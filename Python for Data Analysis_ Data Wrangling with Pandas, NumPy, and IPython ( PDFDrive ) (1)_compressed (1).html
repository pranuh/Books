<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html><head><title>Python for Data Analysis</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
</head>
<body>
<div style="page-break-before:always; page-break-after:always"><div><p>powered by<br/></p>
<p>Wes McKinney<br/></p>
<p>Python for <br/>Data Analysis<br/>DATA WRANGLING WITH PANDAS, <br/>NUMPY, AND IPYTHON<br/></p>
<p>2nd Edition</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div>
</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><i>Wes McKinney<br/></i></p>
<p>Python for Data Analysis<br/><i>Data Wrangling with Pandas, NumPy,<br/></i></p>
<p><i>and IPython<br/></i></p>
<p>SECOND EDITION<br/></p>
<p>Boston Farnham Sebastopol TokyoBeijing</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Python for Data Analysis<br/>by Wes McKinney<br/>Copyright &#169; 2018 William McKinney. All rights reserved.<br/>Printed in the United States of America.<br/>Published by O&#8217;Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.<br/>O&#8217;Reilly books may be purchased for educational, business, or sales promotional use. Online editions are<br/>also available for most titles (<i>http://oreilly.com/safari</i>). For more information, contact our corporate/insti&#8208;<br/>tutional sales department: 800-998-9938 or <i>corporate@oreilly.com</i>.<br/>Editor: Marie Beaugureau<br/>Production Editor: Kristen Brown<br/>Copyeditor: Jasmine Kwityn<br/>Proofreader: Rachel Monaghan<br/></p>
<p>Indexer: Lucie Haskins<br/>Interior Designer: David Futato<br/>Cover Designer: Karen Montgomery<br/>Illustrator: Rebecca Demarest<br/></p>
<p>October 2012:  First Edition<br/>October 2017:  Second Edition<br/></p>
<p>Revision History for the Second Edition<br/>2017-09-25: First Release<br/></p>
<p>See <i>http://oreilly.com/catalog/errata.csp?isbn=9781491957660</i> for release details.<br/></p>
<p>The O&#8217;Reilly logo is a registered trademark of O&#8217;Reilly Media, Inc. <i>Python for Data Analysis</i>, the cover <br/>image, and related trade dress are trademarks of O&#8217;Reilly Media, Inc.<br/>While the publisher and the author have used good faith efforts to ensure that the information and <br/>instructions contained in this work are accurate, the publisher and the author disclaim all responsibility <br/>for errors or omissions, including without limitation responsibility for damages resulting from the use of <br/>or reliance on this work. Use of the information and instructions contained in this work is at your own <br/>risk. If any code samples or other technology this work contains or describes is subject to open source <br/>licenses or the intellectual property rights of others, it is your responsibility to ensure that your use <br/>thereof complies with such licenses and/or rights.<br/></p>
<p>978-1-491-95766-0<br/>[LSI]</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Table of Contents<br/></p>
<p>Preface. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  xi<br/></p>
<p>1. Preliminaries. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  1<br/>1.1 What Is This Book About? 1<br/></p>
<p>What Kinds of Data? 1<br/>1.2 Why Python for Data Analysis? 2<br/></p>
<p>Python as Glue 2<br/>Solving the &#8220;Two-Language&#8221; Problem 3<br/>Why Not Python? 3<br/></p>
<p>1.3 Essential Python Libraries 4<br/>NumPy 4<br/>pandas 4<br/>matplotlib 5<br/>IPython and Jupyter 6<br/>SciPy 6<br/>scikit-learn 7<br/>statsmodels 8<br/></p>
<p>1.4 Installation and Setup 8<br/>Windows 9<br/>Apple (OS X, macOS) 9<br/>GNU/Linux                                                                                                                     9<br/>Installing or Updating Python Packages 10<br/>Python 2 and Python 3 11<br/>Integrated Development Environments (IDEs) and Text Editors 11<br/></p>
<p>1.5 Community and Conferences 12<br/>1.6 Navigating This Book 12<br/></p>
<p>Code Examples 13<br/>Data for Examples 13<br/></p>
<p>iii</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Import Conventions 14<br/>Jargon 14<br/></p>
<p>2. Python Language Basics, IPython, and Jupyter Notebooks. . . . . . . . . . . . . . . . . . . . . . . .  15<br/>2.1 The Python Interpreter 16<br/>2.2 IPython Basics 17<br/></p>
<p>Running the IPython Shell 17<br/>Running the Jupyter Notebook 18<br/>Tab Completion 21<br/>Introspection 23<br/>The %run Command 25<br/>Executing Code from the Clipboard 26<br/>Terminal Keyboard Shortcuts 27<br/>About Magic Commands 28<br/>Matplotlib Integration 29<br/></p>
<p>2.3 Python Language Basics 30<br/>Language Semantics 30<br/>Scalar Types 38<br/>Control Flow 46<br/></p>
<p>3. Built-in Data Structures, Functions, and Files. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  51<br/>3.1 Data Structures and Sequences 51<br/></p>
<p>Tuple 51<br/>List 54<br/>Built-in Sequence Functions 59<br/>dict 61<br/>set                                                                                                                                    65<br/>List, Set, and Dict Comprehensions 67<br/></p>
<p>3.2 Functions 69<br/>Namespaces, Scope, and Local Functions 70<br/>Returning Multiple Values 71<br/>Functions Are Objects 72<br/>Anonymous (Lambda) Functions 73<br/>Currying: Partial Argument Application 74<br/>Generators 75<br/>Errors and Exception Handling 77<br/></p>
<p>3.3 Files and the Operating System 80<br/>Bytes and Unicode with Files 83<br/></p>
<p>3.4 Conclusion 84<br/></p>
<p>4. NumPy Basics: Arrays and Vectorized Computation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  85<br/>4.1 The NumPy ndarray: A Multidimensional Array Object 87<br/></p>
<p>iv | Table of Contents</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Creating ndarrays 88<br/>Data Types for ndarrays 90<br/>Arithmetic with NumPy Arrays 93<br/>Basic Indexing and Slicing 94<br/>Boolean Indexing 99<br/>Fancy Indexing 102<br/>Transposing Arrays and Swapping Axes 103<br/></p>
<p>4.2 Universal Functions: Fast Element-Wise Array Functions 105<br/>4.3 Array-Oriented Programming with Arrays 108<br/></p>
<p>Expressing Conditional Logic as Array Operations 109<br/>Mathematical and Statistical Methods 111<br/>Methods for Boolean Arrays 113<br/>Sorting 113<br/>Unique and Other Set Logic 114<br/></p>
<p>4.4 File Input and Output with Arrays 115<br/>4.5 Linear Algebra 116<br/>4.6 Pseudorandom Number Generation 118<br/>4.7 Example: Random Walks 119<br/></p>
<p>Simulating Many Random Walks at Once 121<br/>4.8 Conclusion 122<br/></p>
<p>5. Getting Started with pandas. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  123<br/>5.1 Introduction to pandas Data Structures 124<br/></p>
<p>Series 124<br/>DataFrame 128<br/>Index Objects 134<br/></p>
<p>5.2 Essential Functionality 136<br/>Reindexing 136<br/>Dropping Entries from an Axis 138<br/>Indexing, Selection, and Filtering 140<br/>Integer Indexes 145<br/>Arithmetic and Data Alignment 146<br/>Function Application and Mapping 151<br/>Sorting and Ranking 153<br/>Axis Indexes with Duplicate Labels 157<br/></p>
<p>5.3 Summarizing and Computing Descriptive Statistics 158<br/>Correlation and Covariance 160<br/>Unique Values, Value Counts, and Membership 162<br/></p>
<p>5.4 Conclusion 165<br/></p>
<p>6. Data Loading, Storage, and File Formats. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  167<br/>6.1 Reading and Writing Data in Text Format 167<br/></p>
<p>Table of Contents | v</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Reading Text Files in Pieces 173<br/>Writing Data to Text Format 175<br/>Working with Delimited Formats 176<br/>JSON Data                                                                                                                   178<br/>XML and HTML: Web Scraping 180<br/></p>
<p>6.2 Binary Data Formats 183<br/>Using HDF5 Format 184<br/>Reading Microsoft Excel Files 186<br/></p>
<p>6.3 Interacting with Web APIs 187<br/>6.4 Interacting with Databases 188<br/>6.5 Conclusion 190<br/></p>
<p>7. Data Cleaning and Preparation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  191<br/>7.1 Handling Missing Data 191<br/></p>
<p>Filtering Out Missing Data 193<br/>Filling In Missing Data 195<br/></p>
<p>7.2 Data Transformation 197<br/>Removing Duplicates                                                                                                 197<br/>Transforming Data Using a Function or Mapping 198<br/>Replacing Values 200<br/>Renaming Axis Indexes 201<br/>Discretization and Binning 203<br/>Detecting and Filtering Outliers 205<br/>Permutation and Random Sampling 206<br/>Computing Indicator/Dummy Variables 208<br/></p>
<p>7.3 String Manipulation 211<br/>String Object Methods 211<br/>Regular Expressions 213<br/>Vectorized String Functions in pandas 216<br/></p>
<p>7.4 Conclusion 219<br/></p>
<p>8. Data Wrangling: Join, Combine, and Reshape. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  221<br/>8.1 Hierarchical Indexing 221<br/></p>
<p>Reordering and Sorting Levels 224<br/>Summary Statistics by Level 225<br/>Indexing with a DataFrame&#8217;s columns 225<br/></p>
<p>8.2 Combining and Merging Datasets 227<br/>Database-Style DataFrame Joins 227<br/>Merging on Index 232<br/>Concatenating Along an Axis 236<br/>Combining Data with Overlap 241<br/></p>
<p>8.3 Reshaping and Pivoting 242<br/></p>
<p>vi | Table of Contents</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Reshaping with Hierarchical Indexing 243<br/>Pivoting &#8220;Long&#8221; to &#8220;Wide&#8221; Format 246<br/>Pivoting &#8220;Wide&#8221; to &#8220;Long&#8221; Format 249<br/></p>
<p>8.4 Conclusion 251<br/></p>
<p>9. Plotting and Visualization. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  253<br/>9.1 A Brief matplotlib API Primer 253<br/></p>
<p>Figures and Subplots 255<br/>Colors, Markers, and Line Styles 259<br/>Ticks, Labels, and Legends 261<br/>Annotations and Drawing on a Subplot 265<br/>Saving Plots to File 267<br/>matplotlib Configuration 268<br/></p>
<p>9.2 Plotting with pandas and seaborn 268<br/>Line Plots 269<br/>Bar Plots 272<br/>Histograms and Density Plots 277<br/>Scatter or Point Plots 280<br/>Facet Grids and Categorical Data 283<br/></p>
<p>9.3 Other Python Visualization Tools 285<br/>9.4 Conclusion 286<br/></p>
<p>10. Data Aggregation and Group Operations. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  287<br/>10.1 GroupBy Mechanics 288<br/></p>
<p>Iterating Over Groups 291<br/>Selecting a Column or Subset of Columns 293<br/>Grouping with Dicts and Series 294<br/>Grouping with Functions 295<br/>Grouping by Index Levels 295<br/></p>
<p>10.2 Data Aggregation 296<br/>Column-Wise and Multiple Function Application 298<br/>Returning Aggregated Data Without Row Indexes 301<br/></p>
<p>10.3 Apply: General split-apply-combine 302<br/>Suppressing the Group Keys 304<br/>Quantile and Bucket Analysis 305<br/>Example: Filling Missing Values with Group-Specific Values 306<br/>Example: Random Sampling and Permutation 308<br/>Example: Group Weighted Average and Correlation 310<br/>Example: Group-Wise Linear Regression 312<br/></p>
<p>10.4 Pivot Tables and Cross-Tabulation 313<br/>Cross-Tabulations: Crosstab 315<br/></p>
<p>10.5 Conclusion 316<br/></p>
<p>Table of Contents | vii</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>11. Time Series. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  317<br/>11.1 Date and Time Data Types and Tools 318<br/></p>
<p>Converting Between String and Datetime 319<br/>11.2 Time Series Basics 322<br/></p>
<p>Indexing, Selection, Subsetting 323<br/>Time Series with Duplicate Indices 326<br/></p>
<p>11.3 Date Ranges, Frequencies, and Shifting 327<br/>Generating Date Ranges 328<br/>Frequencies and Date Offsets 330<br/>Shifting (Leading and Lagging) Data 332<br/></p>
<p>11.4 Time Zone Handling 335<br/>Time Zone Localization and Conversion 335<br/>Operations with Time Zone&#8722;Aware Timestamp Objects 338<br/>Operations Between Different Time Zones 339<br/></p>
<p>11.5 Periods and Period Arithmetic 339<br/>Period Frequency Conversion 340<br/>Quarterly Period Frequencies 342<br/>Converting Timestamps to Periods (and Back) 344<br/>Creating a PeriodIndex from Arrays 345<br/></p>
<p>11.6 Resampling and Frequency Conversion 348<br/>Downsampling 349<br/>Upsampling and Interpolation 352<br/>Resampling with Periods 353<br/></p>
<p>11.7 Moving Window Functions 354<br/>Exponentially Weighted Functions 358<br/>Binary Moving Window Functions 359<br/>User-Defined Moving Window Functions 361<br/></p>
<p>11.8 Conclusion 362<br/></p>
<p>12. Advanced pandas. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  363<br/>12.1 Categorical Data 363<br/></p>
<p>Background and Motivation 363<br/>Categorical Type in pandas 365<br/>Computations with Categoricals 367<br/>Categorical Methods 370<br/></p>
<p>12.2 Advanced GroupBy Use 373<br/>Group Transforms and &#8220;Unwrapped&#8221; GroupBys 373<br/>Grouped Time Resampling 377<br/></p>
<p>12.3 Techniques for Method Chaining 378<br/>The pipe Method 380<br/></p>
<p>12.4 Conclusion 381<br/></p>
<p>viii | Table of Contents</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>13. Introduction to Modeling Libraries in Python. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  383<br/>13.1 Interfacing Between pandas and Model Code                                                  383<br/>13.2 Creating Model Descriptions with Patsy                                                            386<br/></p>
<p>Data Transformations in Patsy Formulas                                                               389<br/>Categorical Data and Patsy                                                                                       390<br/></p>
<p>13.3 Introduction to statsmodels                                                                                 393<br/>Estimating Linear Models                                                                                         393<br/>Estimating Time Series Processes                                                                            396<br/></p>
<p>13.4 Introduction to scikit-learn                                                                                  397<br/>13.5 Continuing Your Education                                                                                 401<br/></p>
<p>14. Data Analysis Examples. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  403<br/>14.1 1.USA.gov Data from Bitly                                                                                   403<br/></p>
<p>Counting Time Zones in Pure Python                                                                    404<br/>Counting Time Zones with pandas                                                                         406<br/></p>
<p>14.2 MovieLens 1M Dataset                                                                                         413<br/>Measuring Rating Disagreement                                                                             418<br/></p>
<p>14.3 US Baby Names 1880&#8211;2010                                                                                 419<br/>Analyzing Naming Trends                                                                                        425<br/></p>
<p>14.4 USDA Food Database                                                                                           434<br/>14.5 2012 Federal Election Commission Database                                                   440<br/></p>
<p>Donation Statistics by Occupation and Employer                                                442<br/>Bucketing Donation Amounts                                                                                 445<br/>Donation Statistics by State                                                                                      447<br/></p>
<p>14.6 Conclusion                                                                                                             448<br/></p>
<p>A. Advanced NumPy. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  449<br/>A.1 ndarray Object Internals                                                                                      449<br/></p>
<p>NumPy dtype Hierarchy                                                                                          450<br/>A.2 Advanced Array Manipulation                                                                            451<br/></p>
<p>Reshaping Arrays                                                                                                      452<br/>C Versus Fortran Order                                                                                           454<br/>Concatenating and Splitting Arrays                                                                       454<br/>Repeating Elements: tile and repeat                                                                       457<br/>Fancy Indexing Equivalents: take and put                                                            459<br/></p>
<p>A.3 Broadcasting                                                                                                           460<br/>Broadcasting Over Other Axes                                                                               462<br/>Setting Array Values by Broadcasting                                                                    465<br/></p>
<p>A.4 Advanced ufunc Usage                                                                                         466<br/>ufunc Instance Methods                                                                                           466<br/>Writing New ufuncs in Python                                                                               468<br/></p>
<p>A.5 Structured and Record Arrays                                                                             469<br/></p>
<p>Table of Contents | ix</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Nested dtypes and Multidimensional Fields                                                         469<br/>Why Use Structured Arrays?                                                                                   470<br/></p>
<p>A.6 More About Sorting                                                                                              471<br/>Indirect Sorts: argsort and lexsort                                                                          472<br/>Alternative Sort Algorithms                                                                                    474<br/>Partially Sorting Arrays                                                                                            474<br/>numpy.searchsorted: Finding Elements in a Sorted Array                                 475<br/></p>
<p>A.7 Writing Fast NumPy Functions with Numba                                                    476<br/>Creating Custom numpy.ufunc Objects with Numba                                         478<br/></p>
<p>A.8 Advanced Array Input and Output                                                                    478<br/>Memory-Mapped Files                                                                                             478<br/>HDF5 and Other Array Storage Options                                                              480<br/></p>
<p>A.9 Performance Tips                                                                                                  480<br/>The Importance of Contiguous Memory                                                              480<br/></p>
<p>B. More on the IPython System. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  483<br/>B.1 Using the Command History                                                                               483<br/></p>
<p>Searching and Reusing the Command History                                                    483<br/>Input and Output Variables                                                                                     484<br/></p>
<p>B.2 Interacting with the Operating System                                                               485<br/>Shell Commands and Aliases                                                                                  486<br/>Directory Bookmark System                                                                                   487<br/></p>
<p>B.3 Software Development Tools                                                                                487<br/>Interactive Debugger                                                                                                488<br/>Timing Code: %time and %timeit                                                                          492<br/>Basic Profiling: %prun and %run -p                                                                      494<br/>Profiling a Function Line by Line                                                                           496<br/></p>
<p>B.4 Tips for Productive Code Development Using IPython                                  498<br/>Reloading Module Dependencies                                                                           498<br/>Code Design Tips                                                                                                      499<br/></p>
<p>B.5 Advanced IPython Features                                                                                 500<br/>Making Your Own Classes IPython-Friendly                                                       500<br/>Profiles and Configuration                                                                                      501<br/></p>
<p>B.6 Conclusion                                                                                                              503<br/></p>
<p>Index. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  505<br/></p>
<p>x | Table of Contents</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Preface<br/></p>
<p>New for the Second Edition<br/>The first edition of this book was published in 2012, during a time when open source<br/>data analysis libraries for Python (such as pandas) were very new and developing rap&#8208;<br/>idly. In this updated and expanded second edition, I have overhauled the chapters to<br/>account both for incompatible changes and deprecations as well as new features that<br/>have occurred in the last five years. I&#8217;ve also added fresh content to introduce tools<br/>that either did not exist in 2012 or had not matured enough to make the first cut.<br/>Finally, I have tried to avoid writing about new or cutting-edge open source projects<br/>that may not have had a chance to mature. I would like readers of this edition to find<br/>that the content is still almost as relevant in 2020 or 2021 as it is in 2017.<br/>The major updates in this second edition include:<br/></p>
<p>&#8226; All code, including the Python tutorial, updated for Python 3.6 (the first edition<br/>used Python 2.7)<br/></p>
<p>&#8226; Updated Python installation instructions for the Anaconda Python Distribution<br/>and other needed Python packages<br/></p>
<p>&#8226; Updates for the latest versions of the pandas library in 2017<br/>&#8226; A new chapter on some more advanced pandas tools, and some other usage tips<br/>&#8226; A brief introduction to using statsmodels and scikit-learn<br/></p>
<p>I also reorganized a significant portion of the content from the first edition to make<br/>the book more accessible to newcomers.<br/></p>
<p>xi</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Conventions Used in This Book<br/>The following typographical conventions are used in this book:<br/><i>Italic<br/></i></p>
<p>Indicates new terms, URLs, email addresses, filenames, and file extensions.<br/>Constant width<br/></p>
<p>Used for program listings, as well as within paragraphs to refer to program ele&#8208;<br/>ments such as variable or function names, databases, data types, environment<br/>variables, statements, and keywords.<br/></p>
<p><b>Constant width bold<br/></b>Shows commands or other text that should be typed literally by the user.<br/></p>
<p><i>Constant width italic<br/></i>Shows text that should be replaced with user-supplied values or by values deter&#8208;<br/>mined by context.<br/></p>
<p>This element signifies a tip or suggestion.<br/></p>
<p>This element signifies a general note.<br/></p>
<p>This element indicates a warning or caution.<br/></p>
<p>Using Code Examples<br/>You can find data files and related material for each chapter is available in this book&#8217;s<br/>GitHub repository at <i>http://github.com/wesm/pydata-book</i>.<br/>This book is here to help you get your job done. In general, if example code is offered<br/>with this book, you may use it in your programs and documentation. You do not<br/>need to contact us for permission unless you&#8217;re reproducing a significant portion of<br/>the code. For example, writing a program that uses several chunks of code from this<br/></p>
<p>xii | Preface</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>book does not require permission. Selling or distributing a CD-ROM of examples<br/>from O&#8217;Reilly books does require permission. Answering a question by citing this<br/>book and quoting example code does not require permission. Incorporating a signifi&#8208;<br/>cant amount of example code from this book into your product&#8217;s documentation does<br/>require permission.<br/>We appreciate, but do not require, attribution. An attribution usually includes the<br/>title, author, publisher, and ISBN. For example: &#8220;<i>Python for Data Analysis</i> by Wes<br/>McKinney (O&#8217;Reilly). Copyright 2017 Wes McKinney, 978-1-491-95766-0.&#8221;<br/>If you feel your use of code examples falls outside fair use or the permission given<br/>above, feel free to contact us at <i>permissions@oreilly.com</i>.<br/></p>
<p>O&#8217;Reilly Safari<br/>Safari (formerly Safari Books Online) is a membership-based<br/>training and reference platform for enterprise, government,<br/>educators, and individuals.<br/></p>
<p>Members have access to thousands of books, training videos, Learning Paths, interac&#8208;<br/>tive tutorials, and curated playlists from over 250 publishers, including O&#8217;Reilly<br/>Media, Harvard Business Review, Prentice Hall Professional, Addison-Wesley Profes&#8208;<br/>sional, Microsoft Press, Sams, Que, Peachpit Press, Adobe, Focal Press, Cisco Press,<br/>John Wiley &amp; Sons, Syngress, Morgan Kaufmann, IBM Redbooks, Packt, Adobe<br/>Press, FT Press, Apress, Manning, New Riders, McGraw-Hill, Jones &amp; Bartlett, and<br/>Course Technology, among others.<br/>For more information, please visit <i>http://oreilly.com/safari</i>.<br/></p>
<p>How to Contact Us<br/>Please address comments and questions concerning this book to the publisher:<br/></p>
<p>O&#8217;Reilly Media, Inc.<br/>1005 Gravenstein Highway North<br/>Sebastopol, CA 95472<br/>800-998-9938 (in the United States or Canada)<br/>707-829-0515 (international or local)<br/>707-829-0104 (fax)<br/></p>
<p>We have a web page for this book, where we list errata, examples, and any additional<br/>information. You can access this page at <i>http://bit.ly/python_data_analysis_2e</i>.<br/></p>
<p>Preface | xiii</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>To comment or ask technical questions about this book, send email to <i>bookques&#8208;<br/>tions@oreilly.com</i>.<br/>For more information about our books, courses, conferences, and news, see our web&#8208;<br/>site at <i>http://www.oreilly.com</i>.<br/>Find us on Facebook: <i>http://facebook.com/oreilly<br/></i>Follow us on Twitter: <i>http://twitter.com/oreillymedia<br/></i>Watch us on YouTube: <i>http://www.youtube.com/oreillymedia<br/></i></p>
<p>Acknowledgments<br/>This work is the product of many years of fruitful discussions, collaborations, and<br/>assistance with and from many people around the world. I&#8217;d like to thank a few of<br/>them.<br/></p>
<p>In Memoriam: John D. Hunter (1968&#8211;2012)<br/>Our dear friend and colleague John D. Hunter passed away after a battle with colon<br/>cancer on August 28, 2012. This was only a short time after I&#8217;d completed the final<br/>manuscript for this book&#8217;s first edition.<br/>John&#8217;s impact and legacy in the Python scientific and data communities would be<br/>hard to overstate. In addition to developing matplotlib in the early 2000s (a time<br/>when Python was not nearly so popular), he helped shape the culture of a critical gen&#8208;<br/>eration of open source developers who&#8217;ve become pillars of the Python ecosystem that<br/>we now often take for granted.<br/>I was lucky enough to connect with John early in my open source career in January<br/>2010, just after releasing pandas 0.1. His inspiration and mentorship helped me push<br/>forward, even in the darkest of times, with my vision for pandas and Python as a<br/>first-class data analysis language.<br/>John was very close with Fernando P&#233;rez and Brian Granger, pioneers of IPython,<br/>Jupyter, and many other initiatives in the Python community. We had hoped to work<br/>on a book together, the four of us, but I ended up being the one with the most free<br/>time. I am sure he would be proud of what we&#8217;ve accomplished, as individuals and as<br/>a community, over the last five years.<br/></p>
<p>Acknowledgments for the Second Edition (2017)<br/>It has been five years almost to the day since I completed the manuscript for this<br/>book&#8217;s first edition in July 2012. A lot has changed. The Python community has<br/>grown immensely, and the ecosystem of open source software around it has<br/>flourished.<br/></p>
<p>xiv | Preface</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>This new edition of the book would not exist if not for the tireless efforts of the pan&#8208;<br/>das core developers, who have grown the project and its user community into one of<br/>the cornerstones of the Python data science ecosystem. These include, but are not<br/>limited to, Tom Augspurger, Joris van den Bossche, Chris Bartak, Phillip Cloud,<br/>gfyoung, Andy Hayden, Masaaki Horikoshi, Stephan Hoyer, Adam Klein, Wouter<br/>Overmeire, Jeff Reback, Chang She, Skipper Seabold, Jeff Tratner, and y-p.<br/>On the actual writing of this second edition, I would like to thank the O&#8217;Reilly staff<br/>who helped me patiently with the writing process. This includes Marie Beaugureau,<br/>Ben Lorica, and Colleen Toporek. I again had outstanding technical reviewers with<br/>Tom Augpurger, Paul Barry, Hugh Brown, Jonathan Coe, and Andreas M&#252;ller contri&#8208;<br/>buting. Thank you.<br/>This book&#8217;s first edition has been translated into many foreign languages, including<br/>Chinese, French, German, Japanese, Korean, and Russian. Translating all this content<br/>and making it available to a broader audience is a huge and often thankless effort.<br/>Thank you for helping more people in the world learn how to program and use data<br/>analysis tools.<br/>I am also lucky to have had support for my continued open source development<br/>efforts from Cloudera and Two Sigma Investments over the last few years. With open<br/>source software projects more thinly resourced than ever relative to the size of user<br/>bases, it is becoming increasingly important for businesses to provide support for<br/>development of key open source projects. It&#8217;s the right thing to do.<br/></p>
<p>Acknowledgments for the First Edition (2012)<br/>It would have been difficult for me to write this book without the support of a large<br/>number of people.<br/>On the O&#8217;Reilly staff, I&#8217;m very grateful for my editors, Meghan Blanchette and Julie<br/>Steele, who guided me through the process. Mike Loukides also worked with me in<br/>the proposal stages and helped make the book a reality.<br/>I received a wealth of technical review from a large cast of characters. In particular,<br/>Martin Blais and Hugh Brown were incredibly helpful in improving the book&#8217;s exam&#8208;<br/>ples, clarity, and organization from cover to cover. James Long, Drew Conway, Fer&#8208;<br/>nando P&#233;rez, Brian Granger, Thomas Kluyver, Adam Klein, Josh Klein, Chang She,<br/>and St&#233;fan van der Walt each reviewed one or more chapters, providing pointed feed&#8208;<br/>back from many different perspectives.<br/>I got many great ideas for examples and datasets from friends and colleagues in the<br/>data community, among them: Mike Dewar, Jeff Hammerbacher, James Johndrow,<br/>Kristian Lum, Adam Klein, Hilary Mason, Chang She, and Ashley Williams.<br/></p>
<p>Preface | xv</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>I am of course indebted to the many leaders in the open source scientific Python<br/>community who&#8217;ve built the foundation for my development work and gave encour&#8208;<br/>agement while I was writing this book: the IPython core team (Fernando P&#233;rez, Brian<br/>Granger, Min Ragan-Kelly, Thomas Kluyver, and others), John Hunter, Skipper Sea&#8208;<br/>bold, Travis Oliphant, Peter Wang, Eric Jones, Robert Kern, Josef Perktold, Francesc<br/>Alted, Chris Fonnesbeck, and too many others to mention. Several other people pro&#8208;<br/>vided a great deal of support, ideas, and encouragement along the way: Drew Con&#8208;<br/>way, Sean Taylor, Giuseppe Paleologo, Jared Lander, David Epstein, John Krowas,<br/>Joshua Bloom, Den Pilsworth, John Myles-White, and many others I&#8217;ve forgotten.<br/>I&#8217;d also like to thank a number of people from my formative years. First, my former<br/>AQR colleagues who&#8217;ve cheered me on in my pandas work over the years: Alex Reyf&#8208;<br/>man, Michael Wong, Tim Sargen, Oktay Kurbanov, Matthew Tschantz, Roni Israelov,<br/>Michael Katz, Chris Uga, Prasad Ramanan, Ted Square, and Hoon Kim. Lastly, my<br/>academic advisors Haynes Miller (MIT) and Mike West (Duke).<br/>I received significant help from Phillip Cloud and Joris Van den Bossche in 2014 to<br/>update the book&#8217;s code examples and fix some other inaccuracies due to changes in<br/>pandas.<br/>On the personal side, Casey provided invaluable day-to-day support during the writ&#8208;<br/>ing process, tolerating my highs and lows as I hacked together the final draft on top<br/>of an already overcommitted schedule. Lastly, my parents, Bill and Kim, taught me to<br/>always follow my dreams and to never settle for less.<br/></p>
<p>xvi | Preface</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>CHAPTER 1<br/>Preliminaries<br/></p>
<p>1.1 What Is This Book About?<br/>This book is concerned with the nuts and bolts of manipulating, processing, cleaning,<br/>and crunching data in Python. My goal is to offer a guide to the parts of the Python<br/>programming language and its data-oriented library ecosystem and tools that will<br/>equip you to become an effective data analyst. While &#8220;data analysis&#8221; is in the title of<br/>the book, the focus is specifically on Python programming, libraries, and tools as<br/>opposed to data analysis methodology. This is the Python programming you need <i>for<br/></i>data analysis.<br/></p>
<p>What Kinds of Data?<br/>When I say &#8220;data,&#8221; what am I referring to exactly? The primary focus is on <i>structured<br/>data</i>, a deliberately vague term that encompasses many different common forms of<br/>data, such as:<br/></p>
<p>&#8226; Tabular or spreadsheet-like data in which each column may be a different type<br/>(string, numeric, date, or otherwise). This includes most kinds of data commonly<br/>stored in relational databases or tab- or comma-delimited text files.<br/></p>
<p>&#8226; Multidimensional arrays (matrices).<br/>&#8226; Multiple tables of data interrelated by key columns (what would be primary or<br/></p>
<p>foreign keys for a SQL user).<br/>&#8226; Evenly or unevenly spaced time series.<br/></p>
<p>This is by no means a complete list. Even though it may not always be obvious, a large<br/>percentage of datasets can be transformed into a structured form that is more suitable<br/>for analysis and modeling. If not, it may be possible to extract features from a dataset<br/></p>
<p>1</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>into a structured form. As an example, a collection of news articles could be pro&#8208;<br/>cessed into a word frequency table, which could then be used to perform sentiment<br/>analysis.<br/>Most users of spreadsheet programs like Microsoft Excel, perhaps the most widely<br/>used data analysis tool in the world, will not be strangers to these kinds of data.<br/></p>
<p>1.2 Why Python for Data Analysis?<br/>For many people, the Python programming language has strong appeal. Since its first<br/>appearance in 1991, Python has become one of the most popular interpreted pro&#8208;<br/>gramming languages, along with Perl, Ruby, and others. Python and Ruby have<br/>become especially popular since 2005 or so for building websites using their numer&#8208;<br/>ous web frameworks, like Rails (Ruby) and Django (Python). Such languages are<br/>often called <i>scripting</i> languages, as they can be used to quickly write small programs,<br/>or <i>scripts</i> to automate other tasks. I don&#8217;t like the term &#8220;scripting language,&#8221; as it car&#8208;<br/>ries a connotation that they cannot be used for building serious software. Among <br/>interpreted languages, for various historical and cultural reasons, Python has devel&#8208;<br/>oped a large and active scientific computing and data analysis community. In the last<br/>10 years, Python has gone from a bleeding-edge or &#8220;at your own risk&#8221; scientific com&#8208;<br/>puting language to one of the most important languages for data science, machine<br/>learning, and general software development in academia and industry.<br/>For data analysis and interactive computing and data visualization, Python will inevi&#8208;<br/>tably draw comparisons with other open source and commercial programming lan&#8208;<br/>guages and tools in wide use, such as R, MATLAB, SAS, Stata, and others. In recent<br/>years, Python&#8217;s improved support for libraries (such as pandas and scikit-learn) has<br/>made it a popular choice for data analysis tasks. Combined with Python&#8217;s overall<br/>strength for general-purpose software engineering, it is an excellent option as a pri&#8208;<br/>mary language for building data applications.<br/></p>
<p>Python as Glue<br/>Part of Python&#8217;s success in scientific computing is the ease of integrating C, C++, and<br/>FORTRAN code. Most modern computing environments share a similar set of legacy<br/>FORTRAN and C libraries for doing linear algebra, optimization, integration, fast<br/>Fourier transforms, and other such algorithms. The same story has held true for<br/>many companies and national labs that have used Python to glue together decades&#8217;<br/>worth of legacy software.<br/>Many programs consist of small portions of code where most of the time is spent,<br/>with large amounts of &#8220;glue code&#8221; that doesn&#8217;t run often. In many cases, the execution<br/>time of the glue code is insignificant; effort is most fruitfully invested in optimizing<br/></p>
<p>2 | Chapter 1: Preliminaries</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>the computational bottlenecks, sometimes by moving the code to a lower-level lan&#8208;<br/>guage like C.<br/></p>
<p>Solving the &#8220;Two-Language&#8221; Problem<br/>In many organizations, it is common to research, prototype, and test new ideas using<br/>a more specialized computing language like SAS or R and then later port those ideas<br/>to be part of a larger production system written in, say, Java, C#, or C++. What people<br/>are increasingly finding is that Python is a suitable language not only for doing<br/>research and prototyping but also for building the production systems. Why main&#8208;<br/>tain two development environments when one will suffice? I believe that more and<br/>more companies will go down this path, as there are often significant organizational<br/>benefits to having both researchers and software engineers using the same set of pro&#8208;<br/>gramming tools.<br/></p>
<p>Why Not Python?<br/>While Python is an excellent environment for building many kinds of analytical<br/>applications and general-purpose systems, there are a number of uses for which<br/>Python may be less suitable.<br/>As Python is an interpreted programming language, in general most Python code will<br/>run substantially slower than code written in a compiled language like Java or C++.<br/>As <i>programmer time</i> is often more valuable than <i>CPU time</i>, many are happy to make<br/>this trade-off. However, in an application with very low latency or demanding<br/>resource utilization requirements (e.g., a high-frequency trading system), the time<br/>spent programming in a lower-level (but also lower-productivity) language like C++<br/>to achieve the maximum possible performance might be time well spent.<br/>Python can be a challenging language for building highly concurrent, multithreaded<br/>applications, particularly applications with many CPU-bound threads. The reason for<br/>this is that it has what is known as the <i>global interpreter lock</i> (GIL), a mechanism that<br/>prevents the interpreter from executing more than one Python instruction at a time.<br/>The technical reasons for why the GIL exists are beyond the scope of this book. While<br/>it is true that in many big data processing applications, a cluster of computers may be<br/>required to process a dataset in a reasonable amount of time, there are still situations<br/>where a single-process, multithreaded system is desirable.<br/>This is not to say that Python cannot execute truly multithreaded, parallel code.<br/>Python C extensions that use native multithreading (in C or C++) can run code in<br/>parallel without being impacted by the GIL, so long as they do not need to regularly<br/>interact with Python objects.<br/></p>
<p>1.2 Why Python for Data Analysis? | 3</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>1.3 Essential Python Libraries<br/>For those who are less familiar with the Python data ecosystem and the libraries used<br/>throughout the book, I will give a brief overview of some of them.<br/></p>
<p>NumPy<br/>NumPy, short for Numerical Python, has long been a cornerstone of numerical com&#8208;<br/>puting in Python. It provides the data structures, algorithms, and library glue needed<br/>for most scientific applications involving numerical data in Python. NumPy contains,<br/>among other things:<br/></p>
<p>&#8226; A fast and efficient multidimensional array object <i>ndarray<br/></i>&#8226; Functions for performing element-wise computations with arrays or mathemati&#8208;<br/></p>
<p>cal operations between arrays<br/>&#8226; Tools for reading and writing array-based datasets to disk<br/>&#8226; Linear algebra operations, Fourier transform, and random number generation<br/>&#8226; A mature C API to enable Python extensions and native C or C++ code to access<br/></p>
<p>NumPy&#8217;s data structures and computational facilities<br/>Beyond the fast array-processing capabilities that NumPy adds to Python, one of its<br/>primary uses in data analysis is as a container for data to be passed between algo&#8208;<br/>rithms and libraries. For numerical data, NumPy arrays are more efficient for storing<br/>and manipulating data than the other built-in Python data structures. Also, libraries<br/>written in a lower-level language, such as C or Fortran, can operate on the data stored<br/>in a NumPy array without copying data into some other memory representation.<br/>Thus, many numerical computing tools for Python either assume NumPy arrays as a<br/>primary data structure or else target seamless interoperability with NumPy.<br/></p>
<p>pandas<br/>pandas provides high-level data structures and functions designed to make working<br/>with structured or tabular data fast, easy, and expressive. Since its emergence in 2010,<br/>it has helped enable Python to be a powerful and productive data analysis environ&#8208;<br/>ment. The primary objects in pandas that will be used in this book are the DataFrame,<br/>a tabular, column-oriented data structure with both row and column labels, and the<br/>Series, a one-dimensional labeled array object.<br/>pandas blends the high-performance, array-computing ideas of NumPy with the flex&#8208;<br/>ible data manipulation capabilities of spreadsheets and relational databases (such as<br/>SQL). It provides sophisticated indexing functionality to make it easy to reshape, slice<br/>and dice, perform aggregations, and select subsets of data. Since data manipulation,<br/></p>
<p>4 | Chapter 1: Preliminaries</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>preparation, and cleaning is such an important skill in data analysis, pandas is one of<br/>the primary focuses of this book.<br/>As a bit of background, I started building pandas in early 2008 during my tenure at<br/>AQR Capital Management, a quantitative investment management firm. At the time,<br/>I had a distinct set of requirements that were not well addressed by any single tool at<br/>my disposal:<br/></p>
<p>&#8226; Data structures with labeled axes supporting automatic or explicit data alignment<br/>&#8212;this prevents common errors resulting from misaligned data and working with<br/>differently indexed data coming from different sources<br/></p>
<p>&#8226; Integrated time series functionality<br/>&#8226; The same data structures handle both time series data and non&#8211;time series data<br/>&#8226; Arithmetic operations and reductions that preserve metadata<br/>&#8226; Flexible handling of missing data<br/>&#8226; Merge and other relational operations found in popular databases (SQL-based,<br/></p>
<p>for example)<br/>I wanted to be able to do all of these things in one place, preferably in a language well<br/>suited to general-purpose software development. Python was a good candidate lan&#8208;<br/>guage for this, but at that time there was not an integrated set of data structures and<br/>tools providing this functionality. As a result of having been built initially to solve<br/>finance and business analytics problems, pandas features especially deep time series<br/>functionality and tools well suited for working with time-indexed data generated by<br/>business processes.<br/>For users of the R language for statistical computing, the DataFrame name will be<br/>familiar, as the object was named after the similar R data.frame object. Unlike<br/>Python, data frames are built into the R programming language and its standard<br/>library. As a result, many features found in pandas are typically either part of the R<br/>core implementation or provided by add-on packages.<br/>The pandas name itself is derived from <i>panel data</i>, an econometrics term for multidi&#8208;<br/>mensional structured datasets, and a play on the phrase <i>Python data analysis</i> itself.<br/></p>
<p>matplotlib<br/>matplotlib is the most popular Python library for producing plots and other two-<br/>dimensional data visualizations. It was originally created by John D. Hunter and is<br/>now maintained by a large team of developers. It is designed for creating plots suit&#8208;<br/>able for publication. While there are other visualization libraries available to Python<br/>programmers, matplotlib is the most widely used and as such has generally good inte&#8208;<br/></p>
<p>1.3 Essential Python Libraries | 5</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>gration with the rest of the ecosystem. I think it is a safe choice as a default visualiza&#8208;<br/>tion tool.<br/></p>
<p>IPython and Jupyter<br/>The IPython project began in 2001 as Fernando P&#233;rez&#8217;s side project to make a better<br/>interactive Python interpreter. In the subsequent 16 years it has become one of the<br/>most important tools in the modern Python data stack. While it does not provide any<br/>computational or data analytical tools by itself, IPython is designed from the ground<br/>up to maximize your productivity in both interactive computing and software devel&#8208;<br/>opment. It encourages an <i>execute-explore</i> workflow instead of the typical <i>edit-compile-<br/>run</i> workflow of many other programming languages. It also provides easy access to<br/>your operating system&#8217;s shell and filesystem. Since much of data analysis coding<br/>involves exploration, trial and error, and iteration, IPython can help you get the job<br/>done faster.<br/>In 2014, Fernando and the IPython team announced the Jupyter project, a broader<br/>initiative to design language-agnostic interactive computing tools. The IPython web<br/>notebook became the Jupyter notebook, with support now for over 40 programming<br/>languages. The IPython system can now be used as a <i>kernel</i> (a programming language<br/>mode) for using Python with Jupyter.<br/>IPython itself has become a component of the much broader Jupyter open source<br/>project, which provides a productive environment for interactive and exploratory<br/>computing. Its oldest and simplest &#8220;mode&#8221; is as an enhanced Python shell designed to<br/>accelerate the writing, testing, and debugging of Python code. You can also use the<br/>IPython system through the Jupyter Notebook, an interactive web-based code &#8220;note&#8208;<br/>book&#8221; offering support for dozens of programming languages. The IPython shell and<br/>Jupyter notebooks are especially useful for data exploration and visualization.<br/>The Jupyter notebook system also allows you to author content in Markdown and<br/>HTML, providing you a means to create rich documents with code and text. Other<br/>programming languages have also implemented kernels for Jupyter to enable you to<br/>use languages other than Python in Jupyter.<br/>For me personally, IPython is usually involved with the majority of my Python work,<br/>including running, debugging, and testing code.<br/>In the accompanying book materials, you will find Jupyter notebooks containing all<br/>the code examples from each chapter.<br/></p>
<p>SciPy<br/>SciPy is a collection of packages addressing a number of different standard problem<br/>domains in scientific computing. Here is a sampling of the packages included:<br/></p>
<p>6 | Chapter 1: Preliminaries</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>scipy.integrate<br/>Numerical integration routines and differential equation solvers<br/></p>
<p>scipy.linalg<br/>Linear algebra routines and matrix decompositions extending beyond those pro&#8208;<br/>vided in numpy.linalg<br/></p>
<p>scipy.optimize<br/>Function optimizers (minimizers) and root finding algorithms<br/></p>
<p>scipy.signal<br/>Signal processing tools<br/></p>
<p>scipy.sparse<br/>Sparse matrices and sparse linear system solvers<br/></p>
<p>scipy.special<br/>Wrapper around SPECFUN, a Fortran library implementing many common<br/>mathematical functions, such as the gamma function<br/></p>
<p>scipy.stats<br/>Standard continuous and discrete probability distributions (density functions,<br/>samplers, continuous distribution functions), various statistical tests, and more<br/>descriptive statistics<br/></p>
<p>Together NumPy and SciPy form a reasonably complete and mature computational<br/>foundation for many traditional scientific computing applications.<br/></p>
<p>scikit-learn<br/>Since the project&#8217;s inception in 2010, scikit-learn has become the premier general-<br/>purpose machine learning toolkit for Python programmers. In just seven years, it has<br/>had over 1,500 contributors from around the world. It includes submodules for such<br/>models as:<br/></p>
<p>&#8226; Classification: SVM, nearest neighbors, random forest, logistic regression, etc.<br/>&#8226; Regression: Lasso, ridge regression, etc.<br/>&#8226; Clustering: <i>k</i>-means, spectral clustering, etc.<br/>&#8226; Dimensionality reduction: PCA, feature selection, matrix factorization, etc.<br/>&#8226; Model selection: Grid search, cross-validation, metrics<br/>&#8226; Preprocessing: Feature extraction, normalization<br/></p>
<p>Along with pandas, statsmodels, and IPython, scikit-learn has been critical for ena&#8208;<br/>bling Python to be a productive data science programming language. While I won&#8217;t<br/></p>
<p>1.3 Essential Python Libraries | 7</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>be able to include a comprehensive guide to scikit-learn in this book, I will give a<br/>brief introduction to some of its models and how to use them with the other tools<br/>presented in the book.<br/></p>
<p>statsmodels<br/>statsmodels is a statistical analysis package that was seeded by work from Stanford<br/>University statistics professor Jonathan Taylor, who implemented a number of regres&#8208;<br/>sion analysis models popular in the R programming language. Skipper Seabold and<br/>Josef Perktold formally created the new statsmodels project in 2010 and since then<br/>have grown the project to a critical mass of engaged users and contributors. Nathaniel<br/>Smith developed the Patsy project, which provides a formula or model specification<br/>framework for statsmodels inspired by R&#8217;s formula system.<br/>Compared with scikit-learn, statsmodels contains algorithms for classical (primarily<br/>frequentist) statistics and econometrics. This includes such submodules as:<br/></p>
<p>&#8226; Regression models: Linear regression, generalized linear models, robust linear<br/>models, linear mixed effects models, etc.<br/></p>
<p>&#8226; Analysis of variance (ANOVA)<br/>&#8226; Time series analysis: AR, ARMA, ARIMA, VAR, and other models<br/>&#8226; Nonparametric methods: Kernel density estimation, kernel regression<br/>&#8226; Visualization of statistical model results<br/></p>
<p>statsmodels is more focused on statistical inference, providing uncertainty estimates<br/>and <i>p</i>-values for parameters. scikit-learn, by contrast, is more prediction-focused.<br/>As with scikit-learn, I will give a brief introduction to statsmodels and how to use it<br/>with NumPy and pandas.<br/></p>
<p>1.4 Installation and Setup<br/>Since everyone uses Python for different applications, there is no single solution for<br/>setting up Python and required add-on packages. Many readers will not have a com&#8208;<br/>plete Python development environment suitable for following along with this book,<br/>so here I will give detailed instructions to get set up on each operating system. I rec&#8208;<br/>ommend using the free Anaconda distribution. At the time of this writing, Anaconda<br/>is offered in both Python 2.7 and 3.6 forms, though this might change at some point<br/>in the future. This book uses Python 3.6, and I encourage you to use Python 3.6 or<br/>higher.<br/></p>
<p>8 | Chapter 1: Preliminaries</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Windows<br/>To get started on Windows, download the Anaconda installer. I recommend follow&#8208;<br/>ing the installation instructions for Windows available on the Anaconda download<br/>page, which may have changed between the time this book was published and when<br/>you are reading this.<br/>Now, let&#8217;s verify that things are configured correctly. To open the Command Prompt<br/>application (also known as <i>cmd.exe</i>), right-click the Start menu and select Command<br/>Prompt. Try starting the Python interpreter by typing <b>python</b>. You should see a mes&#8208;<br/>sage that matches the version of Anaconda you installed:<br/></p>
<p>C:\Users\wesm&gt;python<br/>Python 3.5.2 |Anaconda 4.1.1 (64-bit)| (default, Jul  5 2016, 11:41:13)<br/>[MSC v.1900 64 bit (AMD64)] on win32<br/>&gt;&gt;&gt;<br/></p>
<p>To exit the shell, press Ctrl-D (on Linux or macOS), Ctrl-Z (on Windows), or type<br/>the command <b>exit()</b> and press Enter.<br/></p>
<p>Apple (OS X, macOS)<br/>Download the OS X Anaconda installer, which should be named something like<br/><i>Anaconda3-4.1.0-MacOSX-x86_64.pkg</i>. Double-click the <i>.pkg</i> file to run the installer.<br/>When the installer runs, it automatically appends the Anaconda executable path to<br/>your .bash_profile file. This is located at /Users/$USER/.bash_profile.<br/>To verify everything is working, try launching IPython in the system shell (open the<br/>Terminal application to get a command prompt):<br/></p>
<p>$ ipython<br/></p>
<p>To exit the shell, press Ctrl-D or type <b>exit()</b> and press Enter.<br/></p>
<p>GNU/Linux<br/>Linux details will vary a bit depending on your Linux flavor, but here I give details for<br/>such distributions as Debian, Ubuntu, CentOS, and Fedora. Setup is similar to OS X<br/>with the exception of how Anaconda is installed. The installer is a shell script that<br/>must be executed in the terminal. Depending on whether you have a 32-bit or 64-bit<br/>system, you will either need to install the x86 (32-bit) or x86_64 (64-bit) installer. You<br/>will then have a file named something similar to <i>Anaconda3-4.1.0-Linux-x86_64.sh</i>.<br/>To install it, execute this script with bash:<br/></p>
<p>$ bash Anaconda3-4.1.0-Linux-x86_64.sh<br/></p>
<p>1.4 Installation and Setup | 9</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Some Linux distributions have versions of all the required Python<br/>packages in their package managers and can be installed using a<br/>tool like apt. The setup described here uses Anaconda, as it&#8217;s both<br/>easily reproducible across distributions and simpler to upgrade<br/>packages to their latest versions.<br/></p>
<p>After accepting the license, you will be presented with a choice of where to put the<br/>Anaconda files. I recommend installing the files in the default location in your home<br/>directory&#8212;for example, <i>/home/$USER/anaconda</i> (with your username, naturally).<br/>The Anaconda installer may ask if you wish to prepend its <i>bin/</i> directory to your<br/>$PATH variable. If you have any problems after installation, you can do this yourself by<br/>modifying your <i>.bashrc</i> (or <i>.zshrc</i>, if you are using the zsh shell) with something akin<br/>to:<br/></p>
<p>export PATH=/home/$USER/anaconda/bin:$PATH<br/></p>
<p>After doing this you can either start a new terminal process or execute your <i>.bashrc<br/></i>again with source ~/.bashrc.<br/></p>
<p>Installing or Updating Python Packages<br/>At some point while reading, you may wish to install additional Python packages that<br/>are not included in the Anaconda distribution. In general, these can be installed with<br/>the following command:<br/></p>
<p>conda install <i>package_name<br/></i></p>
<p>If this does not work, you may also be able to install the package using the pip pack&#8208;<br/>age management tool:<br/></p>
<p>pip install <i>package_name<br/></i></p>
<p>You can update packages by using the conda update command:<br/>conda update <i>package_name<br/></i></p>
<p>pip also supports upgrades using the --upgrade flag:<br/>pip install --upgrade <i>package_name<br/></i></p>
<p>You will have several opportunities to try out these commands throughout the book.<br/></p>
<p>While you can use both conda and pip to install packages, you<br/>should not attempt to update conda packages with pip, as doing so<br/>can lead to environment problems. When using Anaconda or Min&#8208;<br/>iconda, it&#8217;s best to first try updating with conda.<br/></p>
<p>10 | Chapter 1: Preliminaries</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Python 2 and Python 3<br/>The first version of the Python 3.x line of interpreters was released at the end of 2008.<br/>It included a number of changes that made some previously written Python 2.x code<br/>incompatible. Because 17 years had passed since the very first release of Python in<br/>1991, creating a &#8220;breaking&#8221; release of Python 3 was viewed to be for the greater good<br/>given the lessons learned during that time.<br/>In 2012, much of the scientific and data analysis community was still using Python<br/>2.x because many packages had not been made fully Python 3 compatible. Thus, the<br/>first edition of this book used Python 2.7. Now, users are free to choose between<br/>Python 2.x and 3.x and in general have full library support with either flavor.<br/>However, Python 2.x will reach its development end of life in 2020 (including critical<br/>security patches), and so it is no longer a good idea to start new projects in Python<br/>2.7. Therefore, this book uses Python 3.6, a widely deployed, well-supported stable<br/>release. We have begun to call Python 2.x &#8220;Legacy Python&#8221; and Python 3.x simply<br/>&#8220;Python.&#8221; I encourage you to do the same.<br/>This book uses Python 3.6 as its basis. Your version of Python may be newer than 3.6,<br/>but the code examples should be forward compatible. Some code examples may work<br/>differently or not at all in Python 2.7.<br/></p>
<p>Integrated Development Environments (IDEs) and Text Editors<br/>When asked about my standard development environment, I almost always say &#8220;IPy&#8208;<br/>thon plus a text editor.&#8221; I typically write a program and iteratively test and debug each<br/>piece of it in IPython or Jupyter notebooks. It is also useful to be able to play around<br/>with data interactively and visually verify that a particular set of data manipulations is<br/>doing the right thing. Libraries like pandas and NumPy are designed to be easy to use<br/>in the shell.<br/>When building software, however, some users may prefer to use a more richly fea&#8208;<br/>tured IDE rather than a comparatively primitive text editor like Emacs or Vim. Here<br/>are some that you can explore:<br/></p>
<p>&#8226; PyDev (free), an IDE built on the Eclipse platform<br/>&#8226; PyCharm from JetBrains (subscription-based for commercial users, free for open<br/></p>
<p>source developers)<br/>&#8226; Python Tools for Visual Studio (for Windows users)<br/>&#8226; Spyder (free), an IDE currently shipped with Anaconda<br/>&#8226; Komodo IDE (commercial)<br/></p>
<p>1.4 Installation and Setup | 11</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Due to the popularity of Python, most text editors, like Atom and Sublime Text 2,<br/>have excellent Python support.<br/></p>
<p>1.5 Community and Conferences<br/>Outside of an internet search, the various scientific and data-related Python mailing<br/>lists are generally helpful and responsive to questions. Some to take a look at include:<br/></p>
<p>&#8226; pydata: A Google Group list for questions related to Python for data analysis and<br/>pandas<br/></p>
<p>&#8226; pystatsmodels: For statsmodels or pandas-related questions<br/>&#8226; Mailing list for scikit-learn (<i>scikit-learn@python.org</i>) and machine learning in<br/></p>
<p>Python, generally<br/>&#8226; numpy-discussion: For NumPy-related questions<br/>&#8226; scipy-user: For general SciPy or scientific Python questions<br/></p>
<p>I deliberately did not post URLs for these in case they change. They can be easily<br/>located via an internet search.<br/>Each year many conferences are held all over the world for Python programmers. If<br/>you would like to connect with other Python programmers who share your interests,<br/>I encourage you to explore attending one, if possible. Many conferences have finan&#8208;<br/>cial support available for those who cannot afford admission or travel to the confer&#8208;<br/>ence. Here are some to consider:<br/></p>
<p>&#8226; PyCon and EuroPython: The two main general Python conferences in North<br/>America and Europe, respectively<br/></p>
<p>&#8226; SciPy and EuroSciPy: Scientific-computing-oriented conferences in North Amer&#8208;<br/>ica and Europe, respectively<br/></p>
<p>&#8226; PyData: A worldwide series of regional conferences targeted at data science and<br/>data analysis use cases<br/></p>
<p>&#8226; International and regional PyCon conferences (see <i>http://pycon.org</i> for a com&#8208;<br/>plete listing)<br/></p>
<p>1.6 Navigating This Book<br/>If you have never programmed in Python before, you will want to spend some time in<br/>Chapters 2 and 3, where I have placed a condensed tutorial on Python language fea&#8208;<br/>tures and the IPython shell and Jupyter notebooks. These things are prerequisite<br/></p>
<p>12 | Chapter 1: Preliminaries</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>knowledge for the remainder of the book. If you have Python experience already, you<br/>may instead choose to skim or skip these chapters.<br/>Next, I give a short introduction to the key features of NumPy, leaving more<br/>advanced NumPy use for Appendix A. Then, I introduce pandas and devote the rest<br/>of the book to data analysis topics applying pandas, NumPy, and matplotlib (for visu&#8208;<br/>alization). I have structured the material in the most incremental way possible,<br/>though there is occasionally some minor cross-over between chapters, with a few iso&#8208;<br/>lated cases where concepts are used that haven&#8217;t necessarily been introduced yet.<br/>While readers may have many different end goals for their work, the tasks required<br/>generally fall into a number of different broad groups:<br/><i>Interacting with the outside world<br/></i></p>
<p>Reading and writing with a variety of file formats and data stores<br/><i>Preparation<br/></i></p>
<p>Cleaning, munging, combining, normalizing, reshaping, slicing and dicing, and<br/>transforming data for analysis<br/></p>
<p><i>Transformation<br/></i>Applying mathematical and statistical operations to groups of datasets to derive<br/>new datasets (e.g., aggregating a large table by group variables)<br/></p>
<p><i>Modeling and computation<br/></i>Connecting your data to statistical models, machine learning algorithms, or other<br/>computational tools<br/></p>
<p><i>Presentation<br/></i>Creating interactive or static graphical visualizations or textual summaries<br/></p>
<p>Code Examples<br/>Most of the code examples in the book are shown with input and output as it would<br/>appear executed in the IPython shell or in Jupyter notebooks:<br/></p>
<p>In [5]: CODE EXAMPLE<br/>Out[5]: OUTPUT<br/></p>
<p>When you see a code example like this, the intent is for you to type in the example<br/>code in the In block in your coding environment and execute it by pressing the Enter<br/>key (or Shift-Enter in Jupyter). You should see output similar to what is shown in the<br/>Out block.<br/></p>
<p>Data for Examples<br/>Datasets for the examples in each chapter are hosted in a GitHub repository. You can<br/>download this data either by using the Git version control system on the command<br/></p>
<p>1.6 Navigating This Book | 13</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>line or by downloading a zip file of the repository from the website. If you run into<br/>problems, navigate to my website for up-to-date instructions about obtaining the<br/>book materials.<br/>I have made every effort to ensure that it contains everything necessary to reproduce<br/>the examples, but I may have made some mistakes or omissions. If so, please send me<br/>an email: <i>book@wesmckinney.com</i>. The best way to report errors in the book is on the<br/>errata page on the O&#8217;Reilly website.<br/></p>
<p>Import Conventions<br/>The Python community has adopted a number of naming conventions for commonly<br/>used modules:<br/></p>
<p>import numpy as np<br/>import matplotlib.pyplot as plt<br/>import pandas as pd<br/>import seaborn as sns<br/>import statsmodels as sm<br/></p>
<p>This means that when you see np.arange, this is a reference to the arange function in<br/>NumPy. This is done because it&#8217;s considered bad practice in Python software develop&#8208;<br/>ment to import everything (from numpy import *) from a large package like NumPy.<br/></p>
<p>Jargon<br/>I&#8217;ll use some terms common both to programming and data science that you may not<br/>be familiar with. Thus, here are some brief definitions:<br/><i>Munge/munging/wrangling<br/></i></p>
<p>Describes the overall process of manipulating unstructured and/or messy data<br/>into a structured or clean form. The word has snuck its way into the jargon of<br/>many modern-day data hackers. &#8220;Munge&#8221; rhymes with &#8220;grunge.&#8221;<br/></p>
<p><i>Pseudocode<br/></i>A description of an algorithm or process that takes a code-like form while likely<br/>not being actual valid source code.<br/></p>
<p><i>Syntactic sugar<br/></i>Programming syntax that does not add new features, but makes something more<br/>convenient or easier to type.<br/></p>
<p>14 | Chapter 1: Preliminaries</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>CHAPTER 2<br/>Python Language Basics, IPython, and<br/></p>
<p>Jupyter Notebooks<br/></p>
<p>When I wrote the first edition of this book in 2011 and 2012, there were fewer resour&#8208;<br/>ces available for learning about doing data analysis in Python. This was partially a<br/>chicken-and-egg problem; many libraries that we now take for granted, like pandas,<br/>scikit-learn, and statsmodels, were comparatively immature back then. In 2017, there<br/>is now a growing literature on data science, data analysis, and machine learning, sup&#8208;<br/>plementing the prior works on general-purpose scientific computing geared toward<br/>computational scientists, physicists, and professionals in other research fields. There<br/>are also excellent books about learning the Python programming language itself and<br/>becoming an effective software engineer.<br/>As this book is intended as an introductory text in working with data in Python, I feel<br/>it is valuable to have a self-contained overview of some of the most important fea&#8208;<br/>tures of Python&#8217;s built-in data structures and libraries from the perspective of data<br/>manipulation. So, I will only present roughly enough information in this chapter and<br/>Chapter 3 to enable you to follow along with the rest of the book.<br/>In my opinion, it is <i>not</i> necessary to become proficient at building good software in<br/>Python to be able to productively do data analysis. I encourage you to use the IPy&#8208;<br/>thon shell and Jupyter notebooks to experiment with the code examples and to<br/>explore the documentation for the various types, functions, and methods. While I&#8217;ve<br/>made best efforts to present the book material in an incremental form, you may occa&#8208;<br/>sionally encounter things that have not yet been fully introduced.<br/>Much of this book focuses on table-based analytics and data preparation tools for<br/>working with large datasets. In order to use those tools you must often first do some<br/>munging to corral messy data into a more nicely tabular (or <i>structured</i>) form. Fortu&#8208;<br/>nately, Python is an ideal language for rapidly whipping your data into shape. The<br/></p>
<p>15</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>greater your facility with Python the language, the easier it will be for you to prepare<br/>new datasets for analysis.<br/>Some of the tools in this book are best explored from a live IPython or Jupyter ses&#8208;<br/>sion. Once you learn how to start up IPython and Jupyter, I recommend that you fol&#8208;<br/>low along with the examples so you can experiment and try different things. As with<br/>any keyboard-driven console-like environment, developing muscle-memory for the<br/>common commands is also part of the learning curve.<br/></p>
<p>There are introductory Python concepts that this chapter does not<br/>cover, like classes and object-oriented programming, which you<br/>may find useful in your foray into data analysis in Python.<br/>To deepen your Python language knowledge, I recommend that<br/>you supplement this chapter with the official Python tutorial and<br/>potentially one of the many excellent books on general-purpose<br/>Python programming. Some recommendations to get you started<br/>include:<br/></p>
<p>&#8226; <i>Python Cookbook</i>, Third Edition, by David Beazley and Brian<br/>K. Jones (O&#8217;Reilly)<br/></p>
<p>&#8226; <i>Fluent Python</i> by Luciano Ramalho (O&#8217;Reilly)<br/>&#8226; <i>Effective Python by Brett Slatkin (Pearson)<br/></i></p>
<p>2.1 The Python Interpreter<br/>Python is an <i>interpreted</i> language. The Python interpreter runs a program by execut&#8208;<br/>ing one statement at a time. The standard interactive Python interpreter can be<br/>invoked on the command line with the python command:<br/></p>
<p>$ python<br/>Python 3.6.0 | packaged by conda-forge | (default, Jan 13 2017, 23:17:12)<br/>[GCC 4.8.2 20140120 (Red Hat 4.8.2-15)] on linux<br/>Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.<br/>&gt;&gt;&gt; a = 5<br/>&gt;&gt;&gt; print(a)<br/>5<br/></p>
<p>The &gt;&gt;&gt; you see is the <i>prompt</i> where you&#8217;ll type code expressions. To exit the Python<br/>interpreter and return to the command prompt, you can either type <b>exit()</b> or press<br/>Ctrl-D.<br/>Running Python programs is as simple as calling python with a <i>.py</i> file as its first<br/>argument. Suppose we had created <i>hello_world.py</i> with these contents:<br/></p>
<p><b>print</b>('Hello world')<br/></p>
<p>16 | Chapter 2: Python Language Basics, IPython, and Jupyter Notebooks</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>You can run it by executing the following command (the <i>hello_world.py</i> file must be<br/>in your current working terminal directory):<br/></p>
<p>$ python hello_world.py<br/>Hello world<br/></p>
<p>While some Python programmers execute all of their Python code in this way, those<br/>doing data analysis or scientific computing make use of IPython, an enhanced Python<br/>interpreter, or Jupyter notebooks, web-based code notebooks originally created<br/>within the IPython project. I give an introduction to using IPython and Jupyter in<br/>this chapter and have included a deeper look at IPython functionality in Appendix A. <br/>When you use the %run command, IPython executes the code in the specified file in<br/>the same process, enabling you to explore the results interactively when it&#8217;s done:<br/></p>
<p>$ ipython<br/>Python 3.6.0 | packaged by conda-forge | (default, Jan 13 2017, 23:17:12)<br/>Type &quot;copyright&quot;, &quot;credits&quot; <b>or</b> &quot;license&quot; <b>for</b> more information.<br/></p>
<p>IPython 5.1.0 -- An enhanced Interactive Python.<br/>?         -&gt; Introduction <b>and</b> overview of IPython's features.<br/>%quickref -&gt; Quick reference.<br/>help      -&gt; Python's own help system.<br/>object?   -&gt; Details about 'object', use 'object??' <b>for</b> extra details.<br/></p>
<p>In [1]: %run hello_world.py<br/>Hello world<br/></p>
<p>In [2]:<br/></p>
<p>The default IPython prompt adopts the numbered In [2]: style compared with the<br/>standard &gt;&gt;&gt; prompt.<br/></p>
<p>2.2 IPython Basics<br/>In this section, we&#8217;ll get you up and running with the IPython shell and Jupyter note&#8208;<br/>book, and introduce you to some of the essential concepts.<br/></p>
<p>Running the IPython Shell<br/>You can launch the IPython shell on the command line just like launching the regular<br/>Python interpreter except with the ipython command:<br/></p>
<p>$ <b>ipython<br/></b>Python 3.6.0 | packaged by conda-forge | (default, Jan 13 2017, 23:17:12)<br/>Type &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.<br/></p>
<p>IPython 5.1.0 -- An enhanced Interactive Python.<br/>?         -&gt; Introduction and overview of IPython's features.<br/>%quickref -&gt; Quick reference.<br/>help      -&gt; Python's own help system.<br/></p>
<p>2.2 IPython Basics | 17</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>object?   -&gt; Details about 'object', use 'object??' for extra details.<br/></p>
<p>In [1]: a = 5<br/></p>
<p>In [2]: a<br/>Out[2]: 5<br/></p>
<p>You can execute arbitrary Python statements by typing them in and pressing Return<br/>(or Enter). When you type just a variable into IPython, it renders a string representa&#8208;<br/>tion of the object:<br/></p>
<p>In [5]: <b>import</b> <b>numpy</b> <b>as</b> <b>np<br/></b></p>
<p>In [6]: data = {i : np.random.randn() <b>for</b> i <b>in</b> range(7)}<br/></p>
<p>In [7]: data<br/>Out[7]: <br/>{0: -0.20470765948471295,<br/> 1: 0.47894333805754824,<br/> 2: -0.5194387150567381,<br/> 3: -0.55573030434749,<br/> 4: 1.9657805725027142,<br/> 5: 1.3934058329729904,<br/> 6: 0.09290787674371767}<br/></p>
<p>The first two lines are Python code statements; the second statement creates a vari&#8208;<br/>able named data that refers to a newly created Python dictionary. The last line prints<br/>the value of data in the console.<br/>Many kinds of Python objects are formatted to be more readable, or <i>pretty-printed</i>,<br/>which is distinct from normal printing with print. If you printed the above data<br/>variable in the standard Python interpreter, it would be much less readable:<br/></p>
<p>&gt;&gt;&gt; <b>from</b> <b>numpy.random</b> <b>import</b> randn<br/>&gt;&gt;&gt; data = {i : randn() <b>for</b> i <b>in</b> range(7)}<br/>&gt;&gt;&gt; <b>print</b>(data)<br/>{0: -1.5948255432744511, 1: 0.10569006472787983, 2: 1.972367135977295,<br/>3: 0.15455217573074576, 4: -0.24058577449429575, 5: -1.2904897053651216,<br/>6: 0.3308507317325902}<br/></p>
<p>IPython also provides facilities to execute arbitrary blocks of code (via a somewhat<br/>glorified copy-and-paste approach) and whole Python scripts. You can also use the<br/>Jupyter notebook to work with larger blocks of code, as we&#8217;ll soon see.<br/></p>
<p>Running the Jupyter Notebook<br/>One of the major components of the Jupyter project is the <i>notebook</i>, a type of interac&#8208;<br/>tive document for code, text (with or without markup), data visualizations, and other<br/>output. The Jupyter notebook interacts with <i>kernels</i>, which are implementations of<br/></p>
<p>18 | Chapter 2: Python Language Basics, IPython, and Jupyter Notebooks</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>the Jupyter interactive computing protocol in any number of programming lan&#8208;<br/>guages. Python&#8217;s Jupyter kernel uses the IPython system for its underlying behavior.<br/>To start up Jupyter, run the command jupyter notebook in a terminal:<br/></p>
<p>$ jupyter notebook<br/>[I 15:20:52.739 NotebookApp] Serving notebooks from local directory:<br/>/home/wesm/code/pydata-book<br/>[I 15:20:52.739 NotebookApp] 0 active kernels<br/>[I 15:20:52.739 NotebookApp] The Jupyter Notebook is running at:<br/>http://localhost:8888/<br/>[I 15:20:52.740 NotebookApp] Use Control-C to stop this server and shut down<br/>all kernels (twice to skip confirmation).<br/>Created new window in existing browser session.<br/></p>
<p>On many platforms, Jupyter will automatically open up in your default web browser<br/>(unless you start it with --no-browser). Otherwise, you can navigate to the HTTP<br/>address printed when you started the notebook, here http://localhost:8888/. See<br/>Figure 2-1 for what this looks like in Google Chrome.<br/></p>
<p>Many people use Jupyter as a local computing environment, but it<br/>can also be deployed on servers and accessed remotely. I won&#8217;t<br/>cover those details here, but encourage you to explore this topic on<br/>the internet if it&#8217;s relevant to your needs.<br/></p>
<p><i>Figure 2-1. Jupyter notebook landing page<br/></i></p>
<p>2.2 IPython Basics | 19</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>To create a new notebook, click the New button and select the &#8220;Python 3&#8221; or &#8220;conda<br/>[default]&#8221; option. You should see something like Figure 2-2. If this is your first time,<br/>try clicking on the empty code &#8220;cell&#8221; and entering a line of Python code. Then press<br/>Shift-Enter to execute it.<br/></p>
<p><i>Figure 2-2. Jupyter new notebook view<br/></i>When you save the notebook (see &#8220;Save and Checkpoint&#8221; under the notebook File<br/>menu), it creates a file with the extension <i>.ipynb</i>. This is a self-contained file format<br/>that contains all of the content (including any evaluated code output) currently in the<br/>notebook. These can be loaded and edited by other Jupyter users. To load an existing<br/>notebook, put the file in the same directory where you started the notebook process<br/>(or in a subfolder within it), then double-click the name from the landing page. You<br/>can try it out with the notebooks from my <i>wesm/pydata-book</i> repository on GitHub.<br/>See Figure 2-3.<br/>While the Jupyter notebook can feel like a distinct experience from the IPython shell,<br/>nearly all of the commands and tools in this chapter can be used in either environ&#8208;<br/>ment.<br/></p>
<p>20 | Chapter 2: Python Language Basics, IPython, and Jupyter Notebooks</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><i>Figure 2-3. Jupyter example view for an existing notebook<br/></i></p>
<p>Tab Completion<br/>On the surface, the IPython shell looks like a cosmetically different version of the<br/>standard terminal Python interpreter (invoked with python). One of the major<br/>improvements over the standard Python shell is <i>tab completion</i>, found in many IDEs<br/>or other interactive computing analysis environments. While entering expressions in<br/>the shell, pressing the Tab key will search the namespace for any variables (objects,<br/>functions, etc.) matching the characters you have typed so far:<br/></p>
<p>In [1]: an_apple = 27<br/></p>
<p>In [2]: an_example = 42<br/></p>
<p>In [3]: an<b>&lt;Tab&gt;<br/></b>an_apple    and         an_example  any<br/></p>
<p>In this example, note that IPython displayed both the two variables I defined as well<br/>as the Python keyword and and built-in function any. Naturally, you can also com&#8208;<br/>plete methods and attributes on any object after typing a period:<br/></p>
<p>2.2 IPython Basics | 21</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In [3]: b = [1, 2, 3]<br/></p>
<p>In [4]: b.<b>&lt;Tab&gt;<br/></b>b.append  b.count   b.insert  b.reverse<br/>b.clear   b.extend  b.pop     b.sort<br/>b.copy    b.index   b.remove<br/></p>
<p>The same goes for modules:<br/>In [1]: import datetime<br/></p>
<p>In [2]: datetime.<b>&lt;Tab&gt;<br/></b>datetime.date          datetime.MAXYEAR       datetime.timedelta<br/>datetime.datetime      datetime.MINYEAR       datetime.timezone<br/>datetime.datetime_CAPI datetime.time          datetime.tzinfo<br/></p>
<p>In the Jupyter notebook and newer versions of IPython (5.0 and higher), the auto&#8208;<br/>completions show up in a drop-down box rather than as text output.<br/></p>
<p>Note that IPython by default hides methods and attributes starting<br/>with underscores, such as magic methods and internal &#8220;private&#8221;<br/>methods and attributes, in order to avoid cluttering the display<br/>(and confusing novice users!). These, too, can be tab-completed,<br/>but you must first type an underscore to see them. If you prefer to<br/>always see such methods in tab completion, you can change this<br/>setting in the IPython configuration. See the IPython documenta&#8208;<br/>tion to find out how to do this.<br/></p>
<p>Tab completion works in many contexts outside of searching the interactive name&#8208;<br/>space and completing object or module attributes. When typing anything that looks<br/>like a file path (even in a Python string), pressing the Tab key will complete anything<br/>on your computer&#8217;s filesystem matching what you&#8217;ve typed:<br/></p>
<p>In [7]: datasets/movielens/<b>&lt;Tab&gt;<br/></b>datasets/movielens/movies.dat    datasets/movielens/README<br/>datasets/movielens/ratings.dat   datasets/movielens/users.dat<br/></p>
<p>In [7]: path = 'datasets/movielens/<b>&lt;Tab&gt;<br/></b>datasets/movielens/movies.dat    datasets/movielens/README<br/>datasets/movielens/ratings.dat   datasets/movielens/users.dat<br/></p>
<p>Combined with the %run command (see &#8220;The %run Command&#8221; on page 25), this<br/>functionality can save you many keystrokes.<br/>Another area where tab completion saves time is in the completion of function key&#8208;<br/>word arguments (and including the = sign!). See Figure 2-4.<br/></p>
<p>22 | Chapter 2: Python Language Basics, IPython, and Jupyter Notebooks</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><i>Figure 2-4. Autocomplete function keywords in Jupyter notebook<br/></i>We&#8217;ll have a closer look at functions in a little bit.<br/></p>
<p>Introspection<br/>Using a question mark (?) before or after a variable will display some general infor&#8208;<br/>mation about the object:<br/></p>
<p>In [8]: b = [1, 2, 3]<br/></p>
<p>In [9]: b?<br/>Type:       list<br/>String Form:[1, 2, 3]<br/>Length:     3<br/>Docstring:<br/>list() -&gt; new empty list<br/>list(iterable) -&gt; new list initialized <b>from</b> <b>iterable</b>'s items<br/></p>
<p>In [10]: <b>print</b>?<br/>Docstring:<br/><b>print</b>(value, ..., sep=' ', end='<b>\n</b>', file=sys.stdout, flush=False)<br/></p>
<p>Prints the values to a stream, <b>or</b> to sys.stdout by default.<br/>Optional keyword arguments:<br/>file:  a file-like object (stream); defaults to the current sys.stdout.<br/>sep:   string inserted between values, default a space.<br/>end:   string appended after the last value, default a newline.<br/>flush: whether to forcibly flush the stream.<br/>Type:      builtin_function_or_method<br/></p>
<p>This is referred to as <i>object introspection</i>. If the object is a function or instance<br/>method, the docstring, if defined, will also be shown. Suppose we&#8217;d written the follow&#8208;<br/>ing function (which you can reproduce in IPython or Jupyter):<br/></p>
<p>2.2 IPython Basics | 23</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><b>def</b> add_numbers(a, b):<br/>    <i>&quot;&quot;&quot;<br/>    Add two numbers together<br/></i></p>
<p><i>    Returns<br/>    -------<br/>    the_sum : type of arguments<br/>    &quot;&quot;&quot;<br/></i>    <b>return</b> a + b<br/></p>
<p>Then using ? shows us the docstring:<br/>In [11]: add_numbers?<br/>Signature: add_numbers(a, b)<br/>Docstring:<br/>Add two numbers together<br/></p>
<p>Returns<br/>-------<br/>the_sum : type of arguments<br/>File:      &lt;ipython-input-9-6a548a216e27&gt;<br/>Type:      function<br/></p>
<p>Using ?? will also show the function&#8217;s source code if possible:<br/>In [12]: add_numbers??<br/>Signature: add_numbers(a, b)<br/>Source:<br/><b>def</b> add_numbers(a, b):<br/>    <i>&quot;&quot;&quot;<br/>    Add two numbers together<br/></i></p>
<p><i>    Returns<br/>    -------<br/>    the_sum : type of arguments<br/>    &quot;&quot;&quot;<br/></i>    <b>return</b> a + b<br/>File:      &lt;ipython-input-9-6a548a216e27&gt;<br/>Type:      function<br/></p>
<p>? has a final usage, which is for searching the IPython namespace in a manner similar<br/>to the standard Unix or Windows command line. A number of characters combined<br/>with the wildcard (*) will show all names matching the wildcard expression. For<br/>example, we could get a list of all functions in the top-level NumPy namespace con&#8208;<br/>taining load:<br/></p>
<p>In [13]: np.*load*?<br/>np.__loader__<br/>np.load<br/>np.loads<br/>np.loadtxt<br/>np.pkgload<br/></p>
<p>24 | Chapter 2: Python Language Basics, IPython, and Jupyter Notebooks</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>The %run Command<br/>You can run any file as a Python program inside the environment of your IPython<br/>session using the %run command. Suppose you had the following simple script stored<br/>in <i>ipython_script_test.py</i>:<br/></p>
<p><b>def</b> f(x, y, z):<br/>    <b>return</b> (x + y) / z<br/></p>
<p>a = 5<br/>b = 6<br/>c = 7.5<br/></p>
<p>result = f(a, b, c)<br/></p>
<p>You can execute this by passing the filename to %run:<br/>In [14]: %run ipython_script_test.py<br/></p>
<p>The script is run in an <i>empty namespace</i> (with no imports or other variables defined)<br/>so that the behavior should be identical to running the program on the command line<br/>using python script.py. All of the variables (imports, functions, and globals)<br/>defined in the file (up until an exception, if any, is raised) will then be accessible in<br/>the IPython shell:<br/></p>
<p>In [15]: c<br/>Out [15]: 7.5<br/></p>
<p>In [16]: result<br/>Out[16]: 1.4666666666666666<br/></p>
<p>If a Python script expects command-line arguments (to be found in sys.argv), these<br/>can be passed after the file path as though run on the command line.<br/></p>
<p>Should you wish to give a script access to variables already defined<br/>in the interactive IPython namespace, use %run -i instead of plain<br/>%run.<br/></p>
<p>In the Jupyter notebook, you may also use the related %load magic function, which<br/>imports a script into a code cell:<br/></p>
<p>&gt;&gt;&gt; %load ipython_script_test.py<br/></p>
<p>    <b>def</b> f(x, y, z):<br/>        <b>return</b> (x + y) / z<br/></p>
<p>    a = 5<br/>    b = 6<br/>    c = 7.5<br/></p>
<p>2.2 IPython Basics | 25</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>    result = f(a, b, c)<br/></p>
<p>Interrupting running code<br/>Pressing Ctrl-C while any code is running, whether a script through %run or a long-<br/>running command, will cause a KeyboardInterrupt to be raised. This will cause<br/>nearly all Python programs to stop immediately except in certain unusual cases.<br/></p>
<p>When a piece of Python code has called into some compiled exten&#8208;<br/>sion modules, pressing Ctrl-C will not always cause the program<br/>execution to stop immediately. In such cases, you will have to<br/>either wait until control is returned to the Python interpreter, or in<br/>more dire circumstances, forcibly terminate the Python process.<br/></p>
<p>Executing Code from the Clipboard<br/>If you are using the Jupyter notebook, you can copy and paste code into any code cell<br/>and execute it. It is also possible to run code from the clipboard in the IPython shell.<br/>Suppose you had the following code in some other application:<br/></p>
<p>x = 5<br/>y = 7<br/><b>if</b> x &gt; 5:<br/>    x += 1<br/></p>
<p>    y = 8<br/></p>
<p>The most foolproof methods are the %paste and %cpaste magic functions. %paste<br/>takes whatever text is in the clipboard and executes it as a single block in the shell:<br/></p>
<p>In [17]: %paste<br/>x = 5<br/>y = 7<br/><b>if</b> x &gt; 5:<br/>    x += 1<br/></p>
<p>    y = 8<br/><i>## -- End pasted text --<br/></i></p>
<p>%cpaste is similar, except that it gives you a special prompt for pasting code into:<br/>In [18]: %cpaste<br/>Pasting code; enter '--' alone on the line to stop <b>or</b> use Ctrl-D.<br/>:x = 5<br/>:y = 7<br/>:<b>if</b> x &gt; 5:<br/>:    x += 1<br/>:<br/></p>
<p>26 | Chapter 2: Python Language Basics, IPython, and Jupyter Notebooks</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>:    y = 8<br/>:--<br/></p>
<p>With the %cpaste block, you have the freedom to paste as much code as you like<br/>before executing it. You might decide to use %cpaste in order to look at the pasted<br/>code before executing it. If you accidentally paste the wrong code, you can break out<br/>of the %cpaste prompt by pressing Ctrl-C.<br/></p>
<p>Terminal Keyboard Shortcuts<br/>IPython has many keyboard shortcuts for navigating the prompt (which will be famil&#8208;<br/>iar to users of the Emacs text editor or the Unix bash shell) and interacting with the<br/>shell&#8217;s command history. Table 2-1 summarizes some of the most commonly used<br/>shortcuts. See Figure 2-5 for an illustration of a few of these, such as cursor<br/>movement.<br/></p>
<p><i>Figure 2-5. Illustration of some keyboard shortcuts in the IPython shell<br/>Table 2-1. Standard IPython keyboard shortcuts<br/></i></p>
<p>Keyboard shortcut Description<br/>Ctrl-P or up-arrow Search backward in command history for commands starting with currently entered text<br/>Ctrl-N or down-arrow Search forward in command history for commands starting with currently entered text<br/>Ctrl-R Readline-style reverse history search (partial matching)<br/>Ctrl-Shift-V Paste text from clipboard<br/>Ctrl-C Interrupt currently executing code<br/>Ctrl-A Move cursor to beginning of line<br/>Ctrl-E Move cursor to end of line<br/>Ctrl-K Delete text from cursor until end of line<br/>Ctrl-U Discard all text on current line<br/>Ctrl-F Move cursor forward one character<br/>Ctrl-B Move cursor back one character<br/>Ctrl-L Clear screen<br/></p>
<p>Note that Jupyter notebooks have a largely separate set of keyboard shortcuts for nav&#8208;<br/>igation and editing. Since these shortcuts have evolved more rapidly than IPython&#8217;s, I<br/>encourage you to explore the integrated help system in the Jupyter notebook&#8217;s menus.<br/></p>
<p>2.2 IPython Basics | 27</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>About Magic Commands<br/>IPython&#8217;s special commands (which are not built into Python itself) are known as<br/>&#8220;magic&#8221; commands. These are designed to facilitate common tasks and enable you to<br/>easily control the behavior of the IPython system. A magic command is any com&#8208;<br/>mand prefixed by the percent symbol %. For example, you can check the execution<br/>time of any Python statement, such as a matrix multiplication, using the %timeit<br/>magic function (which will be discussed in more detail later):<br/></p>
<p>In [20]: a = np.random.randn(100, 100)<br/></p>
<p>In [20]: %timeit np.dot(a, a)<br/>10000 loops, best of 3: 20.9 &#181;s per loop<br/></p>
<p>Magic commands can be viewed as command-line programs to be run within the<br/>IPython system. Many of them have additional &#8220;command-line&#8221; options, which can<br/>all be viewed (as you might expect) using ?:<br/></p>
<p>In [21]: %debug?<br/>Docstring:<br/>::<br/></p>
<p>  %debug [--breakpoint FILE:LINE] [statement [statement ...]]<br/></p>
<p>Activate the interactive debugger.<br/></p>
<p>This magic command support two ways of activating debugger.<br/>One <b>is</b> to activate debugger before executing code.  This way, you<br/>can set a <b>break</b> point, to step through the code <b>from</b> <b>the</b> <b>point.<br/></b>You can use this mode by giving statements to execute <b>and</b> optionally<br/>a breakpoint.<br/></p>
<p>The other one <b>is</b> to activate debugger <b>in</b> post-mortem mode.  You can<br/>activate this mode simply running %debug without any argument.<br/>If an exception has just occurred, this lets you inspect its stack<br/>frames interactively.  Note that this will always work only on the last<br/>traceback that occurred, so you must call this quickly after an<br/>exception that you wish to inspect has fired, because <b>if</b> another one<br/>occurs, it clobbers the previous one.<br/></p>
<p>If you want IPython to automatically do this on every exception, see<br/>the %pdb magic <b>for</b> more details.<br/></p>
<p>positional arguments:<br/>  statement             Code to run <b>in</b> debugger. You can omit this <b>in</b> cell<br/>                        magic mode.<br/></p>
<p>optional arguments:<br/>  --breakpoint &lt;FILE:LINE&gt;, -b &lt;FILE:LINE&gt;<br/>                        Set <b>break</b> point at LINE <b>in</b> FILE.<br/></p>
<p>28 | Chapter 2: Python Language Basics, IPython, and Jupyter Notebooks</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Magic functions can be used by default without the percent sign, as long as no vari&#8208;<br/>able is defined with the same name as the magic function in question. This feature is<br/>called <i>automagic</i> and can be enabled or disabled with %automagic.<br/>Some magic functions behave like Python functions and their output can be assigned<br/>to a variable:<br/></p>
<p>In [22]: %pwd<br/>Out[22]: '/home/wesm/code/pydata-book<br/></p>
<p>In [23]: foo = %pwd<br/></p>
<p>In [24]: foo<br/>Out[24]: '/home/wesm/code/pydata-book'<br/></p>
<p>Since IPython&#8217;s documentation is accessible from within the system, I encourage you<br/>to explore all of the special commands available by typing %quickref or %magic.<br/>Table 2-2 highlights some of the most critical ones for being productive in interactive<br/>computing and Python development in IPython.<br/><i>Table 2-2. Some frequently used IPython magic commands<br/></i></p>
<p>Command Description<br/>%quickref Display the IPython Quick Reference Card<br/>%magic Display detailed documentation for all of the available magic commands<br/>%debug Enter the interactive debugger at the bottom of the last exception traceback<br/>%hist Print command input (and optionally output) history<br/>%pdb Automatically enter debugger after any exception<br/>%paste Execute preformatted Python code from clipboard<br/>%cpaste Open a special prompt for manually pasting Python code to be executed<br/>%reset Delete all variables/names defined in interactive namespace<br/>%page <i>OBJECT </i>Pretty-print the object and display it through a pager<br/>%run <i>script.py </i>Run a Python script inside IPython<br/>%prun <i>statement </i>Execute statement with cProfile and report the profiler output<br/>%time <i>statement </i>Report the execution time of a single statement<br/>%timeit <i>statement </i>Run a statement multiple times to compute an ensemble average execution time; useful for<br/></p>
<p>timing code with very short execution time<br/>%who, %who_ls, %whos Display variables defined in interactive namespace, with varying levels of information/<br/></p>
<p>verbosity<br/>%xdel <i>variable </i>Delete a variable and attempt to clear any references to the object in the IPython internals<br/></p>
<p>Matplotlib Integration<br/>One reason for IPython&#8217;s popularity in analytical computing is that it integrates well<br/>with data visualization and other user interface libraries like matplotlib. Don&#8217;t worry<br/>if you have never used matplotlib before; it will be discussed in more detail later in<br/></p>
<p>2.2 IPython Basics | 29</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>this book. The %matplotlib magic function configures its integration with the IPy&#8208;<br/>thon shell or Jupyter notebook. This is important, as otherwise plots you create will<br/>either not appear (notebook) or take control of the session until closed (shell).<br/>In the IPython shell, running %matplotlib sets up the integration so you can create<br/>multiple plot windows without interfering with the console session:<br/></p>
<p>In [26]: %matplotlib<br/>Using matplotlib backend: Qt4Agg<br/></p>
<p>In Jupyter, the command is a little different (Figure 2-6):<br/>In [26]: %matplotlib inline<br/></p>
<p><i>Figure 2-6. Jupyter inline matplotlib plotting<br/></i></p>
<p>2.3 Python Language Basics<br/>In this section, I will give you an overview of essential Python programming concepts<br/>and language mechanics. In the next chapter, I will go into more detail about Python&#8217;s<br/>data structures, functions, and other built-in tools.<br/></p>
<p>Language Semantics<br/>The Python language design is distinguished by its emphasis on readability, simplic&#8208;<br/>ity, and explicitness. Some people go so far as to liken it to &#8220;executable pseudocode.&#8221;<br/></p>
<p>Indentation, not braces<br/>Python uses whitespace (tabs or spaces) to structure code instead of using braces as in<br/>many other languages like R, C++, Java, and Perl. Consider a for loop from a sorting<br/>algorithm:<br/></p>
<p>30 | Chapter 2: Python Language Basics, IPython, and Jupyter Notebooks</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><b>for</b> x <b>in</b> array:<br/>    <b>if</b> x &lt; pivot:<br/>        less.append(x)<br/>    <b>else</b>:<br/>        greater.append(x)<br/></p>
<p>A colon denotes the start of an indented code block after which all of the code must<br/>be indented by the same amount until the end of the block.<br/>Love it or hate it, significant whitespace is a fact of life for Python programmers, and<br/>in my experience it can make Python code more readable than other languages I&#8217;ve<br/>used. While it may seem foreign at first, you will hopefully grow accustomed in time.<br/></p>
<p>I strongly recommend using <i>four spaces</i> as your default indentation<br/>and replacing tabs with four spaces. Many text editors have a set&#8208;<br/>ting that will replace tab stops with spaces automatically (do this!).<br/>Some people use tabs or a different number of spaces, with two<br/>spaces not being terribly uncommon. By and large, four spaces is<br/>the standard adopted by the vast majority of Python programmers,<br/>so I recommend doing that in the absence of a compelling reason<br/>otherwise.<br/></p>
<p>As you can see by now, Python statements also do not need to be terminated by semi&#8208;<br/>colons. Semicolons can be used, however, to separate multiple statements on a single<br/>line:<br/></p>
<p>a = 5; b = 6; c = 7<br/></p>
<p>Putting multiple statements on one line is generally discouraged in Python as it often<br/>makes code less readable.<br/></p>
<p>Everything is an object<br/>An important characteristic of the Python language is the consistency of its <i>object<br/>model</i>. Every number, string, data structure, function, class, module, and so on exists<br/>in the Python interpreter in its own &#8220;box,&#8221; which is referred to as a <i>Python object</i>.<br/>Each object has an associated <i>type</i> (e.g., <i>string</i> or <i>function</i>) and internal data. In prac&#8208;<br/>tice this makes the language very flexible, as even functions can be treated like any<br/>other object.<br/></p>
<p>Comments<br/>Any text preceded by the hash mark (pound sign) # is ignored by the Python inter&#8208;<br/>preter. This is often used to add comments to code. At times you may also want to<br/>exclude certain blocks of code without deleting them. An easy solution is to <i>comment<br/>out</i> the code:<br/></p>
<p>2.3 Python Language Basics | 31</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>results = []<br/><b>for</b> line <b>in</b> file_handle:<br/>    <i># keep the empty lines for now<br/></i>    <i># if len(line) == 0:<br/></i>    <i>#   continue<br/></i>    results.append(line.replace('foo', 'bar'))<br/></p>
<p>Comments can also occur after a line of executed code. While some programmers<br/>prefer comments to be placed in the line preceding a particular line of code, this can<br/>be useful at times:<br/></p>
<p><b>print</b>(&quot;Reached this line&quot;)  <i># Simple status report<br/></i></p>
<p>Function and object method calls<br/>You call functions using parentheses and passing zero or more arguments, optionally<br/>assigning the returned value to a variable:<br/></p>
<p>result = f(x, y, z)<br/>g()<br/></p>
<p>Almost every object in Python has attached functions, known as <i>methods</i>, that have<br/>access to the object&#8217;s internal contents. You can call them using the following syntax:<br/></p>
<p>obj.some_method(x, y, z)<br/></p>
<p>Functions can take both <i>positional</i> and <i>keyword</i> arguments:<br/>result = f(a, b, c, d=5, e='foo')<br/></p>
<p>More on this later.<br/></p>
<p>Variables and argument passing<br/>When assigning a variable (or <i>name</i>) in Python, you are creating a <i>reference</i> to the<br/>object on the righthand side of the equals sign. In practical terms, consider a list of<br/>integers:<br/></p>
<p>In [8]: a = [1, 2, 3]<br/></p>
<p>Suppose we assign a to a new variable b:<br/>In [9]: b = a<br/></p>
<p>In some languages, this assignment would cause the data [1, 2, 3] to be copied. In<br/>Python, a and b actually now refer to the same object, the original list [1, 2, 3] (see<br/>Figure 2-7 for a mockup). You can prove this to yourself by appending an element to<br/>a and then examining b:<br/></p>
<p>In [10]: a.append(4)<br/></p>
<p>In [11]: b<br/>Out[11]: [1, 2, 3, 4]<br/></p>
<p>32 | Chapter 2: Python Language Basics, IPython, and Jupyter Notebooks</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><i>Figure 2-7. Two references for the same object<br/></i>Understanding the semantics of references in Python and when, how, and why data is<br/>copied is especially critical when you are working with larger datasets in Python.<br/></p>
<p>Assignment is also referred to as <i>binding</i>, as we are binding a name<br/>to an object. Variable names that have been assigned may occasion&#8208;<br/>ally be referred to as bound variables.<br/></p>
<p>When you pass objects as arguments to a function, new local variables are created ref&#8208;<br/>erencing the original objects without any copying. If you bind a new object to a vari&#8208;<br/>able inside a function, that change will not be reflected in the parent scope. It is<br/>therefore possible to alter the internals of a mutable argument. Suppose we had the<br/>following function:<br/></p>
<p><b>def</b> append_element(some_list, element):<br/>    some_list.append(element)<br/></p>
<p>Then we have:<br/>In [27]: data = [1, 2, 3]<br/></p>
<p>In [28]: append_element(data, 4)<br/></p>
<p>In [29]: data<br/>Out[29]: [1, 2, 3, 4]<br/></p>
<p>Dynamic references, strong types<br/>In contrast with many compiled languages, such as Java and C++, object <i>references</i> in<br/>Python have no type associated with them. There is no problem with the following:<br/></p>
<p>In [12]: a = 5<br/></p>
<p>In [13]: type(a)<br/>Out[13]: int<br/></p>
<p>In [14]: a = 'foo'<br/></p>
<p>2.3 Python Language Basics | 33</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In [15]: type(a)<br/>Out[15]: str<br/></p>
<p>Variables are names for objects within a particular namespace; the type information is<br/>stored in the object itself. Some observers might hastily conclude that Python is not a<br/>&#8220;typed language.&#8221; This is not true; consider this example:<br/></p>
<p>In [16]: '5' + 5<br/>---------------------------------------------------------------------------<br/><b>TypeError</b>                                 Traceback (most recent call last)<br/>&lt;ipython-input-16-f9dbf5f0b234&gt; <b>in</b> &lt;module&gt;()<br/>----&gt; 1 '5' + 5<br/><b>TypeError</b>: must be str, <b>not</b> int<br/></p>
<p>In some languages, such as Visual Basic, the string '5' might get implicitly converted<br/>(or <i>casted</i>) to an integer, thus yielding 10. Yet in other languages, such as JavaScript,<br/>the integer 5 might be casted to a string, yielding the concatenated string '55'. In this<br/>regard Python is considered a <i>strongly typed</i> language, which means that every object<br/>has a specific type (or <i>class</i>), and implicit conversions will occur only in certain obvi&#8208;<br/>ous circumstances, such as the following:<br/></p>
<p>In [17]: a = 4.5<br/></p>
<p>In [18]: b = 2<br/></p>
<p><i># String formatting, to be visited later<br/></i>In [19]: <b>print</b>('a is {0}, b is {1}'.format(type(a), type(b)))<br/>a <b>is</b> &lt;<b>class</b> '<b>float</b>'&gt;, b is &lt;class 'int'&gt;<br/></p>
<p>In [20]: a / b<br/>Out[20]: 2.25<br/></p>
<p>Knowing the type of an object is important, and it&#8217;s useful to be able to write func&#8208;<br/>tions that can handle many different kinds of input. You can check that an object is an<br/>instance of a particular type using the isinstance function:<br/></p>
<p>In [21]: a = 5<br/></p>
<p>In [22]: isinstance(a, int)<br/>Out[22]: True<br/></p>
<p>isinstance can accept a tuple of types if you want to check that an object&#8217;s type is<br/>among those present in the tuple:<br/></p>
<p>In [23]: a = 5; b = 4.5<br/></p>
<p>In [24]: isinstance(a, (int, float))<br/>Out[24]: True<br/></p>
<p>In [25]: isinstance(b, (int, float))<br/>Out[25]: True<br/></p>
<p>34 | Chapter 2: Python Language Basics, IPython, and Jupyter Notebooks</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Attributes and methods<br/>Objects in Python typically have both attributes (other Python objects stored &#8220;inside&#8221;<br/>the object) and methods (functions associated with an object that can have access to<br/>the object&#8217;s internal data). Both of them are accessed via the syntax<br/><i>obj.attribute_name</i>:<br/></p>
<p>In [1]: a = 'foo'<br/></p>
<p>In [2]: a.&lt;Press Tab&gt;<br/>a.capitalize  a.format      a.isupper     a.rindex      a.strip<br/>a.center      a.index       a.join        a.rjust       a.swapcase<br/>a.count       a.isalnum     a.ljust       a.rpartition  a.title<br/>a.decode      a.isalpha     a.lower       a.rsplit      a.translate<br/>a.encode      a.isdigit     a.lstrip      a.rstrip      a.upper<br/>a.endswith    a.islower     a.partition   a.split       a.zfill<br/>a.expandtabs  a.isspace     a.replace     a.splitlines<br/>a.find        a.istitle     a.rfind       a.startswith<br/></p>
<p>Attributes and methods can also be accessed by name via the getattr function:<br/>In [27]: getattr(a, 'split')<br/>Out[27]: &lt;function str.split&gt;<br/></p>
<p>In other languages, accessing objects by name is often referred to as &#8220;reflection.&#8221;<br/>While we will not extensively use the functions getattr and related functions<br/>hasattr and setattr in this book, they can be used very effectively to write generic,<br/>reusable code.<br/></p>
<p>Duck typing<br/>Often you may not care about the type of an object but rather only whether it has<br/>certain methods or behavior. This is sometimes called &#8220;duck typing,&#8221; after the saying<br/>&#8220;If it walks like a duck and quacks like a duck, then it&#8217;s a duck.&#8221; For example, you can<br/>verify that an object is iterable if it implemented the <i>iterator protocol</i>. For many<br/>objects, this means it has a __iter__ &#8220;magic method,&#8221; though an alternative and bet&#8208;<br/>ter way to check is to try using the iter function:<br/></p>
<p><b>def</b> isiterable(obj):<br/>    <b>try</b>:<br/>        iter(obj)<br/>        <b>return</b> True<br/>    <b>except</b> <b>TypeError</b>: <i># not iterable<br/></i>        <b>return</b> False<br/></p>
<p>This function would return True for strings as well as most Python collection types:<br/>In [29]: isiterable('a string')<br/>Out[29]: True<br/></p>
<p>In [30]: isiterable([1, 2, 3])<br/></p>
<p>2.3 Python Language Basics | 35</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Out[30]: True<br/></p>
<p>In [31]: isiterable(5)<br/>Out[31]: False<br/></p>
<p>A place where I use this functionality all the time is to write functions that can accept<br/>multiple kinds of input. A common case is writing a function that can accept any<br/>kind of sequence (list, tuple, ndarray) or even an iterator. You can first check if the<br/>object is a list (or a NumPy array) and, if it is not, convert it to be one:<br/></p>
<p><b>if</b> <b>not</b> isinstance(x, list) <b>and</b> isiterable(x):<br/>    x = list(x)<br/></p>
<p>Imports<br/>In Python a <i>module</i> is simply a file with the <i>.py</i> extension containing Python code.<br/>Suppose that we had the following module:<br/></p>
<p><i># some_module.py<br/></i>PI = 3.14159<br/></p>
<p><b>def</b> f(x):<br/>    <b>return</b> x + 2<br/></p>
<p><b>def</b> g(a, b):<br/>    <b>return</b> a + b<br/></p>
<p>If we wanted to access the variables and functions defined in <i>some_module.py</i>, from<br/>another file in the same directory we could do:<br/></p>
<p><b>import</b> <b>some_module<br/></b>result = some_module.f(5)<br/>pi = some_module.PI<br/></p>
<p>Or equivalently:<br/><b>from</b> <b>some_module</b> <b>import</b> f, g, PI<br/>result = g(5, PI)<br/></p>
<p>By using the as keyword you can give imports different variable names:<br/><b>import</b> <b>some_module</b> <b>as</b> <b>sm<br/>from</b> <b>some_module</b> <b>import</b> PI <b>as</b> pi, g <b>as</b> gf<br/></p>
<p>r1 = sm.f(pi)<br/>r2 = gf(6, pi)<br/></p>
<p>Binary operators and comparisons<br/>Most of the binary math operations and comparisons are as you might expect:<br/></p>
<p>In [32]: 5 - 7<br/>Out[32]: -2<br/></p>
<p>36 | Chapter 2: Python Language Basics, IPython, and Jupyter Notebooks</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In [33]: 12 + 21.5<br/>Out[33]: 33.5<br/></p>
<p>In [34]: 5 &lt;= 2<br/>Out[34]: False<br/></p>
<p>See Table 2-3 for all of the available binary operators.<br/>To check if two references refer to the same object, use the is keyword. is not is also<br/>perfectly valid if you want to check that two objects are not the same:<br/></p>
<p>In [35]: a = [1, 2, 3]<br/></p>
<p>In [36]: b = a<br/></p>
<p>In [37]: c = list(a)<br/></p>
<p>In [38]: a <b>is</b> b<br/>Out[38]: True<br/></p>
<p>In [39]: a <b>is</b> <b>not</b> c<br/>Out[39]: True<br/></p>
<p>Since list always creates a new Python list (i.e., a copy), we can be sure that c is dis&#8208;<br/>tinct from a. Comparing with is is not the same as the == operator, because in this<br/>case we have:<br/></p>
<p>In [40]: a == c<br/>Out[40]: True<br/></p>
<p>A very common use of is and is not is to check if a variable is None, since there is<br/>only one instance of None:<br/></p>
<p>In [41]: a = None<br/></p>
<p>In [42]: a <b>is</b> None<br/>Out[42]: True<br/></p>
<p><i>Table 2-3. Binary operators<br/></i>Operation Description<br/>a + b Add a and b<br/>a - b Subtract b from a<br/>a * b Multiply a by b<br/>a / b Divide a by b<br/>a // b Floor-divide a by b, dropping any fractional remainder<br/>a ** b Raise a to the b power<br/>a &amp; b True if both a and b are True; for integers, take the bitwise AND<br/>a | b True if either a or b is True; for integers, take the bitwise OR<br/>a ^ b For booleans, True if a or b is True, but not both; for integers, take the bitwise EXCLUSIVE-OR<br/></p>
<p>2.3 Python Language Basics | 37</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Operation Description<br/>a == b True if a equals b<br/>a != b True if a is not equal to b<br/>a &lt;= b, a &lt; b True if a is less than (less than or equal) to b<br/>a &gt; b, a &gt;= b True if a is greater than (greater than or equal) to b<br/>a is b True if a and b reference the same Python object<br/>a is not b True if a and b reference different Python objects<br/></p>
<p>Mutable and immutable objects<br/>Most objects in Python, such as lists, dicts, NumPy arrays, and most user-defined<br/>types (classes), are mutable. This means that the object or values that they contain can<br/>be modified:<br/></p>
<p>In [43]: a_list = ['foo', 2, [4, 5]]<br/></p>
<p>In [44]: a_list[2] = (3, 4)<br/></p>
<p>In [45]: a_list<br/>Out[45]: ['foo', 2, (3, 4)]<br/></p>
<p>Others, like strings and tuples, are immutable:<br/>In [46]: a_tuple = (3, 5, (4, 5))<br/></p>
<p>In [47]: a_tuple[1] = 'four'<br/>---------------------------------------------------------------------------<br/><b>TypeError</b>                                 Traceback (most recent call last)<br/>&lt;ipython-input-47-b7966a9ae0f1&gt; <b>in</b> &lt;module&gt;()<br/>----&gt; 1 a_tuple[1] = 'four'<br/><b>TypeError</b>: 'tuple' object does <b>not</b> support item assignment<br/></p>
<p>Remember that just because you <i>can</i> mutate an object does not mean that you always<br/><i>should. Such actions are known as side effects. For example, when writing a function,<br/></i>any side effects should be explicitly communicated to the user in the function&#8217;s docu&#8208;<br/>mentation or comments. If possible, I recommend trying to avoid side effects and<br/><i>favor immutability</i>, even though there may be mutable objects involved.<br/></p>
<p>Scalar Types<br/>Python along with its standard library has a small set of built-in types for handling<br/>numerical data, strings, boolean (True or False) values, and dates and time. These<br/>&#8220;single value&#8221; types are sometimes called <i>scalar types</i> and we refer to them in this<br/>book as scalars. See Table 2-4 for a list of the main scalar types. Date and time han&#8208;<br/>dling will be discussed separately, as these are provided by the datetime module in<br/>the standard library.<br/></p>
<p>38 | Chapter 2: Python Language Basics, IPython, and Jupyter Notebooks</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><i>Table 2-4. Standard Python scalar types<br/></i>Type Description<br/>None The Python &#8220;null&#8221; value (only one instance of the None object exists)<br/>str String type; holds Unicode (UTF-8 encoded) strings<br/>bytes Raw ASCII bytes (or Unicode encoded as bytes)<br/>float Double-precision (64-bit) floating-point number (note there is no separate double type)<br/>bool A True or False value<br/>int Arbitrary precision signed integer<br/></p>
<p>Numeric types<br/>The primary Python types for numbers are int and float. An int can store arbitrar&#8208;<br/>ily large numbers:<br/></p>
<p>In [48]: ival = 17239871<br/></p>
<p>In [49]: ival ** 6<br/>Out[49]: 26254519291092456596965462913230729701102721<br/></p>
<p>Floating-point numbers are represented with the Python float type. Under the hood<br/>each one is a double-precision (64-bit) value. They can also be expressed with scien&#8208;<br/>tific notation:<br/></p>
<p>In [50]: fval = 7.243<br/></p>
<p>In [51]: fval2 = 6.78e-5<br/></p>
<p>Integer division not resulting in a whole number will always yield a floating-point<br/>number:<br/></p>
<p>In [52]: 3 / 2<br/>Out[52]: 1.5<br/></p>
<p>To get C-style integer division (which drops the fractional part if the result is not a<br/>whole number), use the floor division operator //:<br/></p>
<p>In [53]: 3 // 2<br/>Out[53]: 1<br/></p>
<p>Strings<br/>Many people use Python for its powerful and flexible built-in string processing capa&#8208;<br/>bilities. You can write <i>string literals</i> using either single quotes ' or double quotes &quot;:<br/></p>
<p>a = 'one way of writing a string'<br/>b = &quot;another way&quot;<br/></p>
<p>For multiline strings with line breaks, you can use triple quotes, either ''' or &quot;&quot;&quot;:<br/></p>
<p>2.3 Python Language Basics | 39</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>c = &quot;&quot;&quot;<br/>This is a longer string that<br/>spans multiple lines<br/>&quot;&quot;&quot;<br/></p>
<p>It may surprise you that this string c actually contains four lines of text; the line<br/>breaks after &quot;&quot;&quot; and after lines are included in the string. We can count the new line<br/>characters with the count method on c:<br/></p>
<p>In [55]: c.count('<b>\n</b>')<br/>Out[55]: 3<br/></p>
<p>Python strings are immutable; you cannot modify a string:<br/>In [56]: a = 'this is a string'<br/></p>
<p>In [57]: a[10] = 'f'<br/>---------------------------------------------------------------------------<br/><b>TypeError</b>                                 Traceback (most recent call last)<br/>&lt;ipython-input-57-5ca625d1e504&gt; <b>in</b> &lt;module&gt;()<br/>----&gt; 1 a[10] = 'f'<br/><b>TypeError</b>: 'str' object does <b>not</b> support item assignment<br/></p>
<p>In [58]: b = a.replace('string', 'longer string')<br/></p>
<p>In [59]: b<br/>Out[59]: 'this is a longer string'<br/></p>
<p>Afer this operation, the variable a is unmodified:<br/>In [60]: a<br/>Out[60]: 'this is a string'<br/></p>
<p>Many Python objects can be converted to a string using the str function:<br/>In [61]: a = 5.6<br/></p>
<p>In [62]: s = str(a)<br/></p>
<p>In [63]: <b>print</b>(s)<br/>5.6<br/></p>
<p>Strings are a sequence of Unicode characters and therefore can be treated like other<br/>sequences, such as lists and tuples (which we will explore in more detail in the next<br/>chapter):<br/></p>
<p>In [64]: s = 'python'<br/></p>
<p>In [65]: list(s)<br/>Out[65]: ['p', 'y', 't', 'h', 'o', 'n']<br/></p>
<p>In [66]: s[:3]<br/>Out[66]: 'pyt'<br/></p>
<p>40 | Chapter 2: Python Language Basics, IPython, and Jupyter Notebooks</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>The syntax s[:3] is called <i>slicing</i> and is implemented for many kinds of Python<br/>sequences. This will be explained in more detail later on, as it is used extensively in<br/>this book.<br/>The backslash character \ is an <i>escape character</i>, meaning that it is used to specify<br/>special characters like newline \n or Unicode characters. To write a string literal with<br/>backslashes, you need to escape them:<br/></p>
<p>In [67]: s = '12<b>\\</b>34'<br/></p>
<p>In [68]: <b>print</b>(s)<br/>12\34<br/></p>
<p>If you have a string with a lot of backslashes and no special characters, you might find<br/>this a bit annoying. Fortunately you can preface the leading quote of the string with r,<br/>which means that the characters should be interpreted as is:<br/></p>
<p>In [69]: s = r'this\has\no\special\characters'<br/></p>
<p>In [70]: s<br/>Out[70]: 'this<b>\\</b>has<b>\\</b>no<b>\\</b>special<b>\\</b>characters'<br/></p>
<p>The r stands for <i>raw</i>.<br/>Adding two strings together concatenates them and produces a new string:<br/></p>
<p>In [71]: a = 'this is the first half '<br/></p>
<p>In [72]: b = 'and this is the second half'<br/></p>
<p>In [73]: a + b<br/>Out[73]: 'this is the first half and this is the second half'<br/></p>
<p>String templating or formatting is another important topic. The number of ways to<br/>do so has expanded with the advent of Python 3, and here I will briefly describe the<br/>mechanics of one of the main interfaces. String objects have a format method that<br/>can be used to substitute formatted arguments into the string, producing a new<br/>string:<br/></p>
<p>In [74]: template = '{0:.2f} {1:s} are worth US${2:d}'<br/></p>
<p>In this string,<br/>&#8226; {0:.2f} means to format the first argument as a floating-point number with two<br/></p>
<p>decimal places.<br/>&#8226; {1:s} means to format the second argument as a string.<br/>&#8226; {2:d} means to format the third argument as an exact integer.<br/></p>
<p>To substitute arguments for these format parameters, we pass a sequence of argu&#8208;<br/>ments to the format method:<br/></p>
<p>2.3 Python Language Basics | 41</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In [75]: template.format(4.5560, 'Argentine Pesos', 1)<br/>Out[75]: '4.56 Argentine Pesos are worth US$1'<br/></p>
<p>String formatting is a deep topic; there are multiple methods and numerous options<br/>and tweaks available to control how values are formatted in the resulting string. To<br/>learn more, I recommend consulting the official Python documentation.<br/>I discuss general string processing as it relates to data analysis in more detail in Chap&#8208;<br/>ter 8.<br/></p>
<p>Bytes and Unicode<br/>In modern Python (i.e., Python 3.0 and up), Unicode has become the first-class string<br/>type to enable more consistent handling of ASCII and non-ASCII text. In older ver&#8208;<br/>sions of Python, strings were all bytes without any explicit Unicode encoding. You<br/>could convert to Unicode assuming you knew the character encoding. Let&#8217;s look at an<br/>example:<br/></p>
<p>In [76]: val = &quot;espa&#241;ol&quot;<br/></p>
<p>In [77]: val<br/>Out[77]: 'espa&#241;ol'<br/></p>
<p>We can convert this Unicode string to its UTF-8 bytes representation using the<br/>encode method:<br/></p>
<p>In [78]: val_utf8 = val.encode('utf-8')<br/></p>
<p>In [79]: val_utf8<br/>Out[79]: b'espa<b>\xc3\xb1</b>ol'<br/></p>
<p>In [80]: type(val_utf8)<br/>Out[80]: bytes<br/></p>
<p>Assuming you know the Unicode encoding of a bytes object, you can go back using <br/>the decode method:<br/></p>
<p>In [81]: val_utf8.decode('utf-8')<br/>Out[81]: 'espa&#241;ol'<br/></p>
<p>While it&#8217;s become preferred to use UTF-8 for any encoding, for historical reasons you<br/>may encounter data in any number of different encodings:<br/></p>
<p>In [82]: val.encode('latin1')<br/>Out[82]: b'espa<b>\xf1</b>ol'<br/></p>
<p>In [83]: val.encode('utf-16')<br/>Out[83]: b'<b>\xff\xfe</b>e<b>\x00</b>s<b>\x00</b>p<b>\x00</b>a<b>\x00\xf1\x00</b>o<b>\x00</b>l<b>\x00</b>'<br/></p>
<p>In [84]: val.encode('utf-16le')<br/>Out[84]: b'e<b>\x00</b>s<b>\x00</b>p<b>\x00</b>a<b>\x00\xf1\x00</b>o<b>\x00</b>l<b>\x00</b>'<br/></p>
<p>42 | Chapter 2: Python Language Basics, IPython, and Jupyter Notebooks</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>It is most common to encounter bytes objects in the context of working with files,<br/>where implicitly decoding all data to Unicode strings may not be desired.<br/>Though you may seldom need to do so, you can define your own byte literals by pre&#8208;<br/>fixing a string with b:<br/></p>
<p>In [85]: bytes_val = b'this is bytes'<br/></p>
<p>In [86]: bytes_val<br/>Out[86]: b'this is bytes'<br/></p>
<p>In [87]: decoded = bytes_val.decode('utf8')<br/></p>
<p>In [88]: decoded  <i># this is str (Unicode) now<br/></i>Out[88]: 'this is bytes'<br/></p>
<p>Booleans<br/>The two boolean values in Python are written as True and False. Comparisons and<br/>other conditional expressions evaluate to either True or False. Boolean values are<br/>combined with the and and or keywords:<br/></p>
<p>In [89]: True <b>and</b> True<br/>Out[89]: True<br/></p>
<p>In [90]: False <b>or</b> True<br/>Out[90]: True<br/></p>
<p>Type casting<br/>The str, bool, int, and float types are also functions that can be used to cast values<br/>to those types:<br/></p>
<p>In [91]: s = '3.14159'<br/></p>
<p>In [92]: fval = float(s)<br/></p>
<p>In [93]: type(fval)<br/>Out[93]: float<br/></p>
<p>In [94]: int(fval)<br/>Out[94]: 3<br/></p>
<p>In [95]: bool(fval)<br/>Out[95]: True<br/></p>
<p>In [96]: bool(0)<br/>Out[96]: False<br/></p>
<p>2.3 Python Language Basics | 43</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>None<br/>None is the Python null value type. If a function does not explicitly return a value, it<br/>implicitly returns None:<br/></p>
<p>In [97]: a = None<br/></p>
<p>In [98]: a <b>is</b> None<br/>Out[98]: True<br/></p>
<p>In [99]: b = 5<br/></p>
<p>In [100]: b <b>is</b> <b>not</b> None<br/>Out[100]: True<br/></p>
<p>None is also a common default value for function arguments:<br/><b>def</b> add_and_maybe_multiply(a, b, c=None):<br/>    result = a + b<br/></p>
<p>    <b>if</b> c <b>is</b> <b>not</b> None:<br/>        result = result * c<br/></p>
<p>    <b>return</b> result<br/></p>
<p>While a technical point, it&#8217;s worth bearing in mind that None is not only a reserved<br/>keyword but also a unique instance of NoneType:<br/></p>
<p>In [101]: type(None)<br/>Out[101]: NoneType<br/></p>
<p>Dates and times<br/>The built-in Python datetime module provides datetime, date, and time types. The<br/>datetime type, as you may imagine, combines the information stored in date and<br/>time and is the most commonly used:<br/></p>
<p>In [102]: <b>from</b> <b>datetime</b> <b>import</b> datetime, date, time<br/></p>
<p>In [103]: dt = datetime(2011, 10, 29, 20, 30, 21)<br/></p>
<p>In [104]: dt.day<br/>Out[104]: 29<br/></p>
<p>In [105]: dt.minute<br/>Out[105]: 30<br/></p>
<p>Given a datetime instance, you can extract the equivalent date and time objects by<br/>calling methods on the datetime of the same name:<br/></p>
<p>In [106]: dt.date()<br/>Out[106]: datetime.date(2011, 10, 29)<br/></p>
<p>44 | Chapter 2: Python Language Basics, IPython, and Jupyter Notebooks</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In [107]: dt.time()<br/>Out[107]: datetime.time(20, 30, 21)<br/></p>
<p>The strftime method formats a datetime as a string:<br/>In [108]: dt.strftime('%m/%d/%Y %H:%M')<br/>Out[108]: '10/29/2011 20:30'<br/></p>
<p>Strings can be converted (parsed) into datetime objects with the strptime function:<br/>In [109]: datetime.strptime('20091031', '%Y%m%d')<br/>Out[109]: datetime.datetime(2009, 10, 31, 0, 0)<br/></p>
<p>See Table 2-5 for a full list of format specifications.<br/>When you are aggregating or otherwise grouping time series data, it will occasionally<br/>be useful to replace time fields of a series of datetimes&#8212;for example, replacing the<br/>minute and second fields with zero:<br/></p>
<p>In [110]: dt.replace(minute=0, second=0)<br/>Out[110]: datetime.datetime(2011, 10, 29, 20, 0)<br/></p>
<p>Since datetime.datetime is an immutable type, methods like these always produce<br/>new objects.<br/>The difference of two datetime objects produces a datetime.timedelta type:<br/></p>
<p>In [111]: dt2 = datetime(2011, 11, 15, 22, 30)<br/></p>
<p>In [112]: delta = dt2 - dt<br/></p>
<p>In [113]: delta<br/>Out[113]: datetime.timedelta(17, 7179)<br/></p>
<p>In [114]: type(delta)<br/>Out[114]: datetime.timedelta<br/></p>
<p>The output timedelta(17, 7179) indicates that the timedelta encodes an offset of 17<br/>days and 7,179 seconds.<br/>Adding a timedelta to a datetime produces a new shifted datetime:<br/></p>
<p>In [115]: dt<br/>Out[115]: datetime.datetime(2011, 10, 29, 20, 30, 21)<br/></p>
<p>In [116]: dt + delta<br/>Out[116]: datetime.datetime(2011, 11, 15, 22, 30)<br/></p>
<p><i>Table 2-5. Datetime format specification (ISO C89 compatible)<br/></i>Type Description<br/>%Y Four-digit year<br/>%y Two-digit year<br/></p>
<p>2.3 Python Language Basics | 45</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Type Description<br/>%m Two-digit month [01, 12]<br/>%d Two-digit day [01, 31]<br/>%H Hour (24-hour clock) [00, 23]<br/>%I Hour (12-hour clock) [01, 12]<br/>%M Two-digit minute [00, 59]<br/>%S Second [00, 61] (seconds 60, 61 account for leap seconds)<br/>%w Weekday as integer [0 (Sunday), 6]<br/>%U Week number of the year [00, 53]; Sunday is considered the first day of the week, and days before the first Sunday of<br/></p>
<p>the year are &#8220;week 0&#8221;<br/>%W Week number of the year [00, 53]; Monday is considered the first day of the week, and days before the first Monday of<br/></p>
<p>the year are &#8220;week 0&#8221;<br/>%z UTC time zone offset as +HHMM or -HHMM; empty if time zone naive<br/>%F Shortcut for %Y-%m-%d (e.g., 2012-4-18)<br/>%D Shortcut for %m/%d/%y (e.g., 04/18/12)<br/></p>
<p>Control Flow<br/>Python has several built-in keywords for conditional logic, loops, and other standard<br/><i>control flow concepts found in other programming languages.<br/></i></p>
<p>if, elif, and else<br/>The if statement is one of the most well-known control flow statement types. It<br/>checks a condition that, if True, evaluates the code in the block that follows:<br/></p>
<p><b>if</b> x &lt; 0:<br/>    <b>print</b>('It's negative')<br/></p>
<p>An if statement can be optionally followed by one or more elif blocks and a catch-<br/>all else block if all of the conditions are False:<br/></p>
<p><b>if</b> x &lt; 0:<br/>    <b>print</b>('It's negative')<br/><b>elif</b> x == 0:<br/>    <b>print</b>('Equal to zero')<br/><b>elif</b> 0 &lt; x &lt; 5:<br/>    <b>print</b>('Positive but smaller than 5')<br/><b>else</b>:<br/>    <b>print</b>('Positive and larger than or equal to 5')<br/></p>
<p>If any of the conditions is True, no further elif or else blocks will be reached. With<br/>a compound condition using and or or, conditions are evaluated left to right and will<br/>short-circuit:<br/></p>
<p>In [117]: a = 5; b = 7<br/></p>
<p>In [118]: c = 8; d = 4<br/></p>
<p>46 | Chapter 2: Python Language Basics, IPython, and Jupyter Notebooks</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In [119]: <b>if</b> a &lt; b <b>or</b> c &gt; d:<br/>   .....:     <b>print</b>('Made it')<br/>Made it<br/></p>
<p>In this example, the comparison c &gt; d never gets evaluated because the first compar&#8208;<br/>ison was True.<br/>It is also possible to chain comparisons:<br/></p>
<p>In [120]: 4 &gt; 3 &gt; 2 &gt; 1<br/>Out[120]: True<br/></p>
<p>for loops<br/>for loops are for iterating over a collection (like a list or tuple) or an iterater. The<br/>standard syntax for a for loop is:<br/></p>
<p><b>for</b> value <b>in</b> collection:<br/>    <i># do something with value<br/></i></p>
<p>You can advance a for loop to the next iteration, skipping the remainder of the block,<br/>using the continue keyword. Consider this code, which sums up integers in a list and<br/>skips None values:<br/></p>
<p>sequence = [1, 2, None, 4, None, 5]<br/>total = 0<br/><b>for</b> value <b>in</b> sequence:<br/>    <b>if</b> value <b>is</b> None:<br/>        <b>continue<br/></b>    total += value<br/></p>
<p>A for loop can be exited altogether with the break keyword. This code sums ele&#8208;<br/>ments of the list until a 5 is reached:<br/></p>
<p>sequence = [1, 2, 0, 4, 6, 5, 2, 1]<br/>total_until_5 = 0<br/><b>for</b> value <b>in</b> sequence:<br/>    <b>if</b> value == 5:<br/>        <b>break<br/></b>    total_until_5 += value<br/></p>
<p>The break keyword only terminates the innermost for loop; any outer for loops will<br/>continue to run:<br/></p>
<p>In [121]: <b>for</b> i <b>in</b> range(4):<br/>   .....:     <b>for</b> j <b>in</b> range(4):<br/>   .....:         <b>if</b> j &gt; i:<br/>   .....:             <b>break<br/></b>   .....:         <b>print</b>((i, j))<br/>   .....:<br/>(0, 0)<br/>(1, 0)<br/></p>
<p>2.3 Python Language Basics | 47</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>(1, 1)<br/>(2, 0)<br/>(2, 1)<br/>(2, 2)<br/>(3, 0)<br/>(3, 1)<br/>(3, 2)<br/>(3, 3)<br/></p>
<p>As we will see in more detail, if the elements in the collection or iterator are sequen&#8208;<br/>ces (tuples or lists, say), they can be conveniently <i>unpacked</i> into variables in the for<br/>loop statement:<br/></p>
<p><b>for</b> a, b, c <b>in</b> iterator:<br/>    <i># do something<br/></i></p>
<p>while loops<br/>A while loop specifies a condition and a block of code that is to be executed until the<br/>condition evaluates to False or the loop is explicitly ended with break:<br/></p>
<p>x = 256<br/>total = 0<br/><b>while</b> x &gt; 0:<br/>    <b>if</b> total &gt; 500:<br/>        <b>break<br/></b>    total += x<br/>    x = x // 2<br/></p>
<p>pass<br/>pass is the &#8220;no-op&#8221; statement in Python. It can be used in blocks where no action is to<br/>be taken (or as a placeholder for code not yet implemented); it is only required<br/>because Python uses whitespace to delimit blocks:<br/></p>
<p><b>if</b> x &lt; 0:<br/>    <b>print</b>('negative!')<br/><b>elif</b> x == 0:<br/>    <i># TODO: put something smart here<br/></i>    <b>pass<br/>else</b>:<br/>    <b>print</b>('positive!')<br/></p>
<p>range<br/>The range function returns an iterator that yields a sequence of evenly spaced<br/>integers:<br/></p>
<p>In [122]: range(10)<br/>Out[122]: range(0, 10)<br/></p>
<p>48 | Chapter 2: Python Language Basics, IPython, and Jupyter Notebooks</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In [123]: list(range(10))<br/>Out[123]: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]<br/></p>
<p>Both a start, end, and step (which may be negative) can be given:<br/>In [124]: list(range(0, 20, 2))<br/>Out[124]: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18]<br/></p>
<p>In [125]: list(range(5, 0, -1))<br/>Out[125]: [5, 4, 3, 2, 1]<br/></p>
<p>As you can see, range produces integers up to but not including the endpoint. A<br/>common use of range is for iterating through sequences by index:<br/></p>
<p>seq = [1, 2, 3, 4]<br/><b>for</b> i <b>in</b> range(len(seq)):<br/>    val = seq[i]<br/></p>
<p>While you can use functions like list to store all the integers generated by range in<br/>some other data structure, often the default iterator form will be what you want. This<br/>snippet sums all numbers from 0 to 99,999 that are multiples of 3 or 5:<br/></p>
<p>sum = 0<br/><b>for</b> i <b>in</b> range(100000):<br/>    <i># % is the modulo operator<br/></i>    <b>if</b> i % 3 == 0 <b>or</b> i % 5 == 0:<br/>        sum += i<br/></p>
<p>While the range generated can be arbitrarily large, the memory use at any given time<br/>may be very small.<br/></p>
<p>Ternary expressions<br/>A <i>ternary expression</i> in Python allows you to combine an if-else block that pro&#8208;<br/>duces a value into a single line or expression. The syntax for this in Python is:<br/></p>
<p>value = <i>true-expr</i> if condition else <i>false-expr<br/></i></p>
<p>Here, <i>true-expr</i> and <i>false-expr</i> can be any Python expressions. It has the identical<br/>effect as the more verbose:<br/></p>
<p>if <i>condition</i>:<br/>    value = <i>true-expr<br/></i>else:<br/>    value = <i>false-expr<br/></i></p>
<p>This is a more concrete example:<br/>In [126]: x = 5<br/></p>
<p>In [127]: 'Non-negative' <b>if</b> x &gt;= 0 <b>else</b> 'Negative'<br/>Out[127]: 'Non-negative'<br/></p>
<p>2.3 Python Language Basics | 49</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>As with if-else blocks, only one of the expressions will be executed. Thus, the &#8220;if &#8221;<br/>and &#8220;else&#8221; sides of the ternary expression could contain costly computations, but only<br/>the true branch is ever evaluated.<br/>While it may be tempting to always use ternary expressions to condense your code,<br/>realize that you may sacrifice readability if the condition as well as the true and false<br/>expressions are very complex.<br/></p>
<p>50 | Chapter 2: Python Language Basics, IPython, and Jupyter Notebooks</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>CHAPTER 3<br/>Built-in Data Structures, Functions,<br/></p>
<p>and Files<br/></p>
<p>This chapter discusses capabilities built into the Python language that will be used<br/>ubiquitously throughout the book. While add-on libraries like pandas and NumPy<br/>add advanced computational functionality for larger datasets, they are designed to be<br/>used together with Python&#8217;s built-in data manipulation tools.<br/>We&#8217;ll start with Python&#8217;s workhorse data structures: tuples, lists, dicts, and sets. Then,<br/>we&#8217;ll discuss creating your own reusable Python functions. Finally, we&#8217;ll look at the<br/>mechanics of Python file objects and interacting with your local hard drive.<br/></p>
<p>3.1 Data Structures and Sequences<br/>Python&#8217;s data structures are simple but powerful. Mastering their use is a critical part<br/>of becoming a proficient Python programmer.<br/></p>
<p>Tuple<br/>A tuple is a fixed-length, immutable sequence of Python objects. The easiest way to<br/>create one is with a comma-separated sequence of values:<br/></p>
<p>In [1]: tup = 4, 5, 6<br/></p>
<p>In [2]: tup<br/>Out[2]: (4, 5, 6)<br/></p>
<p>When you&#8217;re defining tuples in more complicated expressions, it&#8217;s often necessary to<br/>enclose the values in parentheses, as in this example of creating a tuple of tuples:<br/></p>
<p>51</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In [3]: nested_tup = (4, 5, 6), (7, 8)<br/></p>
<p>In [4]: nested_tup<br/>Out[4]: ((4, 5, 6), (7, 8))<br/></p>
<p>You can convert any sequence or iterator to a tuple by invoking tuple:<br/>In [5]: tuple([4, 0, 2])<br/>Out[5]: (4, 0, 2)<br/></p>
<p>In [6]: tup = tuple('string')<br/></p>
<p>In [7]: tup<br/>Out[7]: ('s', 't', 'r', 'i', 'n', 'g')<br/></p>
<p>Elements can be accessed with square brackets [] as with most other sequence types.<br/>As in C, C++, Java, and many other languages, sequences are 0-indexed in Python:<br/></p>
<p>In [8]: tup[0]<br/>Out[8]: 's'<br/></p>
<p>While the objects stored in a tuple may be mutable themselves, once the tuple is cre&#8208;<br/>ated it&#8217;s not possible to modify which object is stored in each slot:<br/></p>
<p>In [9]: tup = tuple(['foo', [1, 2], True])<br/></p>
<p>In [10]: tup[2] = False<br/>---------------------------------------------------------------------------<br/><b>TypeError</b>                                 Traceback (most recent call last)<br/>&lt;ipython-input-10-c7308343b841&gt; <b>in</b> &lt;module&gt;()<br/>----&gt; 1 tup[2] = False<br/><b>TypeError</b>: 'tuple' object does <b>not</b> support item assignment<br/></p>
<p>If an object inside a tuple is mutable, such as a list, you can modify it in-place:<br/>In [11]: tup[1].append(3)<br/></p>
<p>In [12]: tup<br/>Out[12]: ('foo', [1, 2, 3], True)<br/></p>
<p>You can concatenate tuples using the + operator to produce longer tuples:<br/>In [13]: (4, None, 'foo') + (6, 0) + ('bar',)<br/>Out[13]: (4, None, 'foo', 6, 0, 'bar')<br/></p>
<p>Multiplying a tuple by an integer, as with lists, has the effect of concatenating together<br/>that many copies of the tuple:<br/></p>
<p>In [14]: ('foo', 'bar') * 4<br/>Out[14]: ('foo', 'bar', 'foo', 'bar', 'foo', 'bar', 'foo', 'bar')<br/></p>
<p>Note that the objects themselves are not copied, only the references to them.<br/></p>
<p>52 | Chapter 3: Built-in Data Structures, Functions, and Files</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Unpacking tuples<br/>If you try to <i>assign</i> to a tuple-like expression of variables, Python will attempt to<br/><i>unpack</i> the value on the righthand side of the equals sign:<br/></p>
<p>In [15]: tup = (4, 5, 6)<br/></p>
<p>In [16]: a, b, c = tup<br/></p>
<p>In [17]: b<br/>Out[17]: 5<br/></p>
<p>Even sequences with nested tuples can be unpacked:<br/>In [18]: tup = 4, 5, (6, 7)<br/></p>
<p>In [19]: a, b, (c, d) = tup<br/></p>
<p>In [20]: d<br/>Out[20]: 7<br/></p>
<p>Using this functionality you can easily swap variable names, a task which in many<br/>languages might look like:<br/></p>
<p>tmp = a<br/>a = b<br/>b = tmp<br/></p>
<p>But, in Python, the swap can be done like this:<br/>In [21]: a, b = 1, 2<br/></p>
<p>In [22]: a<br/>Out[22]: 1<br/></p>
<p>In [23]: b<br/>Out[23]: 2<br/></p>
<p>In [24]: b, a = a, b<br/></p>
<p>In [25]: a<br/>Out[25]: 2<br/></p>
<p>In [26]: b<br/>Out[26]: 1<br/></p>
<p>A common use of variable unpacking is iterating over sequences of tuples or lists:<br/>In [27]: seq = [(1, 2, 3), (4, 5, 6), (7, 8, 9)]<br/></p>
<p>In [28]: <b>for</b> a, b, c <b>in</b> seq:<br/>   ....:     <b>print</b>('a={0}, b={1}, c={2}'.format(a, b, c))<br/>a=1, b=2, c=3<br/>a=4, b=5, c=6<br/>a=7, b=8, c=9<br/></p>
<p>3.1 Data Structures and Sequences | 53</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Another common use is returning multiple values from a function. I&#8217;ll cover this in<br/>more detail later.<br/>The Python language recently acquired some more advanced tuple unpacking to help<br/>with situations where you may want to &#8220;pluck&#8221; a few elements from the beginning of<br/>a tuple. This uses the special syntax *rest, which is also used in function signatures<br/>to capture an arbitrarily long list of positional arguments:<br/></p>
<p>In [29]: values = 1, 2, 3, 4, 5<br/></p>
<p>In [30]: a, b, *rest = values<br/></p>
<p>In [31]: a, b<br/>Out[31]: (1, 2)<br/></p>
<p>In [32]: rest<br/>Out[32]: [3, 4, 5]<br/></p>
<p>This rest bit is sometimes something you want to discard; there is nothing special<br/>about the rest name. As a matter of convention, many Python programmers will use<br/>the underscore (_) for unwanted variables:<br/></p>
<p>In [33]: a, b, *_ = values<br/></p>
<p>Tuple methods<br/>Since the size and contents of a tuple cannot be modified, it is very light on instance<br/>methods. A particularly useful one (also available on lists) is count, which counts the<br/>number of occurrences of a value:<br/></p>
<p>In [34]: a = (1, 2, 2, 2, 3, 4, 2)<br/></p>
<p>In [35]: a.count(2)<br/>Out[35]: 4<br/></p>
<p>List<br/>In contrast with tuples, lists are variable-length and their contents can be modified<br/>in-place. You can define them using square brackets [] or using the list type func&#8208;<br/>tion:<br/></p>
<p>In [36]: a_list = [2, 3, 7, None]<br/></p>
<p>In [37]: tup = ('foo', 'bar', 'baz')<br/></p>
<p>In [38]: b_list = list(tup)<br/></p>
<p>In [39]: b_list<br/>Out[39]: ['foo', 'bar', 'baz']<br/></p>
<p>In [40]: b_list[1] = 'peekaboo'<br/></p>
<p>54 | Chapter 3: Built-in Data Structures, Functions, and Files</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In [41]: b_list<br/>Out[41]: ['foo', 'peekaboo', 'baz']<br/></p>
<p>Lists and tuples are semantically similar (though tuples cannot be modified) and can<br/>be used interchangeably in many functions.<br/>The list function is frequently used in data processing as a way to materialize an<br/>iterator or generator expression:<br/></p>
<p>In [42]: gen = range(10)<br/></p>
<p>In [43]: gen<br/>Out[43]: range(0, 10)<br/></p>
<p>In [44]: list(gen)<br/>Out[44]: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]<br/></p>
<p>Adding and removing elements<br/>Elements can be appended to the end of the list with the append method:<br/></p>
<p>In [45]: b_list.append('dwarf')<br/></p>
<p>In [46]: b_list<br/>Out[46]: ['foo', 'peekaboo', 'baz', 'dwarf']<br/></p>
<p>Using insert you can insert an element at a specific location in the list:<br/>In [47]: b_list.insert(1, 'red')<br/></p>
<p>In [48]: b_list<br/>Out[48]: ['foo', 'red', 'peekaboo', 'baz', 'dwarf']<br/></p>
<p>The insertion index must be between 0 and the length of the list, inclusive.<br/></p>
<p>insert is computationally expensive compared with append,<br/>because references to subsequent elements have to be shifted inter&#8208;<br/>nally to make room for the new element. If you need to insert ele&#8208;<br/>ments at both the beginning and end of a sequence, you may wish<br/>to explore collections.deque, a double-ended queue, for this pur&#8208;<br/>pose.<br/></p>
<p>The inverse operation to insert is pop, which removes and returns an element at a<br/>particular index:<br/></p>
<p>In [49]: b_list.pop(2)<br/>Out[49]: 'peekaboo'<br/></p>
<p>In [50]: b_list<br/>Out[50]: ['foo', 'red', 'baz', 'dwarf']<br/></p>
<p>3.1 Data Structures and Sequences | 55</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Elements can be removed by value with remove, which locates the first such value and<br/>removes it from the last:<br/></p>
<p>In [51]: b_list.append('foo')<br/></p>
<p>In [52]: b_list<br/>Out[52]: ['foo', 'red', 'baz', 'dwarf', 'foo']<br/></p>
<p>In [53]: b_list.remove('foo')<br/></p>
<p>In [54]: b_list<br/>Out[54]: ['red', 'baz', 'dwarf', 'foo']<br/></p>
<p>If performance is not a concern, by using append and remove, you can use a Python<br/>list as a perfectly suitable &#8220;multiset&#8221; data structure.<br/>Check if a list contains a value using the in keyword:<br/></p>
<p>In [55]: 'dwarf' <b>in</b> b_list<br/>Out[55]: True<br/></p>
<p>The keyword not can be used to negate in:<br/>In [56]: 'dwarf' <b>not</b> <b>in</b> b_list<br/>Out[56]: False<br/></p>
<p>Checking whether a list contains a value is a lot slower than doing so with dicts and<br/>sets (to be introduced shortly), as Python makes a linear scan across the values of the<br/>list, whereas it can check the others (based on hash tables) in constant time.<br/></p>
<p>Concatenating and combining lists<br/>Similar to tuples, adding two lists together with + concatenates them:<br/></p>
<p>In [57]: [4, None, 'foo'] + [7, 8, (2, 3)]<br/>Out[57]: [4, None, 'foo', 7, 8, (2, 3)]<br/></p>
<p>If you have a list already defined, you can append multiple elements to it using the<br/>extend method:<br/></p>
<p>In [58]: x = [4, None, 'foo']<br/></p>
<p>In [59]: x.extend([7, 8, (2, 3)])<br/></p>
<p>In [60]: x<br/>Out[60]: [4, None, 'foo', 7, 8, (2, 3)]<br/></p>
<p>Note that list concatenation by addition is a comparatively expensive operation since<br/>a new list must be created and the objects copied over. Using extend to append ele&#8208;<br/>ments to an existing list, especially if you are building up a large list, is usually pref&#8208;<br/>erable. Thus,<br/></p>
<p>56 | Chapter 3: Built-in Data Structures, Functions, and Files</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>everything = []<br/><b>for</b> chunk <b>in</b> list_of_lists:<br/>    everything.extend(chunk)<br/></p>
<p>is faster than the concatenative alternative:<br/>everything = []<br/><b>for</b> chunk <b>in</b> list_of_lists:<br/>    everything = everything + chunk<br/></p>
<p>Sorting<br/>You can sort a list in-place (without creating a new object) by calling its sort<br/>function:<br/></p>
<p>In [61]: a = [7, 2, 5, 1, 3]<br/></p>
<p>In [62]: a.sort()<br/></p>
<p>In [63]: a<br/>Out[63]: [1, 2, 3, 5, 7]<br/></p>
<p>sort has a few options that will occasionally come in handy. One is the ability to pass<br/>a secondary <i>sort key</i>&#8212;that is, a function that produces a value to use to sort the<br/>objects. For example, we could sort a collection of strings by their lengths:<br/></p>
<p>In [64]: b = ['saw', 'small', 'He', 'foxes', 'six']<br/></p>
<p>In [65]: b.sort(key=len)<br/></p>
<p>In [66]: b<br/>Out[66]: ['He', 'saw', 'six', 'small', 'foxes']<br/></p>
<p>Soon, we&#8217;ll look at the sorted function, which can produce a sorted copy of a general<br/>sequence.<br/></p>
<p>Binary search and maintaining a sorted list<br/>The built-in bisect module implements binary search and insertion into a sorted list.<br/>bisect.bisect finds the location where an element should be inserted to keep it sor&#8208;<br/>ted, while bisect.insort actually inserts the element into that location:<br/></p>
<p>In [67]: <b>import</b> <b>bisect<br/></b></p>
<p>In [68]: c = [1, 2, 2, 2, 3, 4, 7]<br/></p>
<p>In [69]: bisect.bisect(c, 2)<br/>Out[69]: 4<br/></p>
<p>In [70]: bisect.bisect(c, 5)<br/>Out[70]: 6<br/></p>
<p>3.1 Data Structures and Sequences | 57</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In [71]: bisect.insort(c, 6)<br/></p>
<p>In [72]: c<br/>Out[72]: [1, 2, 2, 2, 3, 4, 6, 7]<br/></p>
<p>The bisect module functions do not check whether the list is sor&#8208;<br/>ted, as doing so would be computationally expensive. Thus, using<br/>them with an unsorted list will succeed without error but may lead<br/>to incorrect results.<br/></p>
<p>Slicing<br/>You can select sections of most sequence types by using slice notation, which in its<br/>basic form consists of start:stop passed to the indexing operator []:<br/></p>
<p>In [73]: seq = [7, 2, 3, 7, 5, 6, 0, 1]<br/></p>
<p>In [74]: seq[1:5]<br/>Out[74]: [2, 3, 7, 5]<br/></p>
<p>Slices can also be assigned to with a sequence:<br/>In [75]: seq[3:4] = [6, 3]<br/></p>
<p>In [76]: seq<br/>Out[76]: [7, 2, 3, 6, 3, 5, 6, 0, 1]<br/></p>
<p>While the element at the start index is included, the stop index is <i>not included</i>, so<br/>that the number of elements in the result is stop - start.<br/>Either the start or stop can be omitted, in which case they default to the start of the<br/>sequence and the end of the sequence, respectively:<br/></p>
<p>In [77]: seq[:5]<br/>Out[77]: [7, 2, 3, 6, 3]<br/></p>
<p>In [78]: seq[3:]<br/>Out[78]: [6, 3, 5, 6, 0, 1]<br/></p>
<p>Negative indices slice the sequence relative to the end:<br/>In [79]: seq[-4:]<br/>Out[79]: [5, 6, 0, 1]<br/></p>
<p>In [80]: seq[-6:-2]<br/>Out[80]: [6, 3, 5, 6]<br/></p>
<p>Slicing semantics takes a bit of getting used to, especially if you&#8217;re coming from R or<br/>MATLAB. See Figure 3-1 for a helpful illustration of slicing with positive and nega&#8208;<br/>tive integers. In the figure, the indices are shown at the &#8220;bin edges&#8221; to help show<br/>where the slice selections start and stop using positive or negative indices.<br/></p>
<p>58 | Chapter 3: Built-in Data Structures, Functions, and Files</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>A step can also be used after a second colon to, say, take every other element:<br/>In [81]: seq[::2]<br/>Out[81]: [7, 3, 3, 6, 1]<br/></p>
<p>A clever use of this is to pass -1, which has the useful effect of reversing a list or tuple:<br/>In [82]: seq[::-1]<br/>Out[82]: [1, 0, 6, 5, 3, 6, 3, 2, 7]<br/></p>
<p><i>Figure 3-1. Illustration of Python slicing conventions<br/></i></p>
<p>Built-in Sequence Functions<br/>Python has a handful of useful sequence functions that you should familiarize your&#8208;<br/>self with and use at any opportunity.<br/></p>
<p>enumerate<br/>It&#8217;s common when iterating over a sequence to want to keep track of the index of the<br/>current item. A do-it-yourself approach would look like:<br/></p>
<p>i = 0<br/><b>for</b> value <b>in</b> collection:<br/>   <i># do something with value<br/></i>   i += 1<br/></p>
<p>Since this is so common, Python has a built-in function, enumerate, which returns a<br/>sequence of (i, value) tuples:<br/></p>
<p><b>for</b> i, value <b>in</b> enumerate(collection):<br/>   <i># do something with value<br/></i></p>
<p>When you are indexing data, a helpful pattern that uses enumerate is computing a<br/>dict mapping the values of a sequence (which are assumed to be unique) to their<br/>locations in the sequence:<br/></p>
<p>In [83]: some_list = ['foo', 'bar', 'baz']<br/></p>
<p>In [84]: mapping = {}<br/></p>
<p>3.1 Data Structures and Sequences | 59</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In [85]: <b>for</b> i, v <b>in</b> enumerate(some_list):<br/>   ....:     mapping[v] = i<br/></p>
<p>In [86]: mapping<br/>Out[86]: {'bar': 1, 'baz': 2, 'foo': 0}<br/></p>
<p>sorted<br/>The sorted function returns a new sorted list from the elements of any sequence:<br/></p>
<p>In [87]: sorted([7, 1, 2, 6, 0, 3, 2])<br/>Out[87]: [0, 1, 2, 2, 3, 6, 7]<br/></p>
<p>In [88]: sorted('horse race')<br/>Out[88]: [' ', 'a', 'c', 'e', 'e', 'h', 'o', 'r', 'r', 's']<br/></p>
<p>The sorted function accepts the same arguments as the sort method on lists.<br/></p>
<p>zip<br/>zip &#8220;pairs&#8221; up the elements of a number of lists, tuples, or other sequences to create a<br/>list of tuples:<br/></p>
<p>In [89]: seq1 = ['foo', 'bar', 'baz']<br/></p>
<p>In [90]: seq2 = ['one', 'two', 'three']<br/></p>
<p>In [91]: zipped = zip(seq1, seq2)<br/></p>
<p>In [92]: list(zipped)<br/>Out[92]: [('foo', 'one'), ('bar', 'two'), ('baz', 'three')]<br/></p>
<p>zip can take an arbitrary number of sequences, and the number of elements it pro&#8208;<br/>duces is determined by the <i>shortest</i> sequence:<br/></p>
<p>In [93]: seq3 = [False, True]<br/></p>
<p>In [94]: list(zip(seq1, seq2, seq3))<br/>Out[94]: [('foo', 'one', False), ('bar', 'two', True)]<br/></p>
<p>A very common use of zip is simultaneously iterating over multiple sequences, possi&#8208;<br/>bly also combined with enumerate:<br/></p>
<p>In [95]: <b>for</b> i, (a, b) <b>in</b> enumerate(zip(seq1, seq2)):<br/>   ....:     <b>print</b>('{0}: {1}, {2}'.format(i, a, b))<br/>   ....:<br/>0: foo, one<br/>1: bar, two<br/>2: baz, three<br/></p>
<p>60 | Chapter 3: Built-in Data Structures, Functions, and Files</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Given a &#8220;zipped&#8221; sequence, zip can be applied in a clever way to &#8220;unzip&#8221; the<br/>sequence. Another way to think about this is converting a list of <i>rows</i> into a list of<br/><i>columns</i>. The syntax, which looks a bit magical, is:<br/></p>
<p>In [96]: pitchers = [('Nolan', 'Ryan'), ('Roger', 'Clemens'),<br/>   ....:             ('Schilling', 'Curt')]<br/></p>
<p>In [97]: first_names, last_names = zip(*pitchers)<br/></p>
<p>In [98]: first_names<br/>Out[98]: ('Nolan', 'Roger', 'Schilling')<br/></p>
<p>In [99]: last_names<br/>Out[99]: ('Ryan', 'Clemens', 'Curt')<br/></p>
<p>reversed<br/>reversed iterates over the elements of a sequence in reverse order:<br/></p>
<p>In [100]: list(reversed(range(10)))<br/>Out[100]: [9, 8, 7, 6, 5, 4, 3, 2, 1, 0]<br/></p>
<p>Keep in mind that reversed is a generator (to be discussed in some more detail later),<br/>so it does not create the reversed sequence until materialized (e.g., with list or a for<br/>loop).<br/></p>
<p>dict<br/>dict is likely the most important built-in Python data structure. A more common<br/>name for it is <i>hash map</i> or <i>associative array</i>. It is a flexibly sized collection of <i>key-value<br/></i>pairs, where <i>key</i> and <i>value</i> are Python objects. One approach for creating one is to use<br/>curly braces {} and colons to separate keys and values:<br/></p>
<p>In [101]: empty_dict = {}<br/></p>
<p>In [102]: d1 = {'a' : 'some value', 'b' : [1, 2, 3, 4]}<br/></p>
<p>In [103]: d1<br/>Out[103]: {'a': 'some value', 'b': [1, 2, 3, 4]}<br/></p>
<p>You can access, insert, or set elements using the same syntax as for accessing elements<br/>of a list or tuple:<br/></p>
<p>In [104]: d1[7] = 'an integer'<br/></p>
<p>In [105]: d1<br/>Out[105]: {'a': 'some value', 'b': [1, 2, 3, 4], 7: 'an integer'}<br/></p>
<p>In [106]: d1['b']<br/>Out[106]: [1, 2, 3, 4]<br/></p>
<p>3.1 Data Structures and Sequences | 61</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>You can check if a dict contains a key using the same syntax used for checking<br/>whether a list or tuple contains a value:<br/></p>
<p>In [107]: 'b' <b>in</b> d1<br/>Out[107]: True<br/></p>
<p>You can delete values either using the del keyword or the pop method (which simul&#8208;<br/>taneously returns the value and deletes the key):<br/></p>
<p>In [108]: d1[5] = 'some value'<br/></p>
<p>In [109]: d1<br/>Out[109]: <br/>{'a': 'some value',<br/> 'b': [1, 2, 3, 4],<br/> 7: 'an integer',<br/> 5: 'some value'}<br/></p>
<p>In [110]: d1['dummy'] = 'another value'<br/></p>
<p>In [111]: d1<br/>Out[111]: <br/>{'a': 'some value',<br/> 'b': [1, 2, 3, 4],<br/> 7: 'an integer',<br/> 5: 'some value',<br/> 'dummy': 'another value'}<br/></p>
<p>In [112]: <b>del</b> d1[5]<br/></p>
<p>In [113]: d1<br/>Out[113]: <br/>{'a': 'some value',<br/> 'b': [1, 2, 3, 4],<br/> 7: 'an integer',<br/> 'dummy': 'another value'}<br/></p>
<p>In [114]: ret = d1.pop('dummy')<br/></p>
<p>In [115]: ret<br/>Out[115]: 'another value'<br/></p>
<p>In [116]: d1<br/>Out[116]: {'a': 'some value', 'b': [1, 2, 3, 4], 7: 'an integer'}<br/></p>
<p>The keys and values method give you iterators of the dict&#8217;s keys and values, respec&#8208;<br/>tively. While the key-value pairs are not in any particular order, these functions out&#8208;<br/>put the keys and values in the same order:<br/></p>
<p>In [117]: list(d1.keys())<br/>Out[117]: ['a', 'b', 7]<br/></p>
<p>62 | Chapter 3: Built-in Data Structures, Functions, and Files</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In [118]: list(d1.values())<br/>Out[118]: ['some value', [1, 2, 3, 4], 'an integer']<br/></p>
<p>You can merge one dict into another using the update method:<br/>In [119]: d1.update({'b' : 'foo', 'c' : 12})<br/></p>
<p>In [120]: d1<br/>Out[120]: {'a': 'some value', 'b': 'foo', 7: 'an integer', 'c': 12}<br/></p>
<p>The update method changes dicts in-place, so any existing keys in the data passed to<br/>update will have their old values discarded.<br/></p>
<p>Creating dicts from sequences<br/>It&#8217;s common to occasionally end up with two sequences that you want to pair up<br/>element-wise in a dict. As a first cut, you might write code like this:<br/></p>
<p>mapping = {}<br/><b>for</b> key, value <b>in</b> zip(key_list, value_list):<br/>    mapping[key] = value<br/></p>
<p>Since a dict is essentially a collection of 2-tuples, the dict function accepts a list of<br/>2-tuples:<br/></p>
<p>In [121]: mapping = dict(zip(range(5), reversed(range(5))))<br/></p>
<p>In [122]: mapping<br/>Out[122]: {0: 4, 1: 3, 2: 2, 3: 1, 4: 0}<br/></p>
<p>Later we&#8217;ll talk about <i>dict comprehensions</i>, another elegant way to construct dicts.<br/></p>
<p>Default values<br/>It&#8217;s very common to have logic like:<br/></p>
<p><b>if</b> key <b>in</b> some_dict:<br/>    value = some_dict[key]<br/><b>else</b>:<br/>    value = default_value<br/></p>
<p>Thus, the dict methods get and pop can take a default value to be returned, so that<br/>the above if-else block can be written simply as:<br/></p>
<p>value = some_dict.get(key, default_value)<br/></p>
<p>get by default will return None if the key is not present, while pop will raise an excep&#8208;<br/>tion. With <i>setting</i> values, a common case is for the values in a dict to be other collec&#8208;<br/>tions, like lists. For example, you could imagine categorizing a list of words by their<br/>first letters as a dict of lists:<br/></p>
<p>In [123]: words = ['apple', 'bat', 'bar', 'atom', 'book']<br/></p>
<p>In [124]: by_letter = {}<br/></p>
<p>3.1 Data Structures and Sequences | 63</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In [125]: <b>for</b> word <b>in</b> words:<br/>   .....:     letter = word[0]<br/>   .....:     <b>if</b> letter <b>not</b> <b>in</b> by_letter:<br/>   .....:         by_letter[letter] = [word]<br/>   .....:     <b>else</b>:<br/>   .....:         by_letter[letter].append(word)<br/>   .....:<br/></p>
<p>In [126]: by_letter<br/>Out[126]: {'a': ['apple', 'atom'], 'b': ['bat', 'bar', 'book']}<br/></p>
<p>The setdefault dict method is for precisely this purpose. The preceding for loop<br/>can be rewritten as:<br/></p>
<p><b>for</b> word <b>in</b> words:<br/>    letter = word[0]<br/>    by_letter.setdefault(letter, []).append(word)<br/></p>
<p>The built-in collections module has a useful class, defaultdict, which makes this<br/>even easier. To create one, you pass a type or function for generating the default value<br/>for each slot in the dict:<br/></p>
<p><b>from</b> <b>collections</b> <b>import</b> defaultdict<br/>by_letter = defaultdict(list)<br/><b>for</b> word <b>in</b> words:<br/>    by_letter[word[0]].append(word)<br/></p>
<p>Valid dict key types<br/>While the values of a dict can be any Python object, the keys generally have to be<br/>immutable objects like scalar types (int, float, string) or tuples (all the objects in the<br/>tuple need to be immutable, too). The technical term here is <i>hashability</i>. You can<br/>check whether an object is hashable (can be used as a key in a dict) with the hash<br/>function:<br/></p>
<p>In [127]: hash('string')<br/>Out[127]: 5023931463650008331<br/></p>
<p>In [128]: hash((1, 2, (2, 3)))<br/>Out[128]: 1097636502276347782<br/></p>
<p>In [129]: hash((1, 2, [2, 3])) <i># fails because lists are mutable<br/></i>---------------------------------------------------------------------------<br/><b>TypeError</b>                                 Traceback (most recent call last)<br/>&lt;ipython-input-129-800cd14ba8be&gt; <b>in</b> &lt;module&gt;()<br/>----&gt; 1 hash((1, 2, [2, 3])) <i># fails because lists are mutable<br/><b></b></i><b>TypeError</b>: unhashable type: 'list'<br/></p>
<p>64 | Chapter 3: Built-in Data Structures, Functions, and Files</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>To use a list as a key, one option is to convert it to a tuple, which can be hashed as<br/>long as its elements also can:<br/></p>
<p>In [130]: d = {}<br/></p>
<p>In [131]: d[tuple([1, 2, 3])] = 5<br/></p>
<p>In [132]: d<br/>Out[132]: {(1, 2, 3): 5}<br/></p>
<p>set<br/>A set is an unordered collection of unique elements. You can think of them like dicts,<br/>but keys only, no values. A set can be created in two ways: via the set function or via<br/>a <i>set literal</i> with curly braces:<br/></p>
<p>In [133]: set([2, 2, 2, 1, 3, 3])<br/>Out[133]: {1, 2, 3}<br/></p>
<p>In [134]: {2, 2, 2, 1, 3, 3}<br/>Out[134]: {1, 2, 3}<br/></p>
<p>Sets support mathematical <i>set operations</i> like union, intersection, difference, and<br/>symmetric difference. Consider these two example sets:<br/></p>
<p>In [135]: a = {1, 2, 3, 4, 5}<br/></p>
<p>In [136]: b = {3, 4, 5, 6, 7, 8}<br/></p>
<p>The union of these two sets is the set of distinct elements occurring in either set. This<br/>can be computed with either the union method or the | binary operator:<br/></p>
<p>In [137]: a.union(b)<br/>Out[137]: {1, 2, 3, 4, 5, 6, 7, 8}<br/></p>
<p>In [138]: a | b<br/>Out[138]: {1, 2, 3, 4, 5, 6, 7, 8}<br/></p>
<p>The intersection contains the elements occurring in both sets. The &amp; operator or the<br/>intersection method can be used:<br/></p>
<p>In [139]: a.intersection(b)<br/>Out[139]: {3, 4, 5}<br/></p>
<p>In [140]: a &amp; b<br/>Out[140]: {3, 4, 5}<br/></p>
<p>See Table 3-1 for a list of commonly used set methods.<br/></p>
<p>3.1 Data Structures and Sequences | 65</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><i>Table 3-1. Python set operations<br/></i>Function Alternative<br/></p>
<p>syntax<br/>Description<br/></p>
<p>a.add(x) N/A Add element x to the set a<br/>a.clear() N/A Reset the set a to an empty state, discarding all of<br/></p>
<p>its elements<br/>a.remove(x) N/A Remove element x from the set a<br/>a.pop() N/A Remove an arbitrary element from the set a, raising<br/></p>
<p>KeyError if the set is empty<br/>a.union(b) a | b All of the unique elements in a and b<br/>a.update(b) a |= b Set the contents of a to be the union of the<br/></p>
<p>elements in a and b<br/>a.intersection(b) a &amp; b All of the elements in <i>both </i>a and b<br/>a.intersection_update(b) a &amp;= b Set the contents of a to be the intersection of the<br/></p>
<p>elements in a and b<br/>a.difference(b) a - b The elements in a that are not in b<br/>a.difference_update(b) a -= b Set a to the elements in a that are not in b<br/>a.symmetric_difference(b) a ^ b All of the elements in either a or b but <i>not both<br/></i>a.symmetric_difference_update(b) a ^= b Set a to contain the elements in either a or b but<br/></p>
<p><i>not both<br/></i>a.issubset(b) N/A True if the elements of a are all contained in b<br/>a.issuperset(b) N/A True if the elements of b are all contained in a<br/>a.isdisjoint(b) N/A True if a and b have no elements in common<br/></p>
<p>All of the logical set operations have in-place counterparts, which enable you to<br/>replace the contents of the set on the left side of the operation with the result. For<br/>very large sets, this may be more efficient:<br/></p>
<p>In [141]: c = a.copy()<br/></p>
<p>In [142]: c |= b<br/></p>
<p>In [143]: c<br/>Out[143]: {1, 2, 3, 4, 5, 6, 7, 8}<br/></p>
<p>In [144]: d = a.copy()<br/></p>
<p>In [145]: d &amp;= b<br/></p>
<p>In [146]: d<br/>Out[146]: {3, 4, 5}<br/></p>
<p>Like dicts, set elements generally must be immutable. To have list-like elements, you<br/>must convert it to a tuple:<br/></p>
<p>66 | Chapter 3: Built-in Data Structures, Functions, and Files</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In [147]: my_data = [1, 2, 3, 4]<br/></p>
<p>In [148]: my_set = {tuple(my_data)}<br/></p>
<p>In [149]: my_set<br/>Out[149]: {(1, 2, 3, 4)}<br/></p>
<p>You can also check if a set is a subset of (is contained in) or a superset of (contains all<br/>elements of) another set:<br/></p>
<p>In [150]: a_set = {1, 2, 3, 4, 5}<br/></p>
<p>In [151]: {1, 2, 3}.issubset(a_set)<br/>Out[151]: True<br/></p>
<p>In [152]: a_set.issuperset({1, 2, 3})<br/>Out[152]: True<br/></p>
<p>Sets are equal if and only if their contents are equal:<br/>In [153]: {1, 2, 3} == {3, 2, 1}<br/>Out[153]: True<br/></p>
<p>List, Set, and Dict Comprehensions<br/><i>List comprehensions</i> are one of the most-loved Python language features. They allow<br/>you to concisely form a new list by filtering the elements of a collection, transforming<br/>the elements passing the filter in one concise expression. They take the basic form:<br/></p>
<p>[<i>expr</i> for val in collection if <i>condition</i>]<br/></p>
<p>This is equivalent to the following for loop:<br/>result = []<br/>for val in collection:<br/>    if <i>condition</i>:<br/>        result.append(<i>expr</i>)<br/></p>
<p>The filter condition can be omitted, leaving only the expression. For example, given a<br/>list of strings, we could filter out strings with length 2 or less and also convert them to<br/>uppercase like this:<br/></p>
<p>In [154]: strings = ['a', 'as', 'bat', 'car', 'dove', 'python']<br/></p>
<p>In [155]: [x.upper() <b>for</b> x <b>in</b> strings <b>if</b> len(x) &gt; 2]<br/>Out[155]: ['BAT', 'CAR', 'DOVE', 'PYTHON']<br/></p>
<p>Set and dict comprehensions are a natural extension, producing sets and dicts in an<br/>idiomatically similar way instead of lists. A dict comprehension looks like this:<br/></p>
<p>dict_comp = {<i>key-expr</i> : <i>value-expr</i> for value in collection<br/>             if <i>condition</i>}<br/></p>
<p>3.1 Data Structures and Sequences | 67</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>A set comprehension looks like the equivalent list comprehension except with curly<br/>braces instead of square brackets:<br/></p>
<p>set_comp = {<i>expr</i> for value in collection if <i>condition</i>}<br/></p>
<p>Like list comprehensions, set and dict comprehensions are mostly conveniences, but<br/>they similarly can make code both easier to write and read. Consider the list of strings<br/>from before. Suppose we wanted a set containing just the lengths of the strings con&#8208;<br/>tained in the collection; we could easily compute this using a set comprehension:<br/></p>
<p>In [156]: unique_lengths = {len(x) <b>for</b> x <b>in</b> strings}<br/></p>
<p>In [157]: unique_lengths<br/>Out[157]: {1, 2, 3, 4, 6}<br/></p>
<p>We could also express this more functionally using the map function, introduced<br/>shortly:<br/></p>
<p>In [158]: set(map(len, strings))<br/>Out[158]: {1, 2, 3, 4, 6}<br/></p>
<p>As a simple dict comprehension example, we could create a lookup map of these<br/>strings to their locations in the list:<br/></p>
<p>In [159]: loc_mapping = {val : index <b>for</b> index, val <b>in</b> enumerate(strings)}<br/></p>
<p>In [160]: loc_mapping<br/>Out[160]: {'a': 0, 'as': 1, 'bat': 2, 'car': 3, 'dove': 4, 'python': 5}<br/></p>
<p>Nested list comprehensions<br/>Suppose we have a list of lists containing some English and Spanish names:<br/></p>
<p>In [161]: all_data = [['John', 'Emily', 'Michael', 'Mary', 'Steven'],<br/>   .....:             ['Maria', 'Juan', 'Javier', 'Natalia', 'Pilar']]<br/></p>
<p>You might have gotten these names from a couple of files and decided to organize<br/>them by language. Now, suppose we wanted to get a single list containing all names<br/>with two or more e&#8217;s in them. We could certainly do this with a simple for loop:<br/></p>
<p>names_of_interest = []<br/><b>for</b> names <b>in</b> all_data:<br/>    enough_es = [name <b>for</b> name <b>in</b> names <b>if</b> name.count('e') &gt;= 2]<br/>    names_of_interest.extend(enough_es)<br/></p>
<p>You can actually wrap this whole operation up in a single <i>nested list comprehension</i>,<br/>which will look like:<br/></p>
<p>In [162]: result = [name <b>for</b> names <b>in</b> all_data <b>for</b> name <b>in</b> names<br/>   .....:           <b>if</b> name.count('e') &gt;= 2]<br/></p>
<p>In [163]: result<br/>Out[163]: ['Steven']<br/></p>
<p>68 | Chapter 3: Built-in Data Structures, Functions, and Files</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>At first, nested list comprehensions are a bit hard to wrap your head around. The for<br/>parts of the list comprehension are arranged according to the order of nesting, and<br/>any filter condition is put at the end as before. Here is another example where we<br/>&#8220;flatten&#8221; a list of tuples of integers into a simple list of integers:<br/></p>
<p>In [164]: some_tuples = [(1, 2, 3), (4, 5, 6), (7, 8, 9)]<br/></p>
<p>In [165]: flattened = [x <b>for</b> tup <b>in</b> some_tuples <b>for</b> x <b>in</b> tup]<br/></p>
<p>In [166]: flattened<br/>Out[166]: [1, 2, 3, 4, 5, 6, 7, 8, 9]<br/></p>
<p>Keep in mind that the order of the for expressions would be the same if you wrote a<br/>nested for loop instead of a list comprehension:<br/></p>
<p>flattened = []<br/></p>
<p><b>for</b> tup <b>in</b> some_tuples:<br/>    <b>for</b> x <b>in</b> tup:<br/>        flattened.append(x)<br/></p>
<p>You can have arbitrarily many levels of nesting, though if you have more than two or<br/>three levels of nesting you should probably start to question whether this makes sense<br/>from a code readability standpoint. It&#8217;s important to distinguish the syntax just shown<br/>from a list comprehension inside a list comprehension, which is also perfectly valid:<br/></p>
<p>In [167]: [[x <b>for</b> x <b>in</b> tup] <b>for</b> tup <b>in</b> some_tuples]<br/>Out[167]: [[1, 2, 3], [4, 5, 6], [7, 8, 9]]<br/></p>
<p>This produces a list of lists, rather than a flattened list of all of the inner elements.<br/></p>
<p>3.2 Functions<br/>Functions are the primary and most important method of code organization and<br/>reuse in Python. As a rule of thumb, if you anticipate needing to repeat the same or<br/>very similar code more than once, it may be worth writing a reusable function. Func&#8208;<br/>tions can also help make your code more readable by giving a name to a group of<br/>Python statements.<br/>Functions are declared with the def keyword and returned from with the return key&#8208;<br/>word:<br/></p>
<p><b>def</b> my_function(x, y, z=1.5):<br/>    <b>if</b> z &gt; 1:<br/>        <b>return</b> z * (x + y)<br/>    <b>else</b>:<br/>        <b>return</b> z / (x + y)<br/></p>
<p>3.2 Functions | 69</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>There is no issue with having multiple return statements. If Python reaches the end<br/>of a function without encountering a return statement, None is returned automati&#8208;<br/>cally.<br/>Each function can have <i>positional</i> arguments and <i>keyword</i> arguments. Keyword argu&#8208;<br/>ments are most commonly used to specify default values or optional arguments. In<br/>the preceding function, x and y are positional arguments while z is a keyword argu&#8208;<br/>ment. This means that the function can be called in any of these ways:<br/></p>
<p>my_function(5, 6, z=0.7)<br/>my_function(3.14, 7, 3.5)<br/>my_function(10, 20)<br/></p>
<p>The main restriction on function arguments is that the keyword arguments <i>must</i> fol&#8208;<br/>low the positional arguments (if any). You can specify keyword arguments in any<br/>order; this frees you from having to remember which order the function arguments<br/>were specified in and only what their names are.<br/></p>
<p>It is possible to use keywords for passing positional arguments as<br/>well. In the preceding example, we could also have written:<br/></p>
<p>my_function(x=5, y=6, z=7)<br/>my_function(y=6, x=5, z=7)<br/></p>
<p>In some cases this can help with readability.<br/></p>
<p>Namespaces, Scope, and Local Functions<br/>Functions can access variables in two different scopes: <i>global</i> and <i>local</i>. An alternative<br/>and more descriptive name describing a variable scope in Python is a <i>namespace</i>. Any<br/>variables that are assigned within a function by default are assigned to the local<br/>namespace. The local namespace is created when the function is called and immedi&#8208;<br/>ately populated by the function&#8217;s arguments. After the function is finished, the local<br/>namespace is destroyed (with some exceptions that are outside the purview of this<br/>chapter). Consider the following function:<br/></p>
<p><b>def</b> func():<br/>    a = []<br/>    <b>for</b> i <b>in</b> range(5):<br/>        a.append(i)<br/></p>
<p>When func() is called, the empty list a is created, five elements are appended, and<br/>then a is destroyed when the function exits. Suppose instead we had declared a as<br/>follows:<br/></p>
<p>a = []<br/><b>def</b> func():<br/>    <b>for</b> i <b>in</b> range(5):<br/>        a.append(i)<br/></p>
<p>70 | Chapter 3: Built-in Data Structures, Functions, and Files</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Assigning variables outside of the function&#8217;s scope is possible, but those variables<br/>must be declared as global via the global keyword:<br/></p>
<p>In [168]: a = None<br/></p>
<p>In [169]: <b>def</b> bind_a_variable():<br/>   .....:     <b>global</b> a<br/>   .....:     a = []<br/>   .....: bind_a_variable()<br/>   .....:<br/></p>
<p>In [170]: <b>print</b>(a)<br/>[]<br/></p>
<p>I generally discourage use of the global keyword. Typically global<br/>variables are used to store some kind of state in a system. If you<br/>find yourself using a lot of them, it may indicate a need for object-<br/>oriented programming (using classes).<br/></p>
<p>Returning Multiple Values<br/>When I first programmed in Python after having programmed in Java and C++, one<br/>of my favorite features was the ability to return multiple values from a function with<br/>simple syntax. Here&#8217;s an example:<br/></p>
<p><b>def</b> f():<br/>    a = 5<br/>    b = 6<br/>    c = 7<br/>    <b>return</b> a, b, c<br/></p>
<p>a, b, c = f()<br/></p>
<p>In data analysis and other scientific applications, you may find yourself doing this<br/>often. What&#8217;s happening here is that the function is actually just returning <i>one</i> object,<br/>namely a tuple, which is then being unpacked into the result variables. In the preced&#8208;<br/>ing example, we could have done this instead:<br/></p>
<p>return_value = f()<br/></p>
<p>In this case, return_value would be a 3-tuple with the three returned variables. A<br/>potentially attractive alternative to returning multiple values like before might be to<br/>return a dict instead:<br/></p>
<p><b>def</b> f():<br/>    a = 5<br/>    b = 6<br/>    c = 7<br/>    <b>return</b> {'a' : a, 'b' : b, 'c' : c}<br/></p>
<p>3.2 Functions | 71</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>This alternative technique can be useful depending on what you are trying to do.<br/></p>
<p>Functions Are Objects<br/>Since Python functions are objects, many constructs can be easily expressed that are<br/>difficult to do in other languages. Suppose we were doing some data cleaning and<br/>needed to apply a bunch of transformations to the following list of strings:<br/></p>
<p>In [171]: states = ['   Alabama ', 'Georgia!', 'Georgia', 'georgia', 'FlOrIda',<br/>   .....:           'south   carolina##', 'West virginia?']<br/></p>
<p>Anyone who has ever worked with user-submitted survey data has seen messy results<br/>like these. Lots of things need to happen to make this list of strings uniform and<br/>ready for analysis: stripping whitespace, removing punctuation symbols, and stand&#8208;<br/>ardizing on proper capitalization. One way to do this is to use built-in string methods<br/>along with the re standard library module for regular expressions:<br/></p>
<p><b>import</b> <b>re<br/></b></p>
<p><b>def</b> clean_strings(strings):<br/>    result = []<br/>    <b>for</b> value <b>in</b> strings:<br/>        value = value.strip()<br/>        value = re.sub('[!#?]', '', value)<br/>        value = value.title()<br/>        result.append(value)<br/>    <b>return</b> result<br/></p>
<p>The result looks like this:<br/>In [173]: clean_strings(states)<br/>Out[173]: <br/>['Alabama',<br/> 'Georgia',<br/> 'Georgia',<br/> 'Georgia',<br/> 'Florida',<br/> 'South   Carolina',<br/> 'West Virginia']<br/></p>
<p>An alternative approach that you may find useful is to make a list of the operations<br/>you want to apply to a particular set of strings:<br/></p>
<p><b>def</b> remove_punctuation(value):<br/>    <b>return</b> re.sub('[!#?]', '', value)<br/></p>
<p>clean_ops = [str.strip, remove_punctuation, str.title]<br/></p>
<p><b>def</b> clean_strings(strings, ops):<br/>    result = []<br/>    <b>for</b> value <b>in</b> strings:<br/>        <b>for</b> function <b>in</b> ops:<br/></p>
<p>72 | Chapter 3: Built-in Data Structures, Functions, and Files</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>            value = function(value)<br/>        result.append(value)<br/>    <b>return</b> result<br/></p>
<p>Then we have the following:<br/>In [175]: clean_strings(states, clean_ops)<br/>Out[175]: <br/>['Alabama',<br/> 'Georgia',<br/> 'Georgia',<br/> 'Georgia',<br/> 'Florida',<br/> 'South   Carolina',<br/> 'West Virginia']<br/></p>
<p>A more <i>functional</i> pattern like this enables you to easily modify how the strings are<br/>transformed at a very high level. The clean_strings function is also now more reus&#8208;<br/>able and generic.<br/>You can use functions as arguments to other functions like the built-in map function,<br/>which applies a function to a sequence of some kind:<br/></p>
<p>In [176]: <b>for</b> x <b>in</b> map(remove_punctuation, states):<br/>   .....:     <b>print</b>(x)<br/>Alabama <br/>Georgia<br/>Georgia<br/>georgia<br/>FlOrIda<br/>south   carolina<br/>West virginia<br/></p>
<p>Anonymous (Lambda) Functions<br/>Python has support for so-called <i>anonymous</i> or <i>lambda</i> functions, which are a way of<br/>writing functions consisting of a single statement, the result of which is the return<br/>value. They are defined with the lambda keyword, which has no meaning other than<br/>&#8220;we are declaring an anonymous function&#8221;:<br/></p>
<p><b>def</b> short_function(x):<br/>    <b>return</b> x * 2<br/></p>
<p>equiv_anon = <b>lambda</b> x: x * 2<br/></p>
<p>I usually refer to these as lambda functions in the rest of the book. They are especially<br/>convenient in data analysis because, as you&#8217;ll see, there are many cases where data<br/>transformation functions will take functions as arguments. It&#8217;s often less typing (and<br/>clearer) to pass a lambda function as opposed to writing a full-out function declara&#8208;<br/>tion or even assigning the lambda function to a local variable. For example, consider<br/>this silly example:<br/></p>
<p>3.2 Functions | 73</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><b>def</b> apply_to_list(some_list, f):<br/>    <b>return</b> [f(x) <b>for</b> x <b>in</b> some_list]<br/></p>
<p>ints = [4, 0, 1, 5, 6]<br/>apply_to_list(ints, <b>lambda</b> x: x * 2)<br/></p>
<p>You could also have written [x * 2 for x in ints], but here we were able to suc&#8208;<br/>cinctly pass a custom operator to the apply_to_list function.<br/>As another example, suppose you wanted to sort a collection of strings by the number<br/>of distinct letters in each string:<br/></p>
<p>In [177]: strings = ['foo', 'card', 'bar', 'aaaa', 'abab']<br/></p>
<p>Here we could pass a lambda function to the list&#8217;s sort method:<br/>In [178]: strings.sort(key=<b>lambda</b> x: len(set(list(x))))<br/></p>
<p>In [179]: strings<br/>Out[179]: ['aaaa', 'foo', 'abab', 'bar', 'card']<br/></p>
<p>One reason lambda functions are called anonymous functions is<br/>that , unlike functions declared with the def keyword, the function<br/>object itself is never given an explicit __name__ attribute.<br/></p>
<p>Currying: Partial Argument Application<br/><i>Currying</i> is computer science jargon (named after the mathematician Haskell Curry)<br/>that means deriving new functions from existing ones by <i>partial argument applica&#8208;<br/>tion</i>. For example, suppose we had a trivial function that adds two numbers together:<br/></p>
<p><b>def</b> add_numbers(x, y):<br/>    <b>return</b> x + y<br/></p>
<p>Using this function, we could derive a new function of one variable, add_five, that<br/>adds 5 to its argument:<br/></p>
<p>add_five = <b>lambda</b> y: add_numbers(5, y)<br/></p>
<p>The second argument to add_numbers is said to be <i>curried</i>. There&#8217;s nothing very fancy<br/>here, as all we&#8217;ve really done is define a new function that calls an existing function.<br/>The built-in functools module can simplify this process using the partial function:<br/></p>
<p><b>from</b> <b>functools</b> <b>import</b> partial<br/>add_five = partial(add_numbers, 5)<br/></p>
<p>74 | Chapter 3: Built-in Data Structures, Functions, and Files</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Generators<br/>Having a consistent way to iterate over sequences, like objects in a list or lines in a<br/>file, is an important Python feature. This is accomplished by means of the <i>iterator<br/>protocol</i>, a generic way to make objects iterable. For example, iterating over a dict<br/>yields the dict keys:<br/></p>
<p>In [180]: some_dict = {'a': 1, 'b': 2, 'c': 3}<br/></p>
<p>In [181]: <b>for</b> key <b>in</b> some_dict:<br/>   .....:     <b>print</b>(key)<br/>a<br/>b<br/>c<br/></p>
<p>When you write for key in some_dict, the Python interpreter first attempts to cre&#8208;<br/>ate an iterator out of some_dict:<br/></p>
<p>In [182]: dict_iterator = iter(some_dict)<br/></p>
<p>In [183]: dict_iterator<br/>Out[183]: &lt;dict_keyiterator at 0x7fbbd5a9f908&gt;<br/></p>
<p>An iterator is any object that will yield objects to the Python interpreter when used in<br/>a context like a for loop. Most methods expecting a list or list-like object will also<br/>accept any iterable object. This includes built-in methods such as min, max, and sum,<br/>and type constructors like list and tuple:<br/></p>
<p>In [184]: list(dict_iterator)<br/>Out[184]: ['a', 'b', 'c']<br/></p>
<p>A <i>generator</i> is a concise way to construct a new iterable object. Whereas normal func&#8208;<br/>tions execute and return a single result at a time, generators return a sequence of<br/>multiple results lazily, pausing after each one until the next one is requested. To create<br/>a generator, use the yield keyword instead of return in a function:<br/></p>
<p><b>def</b> squares(n=10):<br/>    <b>print</b>('Generating squares from 1 to {0}'.format(n ** 2))<br/>    <b>for</b> i <b>in</b> range(1, n + 1):<br/>        <b>yield</b> i ** 2<br/></p>
<p>When you actually call the generator, no code is immediately executed:<br/>In [186]: gen = squares()<br/></p>
<p>In [187]: gen<br/>Out[187]: &lt;generator object squares at 0x7fbbd5ab4570&gt;<br/></p>
<p>It is not until you request elements from the generator that it begins executing its<br/>code:<br/></p>
<p>3.2 Functions | 75</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In [188]: <b>for</b> x <b>in</b> gen:<br/>   .....:     <b>print</b>(x, end=' ')<br/>Generating squares <b>from</b> 1 to 100<br/>1 4 9 16 25 36 49 64 81 100<br/></p>
<p>Generator expresssions<br/>Another even more concise way to make a generator is by using a <i>generator expres&#8208;<br/>sion</i>. This is a generator analogue to list, dict, and set comprehensions; to create one,<br/>enclose what would otherwise be a list comprehension within parentheses instead of<br/>brackets:<br/></p>
<p>In [189]: gen = (x ** 2 <b>for</b> x <b>in</b> range(100))<br/></p>
<p>In [190]: gen<br/>Out[190]: &lt;generator object &lt;genexpr&gt; at 0x7fbbd5ab29e8&gt;<br/></p>
<p>This is completely equivalent to the following more verbose generator:<br/><b>def</b> _make_gen():<br/>    <b>for</b> x <b>in</b> range(100):<br/>        <b>yield</b> x ** 2<br/>gen = _make_gen()<br/></p>
<p>Generator expressions can be used instead of list comprehensions as function argu&#8208;<br/>ments in many cases:<br/></p>
<p>In [191]: sum(x ** 2 <b>for</b> x <b>in</b> range(100))<br/>Out[191]: 328350<br/></p>
<p>In [192]: dict((i, i **2) <b>for</b> i <b>in</b> range(5))<br/>Out[192]: {0: 0, 1: 1, 2: 4, 3: 9, 4: 16}<br/></p>
<p>itertools module<br/>The standard library itertools module has a collection of generators for many com&#8208;<br/>mon data algorithms. For example, groupby takes any sequence and a function,<br/>grouping consecutive elements in the sequence by return value of the function. Here&#8217;s<br/>an example:<br/></p>
<p>In [193]: <b>import</b> <b>itertools<br/></b></p>
<p>In [194]: first_letter = <b>lambda</b> x: x[0]<br/></p>
<p>In [195]: names = ['Alan', 'Adam', 'Wes', 'Will', 'Albert', 'Steven']<br/></p>
<p>In [196]: <b>for</b> letter, names <b>in</b> itertools.groupby(names, first_letter):<br/>   .....:     <b>print</b>(letter, list(names)) <i># names is a generator<br/></i>A ['Alan', 'Adam']<br/>W ['Wes', 'Will']<br/>A ['Albert']<br/>S ['Steven']<br/></p>
<p>76 | Chapter 3: Built-in Data Structures, Functions, and Files</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>See Table 3-2 for a list of a few other itertools functions I&#8217;ve frequently found help&#8208;<br/>ful. You may like to check out the official Python documentation for more on this<br/>useful built-in utility module.<br/><i>Table 3-2. Some useful itertools functions<br/></i></p>
<p>Function Description<br/>combinations(iterable, k) Generates a sequence of all possible k-tuples of elements in the iterable,<br/></p>
<p>ignoring order and without replacement (see also the companion function<br/>combinations_with_replacement)<br/></p>
<p>permutations(iterable, k) Generates a sequence of all possible k-tuples of elements in the iterable,<br/>respecting order<br/></p>
<p>groupby(iterable[, keyfunc]) Generates (key, sub-iterator) for each unique key<br/>product(*iterables, repeat=1) Generates the Cartesian product of the input iterables as tuples, similar to a <br/></p>
<p>nested for loop<br/></p>
<p>Errors and Exception Handling<br/>Handling Python errors or <i>exceptions</i> gracefully is an important part of building<br/>robust programs. In data analysis applications, many functions only work on certain<br/>kinds of input. As an example, Python&#8217;s float function is capable of casting a string<br/>to a floating-point number, but fails with ValueError on improper inputs:<br/></p>
<p>In [197]: float('1.2345')<br/>Out[197]: 1.2345<br/></p>
<p>In [198]: float('something')<br/>---------------------------------------------------------------------------<br/><b>ValueError</b>                                Traceback (most recent call last)<br/>&lt;ipython-input-198-439904410854&gt; <b>in</b> &lt;module&gt;()<br/>----&gt; 1 float('something')<br/><b>ValueError</b>: could <b>not</b> convert string to float: 'something'<br/></p>
<p>Suppose we wanted a version of float that fails gracefully, returning the input argu&#8208;<br/>ment. We can do this by writing a function that encloses the call to float in a try/<br/>except block:<br/></p>
<p><b>def</b> attempt_float(x):<br/>    <b>try</b>:<br/>        <b>return</b> float(x)<br/>    <b>except</b>:<br/>        <b>return</b> x<br/></p>
<p>The code in the except part of the block will only be executed if float(x) raises an<br/>exception:<br/></p>
<p>In [200]: attempt_float('1.2345')<br/>Out[200]: 1.2345<br/></p>
<p>3.2 Functions | 77</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In [201]: attempt_float('something')<br/>Out[201]: 'something'<br/></p>
<p>You might notice that float can raise exceptions other than ValueError:<br/>In [202]: float((1, 2))<br/>---------------------------------------------------------------------------<br/><b>TypeError</b>                                 Traceback (most recent call last)<br/>&lt;ipython-input-202-842079ebb635&gt; <b>in</b> &lt;module&gt;()<br/>----&gt; 1 float((1, 2))<br/><b>TypeError</b>: float() argument must be a string <b>or</b> a number, <b>not</b> 'tuple'<br/></p>
<p>You might want to only suppress ValueError, since a TypeError (the input was not a<br/>string or numeric value) might indicate a legitimate bug in your program. To do that,<br/>write the exception type after except:<br/></p>
<p><b>def</b> attempt_float(x):<br/>    <b>try</b>:<br/>        <b>return</b> float(x)<br/>    <b>except</b> <b>ValueError</b>:<br/>        <b>return</b> x<br/></p>
<p>We have then:<br/>In [204]: attempt_float((1, 2))<br/>---------------------------------------------------------------------------<br/><b>TypeError</b>                                 Traceback (most recent call last)<br/>&lt;ipython-input-204-9bdfd730cead&gt; <b>in</b> &lt;module&gt;()<br/>----&gt; 1 attempt_float((1, 2))<br/>&lt;ipython-input-203-3e06b8379b6b&gt; <b>in</b> attempt_float(x)<br/>      1 <b>def</b> attempt_float(x):<br/>      2     <b>try</b>:<br/>----&gt; 3         <b>return</b> float(x)<br/>      4     <b>except</b> <b>ValueError</b>:<br/>      5         <b>return</b> x<br/><b>TypeError</b>: float() argument must be a string <b>or</b> a number, <b>not</b> 'tuple'<br/></p>
<p>You can catch multiple exception types by writing a tuple of exception types instead<br/>(the parentheses are required):<br/></p>
<p><b>def</b> attempt_float(x):<br/>    <b>try</b>:<br/>        <b>return</b> float(x)<br/>    <b>except</b> (<b>TypeError</b>, <b>ValueError</b>):<br/>        <b>return</b> x<br/></p>
<p>In some cases, you may not want to suppress an exception, but you want some code<br/>to be executed regardless of whether the code in the try block succeeds or not. To do<br/>this, use finally:<br/></p>
<p>f = open(path, 'w')<br/></p>
<p><b>try</b>:<br/>    write_to_file(f)<br/></p>
<p>78 | Chapter 3: Built-in Data Structures, Functions, and Files</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><b>finally</b>:<br/>    f.close()<br/></p>
<p>Here, the file handle f will <i>always</i> get closed. Similarly, you can have code that exe&#8208;<br/>cutes only if the try: block succeeds using else:<br/></p>
<p>f = open(path, 'w')<br/></p>
<p><b>try</b>:<br/>    write_to_file(f)<br/><b>except</b>:<br/>    <b>print</b>('Failed')<br/><b>else</b>:<br/>    <b>print</b>('Succeeded')<br/><b>finally</b>:<br/>    f.close()<br/></p>
<p>Exceptions in IPython<br/>If an exception is raised while you are %run-ing a script or executing any statement,<br/>IPython will by default print a full call stack trace (traceback) with a few lines of con&#8208;<br/>text around the position at each point in the stack:<br/></p>
<p>In [10]: %run examples/ipython_bug.py<br/>---------------------------------------------------------------------------<br/><b>AssertionError</b>                            Traceback (most recent call last)<br/>/home/wesm/code/pydata-book/examples/ipython_bug.py <b>in</b> &lt;module&gt;()<br/>     13     throws_an_exception()<br/>     14<br/>---&gt; 15 calling_things()<br/></p>
<p>/home/wesm/code/pydata-book/examples/ipython_bug.py <b>in</b> calling_things()<br/>     11 <b>def</b> calling_things():<br/>     12     works_fine()<br/>---&gt; 13     throws_an_exception()<br/>     14<br/>     15 calling_things()<br/></p>
<p>/home/wesm/code/pydata-book/examples/ipython_bug.py <b>in</b> throws_an_exception()<br/>      7     a = 5<br/>      8     b = 6<br/>----&gt; 9     <b>assert</b>(a + b == 10)<br/>     10<br/>     11 <b>def</b> calling_things():<br/></p>
<p><b>AssertionError</b>:<br/></p>
<p>Having additional context by itself is a big advantage over the standard Python inter&#8208;<br/>preter (which does not provide any additional context). You can control the amount<br/>of context shown using the %xmode magic command, from Plain (same as the stan&#8208;<br/>dard Python interpreter) to Verbose (which inlines function argument values and<br/></p>
<p>3.2 Functions | 79</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>more). As you will see later in the chapter, you can step <i>into the stack</i> (using the<br/>%debug or %pdb magics) after an error has occurred for interactive post-mortem<br/>debugging.<br/></p>
<p>3.3 Files and the Operating System<br/>Most of this book uses high-level tools like pandas.read_csv to read data files from<br/>disk into Python data structures. However, it&#8217;s important to understand the basics of<br/>how to work with files in Python. Fortunately, it&#8217;s very simple, which is one reason<br/>why Python is so popular for text and file munging.<br/>To open a file for reading or writing, use the built-in open function with either a rela&#8208;<br/>tive or absolute file path:<br/></p>
<p>In [207]: path = 'examples/segismundo.txt'<br/></p>
<p>In [208]: f = open(path)<br/></p>
<p>By default, the file is opened in read-only mode 'r'. We can then treat the file handle<br/>f like a list and iterate over the lines like so:<br/></p>
<p><b>for</b> line <b>in</b> f:<br/>    <b>pass<br/></b></p>
<p>The lines come out of the file with the end-of-line (EOL) markers intact, so you&#8217;ll<br/>often see code to get an EOL-free list of lines in a file like:<br/></p>
<p>In [209]: lines = [x.rstrip() <b>for</b> x <b>in</b> open(path)]<br/></p>
<p>In [210]: lines<br/>Out[210]: <br/>['Sue&#241;a el rico en su riqueza,',<br/> 'que m&#225;s cuidados le ofrece;',<br/> '',<br/> 'sue&#241;a el pobre que padece',<br/> 'su miseria y su pobreza;',<br/> '',<br/> 'sue&#241;a el que a medrar empieza,',<br/> 'sue&#241;a el que afana y pretende,',<br/> 'sue&#241;a el que agravia y ofende,',<br/> '',<br/> 'y en el mundo, en conclusi&#243;n,',<br/> 'todos sue&#241;an lo que son,',<br/> 'aunque ninguno lo entiende.',<br/> '']<br/></p>
<p>When you use open to create file objects, it is important to explicitly close the file<br/>when you are finished with it. Closing the file releases its resources back to the oper&#8208;<br/>ating system:<br/></p>
<p>In [211]: f.close()<br/></p>
<p>80 | Chapter 3: Built-in Data Structures, Functions, and Files</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>One of the ways to make it easier to clean up open files is to use the with statement:<br/>In [212]: <b>with</b> open(path) <b>as</b> f:<br/>   .....:     lines = [x.rstrip() <b>for</b> x <b>in</b> f]<br/></p>
<p>This will automatically close the file f when exiting the with block.<br/>If we had typed f = open(path, 'w'), a new file at examples/segismundo.txt would<br/>have been created (be careful!), overwriting any one in its place. There is also the 'x'<br/>file mode, which creates a writable file but fails if the file path already exists. See<br/>Table 3-3 for a list of all valid file read/write modes.<br/>For readable files, some of the most commonly used methods are read, seek, and<br/>tell. read returns a certain number of characters from the file. What constitutes a<br/>&#8220;character&#8221; is determined by the file&#8217;s encoding (e.g., UTF-8) or simply raw bytes if<br/>the file is opened in binary mode:<br/></p>
<p>In [213]: f = open(path)<br/></p>
<p>In [214]: f.read(10)<br/>Out[214]: 'Sue&#241;a el r'<br/></p>
<p>In [215]: f2 = open(path, 'rb')  <i># Binary mode<br/></i></p>
<p>In [216]: f2.read(10)<br/>Out[216]: b'Sue<b>\xc3\xb1</b>a el '<br/></p>
<p>The read method advances the file handle&#8217;s position by the number of bytes read.<br/>tell gives you the current position:<br/></p>
<p>In [217]: f.tell()<br/>Out[217]: 11<br/></p>
<p>In [218]: f2.tell()<br/>Out[218]: 10<br/></p>
<p>Even though we read 10 characters from the file, the position is 11 because it took<br/>that many bytes to decode 10 characters using the default encoding. You can check<br/>the default encoding in the sys module:<br/></p>
<p>In [219]: <b>import</b> <b>sys<br/></b></p>
<p>In [220]: sys.getdefaultencoding()<br/>Out[220]: 'utf-8'<br/></p>
<p>seek changes the file position to the indicated byte in the file:<br/>In [221]: f.seek(3)<br/>Out[221]: 3<br/></p>
<p>In [222]: f.read(1)<br/>Out[222]: '&#241;'<br/></p>
<p>3.3 Files and the Operating System | 81</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Lastly, we remember to close the files:<br/>In [223]: f.close()<br/></p>
<p>In [224]: f2.close()<br/></p>
<p><i>Table 3-3. Python file modes<br/></i>Mode Description<br/>r Read-only mode<br/>w Write-only mode; creates a new file (erasing the data for any file with the same name)<br/>x Write-only mode; creates a new file, but fails if the file path already exists<br/>a Append to existing file (create the file if it does not already exist)<br/>r+ Read and write<br/>b Add to mode for binary files (i.e., 'rb' or 'wb')<br/>t Text mode for files (automatically decoding bytes to Unicode). This is the default if not specified. Add t to other<br/></p>
<p>modes to use this (i.e., 'rt' or 'xt')<br/></p>
<p>To write text to a file, you can use the file&#8217;s write or writelines methods. For exam&#8208;<br/>ple, we could create a version of <i>prof_mod.py</i> with no blank lines like so:<br/></p>
<p>In [225]: <b>with</b> open('tmp.txt', 'w') <b>as</b> handle:<br/>   .....:     handle.writelines(x <b>for</b> x <b>in</b> open(path) <b>if</b> len(x) &gt; 1)<br/></p>
<p>In [226]: <b>with</b> open('tmp.txt') <b>as</b> f:<br/>   .....:     lines = f.readlines()<br/></p>
<p>In [227]: lines<br/>Out[227]: <br/>['Sue&#241;a el rico en su riqueza,<b>\n</b>',<br/> 'que m&#225;s cuidados le ofrece;<b>\n</b>',<br/> 'sue&#241;a el pobre que padece<b>\n</b>',<br/> 'su miseria y su pobreza;<b>\n</b>',<br/> 'sue&#241;a el que a medrar empieza,<b>\n</b>',<br/> 'sue&#241;a el que afana y pretende,<b>\n</b>',<br/> 'sue&#241;a el que agravia y ofende,<b>\n</b>',<br/> 'y en el mundo, en conclusi&#243;n,<b>\n</b>',<br/> 'todos sue&#241;an lo que son,<b>\n</b>',<br/> 'aunque ninguno lo entiende.<b>\n</b>']<br/></p>
<p>See Table 3-4 for many of the most commonly used file methods.<br/><i>Table 3-4. Important Python file methods or attributes<br/></i></p>
<p>Method Description<br/>read([size]) Return data from file as a string, with optional size argument indicating the number of<br/></p>
<p>bytes to read<br/>readlines([size]) Return list of lines in the file, with optional size argument<br/>write(str) Write passed string to file<br/></p>
<p>82 | Chapter 3: Built-in Data Structures, Functions, and Files</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Method Description<br/>writelines(strings) Write passed sequence of strings to the file<br/>close() Close the handle<br/>flush() Flush the internal I/O buffer to disk<br/>seek(pos) Move to indicated file position (integer)<br/>tell() Return current file position as integer<br/>closed True if the file is closed<br/></p>
<p>Bytes and Unicode with Files<br/>The default behavior for Python files (whether readable or writable) is <i>text mode</i>,<br/>which means that you intend to work with Python strings (i.e., Unicode). This con&#8208;<br/>trasts with <i>binary mode</i>, which you can obtain by appending b onto the file mode.<br/>Let&#8217;s look at the file (which contains non-ASCII characters with UTF-8 encoding)<br/>from the previous section:<br/></p>
<p>In [230]: <b>with</b> open(path) <b>as</b> f:<br/>   .....:     chars = f.read(10)<br/></p>
<p>In [231]: chars<br/>Out[231]: 'Sue&#241;a el r'<br/></p>
<p>UTF-8 is a variable-length Unicode encoding, so when I requested some number of<br/>characters from the file, Python reads enough bytes (which could be as few as 10 or as<br/>many as 40 bytes) from the file to decode that many characters. If I open the file in<br/>'rb' mode instead, read requests exact numbers of bytes:<br/></p>
<p>In [232]: <b>with</b> open(path, 'rb') <b>as</b> f:<br/>   .....:     data = f.read(10)<br/></p>
<p>In [233]: data<br/>Out[233]: b'Sue<b>\xc3\xb1</b>a el '<br/></p>
<p>Depending on the text encoding, you may be able to decode the bytes to a str object<br/>yourself, but only if each of the encoded Unicode characters is fully formed:<br/></p>
<p>In [234]: data.decode('utf8')<br/>Out[234]: 'Sue&#241;a el '<br/></p>
<p>In [235]: data[:4].decode('utf8')<br/>---------------------------------------------------------------------------<br/><b>UnicodeDecodeError</b>                        Traceback (most recent call last)<br/>&lt;ipython-input-235-300e0af10bb7&gt; <b>in</b> &lt;module&gt;()<br/>----&gt; 1 data[:4].decode('utf8')<br/><b>UnicodeDecodeError</b>: 'utf-8' codec can't decode byte 0xc3 in position 3: unexpecte<br/>d end of data<br/></p>
<p>Text mode, combined with the encoding option of open, provides a convenient way<br/>to convert from one Unicode encoding to another:<br/></p>
<p>3.3 Files and the Operating System | 83</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In [236]: sink_path = 'sink.txt'<br/></p>
<p>In [237]: <b>with</b> open(path) <b>as</b> source:<br/>   .....:     <b>with</b> open(sink_path, 'xt', encoding='iso-8859-1') <b>as</b> sink:<br/>   .....:         sink.write(source.read())<br/></p>
<p>In [238]: <b>with</b> open(sink_path, encoding='iso-8859-1') <b>as</b> f:<br/>   .....:     <b>print</b>(f.read(10))<br/>Sue&#241;a el r<br/></p>
<p>Beware using seek when opening files in any mode other than binary. If the file posi&#8208;<br/>tion falls in the middle of the bytes defining a Unicode character, then subsequent<br/>reads will result in an error:<br/></p>
<p>In [240]: f = open(path)<br/></p>
<p>In [241]: f.read(5)<br/>Out[241]: 'Sue&#241;a'<br/></p>
<p>In [242]: f.seek(4)<br/>Out[242]: 4<br/></p>
<p>In [243]: f.read(1)<br/>---------------------------------------------------------------------------<br/><b>UnicodeDecodeError</b>                        Traceback (most recent call last)<br/>&lt;ipython-input-243-7841103e33f5&gt; <b>in</b> &lt;module&gt;()<br/>----&gt; 1 f.read(1)<br/>/miniconda/envs/book-env/lib/python3.6/codecs.py <b>in</b> decode(self, input, final)<br/>    319         <i># decode input (taking the buffer into account)<br/></i>    320         data = self.buffer + input<br/>--&gt; 321         (result, consumed) = self._buffer_decode(data, self.errors, final<br/>)<br/>    322         <i># keep undecoded input until the next call<br/></i>    323         self.buffer = data[consumed:]<br/><b>UnicodeDecodeError</b>: 'utf-8' codec can't decode byte 0xb1 in position 0: invalid s<br/>tart byte<br/></p>
<p>In [244]: f.close()<br/></p>
<p>If you find yourself regularly doing data analysis on non-ASCII text data, mastering<br/>Python&#8217;s Unicode functionality will prove valuable. See Python&#8217;s online documenta&#8208;<br/>tion for much more.<br/></p>
<p>3.4 Conclusion<br/>With some of the basics and the Python environment and language now under our<br/>belt, it&#8217;s time to move on and learn about NumPy and array-oriented computing in<br/>Python.<br/></p>
<p>84 | Chapter 3: Built-in Data Structures, Functions, and Files</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>CHAPTER 4<br/>NumPy Basics: Arrays and<br/></p>
<p>Vectorized Computation<br/></p>
<p>NumPy, short for Numerical Python, is one of the most important foundational pack&#8208;<br/>ages for numerical computing in Python. Most computational packages providing<br/>scientific functionality use NumPy&#8217;s array objects as the <i>lingua franca</i> for data<br/>exchange.<br/>Here are some of the things you&#8217;ll find in NumPy:<br/></p>
<p>&#8226; ndarray, an efficient multidimensional array providing fast array-oriented arith&#8208;<br/>metic operations and flexible <i>broadcasting</i> capabilities.<br/></p>
<p>&#8226; Mathematical functions for fast operations on entire arrays of data without hav&#8208;<br/>ing to write loops.<br/></p>
<p>&#8226; Tools for reading/writing array data to disk and working with memory-mapped<br/>files.<br/></p>
<p>&#8226; Linear algebra, random number generation, and Fourier transform capabilities.<br/>&#8226; A C API for connecting NumPy with libraries written in C, C++, or FORTRAN.<br/></p>
<p>Because NumPy provides an easy-to-use C API, it is straightforward to pass data to<br/>external libraries written in a low-level language and also for external libraries to<br/>return data to Python as NumPy arrays. This feature has made Python a language of<br/>choice for wrapping legacy C/C++/Fortran codebases and giving them a dynamic and<br/>easy-to-use interface.<br/>While NumPy by itself does not provide modeling or scientific functionality, having<br/>an understanding of NumPy arrays and array-oriented computing will help you use<br/>tools with array-oriented semantics, like pandas, much more effectively. Since<br/></p>
<p>85</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>NumPy is a large topic, I will cover many advanced NumPy features like broadcasting<br/>in more depth later (see Appendix A).<br/>For most data analysis applications, the main areas of functionality I&#8217;ll focus on are:<br/></p>
<p>&#8226; Fast vectorized array operations for data munging and cleaning, subsetting and<br/>filtering, transformation, and any other kinds of computations<br/></p>
<p>&#8226; Common array algorithms like sorting, unique, and set operations<br/>&#8226; Efficient descriptive statistics and aggregating/summarizing data<br/>&#8226; Data alignment and relational data manipulations for merging and joining<br/></p>
<p>together heterogeneous datasets<br/>&#8226; Expressing conditional logic as array expressions instead of loops with if-elif-<br/>else branches<br/></p>
<p>&#8226; Group-wise data manipulations (aggregation, transformation, function applica&#8208;<br/>tion)<br/></p>
<p>While NumPy provides a computational foundation for general numerical data pro&#8208;<br/>cessing, many readers will want to use pandas as the basis for most kinds of statistics<br/>or analytics, especially on tabular data. pandas also provides some more domain-<br/>specific functionality like time series manipulation, which is not present in NumPy.<br/></p>
<p>Array-oriented computing in Python traces its roots back to 1995,<br/>when Jim Hugunin created the Numeric library. Over the next 10<br/>years, many scientific programming communities began doing<br/>array programming in Python, but the library ecosystem had<br/>become fragmented in the early 2000s. In 2005, Travis Oliphant<br/>was able to forge the NumPy project from the then Numeric and<br/>Numarray projects to bring the community together around a sin&#8208;<br/>gle array computing framework.<br/></p>
<p>One of the reasons NumPy is so important for numerical computations in Python is<br/>because it is designed for efficiency on large arrays of data. There are a number of<br/>reasons for this:<br/></p>
<p>&#8226; NumPy internally stores data in a contiguous block of memory, independent of<br/>other built-in Python objects. NumPy&#8217;s library of algorithms written in the C lan&#8208;<br/>guage can operate on this memory without any type checking or other overhead.<br/>NumPy arrays also use much less memory than built-in Python sequences.<br/></p>
<p>&#8226; NumPy operations perform complex computations on entire arrays without the<br/>need for Python for loops.<br/></p>
<p>86 | Chapter 4: NumPy Basics: Arrays and Vectorized Computation</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>To give you an idea of the performance difference, consider a NumPy array of one<br/>million integers, and the equivalent Python list:<br/></p>
<p>In [7]: <b>import</b> <b>numpy</b> <b>as</b> <b>np<br/></b></p>
<p>In [8]: my_arr = np.arange(1000000)<br/></p>
<p>In [9]: my_list = list(range(1000000))<br/></p>
<p>Now let&#8217;s multiply each sequence by 2:<br/>In [10]: %time <b>for</b> _ <b>in</b> range(10): my_arr2 = my_arr * 2<br/>CPU times: user 20 ms, sys: 50 ms, total: 70 ms<br/>Wall time: 72.4 ms<br/></p>
<p>In [11]: %time <b>for</b> _ <b>in</b> range(10): my_list2 = [x * 2 <b>for</b> x <b>in</b> my_list]<br/>CPU times: user 760 ms, sys: 290 ms, total: 1.05 s<br/>Wall time: 1.05 s<br/></p>
<p>NumPy-based algorithms are generally 10 to 100 times faster (or more) than their<br/>pure Python counterparts and use significantly less memory.<br/></p>
<p>4.1 The NumPy ndarray: A Multidimensional Array Object<br/>One of the key features of NumPy is its N-dimensional array object, or ndarray,<br/>which is a fast, flexible container for large datasets in Python. Arrays enable you to<br/>perform mathematical operations on whole blocks of data using similar syntax to the<br/>equivalent operations between scalar elements.<br/>To give you a flavor of how NumPy enables batch computations with similar syntax<br/>to scalar values on built-in Python objects, I first import NumPy and generate a small<br/>array of random data:<br/></p>
<p>In [12]: <b>import</b> <b>numpy</b> <b>as</b> <b>np<br/></b></p>
<p><i># Generate some random data<br/></i>In [13]: data = np.random.randn(2, 3)<br/></p>
<p>In [14]: data<br/>Out[14]: <br/>array([[-0.2047,  0.4789, -0.5194],<br/>       [-0.5557,  1.9658,  1.3934]])<br/></p>
<p>I then write mathematical operations with data:<br/>In [15]: data * 10<br/>Out[15]: <br/>array([[ -2.0471,   4.7894,  -5.1944],<br/>       [ -5.5573,  19.6578,  13.9341]])<br/></p>
<p>In [16]: data + data<br/>Out[16]: <br/></p>
<p>4.1 The NumPy ndarray: A Multidimensional Array Object | 87</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>array([[-0.4094,  0.9579, -1.0389],<br/>       [-1.1115,  3.9316,  2.7868]])<br/></p>
<p>In the first example, all of the elements have been multiplied by 10. In the second, the<br/>corresponding values in each &#8220;cell&#8221; in the array have been added to each other.<br/></p>
<p>In this chapter and throughout the book, I use the standard<br/>NumPy convention of always using import numpy as np. You are,<br/>of course, welcome to put from numpy import * in your code to<br/>avoid having to write np., but I advise against making a habit of<br/>this. The numpy namespace is large and contains a number of func&#8208;<br/>tions whose names conflict with built-in Python functions (like min<br/>and max).<br/></p>
<p>An ndarray is a generic multidimensional container for homogeneous data; that is, all<br/>of the elements must be the same type. Every array has a shape, a tuple indicating the<br/>size of each dimension, and a dtype, an object describing the <i>data type</i> of the array:<br/></p>
<p>In [17]: data.shape<br/>Out[17]: (2, 3)<br/></p>
<p>In [18]: data.dtype<br/>Out[18]: dtype('float64')<br/></p>
<p>This chapter will introduce you to the basics of using NumPy arrays, and should be<br/>sufficient for following along with the rest of the book. While it&#8217;s not necessary to<br/>have a deep understanding of NumPy for many data analytical applications, becom&#8208;<br/>ing proficient in array-oriented programming and thinking is a key step along the<br/>way to becoming a scientific Python guru.<br/></p>
<p>Whenever you see &#8220;array,&#8221; &#8220;NumPy array,&#8221; or &#8220;ndarray&#8221; in the text,<br/>with few exceptions they all refer to the same thing: the ndarray<br/>object.<br/></p>
<p>Creating ndarrays<br/>The easiest way to create an array is to use the array function. This accepts any<br/>sequence-like object (including other arrays) and produces a new NumPy array con&#8208;<br/>taining the passed data. For example, a list is a good candidate for conversion:<br/></p>
<p>In [19]: data1 = [6, 7.5, 8, 0, 1]<br/></p>
<p>In [20]: arr1 = np.array(data1)<br/></p>
<p>In [21]: arr1<br/>Out[21]: array([ 6. ,  7.5,  8. ,  0. ,  1. ])<br/></p>
<p>88 | Chapter 4: NumPy Basics: Arrays and Vectorized Computation</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Nested sequences, like a list of equal-length lists, will be converted into a multidimen&#8208;<br/>sional array:<br/></p>
<p>In [22]: data2 = [[1, 2, 3, 4], [5, 6, 7, 8]]<br/></p>
<p>In [23]: arr2 = np.array(data2)<br/></p>
<p>In [24]: arr2<br/>Out[24]: <br/>array([[1, 2, 3, 4],<br/>       [5, 6, 7, 8]])<br/></p>
<p>Since data2 was a list of lists, the NumPy array arr2 has two dimensions with shape<br/>inferred from the data. We can confirm this by inspecting the ndim and shape<br/>attributes:<br/></p>
<p>In [25]: arr2.ndim<br/>Out[25]: 2<br/></p>
<p>In [26]: arr2.shape<br/>Out[26]: (2, 4)<br/></p>
<p>Unless explicitly specified (more on this later), np.array tries to infer a good data<br/>type for the array that it creates. The data type is stored in a special dtype metadata<br/>object; for example, in the previous two examples we have:<br/></p>
<p>In [27]: arr1.dtype<br/>Out[27]: dtype('float64')<br/></p>
<p>In [28]: arr2.dtype<br/>Out[28]: dtype('int64')<br/></p>
<p>In addition to np.array, there are a number of other functions for creating new<br/>arrays. As examples, zeros and ones create arrays of 0s or 1s, respectively, with a<br/>given length or shape. empty creates an array without initializing its values to any par&#8208;<br/>ticular value. To create a higher dimensional array with these methods, pass a tuple<br/>for the shape:<br/></p>
<p>In [29]: np.zeros(10)<br/>Out[29]: array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])<br/></p>
<p>In [30]: np.zeros((3, 6))<br/>Out[30]: <br/>array([[ 0.,  0.,  0.,  0.,  0.,  0.],<br/>       [ 0.,  0.,  0.,  0.,  0.,  0.],<br/>       [ 0.,  0.,  0.,  0.,  0.,  0.]])<br/></p>
<p>In [31]: np.empty((2, 3, 2))<br/>Out[31]: <br/>array([[[ 0.,  0.],<br/>        [ 0.,  0.],<br/>        [ 0.,  0.]],<br/></p>
<p>4.1 The NumPy ndarray: A Multidimensional Array Object | 89</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>       [[ 0.,  0.],<br/>        [ 0.,  0.],<br/>        [ 0.,  0.]]])<br/></p>
<p>It&#8217;s not safe to assume that np.empty will return an array of all<br/>zeros. In some cases, it may return uninitialized &#8220;garbage&#8221; values.<br/></p>
<p>arange is an array-valued version of the built-in Python range function:<br/>In [32]: np.arange(15)<br/>Out[32]: array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14])<br/></p>
<p>See Table 4-1 for a short list of standard array creation functions. Since NumPy is<br/>focused on numerical computing, the data type, if not specified, will in many cases be<br/>float64 (floating point).<br/><i>Table 4-1. Array creation functions<br/></i></p>
<p>Function Description<br/>array Convert input data (list, tuple, array, or other sequence type) to an ndarray either by inferring a dtype<br/></p>
<p>or explicitly specifying a dtype; copies the input data by default<br/>asarray Convert input to ndarray, but do not copy if the input is already an ndarray<br/>arange Like the built-in range but returns an ndarray instead of a list<br/>ones, <br/>ones_like<br/></p>
<p>Produce an array of all 1s with the given shape and dtype; ones_like takes another array and<br/>produces a ones array of the same shape and dtype<br/></p>
<p>zeros, <br/>zeros_like<br/></p>
<p>Like ones and ones_like but producing arrays of 0s instead<br/></p>
<p>empty, <br/>empty_like<br/></p>
<p>Create new arrays by allocating new memory, but do not populate with any values like ones and<br/>zeros<br/></p>
<p>full, <br/>full_like<br/></p>
<p>Produce an array of the given shape and dtype with all values set to the indicated &#8220;fill value&#8221;<br/>full_like takes another array and produces a filled array of the same shape and dtype<br/></p>
<p>eye, identity Create a square N &#215; N identity matrix (1s on the diagonal and 0s elsewhere)<br/></p>
<p>Data Types for ndarrays<br/>The <i>data type</i> or dtype is a special object containing the information (or <i>metadata</i>,<br/>data about data) the ndarray needs to interpret a chunk of memory as a particular<br/>type of data:<br/></p>
<p>In [33]: arr1 = np.array([1, 2, 3], dtype=np.float64)<br/></p>
<p>In [34]: arr2 = np.array([1, 2, 3], dtype=np.int32)<br/></p>
<p>In [35]: arr1.dtype<br/>Out[35]: dtype('float64')<br/></p>
<p>90 | Chapter 4: NumPy Basics: Arrays and Vectorized Computation</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In [36]: arr2.dtype<br/>Out[36]: dtype('int32')<br/></p>
<p>dtypes are a source of NumPy&#8217;s flexibility for interacting with data coming from other<br/>systems. In most cases they provide a mapping directly onto an underlying disk or<br/>memory representation, which makes it easy to read and write binary streams of data<br/>to disk and also to connect to code written in a low-level language like C or Fortran.<br/>The numerical dtypes are named the same way: a type name, like float or int, fol&#8208;<br/>lowed by a number indicating the number of bits per element. A standard double-<br/>precision floating-point value (what&#8217;s used under the hood in Python&#8217;s float object)<br/>takes up 8 bytes or 64 bits. Thus, this type is known in NumPy as float64. See<br/>Table 4-2 for a full listing of NumPy&#8217;s supported data types.<br/></p>
<p>Don&#8217;t worry about memorizing the NumPy dtypes, especially if<br/>you&#8217;re a new user. It&#8217;s often only necessary to care about the general<br/><i>kind</i> of data you&#8217;re dealing with, whether floating point, complex,<br/>integer, boolean, string, or general Python object. When you need<br/>more control over how data are stored in memory and on disk,<br/>especially large datasets, it is good to know that you have control<br/>over the storage type.<br/></p>
<p><i>Table 4-2. NumPy data types<br/></i>Type Type code Description<br/>int8, uint8 i1, u1 Signed and unsigned 8-bit (1 byte) integer types<br/>int16, uint16 i2, u2 Signed and unsigned 16-bit integer types<br/>int32, uint32 i4, u4 Signed and unsigned 32-bit integer types<br/>int64, uint64 i8, u8 Signed and unsigned 64-bit integer types<br/>float16 f2 Half-precision floating point<br/>float32 f4 or f Standard single-precision floating point; compatible with C float<br/>float64 f8 or d Standard double-precision floating point; compatible with C double and<br/></p>
<p>Python float object<br/>float128 f16 or g Extended-precision floating point<br/>complex64,<br/>complex128,<br/>complex256<br/></p>
<p>c8, c16, <br/>c32<br/></p>
<p>Complex numbers represented by two 32, 64, or 128 floats, respectively<br/></p>
<p>bool ? Boolean type storing True and False values<br/>object O Python object type; a value can be any Python object<br/>string_ S Fixed-length ASCII string type (1 byte per character); for example, to create a<br/></p>
<p>string dtype with length 10, use 'S10'<br/>unicode_ U Fixed-length Unicode type (number of bytes platform specific); same<br/></p>
<p>specification semantics as string_ (e.g., 'U10')<br/></p>
<p>4.1 The NumPy ndarray: A Multidimensional Array Object | 91</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>You can explicitly convert or <i>cast</i> an array from one dtype to another using ndarray&#8217;s<br/>astype method:<br/></p>
<p>In [37]: arr = np.array([1, 2, 3, 4, 5])<br/></p>
<p>In [38]: arr.dtype<br/>Out[38]: dtype('int64')<br/></p>
<p>In [39]: float_arr = arr.astype(np.float64)<br/></p>
<p>In [40]: float_arr.dtype<br/>Out[40]: dtype('float64')<br/></p>
<p>In this example, integers were cast to floating point. If I cast some floating-point<br/>numbers to be of integer dtype, the decimal part will be truncated:<br/></p>
<p>In [41]: arr = np.array([3.7, -1.2, -2.6, 0.5, 12.9, 10.1])<br/></p>
<p>In [42]: arr<br/>Out[42]: array([  3.7,  -1.2,  -2.6,   0.5,  12.9,  10.1])<br/></p>
<p>In [43]: arr.astype(np.int32)<br/>Out[43]: array([ 3, -1, -2,  0, 12, 10], dtype=int32)<br/></p>
<p>If you have an array of strings representing numbers, you can use astype to convert<br/>them to numeric form:<br/></p>
<p>In [44]: numeric_strings = np.array(['1.25', '-9.6', '42'], dtype=np.string_)<br/></p>
<p>In [45]: numeric_strings.astype(float)<br/>Out[45]: array([  1.25,  -9.6 ,  42.  ])<br/></p>
<p>It&#8217;s important to be cautious when using the numpy.string_ type,<br/>as string data in NumPy is fixed size and may truncate input<br/>without warning. pandas has more intuitive out-of-the-box behav&#8208;<br/>ior on non-numeric data.<br/></p>
<p>If casting were to fail for some reason (like a string that cannot be converted to<br/>float64), a ValueError will be raised. Here I was a bit lazy and wrote float instead<br/>of np.float64; NumPy aliases the Python types to its own equivalent data dtypes.<br/>You can also use another array&#8217;s dtype attribute:<br/></p>
<p>In [46]: int_array = np.arange(10)<br/></p>
<p>In [47]: calibers = np.array([.22, .270, .357, .380, .44, .50], dtype=np.float64)<br/></p>
<p>In [48]: int_array.astype(calibers.dtype)<br/>Out[48]: array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9.])<br/></p>
<p>92 | Chapter 4: NumPy Basics: Arrays and Vectorized Computation</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>There are shorthand type code strings you can also use to refer to a dtype:<br/>In [49]: empty_uint32 = np.empty(8, dtype='u4')<br/></p>
<p>In [50]: empty_uint32<br/>Out[50]: <br/>array([         0, 1075314688,          0, 1075707904,          0,<br/>       1075838976,          0, 1072693248], dtype=uint32)<br/></p>
<p>Calling astype <i>always</i> creates a new array (a copy of the data), even<br/>if the new dtype is the same as the old dtype.<br/></p>
<p>Arithmetic with NumPy Arrays<br/>Arrays are important because they enable you to express batch operations on data<br/>without writing any for loops. NumPy users call this <i>vectorization</i>. Any arithmetic<br/>operations between equal-size arrays applies the operation element-wise:<br/></p>
<p>In [51]: arr = np.array([[1., 2., 3.], [4., 5., 6.]])<br/></p>
<p>In [52]: arr<br/>Out[52]: <br/>array([[ 1.,  2.,  3.],<br/>       [ 4.,  5.,  6.]])<br/></p>
<p>In [53]: arr * arr<br/>Out[53]: <br/>array([[  1.,   4.,   9.],<br/>       [ 16.,  25.,  36.]])<br/></p>
<p>In [54]: arr - arr<br/>Out[54]: <br/>array([[ 0.,  0.,  0.],<br/>       [ 0.,  0.,  0.]])<br/></p>
<p>Arithmetic operations with scalars propagate the scalar argument to each element in<br/>the array:<br/></p>
<p>In [55]: 1 / arr<br/>Out[55]: <br/>array([[ 1.    ,  0.5   ,  0.3333],<br/>       [ 0.25  ,  0.2   ,  0.1667]])<br/></p>
<p>In [56]: arr ** 0.5<br/>Out[56]: <br/>array([[ 1.    ,  1.4142,  1.7321],<br/>       [ 2.    ,  2.2361,  2.4495]])<br/></p>
<p>Comparisons between arrays of the same size yield boolean arrays:<br/></p>
<p>4.1 The NumPy ndarray: A Multidimensional Array Object | 93</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In [57]: arr2 = np.array([[0., 4., 1.], [7., 2., 12.]])<br/></p>
<p>In [58]: arr2<br/>Out[58]: <br/>array([[  0.,   4.,   1.],<br/>       [  7.,   2.,  12.]])<br/></p>
<p>In [59]: arr2 &gt; arr<br/>Out[59]: <br/>array([[False,  True, False],<br/>       [ True, False,  True]], dtype=bool)<br/></p>
<p>Operations between differently sized arrays is called <i>broadcasting</i> and will be dis&#8208;<br/>cussed in more detail in Appendix A. Having a deep understanding of broadcasting is<br/>not necessary for most of this book.<br/></p>
<p>Basic Indexing and Slicing<br/>NumPy array indexing is a rich topic, as there are many ways you may want to select<br/>a subset of your data or individual elements. One-dimensional arrays are simple; on<br/>the surface they act similarly to Python lists:<br/></p>
<p>In [60]: arr = np.arange(10)<br/></p>
<p>In [61]: arr<br/>Out[61]: array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])<br/></p>
<p>In [62]: arr[5]<br/>Out[62]: 5<br/></p>
<p>In [63]: arr[5:8]<br/>Out[63]: array([5, 6, 7])<br/></p>
<p>In [64]: arr[5:8] = 12<br/></p>
<p>In [65]: arr<br/>Out[65]: array([ 0,  1,  2,  3,  4, 12, 12, 12,  8,  9])<br/></p>
<p>As you can see, if you assign a scalar value to a slice, as in arr[5:8] = 12, the value is<br/>propagated (or <i>broadcasted</i> henceforth) to the entire selection. An important first dis&#8208;<br/>tinction from Python&#8217;s built-in lists is that array slices are <i>views</i> on the original array.<br/>This means that the data is not copied, and any modifications to the view will be<br/>reflected in the source array.<br/>To give an example of this, I first create a slice of arr:<br/></p>
<p>In [66]: arr_slice = arr[5:8]<br/></p>
<p>In [67]: arr_slice<br/>Out[67]: array([12, 12, 12])<br/></p>
<p>94 | Chapter 4: NumPy Basics: Arrays and Vectorized Computation</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Now, when I change values in arr_slice, the mutations are reflected in the original<br/>array arr:<br/></p>
<p>In [68]: arr_slice[1] = 12345<br/></p>
<p>In [69]: arr<br/>Out[69]: array([    0,     1,     2,     3,     4,    12, 12345,    12,     8,   <br/>  9])<br/></p>
<p>The &#8220;bare&#8221; slice [:] will assign to all values in an array:<br/>In [70]: arr_slice[:] = 64<br/></p>
<p>In [71]: arr<br/>Out[71]: array([ 0,  1,  2,  3,  4, 64, 64, 64,  8,  9])<br/></p>
<p>If you are new to NumPy, you might be surprised by this, especially if you have used<br/>other array programming languages that copy data more eagerly. As NumPy has been<br/>designed to be able to work with very large arrays, you could imagine performance<br/>and memory problems if NumPy insisted on always copying data.<br/></p>
<p>If you want a copy of a slice of an ndarray instead of a view, you<br/>will need to explicitly copy the array&#8212;for example,<br/>arr[5:8].copy().<br/></p>
<p>With higher dimensional arrays, you have many more options. In a two-dimensional<br/>array, the elements at each index are no longer scalars but rather one-dimensional<br/>arrays:<br/></p>
<p>In [72]: arr2d = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])<br/></p>
<p>In [73]: arr2d[2]<br/>Out[73]: array([7, 8, 9])<br/></p>
<p>Thus, individual elements can be accessed recursively. But that is a bit too much<br/>work, so you can pass a comma-separated list of indices to select individual elements.<br/>So these are equivalent:<br/></p>
<p>In [74]: arr2d[0][2]<br/>Out[74]: 3<br/></p>
<p>In [75]: arr2d[0, 2]<br/>Out[75]: 3<br/></p>
<p>See Figure 4-1 for an illustration of indexing on a two-dimensional array. I find it<br/>helpful to think of axis 0 as the &#8220;rows&#8221; of the array and axis 1 as the &#8220;columns.&#8221;<br/></p>
<p>4.1 The NumPy ndarray: A Multidimensional Array Object | 95</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><i>Figure 4-1. Indexing elements in a NumPy array<br/></i>In multidimensional arrays, if you omit later indices, the returned object will be a<br/>lower dimensional ndarray consisting of all the data along the higher dimensions. So<br/>in the 2 &#215; 2 &#215; 3 array arr3d:<br/></p>
<p>In [76]: arr3d = np.array([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])<br/></p>
<p>In [77]: arr3d<br/>Out[77]: <br/>array([[[ 1,  2,  3],<br/>        [ 4,  5,  6]],<br/>       [[ 7,  8,  9],<br/>        [10, 11, 12]]])<br/></p>
<p>arr3d[0] is a 2 &#215; 3 array:<br/>In [78]: arr3d[0]<br/>Out[78]: <br/>array([[1, 2, 3],<br/>       [4, 5, 6]])<br/></p>
<p>Both scalar values and arrays can be assigned to arr3d[0]:<br/>In [79]: old_values = arr3d[0].copy()<br/></p>
<p>In [80]: arr3d[0] = 42<br/></p>
<p>In [81]: arr3d<br/>Out[81]: <br/>array([[[42, 42, 42],<br/>        [42, 42, 42]],<br/>       [[ 7,  8,  9],<br/>        [10, 11, 12]]])<br/></p>
<p>In [82]: arr3d[0] = old_values<br/></p>
<p>96 | Chapter 4: NumPy Basics: Arrays and Vectorized Computation</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In [83]: arr3d<br/>Out[83]: <br/>array([[[ 1,  2,  3],<br/>        [ 4,  5,  6]],<br/>       [[ 7,  8,  9],<br/>        [10, 11, 12]]])<br/></p>
<p>Similarly, arr3d[1, 0] gives you all of the values whose indices start with (1, 0),<br/>forming a 1-dimensional array:<br/></p>
<p>In [84]: arr3d[1, 0]<br/>Out[84]: array([7, 8, 9])<br/></p>
<p>This expression is the same as though we had indexed in two steps:<br/>In [85]: x = arr3d[1]<br/></p>
<p>In [86]: x<br/>Out[86]: <br/>array([[ 7,  8,  9],<br/>       [10, 11, 12]])<br/></p>
<p>In [87]: x[0]<br/>Out[87]: array([7, 8, 9])<br/></p>
<p>Note that in all of these cases where subsections of the array have been selected, the<br/>returned arrays are views.<br/></p>
<p>Indexing with slices<br/>Like one-dimensional objects such as Python lists, ndarrays can be sliced with the<br/>familiar syntax:<br/></p>
<p>In [88]: arr<br/>Out[88]: array([ 0,  1,  2,  3,  4, 64, 64, 64,  8,  9])<br/></p>
<p>In [89]: arr[1:6]<br/>Out[89]: array([ 1,  2,  3,  4, 64])<br/></p>
<p>Consider the two-dimensional array from before, arr2d. Slicing this array is a bit<br/>different:<br/></p>
<p>In [90]: arr2d<br/>Out[90]: <br/>array([[1, 2, 3],<br/>       [4, 5, 6],<br/>       [7, 8, 9]])<br/></p>
<p>In [91]: arr2d[:2]<br/>Out[91]: <br/>array([[1, 2, 3],<br/>       [4, 5, 6]])<br/></p>
<p>4.1 The NumPy ndarray: A Multidimensional Array Object | 97</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>As you can see, it has sliced along axis 0, the first axis. A slice, therefore, selects a<br/>range of elements along an axis. It can be helpful to read the expression arr2d[:2] as<br/>&#8220;select the first two rows of arr2d.&#8221;<br/>You can pass multiple slices just like you can pass multiple indexes:<br/></p>
<p>In [92]: arr2d[:2, 1:]<br/>Out[92]: <br/>array([[2, 3],<br/>       [5, 6]])<br/></p>
<p>When slicing like this, you always obtain array views of the same number of dimen&#8208;<br/>sions. By mixing integer indexes and slices, you get lower dimensional slices.<br/>For example, I can select the second row but only the first two columns like so:<br/></p>
<p>In [93]: arr2d[1, :2]<br/>Out[93]: array([4, 5])<br/></p>
<p>Similarly, I can select the third column but only the first two rows like so:<br/>In [94]: arr2d[:2, 2]<br/>Out[94]: array([3, 6])<br/></p>
<p>See Figure 4-2 for an illustration. Note that a colon by itself means to take the entire<br/>axis, so you can slice only higher dimensional axes by doing:<br/></p>
<p>In [95]: arr2d[:, :1]<br/>Out[95]: <br/>array([[1],<br/>       [4],<br/>       [7]])<br/></p>
<p>Of course, assigning to a slice expression assigns to the whole selection:<br/>In [96]: arr2d[:2, 1:] = 0<br/></p>
<p>In [97]: arr2d<br/>Out[97]: <br/>array([[1, 0, 0],<br/>       [4, 0, 0],<br/>       [7, 8, 9]])<br/></p>
<p>98 | Chapter 4: NumPy Basics: Arrays and Vectorized Computation</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><i>Figure 4-2. Two-dimensional array slicing<br/></i></p>
<p>Boolean Indexing<br/>Let&#8217;s consider an example where we have some data in an array and an array of names<br/>with duplicates. I&#8217;m going to use here the randn function in numpy.random to generate<br/>some random normally distributed data:<br/></p>
<p>In [98]: names = np.array(['Bob', 'Joe', 'Will', 'Bob', 'Will', 'Joe', 'Joe'])<br/></p>
<p>In [99]: data = np.random.randn(7, 4)<br/></p>
<p>In [100]: names<br/>Out[100]: <br/>array(['Bob', 'Joe', 'Will', 'Bob', 'Will', 'Joe', 'Joe'],<br/>      dtype='&lt;U4')<br/></p>
<p>In [101]: data<br/>Out[101]: <br/>array([[ 0.0929,  0.2817,  0.769 ,  1.2464],<br/>       [ 1.0072, -1.2962,  0.275 ,  0.2289],<br/>       [ 1.3529,  0.8864, -2.0016, -0.3718],<br/>       [ 1.669 , -0.4386, -0.5397,  0.477 ],<br/>       [ 3.2489, -1.0212, -0.5771,  0.1241],<br/></p>
<p>4.1 The NumPy ndarray: A Multidimensional Array Object | 99</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>       [ 0.3026,  0.5238,  0.0009,  1.3438],<br/>       [-0.7135, -0.8312, -2.3702, -1.8608]])<br/></p>
<p>Suppose each name corresponds to a row in the data array and we wanted to select<br/>all the rows with corresponding name 'Bob'. Like arithmetic operations, compari&#8208;<br/>sons (such as ==) with arrays are also vectorized. Thus, comparing names with the<br/>string 'Bob' yields a boolean array:<br/></p>
<p>In [102]: names == 'Bob'<br/>Out[102]: array([ True, False, False,  True, False, False, False], dtype=bool)<br/></p>
<p>This boolean array can be passed when indexing the array:<br/>In [103]: data[names == 'Bob']<br/>Out[103]: <br/>array([[ 0.0929,  0.2817,  0.769 ,  1.2464],<br/>       [ 1.669 , -0.4386, -0.5397,  0.477 ]])<br/></p>
<p>The boolean array must be of the same length as the array axis it&#8217;s indexing. You can<br/>even mix and match boolean arrays with slices or integers (or sequences of integers;<br/>more on this later).<br/></p>
<p>Boolean selection will not fail if the boolean array is not the correct<br/>length, so I recommend care when using this feature.<br/></p>
<p>In these examples, I select from the rows where names == 'Bob' and index the col&#8208;<br/>umns, too:<br/></p>
<p>In [104]: data[names == 'Bob', 2:]<br/>Out[104]: <br/>array([[ 0.769 ,  1.2464],<br/>       [-0.5397,  0.477 ]])<br/></p>
<p>In [105]: data[names == 'Bob', 3]<br/>Out[105]: array([ 1.2464,  0.477 ])<br/></p>
<p>To select everything but 'Bob', you can either use != or negate the condition using ~:<br/>In [106]: names != 'Bob'<br/>Out[106]: array([False,  True,  True, False,  True,  True,  True], dtype=bool)<br/></p>
<p>In [107]: data[~(names == 'Bob')]<br/>Out[107]: <br/>array([[ 1.0072, -1.2962,  0.275 ,  0.2289],<br/>       [ 1.3529,  0.8864, -2.0016, -0.3718],<br/>       [ 3.2489, -1.0212, -0.5771,  0.1241],<br/>       [ 0.3026,  0.5238,  0.0009,  1.3438],<br/>       [-0.7135, -0.8312, -2.3702, -1.8608]])<br/></p>
<p>100 | Chapter 4: NumPy Basics: Arrays and Vectorized Computation</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>The ~ operator can be useful when you want to invert a general condition:<br/>In [108]: cond = names == 'Bob'<br/></p>
<p>In [109]: data[~cond]<br/>Out[109]: <br/>array([[ 1.0072, -1.2962,  0.275 ,  0.2289],<br/>       [ 1.3529,  0.8864, -2.0016, -0.3718],<br/>       [ 3.2489, -1.0212, -0.5771,  0.1241],<br/>       [ 0.3026,  0.5238,  0.0009,  1.3438],<br/>       [-0.7135, -0.8312, -2.3702, -1.8608]])<br/></p>
<p>Selecting two of the three names to combine multiple boolean conditions, use<br/>boolean arithmetic operators like &amp; (and) and | (or):<br/></p>
<p>In [110]: mask = (names == 'Bob') | (names == 'Will')<br/></p>
<p>In [111]: mask<br/>Out[111]: array([ True, False,  True,  True,  True, False, False], dtype=bool)<br/></p>
<p>In [112]: data[mask]<br/>Out[112]: <br/>array([[ 0.0929,  0.2817,  0.769 ,  1.2464],<br/>       [ 1.3529,  0.8864, -2.0016, -0.3718],<br/>       [ 1.669 , -0.4386, -0.5397,  0.477 ],<br/>       [ 3.2489, -1.0212, -0.5771,  0.1241]])<br/></p>
<p>Selecting data from an array by boolean indexing <i>always</i> creates a copy of the data,<br/>even if the returned array is unchanged.<br/></p>
<p>The Python keywords and and or do not work with boolean arrays.<br/>Use &amp; (and) and | (or) instead.<br/></p>
<p>Setting values with boolean arrays works in a common-sense way. To set all of the<br/>negative values in data to 0 we need only do:<br/></p>
<p>In [113]: data[data &lt; 0] = 0<br/></p>
<p>In [114]: data<br/>Out[114]: <br/>array([[ 0.0929,  0.2817,  0.769 ,  1.2464],<br/>       [ 1.0072,  0.    ,  0.275 ,  0.2289],<br/>       [ 1.3529,  0.8864,  0.    ,  0.    ],<br/>       [ 1.669 ,  0.    ,  0.    ,  0.477 ],<br/>       [ 3.2489,  0.    ,  0.    ,  0.1241],<br/>       [ 0.3026,  0.5238,  0.0009,  1.3438],<br/>       [ 0.    ,  0.    ,  0.    ,  0.    ]])<br/></p>
<p>4.1 The NumPy ndarray: A Multidimensional Array Object | 101</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Setting whole rows or columns using a one-dimensional boolean array is also easy:<br/>In [115]: data[names != 'Joe'] = 7<br/></p>
<p>In [116]: data<br/>Out[116]: <br/>array([[ 7.    ,  7.    ,  7.    ,  7.    ],<br/>       [ 1.0072,  0.    ,  0.275 ,  0.2289],<br/>       [ 7.    ,  7.    ,  7.    ,  7.    ],<br/>       [ 7.    ,  7.    ,  7.    ,  7.    ],<br/>       [ 7.    ,  7.    ,  7.    ,  7.    ],<br/>       [ 0.3026,  0.5238,  0.0009,  1.3438],<br/>       [ 0.    ,  0.    ,  0.    ,  0.    ]])<br/></p>
<p>As we will see later, these types of operations on two-dimensional data are convenient<br/>to do with pandas.<br/></p>
<p>Fancy Indexing<br/><i>Fancy indexing</i> is a term adopted by NumPy to describe indexing using integer arrays.<br/>Suppose we had an 8 &#215; 4 array:<br/></p>
<p>In [117]: arr = np.empty((8, 4))<br/></p>
<p>In [118]: <b>for</b> i <b>in</b> range(8):<br/>   .....:     arr[i] = i<br/></p>
<p>In [119]: arr<br/>Out[119]: <br/>array([[ 0.,  0.,  0.,  0.],<br/>       [ 1.,  1.,  1.,  1.],<br/>       [ 2.,  2.,  2.,  2.],<br/>       [ 3.,  3.,  3.,  3.],<br/>       [ 4.,  4.,  4.,  4.],<br/>       [ 5.,  5.,  5.,  5.],<br/>       [ 6.,  6.,  6.,  6.],<br/>       [ 7.,  7.,  7.,  7.]])<br/></p>
<p>To select out a subset of the rows in a particular order, you can simply pass a list or<br/>ndarray of integers specifying the desired order:<br/></p>
<p>In [120]: arr[[4, 3, 0, 6]]<br/>Out[120]: <br/>array([[ 4.,  4.,  4.,  4.],<br/>       [ 3.,  3.,  3.,  3.],<br/>       [ 0.,  0.,  0.,  0.],<br/>       [ 6.,  6.,  6.,  6.]])<br/></p>
<p>Hopefully this code did what you expected! Using negative indices selects rows from<br/>the end:<br/></p>
<p>In [121]: arr[[-3, -5, -7]]<br/>Out[121]: <br/></p>
<p>102 | Chapter 4: NumPy Basics: Arrays and Vectorized Computation</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>array([[ 5.,  5.,  5.,  5.],<br/>       [ 3.,  3.,  3.,  3.],<br/>       [ 1.,  1.,  1.,  1.]])<br/></p>
<p>Passing multiple index arrays does something slightly different; it selects a one-<br/>dimensional array of elements corresponding to each tuple of indices:<br/></p>
<p>In [122]: arr = np.arange(32).reshape((8, 4))<br/></p>
<p>In [123]: arr<br/>Out[123]: <br/>array([[ 0,  1,  2,  3],<br/>       [ 4,  5,  6,  7],<br/>       [ 8,  9, 10, 11],<br/>       [12, 13, 14, 15],<br/>       [16, 17, 18, 19],<br/>       [20, 21, 22, 23],<br/>       [24, 25, 26, 27],<br/>       [28, 29, 30, 31]])<br/></p>
<p>In [124]: arr[[1, 5, 7, 2], [0, 3, 1, 2]]<br/>Out[124]: array([ 4, 23, 29, 10])<br/></p>
<p>We&#8217;ll look at the reshape method in more detail in Appendix A.<br/>Here the elements (1, 0), (5, 3), (7, 1), and (2, 2) were selected. Regardless of<br/>how many dimensions the array has (here, only 2), the result of fancy indexing is<br/>always one-dimensional.<br/>The behavior of fancy indexing in this case is a bit different from what some users<br/>might have expected (myself included), which is the rectangular region formed by<br/>selecting a subset of the matrix&#8217;s rows and columns. Here is one way to get that:<br/></p>
<p>In [125]: arr[[1, 5, 7, 2]][:, [0, 3, 1, 2]]<br/>Out[125]: <br/>array([[ 4,  7,  5,  6],<br/>       [20, 23, 21, 22],<br/>       [28, 31, 29, 30],<br/>       [ 8, 11,  9, 10]])<br/></p>
<p>Keep in mind that fancy indexing, unlike slicing, always copies the data into a new<br/>array.<br/></p>
<p>Transposing Arrays and Swapping Axes<br/>Transposing is a special form of reshaping that similarly returns a view on the under&#8208;<br/>lying data without copying anything. Arrays have the transpose method and also the <br/>special T attribute:<br/></p>
<p>In [126]: arr = np.arange(15).reshape((3, 5))<br/></p>
<p>In [127]: arr<br/></p>
<p>4.1 The NumPy ndarray: A Multidimensional Array Object | 103</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Out[127]: <br/>array([[ 0,  1,  2,  3,  4],<br/>       [ 5,  6,  7,  8,  9],<br/>       [10, 11, 12, 13, 14]])<br/></p>
<p>In [128]: arr.T<br/>Out[128]: <br/>array([[ 0,  5, 10],<br/>       [ 1,  6, 11],<br/>       [ 2,  7, 12],<br/>       [ 3,  8, 13],<br/>       [ 4,  9, 14]])<br/></p>
<p>When doing matrix computations, you may do this very often&#8212;for example, when<br/>computing the inner matrix product using np.dot:<br/></p>
<p>In [129]: arr = np.random.randn(6, 3)<br/></p>
<p>In [130]: arr<br/>Out[130]: <br/>array([[-0.8608,  0.5601, -1.2659],<br/>       [ 0.1198, -1.0635,  0.3329],<br/>       [-2.3594, -0.1995, -1.542 ],<br/>       [-0.9707, -1.307 ,  0.2863],<br/>       [ 0.378 , -0.7539,  0.3313],<br/>       [ 1.3497,  0.0699,  0.2467]])<br/></p>
<p>In [131]: np.dot(arr.T, arr)<br/>Out[131]: <br/>array([[ 9.2291,  0.9394,  4.948 ],<br/>       [ 0.9394,  3.7662, -1.3622],<br/>       [ 4.948 , -1.3622,  4.3437]])<br/></p>
<p>For higher dimensional arrays, transpose will accept a tuple of axis numbers to per&#8208;<br/>mute the axes (for extra mind bending):<br/></p>
<p>In [132]: arr = np.arange(16).reshape((2, 2, 4))<br/></p>
<p>In [133]: arr<br/>Out[133]: <br/>array([[[ 0,  1,  2,  3],<br/>        [ 4,  5,  6,  7]],<br/>       [[ 8,  9, 10, 11],<br/>        [12, 13, 14, 15]]])<br/></p>
<p>In [134]: arr.transpose((1, 0, 2))<br/>Out[134]: <br/>array([[[ 0,  1,  2,  3],<br/>        [ 8,  9, 10, 11]],<br/>       [[ 4,  5,  6,  7],<br/>        [12, 13, 14, 15]]])<br/></p>
<p>104 | Chapter 4: NumPy Basics: Arrays and Vectorized Computation</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Here, the axes have been reordered with the second axis first, the first axis second,<br/>and the last axis unchanged.<br/>Simple transposing with .T is a special case of swapping axes. ndarray has the method<br/>swapaxes, which takes a pair of axis numbers and switches the indicated axes to rear&#8208;<br/>range the data:<br/></p>
<p>In [135]: arr<br/>Out[135]: <br/>array([[[ 0,  1,  2,  3],<br/>        [ 4,  5,  6,  7]],<br/>       [[ 8,  9, 10, 11],<br/>        [12, 13, 14, 15]]])<br/></p>
<p>In [136]: arr.swapaxes(1, 2)<br/>Out[136]: <br/>array([[[ 0,  4],<br/>        [ 1,  5],<br/>        [ 2,  6],<br/>        [ 3,  7]],<br/>       [[ 8, 12],<br/>        [ 9, 13],<br/>        [10, 14],<br/>        [11, 15]]])<br/></p>
<p>swapaxes similarly returns a view on the data without making a copy.<br/></p>
<p>4.2 Universal Functions: Fast Element-Wise Array<br/>Functions<br/>A universal function, or <i>ufunc</i>, is a function that performs element-wise operations<br/>on data in ndarrays. You can think of them as fast vectorized wrappers for simple<br/>functions that take one or more scalar values and produce one or more scalar results.<br/>Many ufuncs are simple element-wise transformations, like sqrt or exp:<br/></p>
<p>In [137]: arr = np.arange(10)<br/></p>
<p>In [138]: arr<br/>Out[138]: array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])<br/></p>
<p>In [139]: np.sqrt(arr)<br/>Out[139]: <br/>array([ 0.    ,  1.    ,  1.4142,  1.7321,  2.    ,  2.2361,  2.4495,<br/>        2.6458,  2.8284,  3.    ])<br/></p>
<p>In [140]: np.exp(arr)<br/>Out[140]: <br/>array([    1.    ,     2.7183,     7.3891,    20.0855,    54.5982,<br/>         148.4132,   403.4288,  1096.6332,  2980.958 ,  8103.0839])<br/></p>
<p>4.2 Universal Functions: Fast Element-Wise Array Functions | 105</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>These are referred to as <i>unary</i> ufuncs. Others, such as add or maximum, take two arrays<br/>(thus, <i>binary</i> ufuncs) and return a single array as the result:<br/></p>
<p>In [141]: x = np.random.randn(8)<br/></p>
<p>In [142]: y = np.random.randn(8)<br/></p>
<p>In [143]: x<br/>Out[143]: <br/>array([-0.0119,  1.0048,  1.3272, -0.9193, -1.5491,  0.0222,  0.7584,<br/>       -0.6605])<br/></p>
<p>In [144]: y<br/>Out[144]: <br/>array([ 0.8626, -0.01  ,  0.05  ,  0.6702,  0.853 , -0.9559, -0.0235,<br/>       -2.3042])<br/></p>
<p>In [145]: np.maximum(x, y)<br/>Out[145]: <br/>array([ 0.8626,  1.0048,  1.3272,  0.6702,  0.853 ,  0.0222,  0.7584,<br/>       -0.6605])<br/></p>
<p>Here, numpy.maximum computed the element-wise maximum of the elements in x and<br/>y.<br/>While not common, a ufunc can return multiple arrays. modf is one example, a vec&#8208;<br/>torized version of the built-in Python divmod; it returns the fractional and integral<br/>parts of a floating-point array:<br/></p>
<p>In [146]: arr = np.random.randn(7) * 5<br/></p>
<p>In [147]: arr<br/>Out[147]: array([-3.2623, -6.0915, -6.663 ,  5.3731,  3.6182,  3.45  ,  5.0077])<br/></p>
<p>In [148]: remainder, whole_part = np.modf(arr)<br/></p>
<p>In [149]: remainder<br/>Out[149]: array([-0.2623, -0.0915, -0.663 ,  0.3731,  0.6182,  0.45  ,  0.0077])<br/></p>
<p>In [150]: whole_part<br/>Out[150]: array([-3., -6., -6.,  5.,  3.,  3.,  5.])<br/></p>
<p>Ufuncs accept an optional out argument that allows them to operate in-place on<br/>arrays:<br/></p>
<p>In [151]: arr<br/>Out[151]: array([-3.2623, -6.0915, -6.663 ,  5.3731,  3.6182,  3.45  ,  5.0077])<br/></p>
<p>In [152]: np.sqrt(arr)<br/>Out[152]: array([    nan,     nan,     nan,  2.318 ,  1.9022,  1.8574,  2.2378])<br/></p>
<p>In [153]: np.sqrt(arr, arr)<br/></p>
<p>106 | Chapter 4: NumPy Basics: Arrays and Vectorized Computation</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Out[153]: array([    nan,     nan,     nan,  2.318 ,  1.9022,  1.8574,  2.2378])<br/></p>
<p>In [154]: arr<br/>Out[154]: array([    nan,     nan,     nan,  2.318 ,  1.9022,  1.8574,  2.2378])<br/></p>
<p>See Tables 4-3 and 4-4 for a listing of available ufuncs.<br/><i>Table 4-3. Unary ufuncs<br/></i></p>
<p>Function Description<br/>abs, fabs Compute the absolute value element-wise for integer, floating-point, or complex values<br/>sqrt Compute the square root of each element (equivalent to arr ** 0.5)<br/>square Compute the square of each element (equivalent to arr ** 2)<br/>exp Compute the exponent ex of each element<br/>log, log10, <br/>log2, log1p<br/></p>
<p>Natural logarithm (base <i>e</i>), log base 10, log base 2, and log(1 + x), respectively<br/></p>
<p>sign Compute the sign of each element: 1 (positive), 0 (zero), or &#8211;1 (negative)<br/>ceil Compute the ceiling of each element (i.e., the smallest integer greater than or equal to that<br/></p>
<p>number)<br/>floor Compute the floor of each element (i.e., the largest integer less than or equal to each element)<br/>rint Round elements to the nearest integer, preserving the dtype<br/>modf Return fractional and integral parts of array as a separate array<br/>isnan Return boolean array indicating whether each value is NaN (Not a Number)<br/>isfinite, isinf Return boolean array indicating whether each element is finite (non-inf, non-NaN) or infinite,<br/></p>
<p>respectively<br/>cos, cosh, sin, <br/>sinh, tan, tanh<br/></p>
<p>Regular and hyperbolic trigonometric functions<br/></p>
<p>arccos, arccosh, <br/>arcsin, arcsinh, <br/>arctan, arctanh<br/></p>
<p>Inverse trigonometric functions<br/></p>
<p>logical_not Compute truth value of not x element-wise (equivalent to ~arr).<br/></p>
<p><i>Table 4-4. Binary universal functions<br/></i>Function Description<br/>add Add corresponding elements in arrays<br/>subtract Subtract elements in second array from first array<br/>multiply Multiply array elements<br/>divide, floor_divide Divide or floor divide (truncating the remainder)<br/>power Raise elements in first array to powers indicated in second array<br/>maximum, fmax Element-wise maximum; fmax ignores NaN<br/>minimum, fmin Element-wise minimum; fmin ignores NaN<br/>mod Element-wise modulus (remainder of division)<br/>copysign Copy sign of values in second argument to values in first argument<br/></p>
<p>4.2 Universal Functions: Fast Element-Wise Array Functions | 107</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Function Description<br/>greater, greater_equal, <br/>less, less_equal, <br/>equal, not_equal<br/></p>
<p>Perform element-wise comparison, yielding boolean array (equivalent to infix <br/>operators &gt;, &gt;=, &lt;, &lt;=, ==, !=)<br/></p>
<p>logical_and, <br/>logical_or, logical_xor<br/></p>
<p>Compute element-wise truth value of logical operation (equivalent to infix operators<br/>&amp; |, ^)<br/></p>
<p>4.3 Array-Oriented Programming with Arrays<br/>Using NumPy arrays enables you to express many kinds of data processing tasks as<br/>concise array expressions that might otherwise require writing loops. This practice of<br/>replacing explicit loops with array expressions is commonly referred to as <i>vectoriza&#8208;<br/>tion</i>. In general, vectorized array operations will often be one or two (or more) orders<br/>of magnitude faster than their pure Python equivalents, with the biggest impact in<br/>any kind of numerical computations. Later, in Appendix A, I explain <i>broadcasting</i>, a<br/>powerful method for vectorizing computations.<br/>As a simple example, suppose we wished to evaluate the function sqrt(x^2 + y^2)<br/>across a regular grid of values. The np.meshgrid function takes two 1D arrays and<br/>produces two 2D matrices corresponding to all pairs of (x, y) in the two arrays:<br/></p>
<p>In [155]: points = np.arange(-5, 5, 0.01) <i># 1000 equally spaced points<br/></i></p>
<p>In [156]: xs, ys = np.meshgrid(points, points)<br/></p>
<p>In [157]: ys<br/>Out[157]: <br/>array([[-5.  , -5.  , -5.  , ..., -5.  , -5.  , -5.  ],<br/>       [-4.99, -4.99, -4.99, ..., -4.99, -4.99, -4.99],<br/>       [-4.98, -4.98, -4.98, ..., -4.98, -4.98, -4.98],<br/>       ..., <br/>       [ 4.97,  4.97,  4.97, ...,  4.97,  4.97,  4.97],<br/>       [ 4.98,  4.98,  4.98, ...,  4.98,  4.98,  4.98],<br/>       [ 4.99,  4.99,  4.99, ...,  4.99,  4.99,  4.99]])<br/></p>
<p>Now, evaluating the function is a matter of writing the same expression you would<br/>write with two points:<br/></p>
<p>In [158]: z = np.sqrt(xs ** 2 + ys ** 2)<br/></p>
<p>In [159]: z<br/>Out[159]: <br/>array([[ 7.0711,  7.064 ,  7.0569, ...,  7.0499,  7.0569,  7.064 ],<br/>       [ 7.064 ,  7.0569,  7.0499, ...,  7.0428,  7.0499,  7.0569],<br/>       [ 7.0569,  7.0499,  7.0428, ...,  7.0357,  7.0428,  7.0499],<br/>       ..., <br/>       [ 7.0499,  7.0428,  7.0357, ...,  7.0286,  7.0357,  7.0428],<br/>       [ 7.0569,  7.0499,  7.0428, ...,  7.0357,  7.0428,  7.0499],<br/>       [ 7.064 ,  7.0569,  7.0499, ...,  7.0428,  7.0499,  7.0569]])<br/></p>
<p>108 | Chapter 4: NumPy Basics: Arrays and Vectorized Computation</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>As a preview of Chapter 9, I use matplotlib to create visualizations of this two-<br/>dimensional array:<br/></p>
<p>In [160]: <b>import</b> <b>matplotlib.pyplot</b> <b>as</b> <b>plt<br/></b></p>
<p>In [161]: plt.imshow(z, cmap=plt.cm.gray); plt.colorbar()<br/>Out[161]: &lt;matplotlib.colorbar.Colorbar at 0x7f715e3fa630&gt;<br/></p>
<p>In [162]: plt.title(&quot;Image plot of $\sqrt{x^2 + y^2}$ for a grid of values&quot;)<br/>Out[162]: &lt;matplotlib.text.Text at 0x7f715d2de748&gt;<br/></p>
<p>See Figure 4-3. Here I used the matplotlib function imshow to create an image plot<br/>from a two-dimensional array of function values.<br/></p>
<p><i>Figure 4-3. Plot of function evaluated on grid<br/></i></p>
<p>Expressing Conditional Logic as Array Operations<br/>The numpy.where function is a vectorized version of the ternary expression x if con<br/>dition else y. Suppose we had a boolean array and two arrays of values:<br/></p>
<p>4.3 Array-Oriented Programming with Arrays | 109</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In [165]: xarr = np.array([1.1, 1.2, 1.3, 1.4, 1.5])<br/></p>
<p>In [166]: yarr = np.array([2.1, 2.2, 2.3, 2.4, 2.5])<br/></p>
<p>In [167]: cond = np.array([True, False, True, True, False])<br/></p>
<p>Suppose we wanted to take a value from xarr whenever the corresponding value in<br/>cond is True, and otherwise take the value from yarr. A list comprehension doing<br/>this might look like:<br/></p>
<p>In [168]: result = [(x <b>if</b> c <b>else</b> y)<br/>   .....:           <b>for</b> x, y, c <b>in</b> zip(xarr, yarr, cond)]<br/></p>
<p>In [169]: result<br/>Out[169]: [1.1000000000000001, 2.2000000000000002, 1.3, 1.3999999999999999, 2.5]<br/></p>
<p>This has multiple problems. First, it will not be very fast for large arrays (because all<br/>the work is being done in interpreted Python code). Second, it will not work with<br/>multidimensional arrays. With np.where you can write this very concisely:<br/></p>
<p>In [170]: result = np.where(cond, xarr, yarr)<br/></p>
<p>In [171]: result<br/>Out[171]: array([ 1.1,  2.2,  1.3,  1.4,  2.5])<br/></p>
<p>The second and third arguments to np.where don&#8217;t need to be arrays; one or both of<br/>them can be scalars. A typical use of where in data analysis is to produce a new array<br/>of values based on another array. Suppose you had a matrix of randomly generated<br/>data and you wanted to replace all positive values with 2 and all negative values with<br/>&#8211;2. This is very easy to do with np.where:<br/></p>
<p>In [172]: arr = np.random.randn(4, 4)<br/></p>
<p>In [173]: arr<br/>Out[173]: <br/>array([[-0.5031, -0.6223, -0.9212, -0.7262],<br/>       [ 0.2229,  0.0513, -1.1577,  0.8167],<br/>       [ 0.4336,  1.0107,  1.8249, -0.9975],<br/>       [ 0.8506, -0.1316,  0.9124,  0.1882]])<br/></p>
<p>In [174]: arr &gt; 0<br/>Out[174]: <br/>array([[False, False, False, False],<br/>       [ True,  True, False,  True],<br/>       [ True,  True,  True, False],<br/>       [ True, False,  True,  True]], dtype=bool)<br/></p>
<p>In [175]: np.where(arr &gt; 0, 2, -2)<br/>Out[175]: <br/>array([[-2, -2, -2, -2],<br/>       [ 2,  2, -2,  2],<br/></p>
<p>110 | Chapter 4: NumPy Basics: Arrays and Vectorized Computation</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>       [ 2,  2,  2, -2],<br/>       [ 2, -2,  2,  2]])<br/></p>
<p>You can combine scalars and arrays when using np.where. For example, I can replace<br/>all positive values in arr with the constant 2 like so:<br/></p>
<p>In [176]: np.where(arr &gt; 0, 2, arr) <i># set only positive values to 2<br/></i>Out[176]: <br/>array([[-0.5031, -0.6223, -0.9212, -0.7262],<br/>       [ 2.    ,  2.    , -1.1577,  2.    ],<br/>       [ 2.    ,  2.    ,  2.    , -0.9975],<br/>       [ 2.    , -0.1316,  2.    ,  2.    ]])<br/></p>
<p>The arrays passed to np.where can be more than just equal-sized arrays or scalars.<br/></p>
<p>Mathematical and Statistical Methods<br/>A set of mathematical functions that compute statistics about an entire array or about<br/>the data along an axis are accessible as methods of the array class. You can use aggre&#8208;<br/>gations (often called <i>reductions</i>) like sum, mean, and std (standard deviation) either by<br/>calling the array instance method or using the top-level NumPy function.<br/>Here I generate some normally distributed random data and compute some aggregate<br/>statistics:<br/></p>
<p>In [177]: arr = np.random.randn(5, 4)<br/></p>
<p>In [178]: arr<br/>Out[178]: <br/>array([[ 2.1695, -0.1149,  2.0037,  0.0296],<br/>       [ 0.7953,  0.1181, -0.7485,  0.585 ],<br/>       [ 0.1527, -1.5657, -0.5625, -0.0327],<br/>       [-0.929 , -0.4826, -0.0363,  1.0954],<br/>       [ 0.9809, -0.5895,  1.5817, -0.5287]])<br/></p>
<p>In [179]: arr.mean()<br/>Out[179]: 0.19607051119998253<br/></p>
<p>In [180]: np.mean(arr)<br/>Out[180]: 0.19607051119998253<br/></p>
<p>In [181]: arr.sum()<br/>Out[181]: 3.9214102239996507<br/></p>
<p>Functions like mean and sum take an optional axis argument that computes the statis&#8208;<br/>tic over the given axis, resulting in an array with one fewer dimension:<br/></p>
<p>In [182]: arr.mean(axis=1)<br/>Out[182]: array([ 1.022 ,  0.1875, -0.502 , -0.0881,  0.3611])<br/></p>
<p>In [183]: arr.sum(axis=0)<br/>Out[183]: array([ 3.1693, -2.6345,  2.2381,  1.1486])<br/></p>
<p>4.3 Array-Oriented Programming with Arrays | 111</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Here, arr.mean(1) means &#8220;compute mean across the columns&#8221; where arr.sum(0)<br/>means &#8220;compute sum down the rows.&#8221;<br/>Other methods like cumsum and cumprod do not aggregate, instead producing an array<br/>of the intermediate results:<br/></p>
<p>In [184]: arr = np.array([0, 1, 2, 3, 4, 5, 6, 7])<br/></p>
<p>In [185]: arr.cumsum()<br/>Out[185]: array([ 0,  1,  3,  6, 10, 15, 21, 28])<br/></p>
<p>In multidimensional arrays, accumulation functions like cumsum return an array of<br/>the same size, but with the partial aggregates computed along the indicated axis<br/>according to each lower dimensional slice:<br/></p>
<p>In [186]: arr = np.array([[0, 1, 2], [3, 4, 5], [6, 7, 8]])<br/></p>
<p>In [187]: arr<br/>Out[187]: <br/>array([[0, 1, 2],<br/>       [3, 4, 5],<br/>       [6, 7, 8]])<br/></p>
<p>In [188]: arr.cumsum(axis=0)<br/>Out[188]: <br/>array([[ 0,  1,  2],<br/>       [ 3,  5,  7],<br/>       [ 9, 12, 15]])<br/></p>
<p>In [189]: arr.cumprod(axis=1)<br/>Out[189]: <br/>array([[  0,   0,   0],<br/>       [  3,  12,  60],<br/>       [  6,  42, 336]])<br/></p>
<p>See Table 4-5 for a full listing. We&#8217;ll see many examples of these methods in action in<br/>later chapters.<br/><i>Table 4-5. Basic array statistical methods<br/></i></p>
<p>Method Description<br/>sum Sum of all the elements in the array or along an axis; zero-length arrays have sum 0<br/>mean Arithmetic mean; zero-length arrays have NaN mean<br/>std, var Standard deviation and variance, respectively, with optional degrees of freedom adjustment (default<br/></p>
<p>denominator n)<br/>min, max Minimum and maximum<br/>argmin, argmax Indices of minimum and maximum elements, respectively<br/>cumsum Cumulative sum of elements starting from 0<br/>cumprod Cumulative product of elements starting from 1<br/></p>
<p>112 | Chapter 4: NumPy Basics: Arrays and Vectorized Computation</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Methods for Boolean Arrays<br/>Boolean values are coerced to 1 (True) and 0 (False) in the preceding methods. Thus,<br/>sum is often used as a means of counting True values in a boolean array:<br/></p>
<p>In [190]: arr = np.random.randn(100)<br/></p>
<p>In [191]: (arr &gt; 0).sum() <i># Number of positive values<br/></i>Out[191]: 42<br/></p>
<p>There are two additional methods, any and all, useful especially for boolean arrays.<br/>any tests whether one or more values in an array is True, while all checks if every<br/>value is True:<br/></p>
<p>In [192]: bools = np.array([False, False, True, False])<br/></p>
<p>In [193]: bools.any()<br/>Out[193]: True<br/></p>
<p>In [194]: bools.all()<br/>Out[194]: False<br/></p>
<p>These methods also work with non-boolean arrays, where non-zero elements evalu&#8208;<br/>ate to True.<br/></p>
<p>Sorting<br/>Like Python&#8217;s built-in list type, NumPy arrays can be sorted in-place with the sort<br/>method:<br/></p>
<p>In [195]: arr = np.random.randn(6)<br/></p>
<p>In [196]: arr<br/>Out[196]: array([ 0.6095, -0.4938,  1.24  , -0.1357,  1.43  , -0.8469])<br/></p>
<p>In [197]: arr.sort()<br/></p>
<p>In [198]: arr<br/>Out[198]: array([-0.8469, -0.4938, -0.1357,  0.6095,  1.24  ,  1.43  ])<br/></p>
<p>You can sort each one-dimensional section of values in a multidimensional array in-<br/>place along an axis by passing the axis number to sort:<br/></p>
<p>In [199]: arr = np.random.randn(5, 3)<br/></p>
<p>In [200]: arr<br/>Out[200]: <br/>array([[ 0.6033,  1.2636, -0.2555],<br/>       [-0.4457,  0.4684, -0.9616],<br/>       [-1.8245,  0.6254,  1.0229],<br/>       [ 1.1074,  0.0909, -0.3501],<br/>       [ 0.218 , -0.8948, -1.7415]])<br/></p>
<p>4.3 Array-Oriented Programming with Arrays | 113</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In [201]: arr.sort(1)<br/></p>
<p>In [202]: arr<br/>Out[202]: <br/>array([[-0.2555,  0.6033,  1.2636],<br/>       [-0.9616, -0.4457,  0.4684],<br/>       [-1.8245,  0.6254,  1.0229],<br/>       [-0.3501,  0.0909,  1.1074],<br/>       [-1.7415, -0.8948,  0.218 ]])<br/></p>
<p>The top-level method np.sort returns a sorted copy of an array instead of modifying<br/>the array in-place. A quick-and-dirty way to compute the quantiles of an array is to<br/>sort it and select the value at a particular rank:<br/></p>
<p>In [203]: large_arr = np.random.randn(1000)<br/></p>
<p>In [204]: large_arr.sort()<br/></p>
<p>In [205]: large_arr[int(0.05 * len(large_arr))] <i># 5% quantile<br/></i>Out[205]: -1.5311513550102103<br/></p>
<p>For more details on using NumPy&#8217;s sorting methods, and more advanced techniques<br/>like indirect sorts, see Appendix A. Several other kinds of data manipulations related<br/>to sorting (e.g., sorting a table of data by one or more columns) can also be found in<br/>pandas.<br/></p>
<p>Unique and Other Set Logic<br/>NumPy has some basic set operations for one-dimensional ndarrays. A commonly<br/>used one is np.unique, which returns the sorted unique values in an array:<br/></p>
<p>In [206]: names = np.array(['Bob', 'Joe', 'Will', 'Bob', 'Will', 'Joe', 'Joe'])<br/></p>
<p>In [207]: np.unique(names)<br/>Out[207]: <br/>array(['Bob', 'Joe', 'Will'],<br/>      dtype='&lt;U4')<br/></p>
<p>In [208]: ints = np.array([3, 3, 3, 2, 2, 1, 1, 4, 4])<br/></p>
<p>In [209]: np.unique(ints)<br/>Out[209]: array([1, 2, 3, 4])<br/></p>
<p>Contrast np.unique with the pure Python alternative:<br/>In [210]: sorted(set(names))<br/>Out[210]: ['Bob', 'Joe', 'Will']<br/></p>
<p>Another function, np.in1d, tests membership of the values in one array in another,<br/>returning a boolean array:<br/></p>
<p>114 | Chapter 4: NumPy Basics: Arrays and Vectorized Computation</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In [211]: values = np.array([6, 0, 0, 3, 2, 5, 6])<br/></p>
<p>In [212]: np.in1d(values, [2, 3, 6])<br/>Out[212]: array([ True, False, False,  True,  True, False,  True], dtype=bool)<br/></p>
<p>See Table 4-6 for a listing of set functions in NumPy.<br/><i>Table 4-6. Array set operations<br/></i></p>
<p>Method Description<br/>unique(x) Compute the sorted, unique elements in x<br/>intersect1d(x, y) Compute the sorted, common elements in x and y<br/>union1d(x, y) Compute the sorted union of elements<br/>in1d(x, y) Compute a boolean array indicating whether each element of x is contained in y<br/>setdiff1d(x, y) Set difference, elements in x that are not in y<br/>setxor1d(x, y) Set symmetric differences; elements that are in either of the arrays, but not both<br/></p>
<p>4.4 File Input and Output with Arrays<br/>NumPy is able to save and load data to and from disk either in text or binary format.<br/>In this section I only discuss NumPy&#8217;s built-in binary format, since most users will<br/>prefer pandas and other tools for loading text or tabular data (see Chapter 6 for much<br/>more).<br/>np.save and np.load are the two workhorse functions for efficiently saving and load&#8208;<br/>ing array data on disk. Arrays are saved by default in an uncompressed raw binary<br/>format with file extension <i>.npy</i>:<br/></p>
<p>In [213]: arr = np.arange(10)<br/></p>
<p>In [214]: np.save('some_array', arr)<br/></p>
<p>If the file path does not already end in <i>.npy</i>, the extension will be appended. The array<br/>on disk can then be loaded with np.load:<br/></p>
<p>In [215]: np.load('some_array.npy')<br/>Out[215]: array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])<br/></p>
<p>You save multiple arrays in an uncompressed archive using np.savez and passing the<br/>arrays as keyword arguments:<br/></p>
<p>In [216]: np.savez('array_archive.npz', a=arr, b=arr)<br/></p>
<p>When loading an <i>.npz</i> file, you get back a dict-like object that loads the individual<br/>arrays lazily:<br/></p>
<p>In [217]: arch = np.load('array_archive.npz')<br/></p>
<p>In [218]: arch['b']<br/>Out[218]: array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])<br/></p>
<p>4.4 File Input and Output with Arrays | 115</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>If your data compresses well, you may wish to use numpy.savez_compressed instead:<br/>In [219]: np.savez_compressed('arrays_compressed.npz', a=arr, b=arr)<br/></p>
<p>4.5 Linear Algebra<br/>Linear algebra, like matrix multiplication, decompositions, determinants, and other<br/>square matrix math, is an important part of any array library. Unlike some languages<br/>like MATLAB, multiplying two two-dimensional arrays with * is an element-wise<br/>product instead of a matrix dot product. Thus, there is a function dot, both an array <br/>method and a function in the numpy namespace, for matrix multiplication:<br/></p>
<p>In [223]: x = np.array([[1., 2., 3.], [4., 5., 6.]])<br/></p>
<p>In [224]: y = np.array([[6., 23.], [-1, 7], [8, 9]])<br/></p>
<p>In [225]: x<br/>Out[225]: <br/>array([[ 1.,  2.,  3.],<br/>       [ 4.,  5.,  6.]])<br/></p>
<p>In [226]: y<br/>Out[226]: <br/>array([[  6.,  23.],<br/>       [ -1.,   7.],<br/>       [  8.,   9.]])<br/></p>
<p>In [227]: x.dot(y)<br/>Out[227]: <br/>array([[  28.,   64.],<br/>       [  67.,  181.]])<br/></p>
<p>x.dot(y) is equivalent to np.dot(x, y):<br/>In [228]: np.dot(x, y)<br/>Out[228]: <br/>array([[  28.,   64.],<br/>       [  67.,  181.]])<br/></p>
<p>A matrix product between a two-dimensional array and a suitably sized one-<br/>dimensional array results in a one-dimensional array:<br/></p>
<p>In [229]: np.dot(x, np.ones(3))<br/>Out[229]: array([  6.,  15.])<br/></p>
<p>The @ symbol (as of Python 3.5) also works as an infix operator that performs matrix<br/>multiplication:<br/></p>
<p>In [230]: x @ np.ones(3)<br/>Out[230]: array([  6.,  15.])<br/></p>
<p>116 | Chapter 4: NumPy Basics: Arrays and Vectorized Computation</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>numpy.linalg has a standard set of matrix decompositions and things like inverse<br/>and determinant. These are implemented under the hood via the same industry-<br/>standard linear algebra libraries used in other languages like MATLAB and R, such as<br/>BLAS, LAPACK, or possibly (depending on your NumPy build) the proprietary Intel<br/>MKL (Math Kernel Library):<br/></p>
<p>In [231]: <b>from</b> <b>numpy.linalg</b> <b>import</b> inv, qr<br/></p>
<p>In [232]: X = np.random.randn(5, 5)<br/></p>
<p>In [233]: mat = X.T.dot(X)<br/></p>
<p>In [234]: inv(mat)<br/>Out[234]: <br/>array([[  933.1189,   871.8258, -1417.6902, -1460.4005,  1782.1391],<br/>       [  871.8258,   815.3929, -1325.9965, -1365.9242,  1666.9347],<br/>       [-1417.6902, -1325.9965,  2158.4424,  2222.0191, -2711.6822],<br/>       [-1460.4005, -1365.9242,  2222.0191,  2289.0575, -2793.422 ],<br/>       [ 1782.1391,  1666.9347, -2711.6822, -2793.422 ,  3409.5128]])<br/></p>
<p>In [235]: mat.dot(inv(mat))<br/>Out[235]: <br/>array([[ 1.,  0., -0., -0., -0.],<br/>       [-0.,  1.,  0.,  0.,  0.],<br/>       [ 0.,  0.,  1.,  0.,  0.],<br/>       [-0.,  0.,  0.,  1., -0.],<br/>       [-0.,  0.,  0.,  0.,  1.]])<br/></p>
<p>In [236]: q, r = qr(mat)<br/></p>
<p>In [237]: r<br/>Out[237]: <br/>array([[-1.6914,  4.38  ,  0.1757,  0.4075, -0.7838],<br/>       [ 0.    , -2.6436,  0.1939, -3.072 , -1.0702],<br/>       [ 0.    ,  0.    , -0.8138,  1.5414,  0.6155],<br/>       [ 0.    ,  0.    ,  0.    , -2.6445, -2.1669],<br/>       [ 0.    ,  0.    ,  0.    ,  0.    ,  0.0002]])<br/></p>
<p>The expression X.T.dot(X) computes the dot product of X with its transpose X.T.<br/>See Table 4-7 for a list of some of the most commonly used linear algebra functions.<br/><i>Table 4-7. Commonly used numpy.linalg functions<br/></i></p>
<p>Function Description<br/>diag Return the diagonal (or off-diagonal) elements of a square matrix as a 1D array, or convert a 1D array into a<br/></p>
<p>square matrix with zeros on the off-diagonal<br/>dot Matrix multiplication<br/>trace Compute the sum of the diagonal elements<br/>det Compute the matrix determinant<br/></p>
<p>4.5 Linear Algebra | 117</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Function Description<br/>eig Compute the eigenvalues and eigenvectors of a square matrix<br/>inv Compute the inverse of a square matrix<br/>pinv Compute the Moore-Penrose pseudo-inverse of a matrix<br/>qr Compute the QR decomposition<br/>svd Compute the singular value decomposition (SVD)<br/>solve Solve the linear system Ax = b for x, where A is a square matrix<br/>lstsq Compute the least-squares solution to Ax = b<br/></p>
<p>4.6 Pseudorandom Number Generation<br/>The numpy.random module supplements the built-in Python random with functions<br/>for efficiently generating whole arrays of sample values from many kinds of probabil&#8208;<br/>ity distributions. For example, you can get a 4 &#215; 4 array of samples from the standard<br/>normal distribution using normal:<br/></p>
<p>In [238]: samples = np.random.normal(size=(4, 4))<br/></p>
<p>In [239]: samples<br/>Out[239]: <br/>array([[ 0.5732,  0.1933,  0.4429,  1.2796],<br/>       [ 0.575 ,  0.4339, -0.7658, -1.237 ],<br/>       [-0.5367,  1.8545, -0.92  , -0.1082],<br/>       [ 0.1525,  0.9435, -1.0953, -0.144 ]])<br/></p>
<p>Python&#8217;s built-in random module, by contrast, only samples one value at a time. As<br/>you can see from this benchmark, numpy.random is well over an order of magnitude<br/>faster for generating very large samples:<br/></p>
<p>In [240]: <b>from</b> <b>random</b> <b>import</b> normalvariate<br/></p>
<p>In [241]: N = 1000000<br/></p>
<p>In [242]: %timeit samples = [normalvariate(0, 1) <b>for</b> _ <b>in</b> range(N)]<br/>1.77 s +- 126 ms per loop (mean +- std. dev. of 7 runs, 1 loop each)<br/></p>
<p>In [243]: %timeit np.random.normal(size=N)<br/>61.7 ms +- 1.32 ms per loop (mean +- std. dev. of 7 runs, 10 loops each)<br/></p>
<p>We say that these are <i>pseudorandom</i> numbers because they are generated by an algo&#8208;<br/>rithm with deterministic behavior based on the <i>seed</i> of the random number genera&#8208;<br/>tor. You can change NumPy&#8217;s random number generation seed using<br/>np.random.seed:<br/></p>
<p>In [244]: np.random.seed(1234)<br/></p>
<p>118 | Chapter 4: NumPy Basics: Arrays and Vectorized Computation</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>The data generation functions in numpy.random use a global random seed. To avoid<br/>global state, you can use numpy.random.RandomState to create a random number<br/>generator isolated from others:<br/></p>
<p>In [245]: rng = np.random.RandomState(1234)<br/></p>
<p>In [246]: rng.randn(10)<br/>Out[246]: <br/>array([ 0.4714, -1.191 ,  1.4327, -0.3127, -0.7206,  0.8872,  0.8596,<br/>       -0.6365,  0.0157, -2.2427])<br/></p>
<p>See Table 4-8 for a partial list of functions available in numpy.random. I&#8217;ll give some<br/>examples of leveraging these functions&#8217; ability to generate large arrays of samples all<br/>at once in the next section.<br/><i>Table 4-8. Partial list of numpy.random functions<br/></i></p>
<p>Function Description<br/>seed Seed the random number generator<br/>permutation Return a random permutation of a sequence, or return a permuted range<br/>shuffle Randomly permute a sequence in-place<br/>rand Draw samples from a uniform distribution<br/>randint Draw random integers from a given low-to-high range<br/>randn Draw samples from a normal distribution with mean 0 and standard deviation 1 (MATLAB-like interface)<br/>binomial Draw samples from a binomial distribution<br/>normal Draw samples from a normal (Gaussian) distribution<br/>beta Draw samples from a beta distribution<br/>chisquare Draw samples from a chi-square distribution<br/>gamma Draw samples from a gamma distribution<br/>uniform Draw samples from a uniform [0, 1) distribution<br/></p>
<p>4.7 Example: Random Walks<br/>The simulation of random walks provides an illustrative application of utilizing array<br/>operations. Let&#8217;s first consider a simple random walk starting at 0 with steps of 1 and<br/>&#8211;1 occurring with equal probability.<br/>Here is a pure Python way to implement a single random walk with 1,000 steps using<br/>the built-in random module:<br/></p>
<p>In [247]: <b>import</b> <b>random<br/></b>   .....: position = 0<br/>   .....: walk = [position]<br/>   .....: steps = 1000<br/>   .....: <b>for</b> i <b>in</b> range(steps):<br/>   .....:     step = 1 <b>if</b> random.randint(0, 1) <b>else</b> -1<br/>   .....:     position += step<br/></p>
<p>4.7 Example: Random Walks | 119</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>   .....:     walk.append(position)<br/>   .....:<br/></p>
<p>See Figure 4-4 for an example plot of the first 100 values on one of these random<br/>walks:<br/></p>
<p>In [249]: plt.plot(walk[:100])<br/></p>
<p><i>Figure 4-4. A simple random walk<br/></i></p>
<p>You might make the observation that walk is simply the cumulative sum of the ran&#8208;<br/>dom steps and could be evaluated as an array expression. Thus, I use the np.random<br/>module to draw 1,000 coin flips at once, set these to 1 and &#8211;1, and compute the<br/>cumulative sum:<br/></p>
<p>In [251]: nsteps = 1000<br/></p>
<p>In [252]: draws = np.random.randint(0, 2, size=nsteps)<br/></p>
<p>In [253]: steps = np.where(draws &gt; 0, 1, -1)<br/></p>
<p>In [254]: walk = steps.cumsum()<br/></p>
<p>From this we can begin to extract statistics like the minimum and maximum value<br/>along the walk&#8217;s trajectory:<br/></p>
<p>In [255]: walk.min()<br/>Out[255]: -3<br/></p>
<p>120 | Chapter 4: NumPy Basics: Arrays and Vectorized Computation</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In [256]: walk.max()<br/>Out[256]: 31<br/></p>
<p>A more complicated statistic is the first crossing time, the step at which the random<br/>walk reaches a particular value. Here we might want to know how long it took the<br/>random walk to get at least 10 steps away from the origin 0 in either direction.<br/>np.abs(walk) &gt;= 10 gives us a boolean array indicating where the walk has reached<br/>or exceeded 10, but we want the index of the first 10 or &#8211;10. Turns out, we can com&#8208;<br/>pute this using argmax, which returns the first index of the maximum value in the<br/>boolean array (True is the maximum value):<br/></p>
<p>In [257]: (np.abs(walk) &gt;= 10).argmax()<br/>Out[257]: 37<br/></p>
<p>Note that using argmax here is not always efficient because it always makes a full scan<br/>of the array. In this special case, once a True is observed we know it to be the maxi&#8208;<br/>mum value.<br/></p>
<p>Simulating Many Random Walks at Once<br/>If your goal was to simulate many random walks, say 5,000 of them, you can generate<br/>all of the random walks with minor modifications to the preceding code. If passed a<br/>2-tuple, the numpy.random functions will generate a two-dimensional array of draws,<br/>and we can compute the cumulative sum across the rows to compute all 5,000 ran&#8208;<br/>dom walks in one shot:<br/></p>
<p>In [258]: nwalks = 5000<br/></p>
<p>In [259]: nsteps = 1000<br/></p>
<p>In [260]: draws = np.random.randint(0, 2, size=(nwalks, nsteps)) <i># 0 or 1<br/></i></p>
<p>In [261]: steps = np.where(draws &gt; 0, 1, -1)<br/></p>
<p>In [262]: walks = steps.cumsum(1)<br/></p>
<p>In [263]: walks<br/>Out[263]: <br/>array([[  1,   0,   1, ...,   8,   7,   8],<br/>       [  1,   0,  -1, ...,  34,  33,  32],<br/>       [  1,   0,  -1, ...,   4,   5,   4],<br/>       ..., <br/>       [  1,   2,   1, ...,  24,  25,  26],<br/>       [  1,   2,   3, ...,  14,  13,  14],<br/>       [ -1,  -2,  -3, ..., -24, -23, -22]])<br/></p>
<p>Now, we can compute the maximum and minimum values obtained over all of the<br/>walks:<br/></p>
<p>4.7 Example: Random Walks | 121</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In [264]: walks.max()<br/>Out[264]: 138<br/></p>
<p>In [265]: walks.min()<br/>Out[265]: -133<br/></p>
<p>Out of these walks, let&#8217;s compute the minimum crossing time to 30 or &#8211;30. This is<br/>slightly tricky because not all 5,000 of them reach 30. We can check this using the any<br/>method:<br/></p>
<p>In [266]: hits30 = (np.abs(walks) &gt;= 30).any(1)<br/></p>
<p>In [267]: hits30<br/>Out[267]: array([False,  True, False, ..., False,  True, False], dtype=bool)<br/></p>
<p>In [268]: hits30.sum() <i># Number that hit 30 or -30<br/></i>Out[268]: 3410<br/></p>
<p>We can use this boolean array to select out the rows of walks that actually cross the<br/>absolute 30 level and call argmax across axis 1 to get the crossing times:<br/></p>
<p>In [269]: crossing_times = (np.abs(walks[hits30]) &gt;= 30).argmax(1)<br/></p>
<p>In [270]: crossing_times.mean()<br/>Out[270]: 498.88973607038122<br/></p>
<p>Feel free to experiment with other distributions for the steps other than equal-sized<br/>coin flips. You need only use a different random number generation function, like<br/>normal to generate normally distributed steps with some mean and standard <br/>deviation:<br/></p>
<p>In [271]: steps = np.random.normal(loc=0, scale=0.25,<br/>   .....:                          size=(nwalks, nsteps))<br/></p>
<p>4.8 Conclusion<br/>While much of the rest of the book will focus on building data wrangling skills with<br/>pandas, we will continue to work in a similar array-based style. In Appendix A, we<br/>will dig deeper into NumPy features to help you further develop your array comput&#8208;<br/>ing skills.<br/></p>
<p>122 | Chapter 4: NumPy Basics: Arrays and Vectorized Computation</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>CHAPTER 5<br/>Getting Started with pandas<br/></p>
<p>pandas will be a major tool of interest throughout much of the rest of the book. It<br/>contains data structures and data manipulation tools designed to make data cleaning<br/>and analysis fast and easy in Python. pandas is often used in tandem with numerical<br/>computing tools like NumPy and SciPy, analytical libraries like statsmodels and<br/>scikit-learn, and data visualization libraries like matplotlib. pandas adopts significant<br/>parts of NumPy&#8217;s idiomatic style of array-based computing, especially array-based<br/>functions and a preference for data processing without for loops.<br/>While pandas adopts many coding idioms from NumPy, the biggest difference is that<br/>pandas is designed for working with tabular or heterogeneous data. NumPy, by con&#8208;<br/>trast, is best suited for working with homogeneous numerical array data.<br/>Since becoming an open source project in 2010, pandas has matured into a quite<br/>large library that&#8217;s applicable in a broad set of real-world use cases. The developer<br/>community has grown to over 800 distinct contributors, who&#8217;ve been helping build<br/>the project as they&#8217;ve used it to solve their day-to-day data problems.<br/>Throughout the rest of the book, I use the following import convention for pandas:<br/></p>
<p>In [1]: <b>import</b> <b>pandas</b> <b>as</b> <b>pd<br/></b></p>
<p>Thus, whenever you see pd. in code, it&#8217;s referring to pandas. You may also find it eas&#8208;<br/>ier to import Series and DataFrame into the local namespace since they are so fre&#8208;<br/>quently used:<br/></p>
<p>In [2]: <b>from</b> <b>pandas</b> <b>import</b> Series, DataFrame<br/></p>
<p>123</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>5.1 Introduction to pandas Data Structures<br/>To get started with pandas, you will need to get comfortable with its two workhorse<br/>data structures: <i>Series</i> and <i>DataFrame</i>. While they are not a universal solution for<br/>every problem, they provide a solid, easy-to-use basis for most applications.<br/></p>
<p>Series<br/>A Series is a one-dimensional array-like object containing a sequence of values (of<br/>similar types to NumPy types) and an associated array of data labels, called its <i>index</i>.<br/>The simplest Series is formed from only an array of data:<br/></p>
<p>In [11]: obj = pd.Series([4, 7, -5, 3])<br/></p>
<p>In [12]: obj<br/>Out[12]: <br/>0    4<br/>1    7<br/>2   -5<br/>3    3<br/>dtype: int64<br/></p>
<p>The string representation of a Series displayed interactively shows the index on the<br/>left and the values on the right. Since we did not specify an index for the data, a<br/>default one consisting of the integers 0 through N - 1 (where N is the length of the<br/>data) is created. You can get the array representation and index object of the Series via<br/>its values and index attributes, respectively:<br/></p>
<p>In [13]: obj.values<br/>Out[13]: array([ 4,  7, -5,  3])<br/></p>
<p>In [14]: obj.index  <i># like range(4)<br/></i>Out[14]: RangeIndex(start=0, stop=4, step=1)<br/></p>
<p>Often it will be desirable to create a Series with an index identifying each data point<br/>with a label:<br/></p>
<p>In [15]: obj2 = pd.Series([4, 7, -5, 3], index=['d', 'b', 'a', 'c'])<br/></p>
<p>In [16]: obj2<br/>Out[16]: <br/>d    4<br/>b    7<br/>a   -5<br/>c    3<br/>dtype: int64<br/></p>
<p>In [17]: obj2.index<br/>Out[17]: Index(['d', 'b', 'a', 'c'], dtype='object')<br/></p>
<p>124 | Chapter 5: Getting Started with pandas</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Compared with NumPy arrays, you can use labels in the index when selecting single<br/>values or a set of values:<br/></p>
<p>In [18]: obj2['a']<br/>Out[18]: -5<br/></p>
<p>In [19]: obj2['d'] = 6<br/></p>
<p>In [20]: obj2[['c', 'a', 'd']]<br/>Out[20]: <br/>c    3<br/>a   -5<br/>d    6<br/>dtype: int64<br/></p>
<p>Here ['c', 'a', 'd'] is interpreted as a list of indices, even though it contains<br/>strings instead of integers.<br/>Using NumPy functions or NumPy-like operations, such as filtering with a boolean<br/>array, scalar multiplication, or applying math functions, will preserve the index-value<br/>link:<br/></p>
<p>In [21]: obj2[obj2 &gt; 0]<br/>Out[21]: <br/>d    6<br/>b    7<br/>c    3<br/>dtype: int64<br/></p>
<p>In [22]: obj2 * 2<br/>Out[22]: <br/>d    12<br/>b    14<br/>a   -10<br/>c     6<br/>dtype: int64<br/></p>
<p>In [23]: np.exp(obj2)<br/>Out[23]: <br/>d     403.428793<br/>b    1096.633158<br/>a       0.006738<br/>c      20.085537<br/>dtype: float64<br/></p>
<p>Another way to think about a Series is as a fixed-length, ordered dict, as it is a map&#8208;<br/>ping of index values to data values. It can be used in many contexts where you might<br/>use a dict:<br/></p>
<p>In [24]: 'b' <b>in</b> obj2<br/>Out[24]: True<br/></p>
<p>5.1 Introduction to pandas Data Structures | 125</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In [25]: 'e' <b>in</b> obj2<br/>Out[25]: False<br/></p>
<p>Should you have data contained in a Python dict, you can create a Series from it by<br/>passing the dict:<br/></p>
<p>In [26]: sdata = {'Ohio': 35000, 'Texas': 71000, 'Oregon': 16000, 'Utah': 5000}<br/></p>
<p>In [27]: obj3 = pd.Series(sdata)<br/></p>
<p>In [28]: obj3<br/>Out[28]: <br/>Ohio      35000<br/>Oregon    16000<br/>Texas     71000<br/>Utah       5000<br/>dtype: int64<br/></p>
<p>When you are only passing a dict, the index in the resulting Series will have the dict&#8217;s<br/>keys in sorted order. You can override this by passing the dict keys in the order you<br/>want them to appear in the resulting Series:<br/></p>
<p>In [29]: states = ['California', 'Ohio', 'Oregon', 'Texas']<br/></p>
<p>In [30]: obj4 = pd.Series(sdata, index=states)<br/></p>
<p>In [31]: obj4<br/>Out[31]: <br/>California        NaN<br/>Ohio          35000.0<br/>Oregon        16000.0<br/>Texas         71000.0<br/>dtype: float64<br/></p>
<p>Here, three values found in sdata were placed in the appropriate locations, but since<br/>no value for 'California' was found, it appears as NaN (not a number), which is con&#8208;<br/>sidered in pandas to mark missing or <i>NA</i> values. Since 'Utah' was not included in<br/>states, it is excluded from the resulting object.<br/>I will use the terms &#8220;missing&#8221; or &#8220;NA&#8221; interchangeably to refer to missing data. The<br/>isnull and notnull functions in pandas should be used to detect missing data:<br/></p>
<p>In [32]: pd.isnull(obj4)<br/>Out[32]: <br/>California     True<br/>Ohio          False<br/>Oregon        False<br/>Texas         False<br/>dtype: bool<br/></p>
<p>In [33]: pd.notnull(obj4)<br/>Out[33]: <br/></p>
<p>126 | Chapter 5: Getting Started with pandas</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>California    False<br/>Ohio           True<br/>Oregon         True<br/>Texas          True<br/>dtype: bool<br/></p>
<p>Series also has these as instance methods:<br/>In [34]: obj4.isnull()<br/>Out[34]: <br/>California     True<br/>Ohio          False<br/>Oregon        False<br/>Texas         False<br/>dtype: bool<br/></p>
<p>I discuss working with missing data in more detail in Chapter 7.<br/>A useful Series feature for many applications is that it automatically aligns by index<br/>label in arithmetic operations:<br/></p>
<p>In [35]: obj3<br/>Out[35]: <br/>Ohio      35000<br/>Oregon    16000<br/>Texas     71000<br/>Utah       5000<br/>dtype: int64<br/></p>
<p>In [36]: obj4<br/>Out[36]: <br/>California        NaN<br/>Ohio          35000.0<br/>Oregon        16000.0<br/>Texas         71000.0<br/>dtype: float64<br/></p>
<p>In [37]: obj3 + obj4<br/>Out[37]: <br/>California         NaN<br/>Ohio           70000.0<br/>Oregon         32000.0<br/>Texas         142000.0<br/>Utah               NaN<br/>dtype: float64<br/></p>
<p>Data alignment features will be addressed in more detail later. If you have experience<br/>with databases, you can think about this as being similar to a join operation.<br/>Both the Series object itself and its index have a name attribute, which integrates with<br/>other key areas of pandas functionality:<br/></p>
<p>5.1 Introduction to pandas Data Structures | 127</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In [38]: obj4.name = 'population'<br/></p>
<p>In [39]: obj4.index.name = 'state'<br/></p>
<p>In [40]: obj4<br/>Out[40]: <br/>state<br/>California        NaN<br/>Ohio          35000.0<br/>Oregon        16000.0<br/>Texas         71000.0<br/>Name: population, dtype: float64<br/></p>
<p>A Series&#8217;s index can be altered in-place by assignment:<br/>In [41]: obj<br/>Out[41]: <br/>0    4<br/>1    7<br/>2   -5<br/>3    3<br/>dtype: int64<br/></p>
<p>In [42]: obj.index = ['Bob', 'Steve', 'Jeff', 'Ryan']<br/></p>
<p>In [43]: obj<br/>Out[43]: <br/>Bob      4<br/>Steve    7<br/>Jeff    -5<br/>Ryan     3<br/>dtype: int64<br/></p>
<p>DataFrame<br/>A DataFrame represents a rectangular table of data and contains an ordered collec&#8208;<br/>tion of columns, each of which can be a different value type (numeric, string,<br/>boolean, etc.). The DataFrame has both a row and column index; it can be thought of<br/>as a dict of Series all sharing the same index. Under the hood, the data is stored as one<br/>or more two-dimensional blocks rather than a list, dict, or some other collection of<br/>one-dimensional arrays. The exact details of DataFrame&#8217;s internals are outside the<br/>scope of this book.<br/></p>
<p>While a DataFrame is physically two-dimensional, you can use it to<br/>represent higher dimensional data in a tabular format using hier&#8208;<br/>archical indexing, a subject we will discuss in Chapter 8 and an<br/>ingredient in some of the more advanced data-handling features in<br/>pandas.<br/></p>
<p>128 | Chapter 5: Getting Started with pandas</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>There are many ways to construct a DataFrame, though one of the most common is<br/>from a dict of equal-length lists or NumPy arrays:<br/></p>
<p>data = {'state': ['Ohio', 'Ohio', 'Ohio', 'Nevada', 'Nevada', 'Nevada'],<br/>        'year': [2000, 2001, 2002, 2001, 2002, 2003],<br/>        'pop': [1.5, 1.7, 3.6, 2.4, 2.9, 3.2]}<br/>frame = pd.DataFrame(data)<br/></p>
<p>The resulting DataFrame will have its index assigned automatically as with Series, and<br/>the columns are placed in sorted order:<br/></p>
<p>In [45]: frame<br/>Out[45]: <br/>   pop   state  year<br/>0  1.5    Ohio  2000<br/>1  1.7    Ohio  2001<br/>2  3.6    Ohio  2002<br/>3  2.4  Nevada  2001<br/>4  2.9  Nevada  2002<br/>5  3.2  Nevada  2003<br/></p>
<p>If you are using the Jupyter notebook, pandas DataFrame objects will be displayed as<br/>a more browser-friendly HTML table.<br/>For large DataFrames, the head method selects only the first five rows:<br/></p>
<p>In [46]: frame.head()<br/>Out[46]: <br/>   pop   state  year<br/>0  1.5    Ohio  2000<br/>1  1.7    Ohio  2001<br/>2  3.6    Ohio  2002<br/>3  2.4  Nevada  2001<br/>4  2.9  Nevada  2002<br/></p>
<p>If you specify a sequence of columns, the DataFrame&#8217;s columns will be arranged in<br/>that order:<br/></p>
<p>In [47]: pd.DataFrame(data, columns=['year', 'state', 'pop'])<br/>Out[47]: <br/>   year   state  pop<br/>0  2000    Ohio  1.5<br/>1  2001    Ohio  1.7<br/>2  2002    Ohio  3.6<br/>3  2001  Nevada  2.4<br/>4  2002  Nevada  2.9<br/>5  2003  Nevada  3.2<br/></p>
<p>If you pass a column that isn&#8217;t contained in the dict, it will appear with missing values<br/>in the result:<br/></p>
<p>In [48]: frame2 = pd.DataFrame(data, columns=['year', 'state', 'pop', 'debt'],<br/>   ....:                       index=['one', 'two', 'three', 'four',<br/>   ....:                              'five', 'six'])<br/></p>
<p>5.1 Introduction to pandas Data Structures | 129</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In [49]: frame2<br/>Out[49]: <br/>       year   state  pop debt<br/>one    2000    Ohio  1.5  NaN<br/>two    2001    Ohio  1.7  NaN<br/>three  2002    Ohio  3.6  NaN<br/>four   2001  Nevada  2.4  NaN<br/>five   2002  Nevada  2.9  NaN<br/>six    2003  Nevada  3.2  NaN<br/></p>
<p>In [50]: frame2.columns<br/>Out[50]: Index(['year', 'state', 'pop', 'debt'], dtype='object')<br/></p>
<p>A column in a DataFrame can be retrieved as a Series either by dict-like notation or<br/>by attribute:<br/></p>
<p>In [51]: frame2['state']<br/>Out[51]: <br/>one        Ohio<br/>two        Ohio<br/>three      Ohio<br/>four     Nevada<br/>five     Nevada<br/>six      Nevada<br/>Name: state, dtype: object<br/></p>
<p>In [52]: frame2.year<br/>Out[52]: <br/>one      2000<br/>two      2001<br/>three    2002<br/>four     2001<br/>five     2002<br/>six      2003<br/>Name: year, dtype: int64<br/></p>
<p>Attribute-like access (e.g., frame2.year) and tab completion of col&#8208;<br/>umn names in IPython is provided as a convenience.<br/>frame2[column] works for any column name, but frame2.column<br/>only works when the column name is a valid Python variable<br/>name.<br/></p>
<p>Note that the returned Series have the same index as the DataFrame, and their name<br/>attribute has been appropriately set.<br/>Rows can also be retrieved by position or name with the special loc attribute (much<br/>more on this later):<br/></p>
<p>130 | Chapter 5: Getting Started with pandas</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In [53]: frame2.loc['three']<br/>Out[53]: <br/>year     2002<br/>state    Ohio<br/>pop       3.6<br/>debt      NaN<br/>Name: three, dtype: object<br/></p>
<p>Columns can be modified by assignment. For example, the empty 'debt' column<br/>could be assigned a scalar value or an array of values:<br/></p>
<p>In [54]: frame2['debt'] = 16.5<br/></p>
<p>In [55]: frame2<br/>Out[55]: <br/>       year   state  pop  debt<br/>one    2000    Ohio  1.5  16.5<br/>two    2001    Ohio  1.7  16.5<br/>three  2002    Ohio  3.6  16.5<br/>four   2001  Nevada  2.4  16.5<br/>five   2002  Nevada  2.9  16.5<br/>six    2003  Nevada  3.2  16.5<br/></p>
<p>In [56]: frame2['debt'] = np.arange(6.)<br/></p>
<p>In [57]: frame2<br/>Out[57]: <br/>       year   state  pop  debt<br/>one    2000    Ohio  1.5   0.0<br/>two    2001    Ohio  1.7   1.0<br/>three  2002    Ohio  3.6   2.0<br/>four   2001  Nevada  2.4   3.0<br/>five   2002  Nevada  2.9   4.0<br/>six    2003  Nevada  3.2   5.0<br/></p>
<p>When you are assigning lists or arrays to a column, the value&#8217;s length must match the<br/>length of the DataFrame. If you assign a Series, its labels will be realigned exactly to<br/>the DataFrame&#8217;s index, inserting missing values in any holes:<br/></p>
<p>In [58]: val = pd.Series([-1.2, -1.5, -1.7], index=['two', 'four', 'five'])<br/></p>
<p>In [59]: frame2['debt'] = val<br/></p>
<p>In [60]: frame2<br/>Out[60]: <br/>       year   state  pop  debt<br/>one    2000    Ohio  1.5   NaN<br/>two    2001    Ohio  1.7  -1.2<br/>three  2002    Ohio  3.6   NaN<br/>four   2001  Nevada  2.4  -1.5<br/>five   2002  Nevada  2.9  -1.7<br/>six    2003  Nevada  3.2   NaN<br/></p>
<p>5.1 Introduction to pandas Data Structures | 131</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Assigning a column that doesn&#8217;t exist will create a new column. The del keyword will<br/>delete columns as with a dict.<br/>As an example of del, I first add a new column of boolean values where the state<br/>column equals 'Ohio':<br/></p>
<p>In [61]: frame2['eastern'] = frame2.state == 'Ohio'<br/></p>
<p>In [62]: frame2<br/>Out[62]: <br/>       year   state  pop  debt  eastern<br/>one    2000    Ohio  1.5   NaN     True<br/>two    2001    Ohio  1.7  -1.2     True<br/>three  2002    Ohio  3.6   NaN     True<br/>four   2001  Nevada  2.4  -1.5    False<br/>five   2002  Nevada  2.9  -1.7    False<br/>six    2003  Nevada  3.2   NaN    False<br/></p>
<p>New columns cannot be created with the frame2.eastern syntax.<br/></p>
<p>The del method can then be used to remove this column:<br/>In [63]: <b>del</b> frame2['eastern']<br/></p>
<p>In [64]: frame2.columns<br/>Out[64]: Index(['year', 'state', 'pop', 'debt'], dtype='object')<br/></p>
<p>The column returned from indexing a DataFrame is a <i>view</i> on the<br/>underlying data, not a copy. Thus, any in-place modifications to the<br/>Series will be reflected in the DataFrame. The column can be<br/>explicitly copied with the Series&#8217;s copy method.<br/></p>
<p>Another common form of data is a nested dict of dicts:<br/>In [65]: pop = {'Nevada': {2001: 2.4, 2002: 2.9},<br/>   ....:        'Ohio': {2000: 1.5, 2001: 1.7, 2002: 3.6}}<br/></p>
<p>If the nested dict is passed to the DataFrame, pandas will interpret the outer dict keys<br/>as the columns and the inner keys as the row indices:<br/></p>
<p>In [66]: frame3 = pd.DataFrame(pop)<br/></p>
<p>In [67]: frame3<br/>Out[67]: <br/>      Nevada  Ohio<br/>2000     NaN   1.5<br/></p>
<p>132 | Chapter 5: Getting Started with pandas</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>2001     2.4   1.7<br/>2002     2.9   3.6<br/></p>
<p>You can transpose the DataFrame (swap rows and columns) with similar syntax to a<br/>NumPy array:<br/></p>
<p>In [68]: frame3.T<br/>Out[68]: <br/>        2000  2001  2002<br/>Nevada   NaN   2.4   2.9<br/>Ohio     1.5   1.7   3.6<br/></p>
<p>The keys in the inner dicts are combined and sorted to form the index in the result.<br/>This isn&#8217;t true if an explicit index is specified:<br/></p>
<p>In [69]: pd.DataFrame(pop, index=[2001, 2002, 2003])<br/>Out[69]: <br/>      Nevada  Ohio<br/>2001     2.4   1.7<br/>2002     2.9   3.6<br/>2003     NaN   NaN<br/></p>
<p>Dicts of Series are treated in much the same way:<br/>In [70]: pdata = {'Ohio': frame3['Ohio'][:-1],<br/>   ....:          'Nevada': frame3['Nevada'][:2]}<br/></p>
<p>In [71]: pd.DataFrame(pdata)<br/>Out[71]: <br/>      Nevada  Ohio<br/>2000     NaN   1.5<br/>2001     2.4   1.7<br/></p>
<p>For a complete list of things you can pass the DataFrame constructor, see Table 5-1.<br/>If a DataFrame&#8217;s index and columns have their name attributes set, these will also be<br/>displayed:<br/></p>
<p>In [72]: frame3.index.name = 'year'; frame3.columns.name = 'state'<br/></p>
<p>In [73]: frame3<br/>Out[73]: <br/>state  Nevada  Ohio<br/>year               <br/>2000      NaN   1.5<br/>2001      2.4   1.7<br/>2002      2.9   3.6<br/></p>
<p>As with Series, the values attribute returns the data contained in the DataFrame as a<br/>two-dimensional ndarray:<br/></p>
<p>In [74]: frame3.values<br/>Out[74]: <br/>array([[ nan,  1.5],<br/></p>
<p>5.1 Introduction to pandas Data Structures | 133</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>       [ 2.4,  1.7],<br/>       [ 2.9,  3.6]])<br/></p>
<p>If the DataFrame&#8217;s columns are different dtypes, the dtype of the values array will be<br/>chosen to accommodate all of the columns:<br/></p>
<p>In [75]: frame2.values<br/>Out[75]: <br/>array([[2000, 'Ohio', 1.5, nan],<br/>       [2001, 'Ohio', 1.7, -1.2],<br/>       [2002, 'Ohio', 3.6, nan],<br/>       [2001, 'Nevada', 2.4, -1.5],<br/>       [2002, 'Nevada', 2.9, -1.7],<br/>       [2003, 'Nevada', 3.2, nan]], dtype=object)<br/></p>
<p><i>Table 5-1. Possible data inputs to DataFrame constructor<br/></i>Type Notes<br/>2D ndarray A matrix of data, passing optional row and column labels<br/>dict of arrays, lists, or tuples Each sequence becomes a column in the DataFrame; all sequences must be the same length<br/>NumPy structured/record<br/>array<br/></p>
<p>Treated as the &#8220;dict of arrays&#8221; case<br/></p>
<p>dict of Series Each value becomes a column; indexes from each Series are unioned together to form the<br/>result&#8217;s row index if no explicit index is passed<br/></p>
<p>dict of dicts Each inner dict becomes a column; keys are unioned to form the row index as in the &#8220;dict of<br/>Series&#8221; case<br/></p>
<p>List of dicts or Series Each item becomes a row in the DataFrame; union of dict keys or Series indexes become the<br/>DataFrame&#8217;s column labels<br/></p>
<p>List of lists or tuples Treated as the &#8220;2D ndarray&#8221; case<br/>Another DataFrame The DataFrame&#8217;s indexes are used unless different ones are passed<br/>NumPy MaskedArray Like the &#8220;2D ndarray&#8221; case except masked values become NA/missing in the DataFrame result<br/></p>
<p>Index Objects<br/>pandas&#8217;s Index objects are responsible for holding the axis labels and other metadata<br/>(like the axis name or names). Any array or other sequence of labels you use when<br/>constructing a Series or DataFrame is internally converted to an Index:<br/></p>
<p>In [76]: obj = pd.Series(range(3), index=['a', 'b', 'c'])<br/></p>
<p>In [77]: index = obj.index<br/></p>
<p>In [78]: index<br/>Out[78]: Index(['a', 'b', 'c'], dtype='object')<br/></p>
<p>In [79]: index[1:]<br/>Out[79]: Index(['b', 'c'], dtype='object')<br/></p>
<p>Index objects are immutable and thus can&#8217;t be modified by the user:<br/></p>
<p>134 | Chapter 5: Getting Started with pandas</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>index[1] = 'd'  <i># TypeError<br/></i></p>
<p>Immutability makes it safer to share Index objects among data structures:<br/>In [80]: labels = pd.Index(np.arange(3))<br/></p>
<p>In [81]: labels<br/>Out[81]: Int64Index([0, 1, 2], dtype='int64')<br/></p>
<p>In [82]: obj2 = pd.Series([1.5, -2.5, 0], index=labels)<br/></p>
<p>In [83]: obj2<br/>Out[83]: <br/>0    1.5<br/>1   -2.5<br/>2    0.0<br/>dtype: float64<br/></p>
<p>In [84]: obj2.index <b>is</b> labels<br/>Out[84]: True<br/></p>
<p>Some users will not often take advantage of the capabilities pro&#8208;<br/>vided by indexes, but because some operations will yield results<br/>containing indexed data, it&#8217;s important to understand how they<br/>work.<br/></p>
<p>In addition to being array-like, an Index also behaves like a fixed-size set:<br/>In [85]: frame3<br/>Out[85]: <br/>state  Nevada  Ohio<br/>year               <br/>2000      NaN   1.5<br/>2001      2.4   1.7<br/>2002      2.9   3.6<br/></p>
<p>In [86]: frame3.columns<br/>Out[86]: Index(['Nevada', 'Ohio'], dtype='object', name='state')<br/></p>
<p>In [87]: 'Ohio' <b>in</b> frame3.columns<br/>Out[87]: True<br/></p>
<p>In [88]: 2003 <b>in</b> frame3.index<br/>Out[88]: False<br/></p>
<p>Unlike Python sets, a pandas Index can contain duplicate labels:<br/>In [89]: dup_labels = pd.Index(['foo', 'foo', 'bar', 'bar'])<br/></p>
<p>In [90]: dup_labels<br/>Out[90]: Index(['foo', 'foo', 'bar', 'bar'], dtype='object')<br/></p>
<p>5.1 Introduction to pandas Data Structures | 135</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Selections with duplicate labels will select all occurrences of that label.<br/>Each Index has a number of methods and properties for set logic, which answer other<br/>common questions about the data it contains. Some useful ones are summarized in<br/>Table 5-2.<br/><i>Table 5-2. Some Index methods and properties<br/></i></p>
<p>Method Description<br/>append Concatenate with additional Index objects, producing a new Index<br/>difference Compute set difference as an Index<br/>intersection Compute set intersection<br/>union Compute set union<br/>isin Compute boolean array indicating whether each value is contained in the passed collection<br/>delete Compute new Index with element at index i deleted<br/>drop Compute new Index by deleting passed values<br/>insert Compute new Index by inserting element at index i<br/>is_monotonic Returns True if each element is greater than or equal to the previous element<br/>is_unique Returns True if the Index has no duplicate values<br/>unique Compute the array of unique values in the Index<br/></p>
<p>5.2 Essential Functionality<br/>This section will walk you through the fundamental mechanics of interacting with the<br/>data contained in a Series or DataFrame. In the chapters to come, we will delve more<br/>deeply into data analysis and manipulation topics using pandas. This book is not<br/>intended to serve as exhaustive documentation for the pandas library; instead, we&#8217;ll<br/>focus on the most important features, leaving the less common (i.e., more esoteric)<br/>things for you to explore on your own.<br/></p>
<p>Reindexing<br/>An important method on pandas objects is reindex, which means to create a new<br/>object with the data <i>conformed</i> to a new index. Consider an example:<br/></p>
<p>In [91]: obj = pd.Series([4.5, 7.2, -5.3, 3.6], index=['d', 'b', 'a', 'c'])<br/></p>
<p>In [92]: obj<br/>Out[92]: <br/>d    4.5<br/>b    7.2<br/>a   -5.3<br/>c    3.6<br/>dtype: float64<br/></p>
<p>136 | Chapter 5: Getting Started with pandas</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Calling reindex on this Series rearranges the data according to the new index, intro&#8208;<br/>ducing missing values if any index values were not already present:<br/></p>
<p>In [93]: obj2 = obj.reindex(['a', 'b', 'c', 'd', 'e'])<br/></p>
<p>In [94]: obj2<br/>Out[94]: <br/>a   -5.3<br/>b    7.2<br/>c    3.6<br/>d    4.5<br/>e    NaN<br/>dtype: float64<br/></p>
<p>For ordered data like time series, it may be desirable to do some interpolation or fill&#8208;<br/>ing of values when reindexing. The method option allows us to do this, using a<br/>method such as ffill, which forward-fills the values:<br/></p>
<p>In [95]: obj3 = pd.Series(['blue', 'purple', 'yellow'], index=[0, 2, 4])<br/></p>
<p>In [96]: obj3<br/>Out[96]: <br/>0      blue<br/>2    purple<br/>4    yellow<br/>dtype: object<br/></p>
<p>In [97]: obj3.reindex(range(6), method='ffill')<br/>Out[97]: <br/>0      blue<br/>1      blue<br/>2    purple<br/>3    purple<br/>4    yellow<br/>5    yellow<br/>dtype: object<br/></p>
<p>With DataFrame, reindex can alter either the (row) index, columns, or both. When<br/>passed only a sequence, it reindexes the rows in the result:<br/></p>
<p>In [98]: frame = pd.DataFrame(np.arange(9).reshape((3, 3)),<br/>   ....:                      index=['a', 'c', 'd'],<br/>   ....:                      columns=['Ohio', 'Texas', 'California'])<br/></p>
<p>In [99]: frame<br/>Out[99]: <br/>   Ohio  Texas  California<br/>a     0      1           2<br/>c     3      4           5<br/>d     6      7           8<br/></p>
<p>In [100]: frame2 = frame.reindex(['a', 'b', 'c', 'd'])<br/></p>
<p>5.2 Essential Functionality | 137</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In [101]: frame2<br/>Out[101]: <br/>   Ohio  Texas  California<br/>a   0.0    1.0         2.0<br/>b   NaN    NaN         NaN<br/>c   3.0    4.0         5.0<br/>d   6.0    7.0         8.0<br/></p>
<p>The columns can be reindexed with the columns keyword:<br/>In [102]: states = ['Texas', 'Utah', 'California']<br/></p>
<p>In [103]: frame.reindex(columns=states)<br/>Out[103]: <br/>   Texas  Utah  California<br/>a      1   NaN           2<br/>c      4   NaN           5<br/>d      7   NaN           8<br/></p>
<p>See Table 5-3 for more about the arguments to reindex.<br/>As we&#8217;ll explore in more detail, you can reindex more succinctly by label-indexing<br/>with loc, and many users prefer to use it exclusively:<br/></p>
<p>In [104]: frame.loc[['a', 'b', 'c', 'd'], states]<br/>Out[104]: <br/>   Texas  Utah  California<br/>a    1.0   NaN         2.0<br/>b    NaN   NaN         NaN<br/>c    4.0   NaN         5.0<br/>d    7.0   NaN         8.0<br/></p>
<p><i>Table 5-3. reindex function arguments<br/></i>Argument Description<br/>index New sequence to use as index. Can be Index instance or any other sequence-like Python data structure. An<br/></p>
<p>Index will be used exactly as is without any copying.<br/>method Interpolation (fill) method; 'ffill' fills forward, while 'bfill' fills backward.<br/>fill_value Substitute value to use when introducing missing data by reindexing.<br/>limit When forward- or backfilling, maximum size gap (in number of elements) to fill.<br/>tolerance When forward- or backfilling, maximum size gap (in absolute numeric distance) to fill for inexact matches.<br/>level Match simple Index on level of MultiIndex; otherwise select subset of.<br/>copy If True, always copy underlying data even if new index is equivalent to old index; if False, do not copy<br/></p>
<p>the data when the indexes are equivalent.<br/></p>
<p>Dropping Entries from an Axis<br/>Dropping one or more entries from an axis is easy if you already have an index array<br/>or list without those entries. As that can require a bit of munging and set logic, the<br/></p>
<p>138 | Chapter 5: Getting Started with pandas</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>drop method will return a new object with the indicated value or values deleted from<br/>an axis:<br/></p>
<p>In [105]: obj = pd.Series(np.arange(5.), index=['a', 'b', 'c', 'd', 'e'])<br/></p>
<p>In [106]: obj<br/>Out[106]: <br/>a    0.0<br/>b    1.0<br/>c    2.0<br/>d    3.0<br/>e    4.0<br/>dtype: float64<br/></p>
<p>In [107]: new_obj = obj.drop('c')<br/></p>
<p>In [108]: new_obj<br/>Out[108]: <br/>a    0.0<br/>b    1.0<br/>d    3.0<br/>e    4.0<br/>dtype: float64<br/></p>
<p>In [109]: obj.drop(['d', 'c'])<br/>Out[109]: <br/>a    0.0<br/>b    1.0<br/>e    4.0<br/>dtype: float64<br/></p>
<p>With DataFrame, index values can be deleted from either axis. To illustrate this, we<br/>first create an example DataFrame:<br/></p>
<p>In [110]: data = pd.DataFrame(np.arange(16).reshape((4, 4)),<br/>   .....:                     index=['Ohio', 'Colorado', 'Utah', 'New York'],<br/>   .....:                     columns=['one', 'two', 'three', 'four'])<br/></p>
<p>In [111]: data<br/>Out[111]: <br/>          one  two  three  four<br/>Ohio        0    1      2     3<br/>Colorado    4    5      6     7<br/>Utah        8    9     10    11<br/>New York   12   13     14    15<br/></p>
<p>Calling drop with a sequence of labels will drop values from the row labels (axis 0):<br/>In [112]: data.drop(['Colorado', 'Ohio'])<br/>Out[112]: <br/>          one  two  three  four<br/>Utah        8    9     10    11<br/>New York   12   13     14    15<br/></p>
<p>5.2 Essential Functionality | 139</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>You can drop values from the columns by passing axis=1 or axis='columns':<br/>In [113]: data.drop('two', axis=1)<br/>Out[113]: <br/>          one  three  four<br/>Ohio        0      2     3<br/>Colorado    4      6     7<br/>Utah        8     10    11<br/>New York   12     14    15<br/></p>
<p>In [114]: data.drop(['two', 'four'], axis='columns')<br/>Out[114]: <br/>          one  three<br/>Ohio        0      2<br/>Colorado    4      6<br/>Utah        8     10<br/>New York   12     14<br/></p>
<p>Many functions, like drop, which modify the size or shape of a Series or DataFrame,<br/>can manipulate an object <i>in-place</i> without returning a new object:<br/></p>
<p>In [115]: obj.drop('c', inplace=True)<br/></p>
<p>In [116]: obj<br/>Out[116]: <br/>a    0.0<br/>b    1.0<br/>d    3.0<br/>e    4.0<br/>dtype: float64<br/></p>
<p>Be careful with the inplace, as it destroys any data that is dropped.<br/></p>
<p>Indexing, Selection, and Filtering<br/>Series indexing (obj[...]) works analogously to NumPy array indexing, except you<br/>can use the Series&#8217;s index values instead of only integers. Here are some examples of<br/>this:<br/></p>
<p>In [117]: obj = pd.Series(np.arange(4.), index=['a', 'b', 'c', 'd'])<br/></p>
<p>In [118]: obj<br/>Out[118]: <br/>a    0.0<br/>b    1.0<br/>c    2.0<br/>d    3.0<br/>dtype: float64<br/></p>
<p>In [119]: obj['b']<br/>Out[119]: 1.0<br/></p>
<p>140 | Chapter 5: Getting Started with pandas</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In [120]: obj[1]<br/>Out[120]: 1.0<br/></p>
<p>In [121]: obj[2:4]<br/>Out[121]: <br/>c    2.0<br/>d    3.0<br/>dtype: float64<br/></p>
<p>In [122]: obj[['b', 'a', 'd']]<br/>Out[122]: <br/>b    1.0<br/>a    0.0<br/>d    3.0<br/>dtype: float64<br/></p>
<p>In [123]: obj[[1, 3]]<br/>Out[123]: <br/>b    1.0<br/>d    3.0<br/>dtype: float64<br/></p>
<p>In [124]: obj[obj &lt; 2]<br/>Out[124]: <br/>a    0.0<br/>b    1.0<br/>dtype: float64<br/></p>
<p>Slicing with labels behaves differently than normal Python slicing in that the end&#8208;<br/>point is inclusive:<br/></p>
<p>In [125]: obj['b':'c']<br/>Out[125]: <br/>b    1.0<br/>c    2.0<br/>dtype: float64<br/></p>
<p><i>Setting</i> using these methods modifies the corresponding section of the Series:<br/>In [126]: obj['b':'c'] = 5<br/></p>
<p>In [127]: obj<br/>Out[127]: <br/>a    0.0<br/>b    5.0<br/>c    5.0<br/>d    3.0<br/>dtype: float64<br/></p>
<p>Indexing into a DataFrame is for retrieving one or more columns either with a single<br/>value or sequence:<br/></p>
<p>5.2 Essential Functionality | 141</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In [128]: data = pd.DataFrame(np.arange(16).reshape((4, 4)),<br/>   .....:                     index=['Ohio', 'Colorado', 'Utah', 'New York'],<br/>   .....:                     columns=['one', 'two', 'three', 'four'])<br/></p>
<p>In [129]: data<br/>Out[129]: <br/>          one  two  three  four<br/>Ohio        0    1      2     3<br/>Colorado    4    5      6     7<br/>Utah        8    9     10    11<br/>New York   12   13     14    15<br/></p>
<p>In [130]: data['two']<br/>Out[130]: <br/>Ohio         1<br/>Colorado     5<br/>Utah         9<br/>New York    13<br/>Name: two, dtype: int64<br/></p>
<p>In [131]: data[['three', 'one']]<br/>Out[131]: <br/>          three  one<br/>Ohio          2    0<br/>Colorado      6    4<br/>Utah         10    8<br/>New York     14   12<br/></p>
<p>Indexing like this has a few special cases. First, slicing or selecting data with a boolean<br/>array:<br/></p>
<p>In [132]: data[:2]<br/>Out[132]: <br/>          one  two  three  four<br/>Ohio        0    1      2     3<br/>Colorado    4    5      6     7<br/></p>
<p>In [133]: data[data['three'] &gt; 5]<br/>Out[133]: <br/>          one  two  three  four<br/>Colorado    4    5      6     7<br/>Utah        8    9     10    11<br/>New York   12   13     14    15<br/></p>
<p>The row selection syntax data[:2] is provided as a convenience. Passing a single ele&#8208;<br/>ment or a list to the [] operator selects columns.<br/>Another use case is in indexing with a boolean DataFrame, such as one produced by a<br/>scalar comparison:<br/></p>
<p>142 | Chapter 5: Getting Started with pandas</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In [134]: data &lt; 5<br/>Out[134]: <br/>            one    two  three   four<br/>Ohio       True   True   True   True<br/>Colorado   True  False  False  False<br/>Utah      False  False  False  False<br/>New York  False  False  False  False<br/></p>
<p>In [135]: data[data &lt; 5] = 0<br/></p>
<p>In [136]: data<br/>Out[136]: <br/>          one  two  three  four<br/>Ohio        0    0      0     0<br/>Colorado    0    5      6     7<br/>Utah        8    9     10    11<br/>New York   12   13     14    15<br/></p>
<p>This makes DataFrame syntactically more like a two-dimensional NumPy array in<br/>this particular case.<br/></p>
<p>Selection with loc and iloc<br/>For DataFrame label-indexing on the rows, I introduce the special indexing operators<br/>loc and iloc. They enable you to select a subset of the rows and columns from a<br/>DataFrame with NumPy-like notation using either axis labels (loc) or integers<br/>(iloc).<br/>As a preliminary example, let&#8217;s select a single row and multiple columns by label:<br/></p>
<p>In [137]: data.loc['Colorado', ['two', 'three']]<br/>Out[137]: <br/>two      5<br/>three    6<br/>Name: Colorado, dtype: int64<br/></p>
<p>We&#8217;ll then perform some similar selections with integers using iloc:<br/>In [138]: data.iloc[2, [3, 0, 1]]<br/>Out[138]: <br/>four    11<br/>one      8<br/>two      9<br/>Name: Utah, dtype: int64<br/></p>
<p>In [139]: data.iloc[2]<br/>Out[139]: <br/>one       8<br/>two       9<br/>three    10<br/>four     11<br/>Name: Utah, dtype: int64<br/></p>
<p>5.2 Essential Functionality | 143</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In [140]: data.iloc[[1, 2], [3, 0, 1]]<br/>Out[140]: <br/>          four  one  two<br/>Colorado     7    0    5<br/>Utah        11    8    9<br/></p>
<p>Both indexing functions work with slices in addition to single labels or lists of labels:<br/>In [141]: data.loc[:'Utah', 'two']<br/>Out[141]: <br/>Ohio        0<br/>Colorado    5<br/>Utah        9<br/>Name: two, dtype: int64<br/></p>
<p>In [142]: data.iloc[:, :3][data.three &gt; 5]<br/>Out[142]: <br/>          one  two  three<br/>Colorado    0    5      6<br/>Utah        8    9     10<br/>New York   12   13     14<br/></p>
<p>So there are many ways to select and rearrange the data contained in a pandas object.<br/>For DataFrame, Table 5-4 provides a short summary of many of them. As you&#8217;ll see<br/>later, there are a number of additional options for working with hierarchical indexes.<br/></p>
<p>When originally designing pandas, I felt that having to type<br/>frame[:, col] to select a column was too verbose (and error-<br/>prone), since column selection is one of the most common opera&#8208;<br/>tions. I made the design trade-off to push all of the fancy indexing<br/>behavior (both labels and integers) into the ix operator. In practice,<br/>this led to many edge cases in data with integer axis labels, so the<br/>pandas team decided to create the loc and iloc operators to deal<br/>with strictly label-based and integer-based indexing, respectively.<br/>The ix indexing operator still exists, but it is deprecated. I do not<br/>recommend using it.<br/></p>
<p><i>Table 5-4. Indexing options with DataFrame<br/></i>Type Notes<br/>df[val] Select single column or sequence of columns from the DataFrame; special case<br/></p>
<p>conveniences: boolean array (filter rows), slice (slice rows), or boolean DataFrame<br/>(set values based on some criterion)<br/></p>
<p>df.loc[val] Selects single row or subset of rows from the DataFrame by label<br/>df.loc[:, val] Selects single column or subset of columns by label<br/>df.loc[val1, val2] Select both rows and columns by label<br/>df.iloc[where] Selects single row or subset of rows from the DataFrame by integer position<br/></p>
<p>144 | Chapter 5: Getting Started with pandas</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Type Notes<br/>df.iloc[:, where] Selects single column or subset of columns by integer position<br/>df.iloc[where_i, where_j] Select both rows and columns by integer position<br/>df.at[label_i, label_j] Select a single scalar value by row and column label<br/>df.iat[i, j] Select a single scalar value by row and column position (integers)<br/>reindex method Select either rows or columns by labels<br/>get_value, set_value methods Select single value by row and column label<br/></p>
<p>Integer Indexes<br/>Working with pandas objects indexed by integers is something that often trips up<br/>new users due to some differences with indexing semantics on built-in Python data<br/>structures like lists and tuples. For example, you might not expect the following code<br/>to generate an error:<br/></p>
<p>ser = pd.Series(np.arange(3.))<br/>ser<br/>ser[-1]<br/></p>
<p>In this case, pandas could &#8220;fall back&#8221; on integer indexing, but it&#8217;s difficult to do this in<br/>general without introducing subtle bugs. Here we have an index containing 0, 1, 2,<br/>but inferring what the user wants (label-based indexing or position-based) is difficult:<br/></p>
<p>In [144]: ser<br/>Out[144]: <br/>0    0.0<br/>1    1.0<br/>2    2.0<br/>dtype: float64<br/></p>
<p>On the other hand, with a non-integer index, there is no potential for ambiguity:<br/>In [145]: ser2 = pd.Series(np.arange(3.), index=['a', 'b', 'c'])<br/></p>
<p>In [146]: ser2[-1]<br/>Out[146]: 2.0<br/></p>
<p>To keep things consistent, if you have an axis index containing integers, data selection<br/>will always be label-oriented. For more precise handling, use loc (for labels) or iloc<br/>(for integers):<br/></p>
<p>In [147]: ser[:1]<br/>Out[147]: <br/>0    0.0<br/>dtype: float64<br/></p>
<p>In [148]: ser.loc[:1]<br/>Out[148]: <br/>0    0.0<br/>1    1.0<br/></p>
<p>5.2 Essential Functionality | 145</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>dtype: float64<br/></p>
<p>In [149]: ser.iloc[:1]<br/>Out[149]: <br/>0    0.0<br/>dtype: float64<br/></p>
<p>Arithmetic and Data Alignment<br/>An important pandas feature for some applications is the behavior of arithmetic<br/>between objects with different indexes. When you are adding together objects, if any<br/>index pairs are not the same, the respective index in the result will be the union of the<br/>index pairs. For users with database experience, this is similar to an automatic outer<br/>join on the index labels. Let&#8217;s look at an example:<br/></p>
<p>In [150]: s1 = pd.Series([7.3, -2.5, 3.4, 1.5], index=['a', 'c', 'd', 'e'])<br/></p>
<p>In [151]: s2 = pd.Series([-2.1, 3.6, -1.5, 4, 3.1],<br/>   .....:                index=['a', 'c', 'e', 'f', 'g'])<br/></p>
<p>In [152]: s1<br/>Out[152]: <br/>a    7.3<br/>c   -2.5<br/>d    3.4<br/>e    1.5<br/>dtype: float64<br/></p>
<p>In [153]: s2<br/>Out[153]: <br/>a   -2.1<br/>c    3.6<br/>e   -1.5<br/>f    4.0<br/>g    3.1<br/>dtype: float64<br/></p>
<p>Adding these together yields:<br/>In [154]: s1 + s2<br/>Out[154]: <br/>a    5.2<br/>c    1.1<br/>d    NaN<br/>e    0.0<br/>f    NaN<br/>g    NaN<br/>dtype: float64<br/></p>
<p>The internal data alignment introduces missing values in the label locations that don&#8217;t<br/>overlap. Missing values will then propagate in further arithmetic computations.<br/></p>
<p>146 | Chapter 5: Getting Started with pandas</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In the case of DataFrame, alignment is performed on both the rows and the columns:<br/>In [155]: df1 = pd.DataFrame(np.arange(9.).reshape((3, 3)), columns=list('bcd'),<br/>   .....:                    index=['Ohio', 'Texas', 'Colorado'])<br/></p>
<p>In [156]: df2 = pd.DataFrame(np.arange(12.).reshape((4, 3)), columns=list('bde'),<br/>   .....:                    index=['Utah', 'Ohio', 'Texas', 'Oregon'])<br/></p>
<p>In [157]: df1<br/>Out[157]: <br/>            b    c    d<br/>Ohio      0.0  1.0  2.0<br/>Texas     3.0  4.0  5.0<br/>Colorado  6.0  7.0  8.0<br/></p>
<p>In [158]: df2<br/>Out[158]: <br/>          b     d     e<br/>Utah    0.0   1.0   2.0<br/>Ohio    3.0   4.0   5.0<br/>Texas   6.0   7.0   8.0<br/>Oregon  9.0  10.0  11.0<br/></p>
<p>Adding these together returns a DataFrame whose index and columns are the unions<br/>of the ones in each DataFrame:<br/></p>
<p>In [159]: df1 + df2<br/>Out[159]: <br/>            b   c     d   e<br/>Colorado  NaN NaN   NaN NaN<br/>Ohio      3.0 NaN   6.0 NaN<br/>Oregon    NaN NaN   NaN NaN<br/>Texas     9.0 NaN  12.0 NaN<br/>Utah      NaN NaN   NaN NaN<br/></p>
<p>Since the 'c' and 'e' columns are not found in both DataFrame objects, they appear<br/>as all missing in the result. The same holds for the rows whose labels are not common<br/>to both objects.<br/>If you add DataFrame objects with no column or row labels in common, the result<br/>will contain all nulls:<br/></p>
<p>In [160]: df1 = pd.DataFrame({'A': [1, 2]})<br/></p>
<p>In [161]: df2 = pd.DataFrame({'B': [3, 4]})<br/></p>
<p>In [162]: df1<br/>Out[162]: <br/>   A<br/>0  1<br/>1  2<br/></p>
<p>In [163]: df2<br/></p>
<p>5.2 Essential Functionality | 147</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Out[163]: <br/>   B<br/>0  3<br/>1  4<br/></p>
<p>In [164]: df1 - df2<br/>Out[164]: <br/>    A   B<br/>0 NaN NaN<br/>1 NaN NaN<br/></p>
<p>Arithmetic methods with fill values<br/>In arithmetic operations between differently indexed objects, you might want to fill<br/>with a special value, like 0, when an axis label is found in one object but not the other:<br/></p>
<p>In [165]: df1 = pd.DataFrame(np.arange(12.).reshape((3, 4)),<br/>   .....:                    columns=list('abcd'))<br/></p>
<p>In [166]: df2 = pd.DataFrame(np.arange(20.).reshape((4, 5)),<br/>   .....:                    columns=list('abcde'))<br/></p>
<p>In [167]: df2.loc[1, 'b'] = np.nan<br/></p>
<p>In [168]: df1<br/>Out[168]: <br/>     a    b     c     d<br/>0  0.0  1.0   2.0   3.0<br/>1  4.0  5.0   6.0   7.0<br/>2  8.0  9.0  10.0  11.0<br/></p>
<p>In [169]: df2<br/>Out[169]: <br/>      a     b     c     d     e<br/>0   0.0   1.0   2.0   3.0   4.0<br/>1   5.0   NaN   7.0   8.0   9.0<br/>2  10.0  11.0  12.0  13.0  14.0<br/>3  15.0  16.0  17.0  18.0  19.0<br/></p>
<p>Adding these together results in NA values in the locations that don&#8217;t overlap:<br/>In [170]: df1 + df2<br/>Out[170]: <br/>      a     b     c     d   e<br/>0   0.0   2.0   4.0   6.0 NaN<br/>1   9.0   NaN  13.0  15.0 NaN<br/>2  18.0  20.0  22.0  24.0 NaN<br/>3   NaN   NaN   NaN   NaN NaN<br/></p>
<p>Using the add method on df1, I pass df2 and an argument to fill_value:<br/>In [171]: df1.add(df2, fill_value=0)<br/>Out[171]: <br/></p>
<p>148 | Chapter 5: Getting Started with pandas</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>      a     b     c     d     e<br/>0   0.0   2.0   4.0   6.0   4.0<br/>1   9.0   5.0  13.0  15.0   9.0<br/>2  18.0  20.0  22.0  24.0  14.0<br/>3  15.0  16.0  17.0  18.0  19.0<br/></p>
<p>See Table 5-5 for a listing of Series and DataFrame methods for arithmetic. Each of<br/>them has a counterpart, starting with the letter r, that has arguments flipped. So these<br/>two statements are equivalent:<br/></p>
<p>In [172]: 1 / df1<br/>Out[172]: <br/>          a         b         c         d<br/>0       inf  1.000000  0.500000  0.333333<br/>1  0.250000  0.200000  0.166667  0.142857<br/>2  0.125000  0.111111  0.100000  0.090909<br/></p>
<p>In [173]: df1.rdiv(1)<br/>Out[173]: <br/>          a         b         c         d<br/>0       inf  1.000000  0.500000  0.333333<br/>1  0.250000  0.200000  0.166667  0.142857<br/>2  0.125000  0.111111  0.100000  0.090909<br/></p>
<p>Relatedly, when reindexing a Series or DataFrame, you can also specify a different fill<br/>value:<br/></p>
<p>In [174]: df1.reindex(columns=df2.columns, fill_value=0)<br/>Out[174]: <br/>     a    b     c     d  e<br/>0  0.0  1.0   2.0   3.0  0<br/>1  4.0  5.0   6.0   7.0  0<br/>2  8.0  9.0  10.0  11.0  0<br/></p>
<p><i>Table 5-5. Flexible arithmetic methods<br/></i>Method Description<br/>add, radd Methods for addition (+)<br/>sub, rsub Methods for subtraction (-)<br/>div, rdiv Methods for division (/)<br/>floordiv, rfloordiv Methods for floor division (//)<br/>mul, rmul Methods for multiplication (*)<br/>pow, rpow Methods for exponentiation (**)<br/></p>
<p>Operations between DataFrame and Series<br/>As with NumPy arrays of different dimensions, arithmetic between DataFrame and<br/>Series is also defined. First, as a motivating example, consider the difference between<br/>a two-dimensional array and one of its rows:<br/></p>
<p>5.2 Essential Functionality | 149</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In [175]: arr = np.arange(12.).reshape((3, 4))<br/></p>
<p>In [176]: arr<br/>Out[176]: <br/>array([[  0.,   1.,   2.,   3.],<br/>       [  4.,   5.,   6.,   7.],<br/>       [  8.,   9.,  10.,  11.]])<br/></p>
<p>In [177]: arr[0]<br/>Out[177]: array([ 0.,  1.,  2.,  3.])<br/></p>
<p>In [178]: arr - arr[0]<br/>Out[178]: <br/>array([[ 0.,  0.,  0.,  0.],<br/>       [ 4.,  4.,  4.,  4.],<br/>       [ 8.,  8.,  8.,  8.]])<br/></p>
<p>When we subtract arr[0] from arr, the subtraction is performed once for each row.<br/>This is referred to as <i>broadcasting</i> and is explained in more detail as it relates to gen&#8208;<br/>eral NumPy arrays in Appendix A. Operations between a DataFrame and a Series are<br/>similar:<br/></p>
<p>In [179]: frame = pd.DataFrame(np.arange(12.).reshape((4, 3)),<br/>   .....:                      columns=list('bde'),<br/>   .....:                      index=['Utah', 'Ohio', 'Texas', 'Oregon'])<br/></p>
<p>In [180]: series = frame.iloc[0]<br/></p>
<p>In [181]: frame<br/>Out[181]: <br/>          b     d     e<br/>Utah    0.0   1.0   2.0<br/>Ohio    3.0   4.0   5.0<br/>Texas   6.0   7.0   8.0<br/>Oregon  9.0  10.0  11.0<br/></p>
<p>In [182]: series<br/>Out[182]: <br/>b    0.0<br/>d    1.0<br/>e    2.0<br/>Name: Utah, dtype: float64<br/></p>
<p>By default, arithmetic between DataFrame and Series matches the index of the Series<br/>on the DataFrame&#8217;s columns, broadcasting down the rows:<br/></p>
<p>In [183]: frame - series<br/>Out[183]: <br/>          b    d    e<br/>Utah    0.0  0.0  0.0<br/>Ohio    3.0  3.0  3.0<br/>Texas   6.0  6.0  6.0<br/>Oregon  9.0  9.0  9.0<br/></p>
<p>150 | Chapter 5: Getting Started with pandas</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>If an index value is not found in either the DataFrame&#8217;s columns or the Series&#8217;s index,<br/>the objects will be reindexed to form the union:<br/></p>
<p>In [184]: series2 = pd.Series(range(3), index=['b', 'e', 'f'])<br/></p>
<p>In [185]: frame + series2<br/>Out[185]: <br/>          b   d     e   f<br/>Utah    0.0 NaN   3.0 NaN<br/>Ohio    3.0 NaN   6.0 NaN<br/>Texas   6.0 NaN   9.0 NaN<br/>Oregon  9.0 NaN  12.0 NaN<br/></p>
<p>If you want to instead broadcast over the columns, matching on the rows, you have to<br/>use one of the arithmetic methods. For example:<br/></p>
<p>In [186]: series3 = frame['d']<br/></p>
<p>In [187]: frame<br/>Out[187]: <br/>          b     d     e<br/>Utah    0.0   1.0   2.0<br/>Ohio    3.0   4.0   5.0<br/>Texas   6.0   7.0   8.0<br/>Oregon  9.0  10.0  11.0<br/></p>
<p>In [188]: series3<br/>Out[188]: <br/>Utah       1.0<br/>Ohio       4.0<br/>Texas      7.0<br/>Oregon    10.0<br/>Name: d, dtype: float64<br/></p>
<p>In [189]: frame.sub(series3, axis='index')<br/>Out[189]: <br/>          b    d    e<br/>Utah   -1.0  0.0  1.0<br/>Ohio   -1.0  0.0  1.0<br/>Texas  -1.0  0.0  1.0<br/>Oregon -1.0  0.0  1.0<br/></p>
<p>The axis number that you pass is the <i>axis to match on</i>. In this case we mean to match<br/>on the DataFrame&#8217;s row index (axis='index' or axis=0) and broadcast across.<br/></p>
<p>Function Application and Mapping<br/>NumPy ufuncs (element-wise array methods) also work with pandas objects:<br/></p>
<p>In [190]: frame = pd.DataFrame(np.random.randn(4, 3), columns=list('bde'),<br/>   .....:                      index=['Utah', 'Ohio', 'Texas', 'Oregon'])<br/></p>
<p>5.2 Essential Functionality | 151</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In [191]: frame<br/>Out[191]: <br/>               b         d         e<br/>Utah   -0.204708  0.478943 -0.519439<br/>Ohio   -0.555730  1.965781  1.393406<br/>Texas   0.092908  0.281746  0.769023<br/>Oregon  1.246435  1.007189 -1.296221<br/></p>
<p>In [192]: np.abs(frame)<br/>Out[192]: <br/>               b         d         e<br/>Utah    0.204708  0.478943  0.519439<br/>Ohio    0.555730  1.965781  1.393406<br/>Texas   0.092908  0.281746  0.769023<br/>Oregon  1.246435  1.007189  1.296221<br/></p>
<p>Another frequent operation is applying a function on one-dimensional arrays to each<br/>column or row. DataFrame&#8217;s apply method does exactly this:<br/></p>
<p>In [193]: f = <b>lambda</b> x: x.max() - x.min()<br/></p>
<p>In [194]: frame.apply(f)<br/>Out[194]: <br/>b    1.802165<br/>d    1.684034<br/>e    2.689627<br/>dtype: float64<br/></p>
<p>Here the function f, which computes the difference between the maximum and mini&#8208;<br/>mum of a Series, is invoked once on each column in frame. The result is a Series hav&#8208;<br/>ing the columns of frame as its index.<br/>If you pass axis='columns' to apply, the function will be invoked once per row<br/>instead:<br/></p>
<p>In [195]: frame.apply(f, axis='columns')<br/>Out[195]: <br/>Utah      0.998382<br/>Ohio      2.521511<br/>Texas     0.676115<br/>Oregon    2.542656<br/>dtype: float64<br/></p>
<p>Many of the most common array statistics (like sum and mean) are DataFrame meth&#8208;<br/>ods, so using apply is not necessary.<br/>The function passed to apply need not return a scalar value; it can also return a Series<br/>with multiple values:<br/></p>
<p>In [196]: <b>def</b> f(x):<br/>   .....:     <b>return</b> pd.Series([x.min(), x.max()], index=['min', 'max'])<br/></p>
<p>In [197]: frame.apply(f)<br/></p>
<p>152 | Chapter 5: Getting Started with pandas</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Out[197]: <br/>            b         d         e<br/>min -0.555730  0.281746 -1.296221<br/>max  1.246435  1.965781  1.393406<br/></p>
<p>Element-wise Python functions can be used, too. Suppose you wanted to compute a<br/>formatted string from each floating-point value in frame. You can do this with apply<br/>map:<br/></p>
<p>In [198]: format = <b>lambda</b> x: '%.2f' % x<br/></p>
<p>In [199]: frame.applymap(format)<br/>Out[199]: <br/>            b     d      e<br/>Utah    -0.20  0.48  -0.52<br/>Ohio    -0.56  1.97   1.39<br/>Texas    0.09  0.28   0.77<br/>Oregon   1.25  1.01  -1.30<br/></p>
<p>The reason for the name applymap is that Series has a map method for applying an<br/>element-wise function:<br/></p>
<p>In [200]: frame['e'].map(format)<br/>Out[200]: <br/>Utah      -0.52<br/>Ohio       1.39<br/>Texas      0.77<br/>Oregon    -1.30<br/>Name: e, dtype: object<br/></p>
<p>Sorting and Ranking<br/>Sorting a dataset by some criterion is another important built-in operation. To sort<br/>lexicographically by row or column index, use the sort_index method, which returns<br/>a new, sorted object:<br/></p>
<p>In [201]: obj = pd.Series(range(4), index=['d', 'a', 'b', 'c'])<br/></p>
<p>In [202]: obj.sort_index()<br/>Out[202]: <br/>a    1<br/>b    2<br/>c    3<br/>d    0<br/>dtype: int64<br/></p>
<p>With a DataFrame, you can sort by index on either axis:<br/>In [203]: frame = pd.DataFrame(np.arange(8).reshape((2, 4)),<br/>   .....:                      index=['three', 'one'],<br/>   .....:                      columns=['d', 'a', 'b', 'c'])<br/></p>
<p>In [204]: frame.sort_index()<br/></p>
<p>5.2 Essential Functionality | 153</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Out[204]: <br/>       d  a  b  c<br/>one    4  5  6  7<br/>three  0  1  2  3<br/></p>
<p>In [205]: frame.sort_index(axis=1)<br/>Out[205]: <br/>       a  b  c  d<br/>three  1  2  3  0<br/>one    5  6  7  4<br/></p>
<p>The data is sorted in ascending order by default, but can be sorted in descending<br/>order, too:<br/></p>
<p>In [206]: frame.sort_index(axis=1, ascending=False)<br/>Out[206]: <br/>       d  c  b  a<br/>three  0  3  2  1<br/>one    4  7  6  5<br/></p>
<p>To sort a Series by its values, use its sort_values method:<br/>In [207]: obj = pd.Series([4, 7, -3, 2])<br/></p>
<p>In [208]: obj.sort_values()<br/>Out[208]: <br/>2   -3<br/>3    2<br/>0    4<br/>1    7<br/>dtype: int64<br/></p>
<p>Any missing values are sorted to the end of the Series by default:<br/>In [209]: obj = pd.Series([4, np.nan, 7, np.nan, -3, 2])<br/></p>
<p>In [210]: obj.sort_values()<br/>Out[210]: <br/>4   -3.0<br/>5    2.0<br/>0    4.0<br/>2    7.0<br/>1    NaN<br/>3    NaN<br/>dtype: float64<br/></p>
<p>When sorting a DataFrame, you can use the data in one or more columns as the sort<br/>keys. To do so, pass one or more column names to the by option of sort_values:<br/></p>
<p>In [211]: frame = pd.DataFrame({'b': [4, 7, -3, 2], 'a': [0, 1, 0, 1]})<br/></p>
<p>In [212]: frame<br/>Out[212]: <br/>   a  b<br/></p>
<p>154 | Chapter 5: Getting Started with pandas</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>0  0  4<br/>1  1  7<br/>2  0 -3<br/>3  1  2<br/></p>
<p>In [213]: frame.sort_values(by='b')<br/>Out[213]: <br/>   a  b<br/>2  0 -3<br/>3  1  2<br/>0  0  4<br/>1  1  7<br/></p>
<p>To sort by multiple columns, pass a list of names:<br/>In [214]: frame.sort_values(by=['a', 'b'])<br/>Out[214]: <br/>   a  b<br/>2  0 -3<br/>0  0  4<br/>3  1  2<br/>1  1  7<br/></p>
<p><i>Ranking</i> assigns ranks from one through the number of valid data points in an array. <br/>The rank methods for Series and DataFrame are the place to look; by default rank<br/>breaks ties by assigning each group the mean rank:<br/></p>
<p>In [215]: obj = pd.Series([7, -5, 7, 4, 2, 0, 4])<br/></p>
<p>In [216]: obj.rank()<br/>Out[216]: <br/>0    6.5<br/>1    1.0<br/>2    6.5<br/>3    4.5<br/>4    3.0<br/>5    2.0<br/>6    4.5<br/>dtype: float64<br/></p>
<p>Ranks can also be assigned according to the order in which they&#8217;re observed in the<br/>data:<br/></p>
<p>In [217]: obj.rank(method='first')<br/>Out[217]: <br/>0    6.0<br/>1    1.0<br/>2    7.0<br/>3    4.0<br/>4    3.0<br/>5    2.0<br/>6    5.0<br/>dtype: float64<br/></p>
<p>5.2 Essential Functionality | 155</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Here, instead of using the average rank 6.5 for the entries 0 and 2, they instead have<br/>been set to 6 and 7 because label 0 precedes label 2 in the data.<br/>You can rank in descending order, too:<br/></p>
<p><i># Assign tie values the maximum rank in the group<br/></i>In [218]: obj.rank(ascending=False, method='max')<br/>Out[218]: <br/>0    2.0<br/>1    7.0<br/>2    2.0<br/>3    4.0<br/>4    5.0<br/>5    6.0<br/>6    4.0<br/>dtype: float64<br/></p>
<p>See Table 5-6 for a list of tie-breaking methods available.<br/>DataFrame can compute ranks over the rows or the columns:<br/></p>
<p>In [219]: frame = pd.DataFrame({'b': [4.3, 7, -3, 2], 'a': [0, 1, 0, 1],<br/>   .....:                       'c': [-2, 5, 8, -2.5]})<br/></p>
<p>In [220]: frame<br/>Out[220]: <br/>   a    b    c<br/>0  0  4.3 -2.0<br/>1  1  7.0  5.0<br/>2  0 -3.0  8.0<br/>3  1  2.0 -2.5<br/></p>
<p>In [221]: frame.rank(axis='columns')<br/>Out[221]: <br/>     a    b    c<br/>0  2.0  3.0  1.0<br/>1  1.0  3.0  2.0<br/>2  2.0  1.0  3.0<br/>3  2.0  3.0  1.0<br/></p>
<p><i>Table 5-6. Tie-breaking methods with rank<br/></i>Method Description<br/>'average' Default: assign the average rank to each entry in the equal group<br/>'min' Use the minimum rank for the whole group<br/>'max' Use the maximum rank for the whole group<br/>'first' Assign ranks in the order the values appear in the data<br/>'dense' Like method='min', but ranks always increase by 1 in between groups rather than the number of equal<br/></p>
<p>elements in a group<br/></p>
<p>156 | Chapter 5: Getting Started with pandas</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Axis Indexes with Duplicate Labels<br/>Up until now all of the examples we&#8217;ve looked at have had unique axis labels (index<br/>values). While many pandas functions (like reindex) require that the labels be<br/>unique, it&#8217;s not mandatory. Let&#8217;s consider a small Series with duplicate indices:<br/></p>
<p>In [222]: obj = pd.Series(range(5), index=['a', 'a', 'b', 'b', 'c'])<br/></p>
<p>In [223]: obj<br/>Out[223]: <br/>a    0<br/>a    1<br/>b    2<br/>b    3<br/>c    4<br/>dtype: int64<br/></p>
<p>The index&#8217;s is_unique property can tell you whether its labels are unique or not:<br/>In [224]: obj.index.is_unique<br/>Out[224]: False<br/></p>
<p>Data selection is one of the main things that behaves differently with duplicates.<br/>Indexing a label with multiple entries returns a Series, while single entries return a<br/>scalar value:<br/></p>
<p>In [225]: obj['a']<br/>Out[225]: <br/>a    0<br/>a    1<br/>dtype: int64<br/></p>
<p>In [226]: obj['c']<br/>Out[226]: 4<br/></p>
<p>This can make your code more complicated, as the output type from indexing can<br/>vary based on whether a label is repeated or not.<br/>The same logic extends to indexing rows in a DataFrame:<br/></p>
<p>In [227]: df = pd.DataFrame(np.random.randn(4, 3), index=['a', 'a', 'b', 'b'])<br/></p>
<p>In [228]: df<br/>Out[228]: <br/>          0         1         2<br/>a  0.274992  0.228913  1.352917<br/>a  0.886429 -2.001637 -0.371843<br/>b  1.669025 -0.438570 -0.539741<br/>b  0.476985  3.248944 -1.021228<br/></p>
<p>In [229]: df.loc['b']<br/>Out[229]: <br/>          0         1         2<br/></p>
<p>5.2 Essential Functionality | 157</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>b  1.669025 -0.438570 -0.539741<br/>b  0.476985  3.248944 -1.021228<br/></p>
<p>5.3 Summarizing and Computing Descriptive Statistics<br/>pandas objects are equipped with a set of common mathematical and statistical meth&#8208;<br/>ods. Most of these fall into the category of <i>reductions</i> or <i>summary statistics</i>, methods<br/>that extract a single value (like the sum or mean) from a Series or a Series of values<br/>from the rows or columns of a DataFrame. Compared with the similar methods<br/>found on NumPy arrays, they have built-in handling for missing data. Consider a<br/>small DataFrame:<br/></p>
<p>In [230]: df = pd.DataFrame([[1.4, np.nan], [7.1, -4.5],<br/>   .....:                    [np.nan, np.nan], [0.75, -1.3]],<br/>   .....:                   index=['a', 'b', 'c', 'd'],<br/>   .....:                   columns=['one', 'two'])<br/></p>
<p>In [231]: df<br/>Out[231]: <br/>    one  two<br/>a  1.40  NaN<br/>b  7.10 -4.5<br/>c   NaN  NaN<br/>d  0.75 -1.3<br/></p>
<p>Calling DataFrame&#8217;s sum method returns a Series containing column sums:<br/>In [232]: df.sum()<br/>Out[232]: <br/>one    9.25<br/>two   -5.80<br/>dtype: float64<br/></p>
<p>Passing axis='columns' or axis=1 sums across the columns instead:<br/>In [233]: df.sum(axis='columns')<br/>Out[233]: <br/>a    1.40<br/>b    2.60<br/>c     NaN<br/>d   -0.55<br/>dtype: float64<br/></p>
<p>NA values are excluded unless the entire slice (row or column in this case) is NA.<br/>This can be disabled with the skipna option:<br/></p>
<p>In [234]: df.mean(axis='columns', skipna=False)<br/>Out[234]: <br/>a      NaN<br/>b    1.300<br/>c      NaN<br/></p>
<p>158 | Chapter 5: Getting Started with pandas</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>d   -0.275<br/>dtype: float64<br/></p>
<p>See Table 5-7 for a list of common options for each reduction method.<br/><i>Table 5-7. Options for reduction methods<br/></i></p>
<p>Method Description<br/>axis Axis to reduce over; 0 for DataFrame&#8217;s rows and 1 for columns<br/>skipna Exclude missing values; True by default<br/>level Reduce grouped by level if the axis is hierarchically indexed (MultiIndex)<br/></p>
<p>Some methods, like idxmin and idxmax, return indirect statistics like the index value<br/>where the minimum or maximum values are attained:<br/></p>
<p>In [235]: df.idxmax()<br/>Out[235]: <br/>one    b<br/>two    d<br/>dtype: object<br/></p>
<p>Other methods are <i>accumulations</i>:<br/>In [236]: df.cumsum()<br/>Out[236]: <br/>    one  two<br/>a  1.40  NaN<br/>b  8.50 -4.5<br/>c   NaN  NaN<br/>d  9.25 -5.8<br/></p>
<p>Another type of method is neither a reduction nor an accumulation. describe is one<br/>such example, producing multiple summary statistics in one shot:<br/></p>
<p>In [237]: df.describe()<br/>Out[237]: <br/>            one       two<br/>count  3.000000  2.000000<br/>mean   3.083333 -2.900000<br/>std    3.493685  2.262742<br/>min    0.750000 -4.500000<br/>25%    1.075000 -3.700000<br/>50%    1.400000 -2.900000<br/>75%    4.250000 -2.100000<br/>max    7.100000 -1.300000<br/></p>
<p>On non-numeric data, describe produces alternative summary statistics:<br/>In [238]: obj = pd.Series(['a', 'a', 'b', 'c'] * 4)<br/></p>
<p>In [239]: obj.describe()<br/>Out[239]: <br/>count     16<br/></p>
<p>5.3 Summarizing and Computing Descriptive Statistics | 159</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>unique     3<br/>top        a<br/>freq       8<br/>dtype: object<br/></p>
<p>See Table 5-8 for a full list of summary statistics and related methods.<br/><i>Table 5-8. Descriptive and summary statistics<br/></i></p>
<p>Method Description<br/>count Number of non-NA values<br/>describe Compute set of summary statistics for Series or each DataFrame column<br/>min, max Compute minimum and maximum values<br/>argmin, argmax Compute index locations (integers) at which minimum or maximum value obtained, respectively<br/>idxmin, idxmax Compute index labels at which minimum or maximum value obtained, respectively<br/>quantile Compute sample quantile ranging from 0 to 1<br/>sum Sum of values<br/>mean Mean of values<br/>median Arithmetic median (50% quantile) of values<br/>mad Mean absolute deviation from mean value<br/>prod Product of all values<br/>var Sample variance of values<br/>std Sample standard deviation of values<br/>skew Sample skewness (third moment) of values<br/>kurt Sample kurtosis (fourth moment) of values<br/>cumsum Cumulative sum of values<br/>cummin, cummax Cumulative minimum or maximum of values, respectively<br/>cumprod Cumulative product of values<br/>diff Compute first arithmetic difference (useful for time series)<br/>pct_change Compute percent changes<br/></p>
<p>Correlation and Covariance<br/>Some summary statistics, like correlation and covariance, are computed from pairs of<br/>arguments. Let&#8217;s consider some DataFrames of stock prices and volumes obtained<br/>from Yahoo! Finance using the add-on pandas-datareader package. If you don&#8217;t<br/>have it installed already, it can be obtained via conda or pip:<br/></p>
<p>conda install pandas-datareader<br/></p>
<p>I use the pandas_datareader module to download some data for a few stock tickers:<br/><b>import</b> <b>pandas_datareader.data</b> <b>as</b> <b>web<br/></b>all_data = {ticker: web.get_data_yahoo(ticker)<br/>            <b>for</b> ticker <b>in</b> ['AAPL', 'IBM', 'MSFT', 'GOOG']}<br/></p>
<p>160 | Chapter 5: Getting Started with pandas</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>price = pd.DataFrame({ticker: data['Adj Close']<br/>                     <b>for</b> ticker, data <b>in</b> all_data.items()})<br/>volume = pd.DataFrame({ticker: data['Volume']<br/>                      <b>for</b> ticker, data <b>in</b> all_data.items()})<br/></p>
<p>It&#8217;s possible by the time you are reading this that Yahoo! Finance no<br/>longer exists since Yahoo! was acquired by Verizon in 2017. Refer<br/>to the pandas-datareader documentation online for the latest<br/>functionality.<br/></p>
<p>I now compute percent changes of the prices, a time series operation which will be<br/>explored further in Chapter 11:<br/></p>
<p>In [242]: returns = price.pct_change()<br/></p>
<p>In [243]: returns.tail()<br/>Out[243]: <br/>                AAPL      GOOG       IBM      MSFT<br/>Date                                              <br/>2016-10-17 -0.000680  0.001837  0.002072 -0.003483<br/>2016-10-18 -0.000681  0.019616 -0.026168  0.007690<br/>2016-10-19 -0.002979  0.007846  0.003583 -0.002255<br/>2016-10-20 -0.000512 -0.005652  0.001719 -0.004867<br/>2016-10-21 -0.003930  0.003011 -0.012474  0.042096<br/></p>
<p>The corr method of Series computes the correlation of the overlapping, non-NA,<br/>aligned-by-index values in two Series. Relatedly, cov computes the covariance:<br/></p>
<p>In [244]: returns['MSFT'].corr(returns['IBM'])<br/>Out[244]: 0.49976361144151144<br/></p>
<p>In [245]: returns['MSFT'].cov(returns['IBM'])<br/>Out[245]: 8.8706554797035462e-05<br/></p>
<p>Since MSFT is a valid Python attribute, we can also select these columns using more<br/>concise syntax:<br/></p>
<p>In [246]: returns.MSFT.corr(returns.IBM)<br/>Out[246]: 0.49976361144151144<br/></p>
<p>DataFrame&#8217;s corr and cov methods, on the other hand, return a full correlation or<br/>covariance matrix as a DataFrame, respectively:<br/></p>
<p>In [247]: returns.corr()<br/>Out[247]: <br/>          AAPL      GOOG       IBM      MSFT<br/>AAPL  1.000000  0.407919  0.386817  0.389695<br/>GOOG  0.407919  1.000000  0.405099  0.465919<br/>IBM   0.386817  0.405099  1.000000  0.499764<br/>MSFT  0.389695  0.465919  0.499764  1.000000<br/></p>
<p>5.3 Summarizing and Computing Descriptive Statistics | 161</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In [248]: returns.cov()<br/>Out[248]: <br/>          AAPL      GOOG       IBM      MSFT<br/>AAPL  0.000277  0.000107  0.000078  0.000095<br/>GOOG  0.000107  0.000251  0.000078  0.000108<br/>IBM   0.000078  0.000078  0.000146  0.000089<br/>MSFT  0.000095  0.000108  0.000089  0.000215<br/></p>
<p>Using DataFrame&#8217;s corrwith method, you can compute pairwise correlations<br/>between a DataFrame&#8217;s columns or rows with another Series or DataFrame. Passing a<br/>Series returns a Series with the correlation value computed for each column:<br/></p>
<p>In [249]: returns.corrwith(returns.IBM)<br/>Out[249]: <br/>AAPL    0.386817<br/>GOOG    0.405099<br/>IBM     1.000000<br/>MSFT    0.499764<br/>dtype: float64<br/></p>
<p>Passing a DataFrame computes the correlations of matching column names. Here I<br/>compute correlations of percent changes with volume:<br/></p>
<p>In [250]: returns.corrwith(volume)<br/>Out[250]: <br/>AAPL   -0.075565<br/>GOOG   -0.007067<br/>IBM    -0.204849<br/>MSFT   -0.092950<br/>dtype: float64<br/></p>
<p>Passing axis='columns' does things row-by-row instead. In all cases, the data points<br/>are aligned by label before the correlation is computed.<br/></p>
<p>Unique Values, Value Counts, and Membership<br/>Another class of related methods extracts information about the values contained in a<br/>one-dimensional Series. To illustrate these, consider this example:<br/></p>
<p>In [251]: obj = pd.Series(['c', 'a', 'd', 'a', 'a', 'b', 'b', 'c', 'c'])<br/></p>
<p>The first function is unique, which gives you an array of the unique values in a Series:<br/>In [252]: uniques = obj.unique()<br/></p>
<p>In [253]: uniques<br/>Out[253]: array(['c', 'a', 'd', 'b'], dtype=object)<br/></p>
<p>The unique values are not necessarily returned in sorted order, but could be sorted<br/>after the fact if needed (uniques.sort()). Relatedly, value_counts computes a Series<br/>containing value frequencies:<br/></p>
<p>162 | Chapter 5: Getting Started with pandas</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In [254]: obj.value_counts()<br/>Out[254]: <br/>c    3<br/>a    3<br/>b    2<br/>d    1<br/>dtype: int64<br/></p>
<p>The Series is sorted by value in descending order as a convenience. value_counts is<br/>also available as a top-level pandas method that can be used with any array or<br/>sequence:<br/></p>
<p>In [255]: pd.value_counts(obj.values, sort=False)<br/>Out[255]: <br/>a    3<br/>b    2<br/>c    3<br/>d    1<br/>dtype: int64<br/></p>
<p>isin performs a vectorized set membership check and can be useful in filtering a<br/>dataset down to a subset of values in a Series or column in a DataFrame:<br/></p>
<p>In [256]: obj<br/>Out[256]: <br/>0    c<br/>1    a<br/>2    d<br/>3    a<br/>4    a<br/>5    b<br/>6    b<br/>7    c<br/>8    c<br/>dtype: object<br/></p>
<p>In [257]: mask = obj.isin(['b', 'c'])<br/></p>
<p>In [258]: mask<br/>Out[258]: <br/>0     True<br/>1    False<br/>2    False<br/>3    False<br/>4    False<br/>5     True<br/>6     True<br/>7     True<br/>8     True<br/>dtype: bool<br/></p>
<p>In [259]: obj[mask]<br/>Out[259]: <br/></p>
<p>5.3 Summarizing and Computing Descriptive Statistics | 163</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>0    c<br/>5    b<br/>6    b<br/>7    c<br/>8    c<br/>dtype: object<br/></p>
<p>Related to isin is the Index.get_indexer method, which gives you an index array<br/>from an array of possibly non-distinct values into another array of distinct values:<br/></p>
<p>In [260]: to_match = pd.Series(['c', 'a', 'b', 'b', 'c', 'a'])<br/></p>
<p>In [261]: unique_vals = pd.Series(['c', 'b', 'a'])<br/></p>
<p>In [262]: pd.Index(unique_vals).get_indexer(to_match)<br/>Out[262]: array([0, 2, 1, 1, 0, 2])<br/></p>
<p>See Table 5-9 for a reference on these methods.<br/><i>Table 5-9. Unique, value counts, and set membership methods<br/></i></p>
<p>Method Description<br/>isin Compute boolean array indicating whether each Series value is contained in the passed sequence of<br/></p>
<p>values<br/>match Compute integer indices for each value in an array into another array of distinct values; helpful for data<br/></p>
<p>alignment and join-type operations<br/>unique Compute array of unique values in a Series, returned in the order observed<br/>value_counts Return a Series containing unique values as its index and frequencies as its values, ordered count in<br/></p>
<p>descending order<br/></p>
<p>In some cases, you may want to compute a histogram on multiple related columns in<br/>a DataFrame. Here&#8217;s an example:<br/></p>
<p>In [263]: data = pd.DataFrame({'Qu1': [1, 3, 4, 3, 4],<br/>   .....:                      'Qu2': [2, 3, 1, 2, 3],<br/>   .....:                      'Qu3': [1, 5, 2, 4, 4]})<br/></p>
<p>In [264]: data<br/>Out[264]: <br/>   Qu1  Qu2  Qu3<br/>0    1    2    1<br/>1    3    3    5<br/>2    4    1    2<br/>3    3    2    4<br/>4    4    3    4<br/></p>
<p>Passing pandas.value_counts to this DataFrame&#8217;s apply function gives:<br/>In [265]: result = data.apply(pd.value_counts).fillna(0)<br/></p>
<p>In [266]: result<br/>Out[266]: <br/></p>
<p>164 | Chapter 5: Getting Started with pandas</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>   Qu1  Qu2  Qu3<br/>1  1.0  1.0  1.0<br/>2  0.0  2.0  1.0<br/>3  2.0  2.0  0.0<br/>4  2.0  0.0  2.0<br/>5  0.0  0.0  1.0<br/></p>
<p>Here, the row labels in the result are the distinct values occurring in all of the col&#8208;<br/>umns. The values are the respective counts of these values in each column.<br/></p>
<p>5.4 Conclusion<br/>In the next chapter, we&#8217;ll discuss tools for reading (or <i>loading</i>) and writing datasets<br/>with pandas. After that, we&#8217;ll dig deeper into data cleaning, wrangling, analysis, and<br/>visualization tools using pandas.<br/></p>
<p>5.4 Conclusion | 165</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div>
</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>CHAPTER 6<br/>Data Loading, Storage, and File Formats<br/></p>
<p>Accessing data is a necessary first step for using most of the tools in this book. I&#8217;m<br/>going to be focused on data input and output using pandas, though there are numer&#8208;<br/>ous tools in other libraries to help with reading and writing data in various formats.<br/>Input and output typically falls into a few main categories: reading text files and other<br/>more efficient on-disk formats, loading data from databases, and interacting with net&#8208;<br/>work sources like web APIs.<br/></p>
<p>6.1 Reading and Writing Data in Text Format<br/>pandas features a number of functions for reading tabular data as a DataFrame<br/>object. Table 6-1 summarizes some of them, though read_csv and read_table are<br/>likely the ones you&#8217;ll use the most.<br/><i>Table 6-1. Parsing functions in pandas<br/></i></p>
<p>Function Description<br/>read_csv Load delimited data from a file, URL, or file-like object; use comma as default delimiter<br/>read_table Load delimited data from a file, URL, or file-like object; use tab ('\t') as default delimiter<br/>read_fwf Read data in fixed-width column format (i.e., no delimiters)<br/>read_clipboard Version of read_table that reads data from the clipboard; useful for converting tables from web<br/></p>
<p>pages<br/>read_excel Read tabular data from an Excel XLS or XLSX file<br/>read_hdf Read HDF5 files written by pandas<br/>read_html Read all tables found in the given HTML document<br/>read_json Read data from a JSON (JavaScript Object Notation) string representation<br/>read_msgpack Read pandas data encoded using the MessagePack binary format<br/>read_pickle Read an arbitrary object stored in Python pickle format<br/></p>
<p>167</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Function Description<br/>read_sas Read a SAS dataset stored in one of the SAS system&#8217;s custom storage formats<br/>read_sql Read the results of a SQL query (using SQLAlchemy) as a pandas DataFrame<br/>read_stata Read a dataset from Stata file format<br/>read_feather Read the Feather binary file format<br/></p>
<p>I&#8217;ll give an overview of the mechanics of these functions, which are meant to convert<br/>text data into a DataFrame. The optional arguments for these functions may fall into<br/>a few categories:<br/><i>Indexing<br/></i></p>
<p>Can treat one or more columns as the returned DataFrame, and whether to get<br/>column names from the file, the user, or not at all.<br/></p>
<p><i>Type inference and data conversion<br/></i>This includes the user-defined value conversions and custom list of missing value<br/>markers.<br/></p>
<p><i>Datetime parsing<br/></i>Includes combining capability, including combining date and time information<br/>spread over multiple columns into a single column in the result.<br/></p>
<p><i>Iterating<br/></i>Support for iterating over chunks of very large files.<br/></p>
<p><i>Unclean data issues<br/></i>Skipping rows or a footer, comments, or other minor things like numeric data<br/>with thousands separated by commas.<br/></p>
<p>Because of how messy data in the real world can be, some of the data loading func&#8208;<br/>tions (especially read_csv) have grown very complex in their options over time. It&#8217;s<br/>normal to feel overwhelmed by the number of different parameters (read_csv has<br/>over 50 as of this writing). The online pandas documentation has many examples<br/>about how each of them works, so if you&#8217;re struggling to read a particular file, there<br/>might be a similar enough example to help you find the right parameters.<br/>Some of these functions, like pandas.read_csv, perform <i>type inference</i>, because the<br/>column data types are not part of the data format. That means you don&#8217;t necessarily<br/>have to specify which columns are numeric, integer, boolean, or string. Other data<br/>formats, like HDF5, Feather, and msgpack, have the data types stored in the format.<br/>Handling dates and other custom types can require extra effort. Let&#8217;s start with a<br/>small comma-separated (CSV) text file:<br/></p>
<p>In [8]: !cat examples/ex1.csv<br/>a,b,c,d,message<br/>1,2,3,4,hello<br/></p>
<p>168 | Chapter 6: Data Loading, Storage, and File Formats</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>5,6,7,8,world<br/>9,10,11,12,foo<br/></p>
<p>Here I used the Unix cat shell command to print the raw contents<br/>of the file to the screen. If you&#8217;re on Windows, you can use type<br/>instead of cat to achieve the same effect.<br/></p>
<p>Since this is comma-delimited, we can use read_csv to read it into a DataFrame:<br/>In [9]: df = pd.read_csv('examples/ex1.csv')<br/></p>
<p>In [10]: df<br/>Out[10]: <br/>   a   b   c   d message<br/>0  1   2   3   4   hello<br/>1  5   6   7   8   world<br/>2  9  10  11  12     foo<br/></p>
<p>We could also have used read_table and specified the delimiter:<br/>In [11]: pd.read_table('examples/ex1.csv', sep=',')<br/>Out[11]: <br/>   a   b   c   d message<br/>0  1   2   3   4   hello<br/>1  5   6   7   8   world<br/>2  9  10  11  12     foo<br/></p>
<p>A file will not always have a header row. Consider this file:<br/>In [12]: !cat examples/ex2.csv<br/>1,2,3,4,hello<br/>5,6,7,8,world<br/>9,10,11,12,foo<br/></p>
<p>To read this file, you have a couple of options. You can allow pandas to assign default<br/>column names, or you can specify names yourself:<br/></p>
<p>In [13]: pd.read_csv('examples/ex2.csv', header=None)<br/>Out[13]: <br/>   0   1   2   3      4<br/>0  1   2   3   4  hello<br/>1  5   6   7   8  world<br/>2  9  10  11  12    foo<br/></p>
<p>In [14]: pd.read_csv('examples/ex2.csv', names=['a', 'b', 'c', 'd', 'message'])<br/>Out[14]: <br/>   a   b   c   d message<br/>0  1   2   3   4   hello<br/>1  5   6   7   8   world<br/>2  9  10  11  12     foo<br/></p>
<p>6.1 Reading and Writing Data in Text Format | 169</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Suppose you wanted the message column to be the index of the returned DataFrame.<br/>You can either indicate you want the column at index 4 or named 'message' using<br/>the index_col argument:<br/></p>
<p>In [15]: names = ['a', 'b', 'c', 'd', 'message']<br/></p>
<p>In [16]: pd.read_csv('examples/ex2.csv', names=names, index_col='message')<br/>Out[16]: <br/>         a   b   c   d<br/>message               <br/>hello    1   2   3   4<br/>world    5   6   7   8<br/>foo      9  10  11  12<br/></p>
<p>In the event that you want to form a hierarchical index from multiple columns, pass a<br/>list of column numbers or names:<br/></p>
<p>In [17]: !cat examples/csv_mindex.csv<br/>key1,key2,value1,value2<br/>one,a,1,2<br/>one,b,3,4<br/>one,c,5,6<br/>one,d,7,8<br/>two,a,9,10<br/>two,b,11,12<br/>two,c,13,14<br/>two,d,15,16<br/></p>
<p>In [18]: parsed = pd.read_csv('examples/csv_mindex.csv',<br/>   ....:                      index_col=['key1', 'key2'])<br/></p>
<p>In [19]: parsed<br/>Out[19]: <br/>           value1  value2<br/>key1 key2                <br/>one  a          1       2<br/>     b          3       4<br/>     c          5       6<br/>     d          7       8<br/>two  a          9      10<br/>     b         11      12<br/>     c         13      14<br/>     d         15      16<br/></p>
<p>In some cases, a table might not have a fixed delimiter, using whitespace or some<br/>other pattern to separate fields. Consider a text file that looks like this:<br/></p>
<p>In [20]: list(open('examples/ex3.txt'))<br/>Out[20]: <br/>['            A         B         C<b>\n</b>',<br/> 'aaa -0.264438 -1.026059 -0.619500<b>\n</b>',<br/> 'bbb  0.927272  0.302904 -0.032399<b>\n</b>',<br/></p>
<p>170 | Chapter 6: Data Loading, Storage, and File Formats</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p> 'ccc -0.264273 -0.386314 -0.217601<b>\n</b>',<br/> 'ddd -0.871858 -0.348382  1.100491<b>\n</b>']<br/></p>
<p>While you could do some munging by hand, the fields here are separated by a vari&#8208;<br/>able amount of whitespace. In these cases, you can pass a regular expression as a<br/>delimiter for read_table. This can be expressed by the regular expression \s+, so we<br/>have then:<br/></p>
<p>In [21]: result = pd.read_table('examples/ex3.txt', sep='\s+')<br/></p>
<p>In [22]: result<br/>Out[22]: <br/>            A         B         C<br/>aaa -0.264438 -1.026059 -0.619500<br/>bbb  0.927272  0.302904 -0.032399<br/>ccc -0.264273 -0.386314 -0.217601<br/>ddd -0.871858 -0.348382  1.100491<br/></p>
<p>Because there was one fewer column name than the number of data rows,<br/>read_table infers that the first column should be the DataFrame&#8217;s index in this spe&#8208;<br/>cial case.<br/>The parser functions have many additional arguments to help you handle the wide<br/>variety of exception file formats that occur (see a partial listing in Table 6-2). For<br/>example, you can skip the first, third, and fourth rows of a file with skiprows:<br/></p>
<p>In [23]: !cat examples/ex4.csv<br/><i># hey!<br/></i>a,b,c,d,message<br/><i># just wanted to make things more difficult for you<br/># who reads CSV files with computers, anyway?<br/></i>1,2,3,4,hello<br/>5,6,7,8,world<br/>9,10,11,12,foo<br/>In [24]: pd.read_csv('examples/ex4.csv', skiprows=[0, 2, 3])<br/>Out[24]: <br/>   a   b   c   d message<br/>0  1   2   3   4   hello<br/>1  5   6   7   8   world<br/>2  9  10  11  12     foo<br/></p>
<p>Handling missing values is an important and frequently nuanced part of the file pars&#8208;<br/>ing process. Missing data is usually either not present (empty string) or marked by<br/>some <i>sentinel</i> value. By default, pandas uses a set of commonly occurring sentinels,<br/>such as NA and NULL:<br/></p>
<p>In [25]: !cat examples/ex5.csv<br/>something,a,b,c,d,message<br/>one,1,2,3,4,NA<br/>two,5,6,,8,world<br/>three,9,10,11,12,foo<br/>In [26]: result = pd.read_csv('examples/ex5.csv')<br/></p>
<p>6.1 Reading and Writing Data in Text Format | 171</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In [27]: result<br/>Out[27]: <br/>  something  a   b     c   d message<br/>0       one  1   2   3.0   4     NaN<br/>1       two  5   6   NaN   8   world<br/>2     three  9  10  11.0  12     foo<br/></p>
<p>In [28]: pd.isnull(result)<br/>Out[28]: <br/>   something      a      b      c      d  message<br/>0      False  False  False  False  False     True<br/>1      False  False  False   True  False    False<br/>2      False  False  False  False  False    False<br/></p>
<p>The na_values option can take either a list or set of strings to consider missing<br/>values:<br/></p>
<p>In [29]: result = pd.read_csv('examples/ex5.csv', na_values=['NULL'])<br/></p>
<p>In [30]: result<br/>Out[30]: <br/>  something  a   b     c   d message<br/>0       one  1   2   3.0   4     NaN<br/>1       two  5   6   NaN   8   world<br/>2     three  9  10  11.0  12     foo<br/></p>
<p>Different NA sentinels can be specified for each column in a dict:<br/>In [31]: sentinels = {'message': ['foo', 'NA'], 'something': ['two']}<br/></p>
<p>In [32]: pd.read_csv('examples/ex5.csv', na_values=sentinels)<br/>Out[32]: <br/>  something  a   b     c   d message<br/>0       one  1   2   3.0   4     NaN<br/>1       NaN  5   6   NaN   8   world<br/>2     three  9  10  11.0  12     NaN<br/></p>
<p>Table 6-2 lists some frequently used options in pandas.read_csv and pan<br/>das.read_table.<br/><i>Table 6-2. Some read_csv/read_table function arguments<br/></i></p>
<p>Argument Description<br/>path String indicating filesystem location, URL, or file-like object<br/>sep or delimiter Character sequence or regular expression to use to split fields in each row<br/>header Row number to use as column names; defaults to 0 (first row), but should be None if there is no<br/></p>
<p>header row<br/>index_col Column numbers or names to use as the row index in the result; can be a single name/number or a<br/></p>
<p>list of them for a hierarchical index<br/>names List of column names for result, combine with header=None<br/></p>
<p>172 | Chapter 6: Data Loading, Storage, and File Formats</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Argument Description<br/>skiprows Number of rows at beginning of file to ignore or list of row numbers (starting from 0) to skip.<br/>na_values Sequence of values to replace with NA.<br/>comment Character(s) to split comments off the end of lines.<br/>parse_dates Attempt to parse data to datetime; False by default. If True, will attempt to parse all columns.<br/></p>
<p>Otherwise can specify a list of column numbers or name to parse. If element of list is tuple or list, will<br/>combine multiple columns together and parse to date (e.g., if date/time split across two columns).<br/></p>
<p>keep_date_col If joining columns to parse date, keep the joined columns; False by default.<br/>converters Dict containing column number of name mapping to functions (e.g., {'foo': f} would apply the<br/></p>
<p>function f to all values in the 'foo' column).<br/>dayfirst When parsing potentially ambiguous dates, treat as international format (e.g., 7/6/2012 -&gt; June 7,<br/></p>
<p>2012); False by default.<br/>date_parser Function to use to parse dates.<br/>nrows Number of rows to read from beginning of file.<br/>iterator Return a TextParser object for reading file piecemeal.<br/>chunksize For iteration, size of file chunks.<br/>skip_footer Number of lines to ignore at end of file.<br/>verbose Print various parser output information, like the number of missing values placed in non-numeric<br/></p>
<p>columns.<br/>encoding Text encoding for Unicode (e.g., 'utf-8' for UTF-8 encoded text).<br/>squeeze If the parsed data only contains one column, return a Series.<br/>thousands Separator for thousands (e.g., ',' or '.').<br/></p>
<p>Reading Text Files in Pieces<br/>When processing very large files or figuring out the right set of arguments to cor&#8208;<br/>rectly process a large file, you may only want to read in a small piece of a file or iterate<br/>through smaller chunks of the file.<br/>Before we look at a large file, we make the pandas display settings more compact:<br/></p>
<p>In [33]: pd.options.display.max_rows = 10<br/></p>
<p>Now we have:<br/>In [34]: result = pd.read_csv('examples/ex6.csv')<br/></p>
<p>In [35]: result<br/>Out[35]: <br/>           one       two     three      four key<br/>0     0.467976 -0.038649 -0.295344 -1.824726   L<br/>1    -0.358893  1.404453  0.704965 -0.200638   B<br/>2    -0.501840  0.659254 -0.421691 -0.057688   G<br/>3     0.204886  1.074134  1.388361 -0.982404   R<br/>4     0.354628 -0.133116  0.283763 -0.837063   Q<br/>...        ...       ...       ...       ...  ..<br/>9995  2.311896 -0.417070 -1.409599 -0.515821   L<br/></p>
<p>6.1 Reading and Writing Data in Text Format | 173</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>9996 -0.479893 -0.650419  0.745152 -0.646038   E<br/>9997  0.523331  0.787112  0.486066  1.093156   K<br/>9998 -0.362559  0.598894 -1.843201  0.887292   G<br/>9999 -0.096376 -1.012999 -0.657431 -0.573315   0<br/>[10000 rows x 5 columns]<br/></p>
<p>If you want to only read a small number of rows (avoiding reading the entire file),<br/>specify that with nrows:<br/></p>
<p>In [36]: pd.read_csv('examples/ex6.csv', nrows=5)<br/>Out[36]: <br/>        one       two     three      four key<br/>0  0.467976 -0.038649 -0.295344 -1.824726   L<br/>1 -0.358893  1.404453  0.704965 -0.200638   B<br/>2 -0.501840  0.659254 -0.421691 -0.057688   G<br/>3  0.204886  1.074134  1.388361 -0.982404   R<br/>4  0.354628 -0.133116  0.283763 -0.837063   Q<br/></p>
<p>To read a file in pieces, specify a chunksize as a number of rows:<br/>In [37]: chunker = pd.read_csv('examples/ex6.csv', chunksize=1000)<br/></p>
<p>In [38]: chunker<br/>Out[38]: &lt;pandas.io.parsers.TextFileReader at 0x7f6b1e2672e8&gt;<br/></p>
<p>The TextParser object returned by read_csv allows you to iterate over the parts of<br/>the file according to the chunksize. For example, we can iterate over ex6.csv, aggre&#8208;<br/>gating the value counts in the 'key' column like so:<br/></p>
<p>chunker = pd.read_csv('examples/ex6.csv', chunksize=1000)<br/></p>
<p>tot = pd.Series([])<br/><b>for</b> piece <b>in</b> chunker:<br/>    tot = tot.add(piece['key'].value_counts(), fill_value=0)<br/></p>
<p>tot = tot.sort_values(ascending=False)<br/></p>
<p>We have then:<br/>In [40]: tot[:10]<br/>Out[40]: <br/>E    368.0<br/>X    364.0<br/>L    346.0<br/>O    343.0<br/>Q    340.0<br/>M    338.0<br/>J    337.0<br/>F    335.0<br/>K    334.0<br/>H    330.0<br/>dtype: float64<br/></p>
<p>174 | Chapter 6: Data Loading, Storage, and File Formats</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>TextParser is also equipped with a get_chunk method that enables you to read<br/>pieces of an arbitrary size.<br/></p>
<p>Writing Data to Text Format<br/>Data can also be exported to a delimited format. Let&#8217;s consider one of the CSV files<br/>read before:<br/></p>
<p>In [41]: data = pd.read_csv('examples/ex5.csv')<br/></p>
<p>In [42]: data<br/>Out[42]: <br/>  something  a   b     c   d message<br/>0       one  1   2   3.0   4     NaN<br/>1       two  5   6   NaN   8   world<br/>2     three  9  10  11.0  12     foo<br/></p>
<p>Using DataFrame&#8217;s to_csv method, we can write the data out to a comma-separated<br/>file:<br/></p>
<p>In [43]: data.to_csv('examples/out.csv')<br/></p>
<p>In [44]: !cat examples/out.csv<br/>,something,a,b,c,d,message<br/>0,one,1,2,3.0,4,<br/>1,two,5,6,,8,world<br/>2,three,9,10,11.0,12,foo<br/></p>
<p>Other delimiters can be used, of course (writing to sys.stdout so it prints the text<br/>result to the console):<br/></p>
<p>In [45]: <b>import</b> <b>sys<br/></b></p>
<p>In [46]: data.to_csv(sys.stdout, sep='|')<br/>|something|a|b|c|d|message<br/>0|one|1|2|3.0|4|<br/>1|two|5|6||8|world<br/>2|three|9|10|11.0|12|foo<br/></p>
<p>Missing values appear as empty strings in the output. You might want to denote them<br/>by some other sentinel value:<br/></p>
<p>In [47]: data.to_csv(sys.stdout, na_rep='NULL')<br/>,something,a,b,c,d,message<br/>0,one,1,2,3.0,4,NULL<br/>1,two,5,6,NULL,8,world<br/>2,three,9,10,11.0,12,foo<br/></p>
<p>With no other options specified, both the row and column labels are written. Both of<br/>these can be disabled:<br/></p>
<p>6.1 Reading and Writing Data in Text Format | 175</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In [48]: data.to_csv(sys.stdout, index=False, header=False)<br/>one,1,2,3.0,4,<br/>two,5,6,,8,world<br/>three,9,10,11.0,12,foo<br/></p>
<p>You can also write only a subset of the columns, and in an order of your choosing:<br/>In [49]: data.to_csv(sys.stdout, index=False, columns=['a', 'b', 'c'])<br/>a,b,c<br/>1,2,3.0<br/>5,6,<br/>9,10,11.0<br/></p>
<p>Series also has a to_csv method:<br/>In [50]: dates = pd.date_range('1/1/2000', periods=7)<br/></p>
<p>In [51]: ts = pd.Series(np.arange(7), index=dates)<br/></p>
<p>In [52]: ts.to_csv('examples/tseries.csv')<br/></p>
<p>In [53]: !cat examples/tseries.csv<br/>2000-01-01,0<br/>2000-01-02,1<br/>2000-01-03,2<br/>2000-01-04,3<br/>2000-01-05,4<br/>2000-01-06,5<br/>2000-01-07,6<br/></p>
<p>Working with Delimited Formats<br/>It&#8217;s possible to load most forms of tabular data from disk using functions like pan<br/>das.read_table. In some cases, however, some manual processing may be necessary.<br/>It&#8217;s not uncommon to receive a file with one or more malformed lines that trip up<br/>read_table. To illustrate the basic tools, consider a small CSV file:<br/></p>
<p>In [54]: !cat examples/ex7.csv<br/>&quot;a&quot;,&quot;b&quot;,&quot;c&quot;<br/>&quot;1&quot;,&quot;2&quot;,&quot;3&quot;<br/>&quot;1&quot;,&quot;2&quot;,&quot;3&quot;<br/></p>
<p>For any file with a single-character delimiter, you can use Python&#8217;s built-in csv mod&#8208;<br/>ule. To use it, pass any open file or file-like object to csv.reader:<br/></p>
<p><b>import</b> <b>csv<br/></b>f = open('examples/ex7.csv')<br/></p>
<p>reader = csv.reader(f)<br/></p>
<p>Iterating through the reader like a file yields tuples of values with any quote charac&#8208;<br/>ters removed:<br/></p>
<p>176 | Chapter 6: Data Loading, Storage, and File Formats</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In [56]: <b>for</b> line <b>in</b> reader:<br/>   ....:     <b>print</b>(line)<br/>['a', 'b', 'c']<br/>['1', '2', '3']<br/>['1', '2', '3']<br/></p>
<p>From there, it&#8217;s up to you to do the wrangling necessary to put the data in the form<br/>that you need it. Let&#8217;s take this step by step. First, we read the file into a list of lines:<br/></p>
<p>In [57]: <b>with</b> open('examples/ex7.csv') <b>as</b> f:<br/>   ....:     lines = list(csv.reader(f))<br/></p>
<p>Then, we split the lines into the header line and the data lines:<br/>In [58]: header, values = lines[0], lines[1:]<br/></p>
<p>Then we can create a dictionary of data columns using a dictionary comprehension<br/>and the expression zip(*values), which transposes rows to columns:<br/></p>
<p>In [59]: data_dict = {h: v <b>for</b> h, v <b>in</b> zip(header, zip(*values))}<br/></p>
<p>In [60]: data_dict<br/>Out[60]: {'a': ('1', '1'), 'b': ('2', '2'), 'c': ('3', '3')}<br/></p>
<p>CSV files come in many different flavors. To define a new format with a different<br/>delimiter, string quoting convention, or line terminator, we define a simple subclass<br/>of csv.Dialect:<br/></p>
<p><b>class</b> <b>my_dialect</b>(csv.Dialect):<br/>    lineterminator = '<b>\n</b>'<br/>    delimiter = ';'<br/>    quotechar = '&quot;'<br/>    quoting = csv.QUOTE_MINIMAL<br/></p>
<p>reader = csv.reader(f, dialect=my_dialect)<br/></p>
<p>We can also give individual CSV dialect parameters as keywords to csv.reader<br/>without having to define a subclass:<br/></p>
<p>reader = csv.reader(f, delimiter='|')<br/></p>
<p>The possible options (attributes of csv.Dialect) and what they do can be found in<br/>Table 6-3.<br/><i>Table 6-3. CSV dialect options<br/></i></p>
<p>Argument Description<br/>delimiter One-character string to separate fields; defaults to ','.<br/>lineterminator Line terminator for writing; defaults to '\r\n'. Reader ignores this and recognizes cross-platform<br/></p>
<p>line terminators.<br/>quotechar Quote character for fields with special characters (like a delimiter); default is '&quot;'.<br/></p>
<p>6.1 Reading and Writing Data in Text Format | 177</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Argument Description<br/>quoting Quoting convention. Options include csv.QUOTE_ALL (quote all fields), csv.QUOTE_MINI<br/></p>
<p>MAL (only fields with special characters like the delimiter), csv.QUOTE_NONNUMERIC, and<br/>csv.QUOTE_NONE (no quoting). See Python&#8217;s documentation for full details. Defaults to<br/>QUOTE_MINIMAL.<br/></p>
<p>skipinitialspace Ignore whitespace after each delimiter; default is False.<br/>doublequote How to handle quoting character inside a field; if True, it is doubled (see online documentation<br/></p>
<p>for full detail and behavior).<br/>escapechar String to escape the delimiter if quoting is set to csv.QUOTE_NONE; disabled by default.<br/></p>
<p>For files with more complicated or fixed multicharacter delimiters,<br/>you will not be able to use the csv module. In those cases, you&#8217;ll<br/>have to do the line splitting and other cleanup using string&#8217;s split<br/>method or the regular expression method re.split.<br/></p>
<p>To <i>write</i> delimited files manually, you can use csv.writer. It accepts an open, writa&#8208;<br/>ble file object and the same dialect and format options as csv.reader:<br/></p>
<p><b>with</b> open('mydata.csv', 'w') <b>as</b> f:<br/>    writer = csv.writer(f, dialect=my_dialect)<br/>    writer.writerow(('one', 'two', 'three'))<br/>    writer.writerow(('1', '2', '3'))<br/>    writer.writerow(('4', '5', '6'))<br/>    writer.writerow(('7', '8', '9'))<br/></p>
<p>JSON Data<br/>JSON (short for JavaScript Object Notation) has become one of the standard formats<br/>for sending data by HTTP request between web browsers and other applications. It is<br/>a much more free-form data format than a tabular text form like CSV. Here is an<br/>example:<br/></p>
<p>obj = &quot;&quot;&quot;<br/>{&quot;name&quot;: &quot;Wes&quot;,<br/> &quot;places_lived&quot;: [&quot;United States&quot;, &quot;Spain&quot;, &quot;Germany&quot;],<br/> &quot;pet&quot;: null,<br/> &quot;siblings&quot;: [{&quot;name&quot;: &quot;Scott&quot;, &quot;age&quot;: 30, &quot;pets&quot;: [&quot;Zeus&quot;, &quot;Zuko&quot;]},<br/>              {&quot;name&quot;: &quot;Katie&quot;, &quot;age&quot;: 38,<br/>               &quot;pets&quot;: [&quot;Sixes&quot;, &quot;Stache&quot;, &quot;Cisco&quot;]}]<br/>}<br/>&quot;&quot;&quot;<br/></p>
<p>JSON is very nearly valid Python code with the exception of its null value null and<br/>some other nuances (such as disallowing trailing commas at the end of lists). The<br/>basic types are objects (dicts), arrays (lists), strings, numbers, booleans, and nulls. All<br/>of the keys in an object must be strings. There are several Python libraries for reading<br/></p>
<p>178 | Chapter 6: Data Loading, Storage, and File Formats</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>and writing JSON data. I&#8217;ll use json here, as it is built into the Python standard<br/>library. To convert a JSON string to Python form, use json.loads:<br/></p>
<p>In [62]: <b>import</b> <b>json<br/></b></p>
<p>In [63]: result = json.loads(obj)<br/></p>
<p>In [64]: result<br/>Out[64]: <br/>{'name': 'Wes',<br/> 'pet': None,<br/> 'places_lived': ['United States', 'Spain', 'Germany'],<br/> 'siblings': [{'age': 30, 'name': 'Scott', 'pets': ['Zeus', 'Zuko']},<br/>  {'age': 38, 'name': 'Katie', 'pets': ['Sixes', 'Stache', 'Cisco']}]}<br/></p>
<p>json.dumps, on the other hand, converts a Python object back to JSON:<br/>In [65]: asjson = json.dumps(result)<br/></p>
<p>How you convert a JSON object or list of objects to a DataFrame or some other data<br/>structure for analysis will be up to you. Conveniently, you can pass a list of dicts<br/>(which were previously JSON objects) to the DataFrame constructor and select a sub&#8208;<br/>set of the data fields:<br/></p>
<p>In [66]: siblings = pd.DataFrame(result['siblings'], columns=['name', 'age'])<br/></p>
<p>In [67]: siblings<br/>Out[67]: <br/>    name  age<br/>0  Scott   30<br/>1  Katie   38<br/></p>
<p>The pandas.read_json can automatically convert JSON datasets in specific arrange&#8208;<br/>ments into a Series or DataFrame. For example:<br/></p>
<p>In [68]: !cat examples/example.json<br/>[{&quot;a&quot;: 1, &quot;b&quot;: 2, &quot;c&quot;: 3},<br/> {&quot;a&quot;: 4, &quot;b&quot;: 5, &quot;c&quot;: 6},<br/> {&quot;a&quot;: 7, &quot;b&quot;: 8, &quot;c&quot;: 9}]<br/></p>
<p>The default options for pandas.read_json assume that each object in the JSON array<br/>is a row in the table:<br/></p>
<p>In [69]: data = pd.read_json('examples/example.json')<br/></p>
<p>In [70]: data<br/>Out[70]: <br/>   a  b  c<br/>0  1  2  3<br/>1  4  5  6<br/>2  7  8  9<br/></p>
<p>6.1 Reading and Writing Data in Text Format | 179</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>1 For the full list, see <i>https://www.fdic.gov/bank/individual/failed/banklist.html</i>.<br/></p>
<p>For an extended example of reading and manipulating JSON data (including nested<br/>records), see the USDA Food Database example in Chapter 7.<br/>If you need to export data from pandas to JSON, one way is to use the to_json meth&#8208;<br/>ods on Series and DataFrame:<br/></p>
<p>In [71]: <b>print</b>(data.to_json())<br/>{&quot;a&quot;:{&quot;0&quot;:1,&quot;1&quot;:4,&quot;2&quot;:7},&quot;b&quot;:{&quot;0&quot;:2,&quot;1&quot;:5,&quot;2&quot;:8},&quot;c&quot;:{&quot;0&quot;:3,&quot;1&quot;:6,&quot;2&quot;:9}}<br/></p>
<p>In [72]: <b>print</b>(data.to_json(orient='records'))<br/>[{&quot;a&quot;:1,&quot;b&quot;:2,&quot;c&quot;:3},{&quot;a&quot;:4,&quot;b&quot;:5,&quot;c&quot;:6},{&quot;a&quot;:7,&quot;b&quot;:8,&quot;c&quot;:9}]<br/></p>
<p>XML and HTML: Web Scraping<br/>Python has many libraries for reading and writing data in the ubiquitous HTML and<br/>XML formats. Examples include lxml, Beautiful Soup, and html5lib. While lxml is<br/>comparatively much faster in general, the other libraries can better handle malformed<br/>HTML or XML files.<br/>pandas has a built-in function, read_html, which uses libraries like lxml and Beauti&#8208;<br/>ful Soup to automatically parse tables out of HTML files as DataFrame objects. To<br/>show how this works, I downloaded an HTML file (used in the pandas documenta&#8208;<br/>tion) from the United States FDIC government agency showing bank failures.1 First,<br/>you must install some additional libraries used by read_html:<br/></p>
<p>conda install lxml<br/>pip install beautifulsoup4 html5lib<br/></p>
<p>If you are not using conda, pip install lxml will likely also work.<br/>The pandas.read_html function has a number of options, but by default it searches<br/>for and attempts to parse all tabular data contained within &lt;table&gt; tags. The result is<br/>a list of DataFrame objects:<br/></p>
<p>In [73]: tables = pd.read_html('examples/fdic_failed_bank_list.html')<br/></p>
<p>In [74]: len(tables)<br/>Out[74]: 1<br/></p>
<p>In [75]: failures = tables[0]<br/></p>
<p>In [76]: failures.head()<br/>Out[76]: <br/>                      Bank Name             City  ST   CERT  \<br/>0                   Allied Bank         Mulberry  AR     91   <br/>1  The Woodbury Banking Company         Woodbury  GA  11297   <br/>2        First CornerStone Bank  King of Prussia  PA  35312   <br/></p>
<p>180 | Chapter 6: Data Loading, Storage, and File Formats</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>3            Trust Company Bank          Memphis  TN   9956   <br/>4    North Milwaukee State Bank        Milwaukee  WI  20364   <br/>                 Acquiring Institution        Closing Date       Updated Date  <br/>0                         Today's Bank  September 23, 2016  November 17, 2016  <br/>1                          United Bank     August 19, 2016  November 17, 2016  <br/>2  First-Citizens Bank &amp; Trust Company         May 6, 2016  September 6, 2016  <br/>3           The Bank of Fayette County      April 29, 2016  September 6, 2016  <br/>4  First-Citizens Bank &amp; Trust Company      March 11, 2016      June 16, 2016  <br/></p>
<p>Because failures has many columns, pandas inserts a line break character \.<br/>As you will learn in later chapters, from here we could proceed to do some data<br/>cleaning and analysis, like computing the number of bank failures by year:<br/></p>
<p>In [77]: close_timestamps = pd.to_datetime(failures['Closing Date'])<br/></p>
<p>In [78]: close_timestamps.dt.year.value_counts()<br/>Out[78]: <br/>2010    157<br/>2009    140<br/>2011     92<br/>2012     51<br/>2008     25<br/>       ... <br/>2004      4<br/>2001      4<br/>2007      3<br/>2003      3<br/>2000      2<br/>Name: Closing Date, Length: 15, dtype: int64<br/></p>
<p>Parsing XML with lxml.objectify<br/>XML (eXtensible Markup Language) is another common structured data format sup&#8208;<br/>porting hierarchical, nested data with metadata. The book you are currently reading<br/>was actually created from a series of large XML documents.<br/>Earlier, I showed the pandas.read_html function, which uses either lxml or Beautiful<br/>Soup under the hood to parse data from HTML. XML and HTML are structurally<br/>similar, but XML is more general. Here, I will show an example of how to use lxml to<br/>parse data from a more general XML format.<br/>The New York Metropolitan Transportation Authority (MTA) publishes a number of<br/>data series about its bus and train services. Here we&#8217;ll look at the performance data,<br/>which is contained in a set of XML files. Each train or bus service has a different file<br/>(like <i>Performance_MNR.xml</i> for the Metro-North Railroad) containing monthly data<br/>as a series of XML records that look like this:<br/></p>
<p><b>&lt;INDICATOR&gt;<br/></b>  <b>&lt;INDICATOR_SEQ&gt;</b>373889<b>&lt;/INDICATOR_SEQ&gt;<br/></b>  <b>&lt;PARENT_SEQ&gt;&lt;/PARENT_SEQ&gt;<br/></b></p>
<p>6.1 Reading and Writing Data in Text Format | 181</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>  <b>&lt;AGENCY_NAME&gt;</b>Metro-North Railroad<b>&lt;/AGENCY_NAME&gt;<br/></b>  <b>&lt;INDICATOR_NAME&gt;</b>Escalator Availability<b>&lt;/INDICATOR_NAME&gt;<br/></b>  <b>&lt;DESCRIPTION&gt;</b>Percent of the time that escalators are operational<br/>  systemwide. The availability rate is based on physical observations performed<br/>  the morning of regular business days only. This is a new indicator the agency<br/>  began reporting in 2009.<b>&lt;/DESCRIPTION&gt;<br/></b>  <b>&lt;PERIOD_YEAR&gt;</b>2011<b>&lt;/PERIOD_YEAR&gt;<br/></b>  <b>&lt;PERIOD_MONTH&gt;</b>12<b>&lt;/PERIOD_MONTH&gt;<br/></b>  <b>&lt;CATEGORY&gt;</b>Service Indicators<b>&lt;/CATEGORY&gt;<br/></b>  <b>&lt;FREQUENCY&gt;</b>M<b>&lt;/FREQUENCY&gt;<br/></b>  <b>&lt;DESIRED_CHANGE&gt;</b>U<b>&lt;/DESIRED_CHANGE&gt;<br/></b>  <b>&lt;INDICATOR_UNIT&gt;</b>%<b>&lt;/INDICATOR_UNIT&gt;<br/></b>  <b>&lt;DECIMAL_PLACES&gt;</b>1<b>&lt;/DECIMAL_PLACES&gt;<br/></b>  <b>&lt;YTD_TARGET&gt;</b>97.00<b>&lt;/YTD_TARGET&gt;<br/></b>  <b>&lt;YTD_ACTUAL&gt;&lt;/YTD_ACTUAL&gt;<br/></b>  <b>&lt;MONTHLY_TARGET&gt;</b>97.00<b>&lt;/MONTHLY_TARGET&gt;<br/></b>  <b>&lt;MONTHLY_ACTUAL&gt;&lt;/MONTHLY_ACTUAL&gt;<br/>&lt;/INDICATOR&gt;<br/></b></p>
<p>Using lxml.objectify, we parse the file and get a reference to the root node of the<br/>XML file with getroot:<br/></p>
<p><b>from</b> <b>lxml</b> <b>import</b> objectify<br/></p>
<p>path = 'examples/mta_perf/Performance_MNR.xml'<br/>parsed = objectify.parse(open(path))<br/>root = parsed.getroot()<br/></p>
<p>root.INDICATOR returns a generator yielding each &lt;INDICATOR&gt; XML element. For<br/>each record, we can populate a dict of tag names (like YTD_ACTUAL) to data values<br/>(excluding a few tags):<br/></p>
<p>data = []<br/></p>
<p>skip_fields = ['PARENT_SEQ', 'INDICATOR_SEQ',<br/>               'DESIRED_CHANGE', 'DECIMAL_PLACES']<br/></p>
<p><b>for</b> elt <b>in</b> root.INDICATOR:<br/>    el_data = {}<br/>    <b>for</b> child <b>in</b> elt.getchildren():<br/>        <b>if</b> child.tag <b>in</b> skip_fields:<br/>            <b>continue<br/></b>        el_data[child.tag] = child.pyval<br/>    data.append(el_data)<br/></p>
<p>Lastly, convert this list of dicts into a DataFrame:<br/>In [81]: perf = pd.DataFrame(data)<br/></p>
<p>In [82]: perf.head()<br/>Out[82]: <br/>Empty DataFrame<br/></p>
<p>182 | Chapter 6: Data Loading, Storage, and File Formats</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Columns: []<br/>Index: []<br/></p>
<p>XML data can get much more complicated than this example. Each tag can have<br/>metadata, too. Consider an HTML link tag, which is also valid XML:<br/></p>
<p><b>from</b> <b>io</b> <b>import</b> StringIO<br/>tag = '&lt;a href=&quot;http://www.google.com&quot;&gt;Google&lt;/a&gt;'<br/>root = objectify.parse(StringIO(tag)).getroot()<br/></p>
<p>You can now access any of the fields (like href) in the tag or the link text:<br/>In [84]: root<br/>Out[84]: &lt;Element a at 0x7f6b15817748&gt;<br/></p>
<p>In [85]: root.get('href')<br/>Out[85]: 'http://www.google.com'<br/></p>
<p>In [86]: root.text<br/>Out[86]: 'Google'<br/></p>
<p>6.2 Binary Data Formats<br/>One of the easiest ways to store data (also known as <i>serialization</i>) efficiently in binary<br/>format is using Python&#8217;s built-in pickle serialization. pandas objects all have a<br/>to_pickle method that writes the data to disk in pickle format:<br/></p>
<p>In [87]: frame = pd.read_csv('examples/ex1.csv')<br/></p>
<p>In [88]: frame<br/>Out[88]: <br/>   a   b   c   d message<br/>0  1   2   3   4   hello<br/>1  5   6   7   8   world<br/>2  9  10  11  12     foo<br/></p>
<p>In [89]: frame.to_pickle('examples/frame_pickle')<br/></p>
<p>You can read any &#8220;pickled&#8221; object stored in a file by using the built-in pickle directly,<br/>or even more conveniently using pandas.read_pickle:<br/></p>
<p>In [90]: pd.read_pickle('examples/frame_pickle')<br/>Out[90]: <br/>   a   b   c   d message<br/>0  1   2   3   4   hello<br/>1  5   6   7   8   world<br/>2  9  10  11  12     foo<br/></p>
<p>6.2 Binary Data Formats | 183</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>pickle is only recommended as a short-term storage format. The<br/>problem is that it is hard to guarantee that the format will be stable<br/>over time; an object pickled today may not unpickle with a later<br/>version of a library. We have tried to maintain backward compati&#8208;<br/>bility when possible, but at some point in the future it may be nec&#8208;<br/>essary to &#8220;break&#8221; the pickle format.<br/></p>
<p>pandas has built-in support for two more binary data formats: HDF5 and Message&#8208;<br/>Pack. I will give some HDF5 examples in the next section, but I encourage you to<br/>explore different file formats to see how fast they are and how well they work for your<br/>analysis. Some other storage formats for pandas or NumPy data include:<br/><i>bcolz<br/></i></p>
<p>A compressable column-oriented binary format based on the Blosc compression<br/>library.<br/></p>
<p><i>Feather<br/></i>A cross-language column-oriented file format I designed with the R program&#8208;<br/>ming community&#8217;s Hadley Wickham. Feather uses the Apache Arrow columnar<br/>memory format.<br/></p>
<p>Using HDF5 Format<br/>HDF5 is a well-regarded file format intended for storing large quantities of scientific<br/>array data. It is available as a C library, and it has interfaces available in many other<br/>languages, including Java, Julia, MATLAB, and Python. The &#8220;HDF&#8221; in HDF5 stands<br/>for <i>hierarchical data format</i>. Each HDF5 file can store multiple datasets and support&#8208;<br/>ing metadata. Compared with simpler formats, HDF5 supports on-the-fly compres&#8208;<br/>sion with a variety of compression modes, enabling data with repeated patterns to be<br/>stored more efficiently. HDF5 can be a good choice for working with very large data&#8208;<br/>sets that don&#8217;t fit into memory, as you can efficiently read and write small sections of<br/>much larger arrays.<br/>While it&#8217;s possible to directly access HDF5 files using either the PyTables or h5py<br/>libraries, pandas provides a high-level interface that simplifies storing Series and<br/>DataFrame object. The HDFStore class works like a dict and handles the low-level<br/>details:<br/></p>
<p>In [92]: frame = pd.DataFrame({'a': np.random.randn(100)})<br/></p>
<p>In [93]: store = pd.HDFStore('mydata.h5')<br/></p>
<p>In [94]: store['obj1'] = frame<br/></p>
<p>In [95]: store['obj1_col'] = frame['a']<br/></p>
<p>In [96]: store<br/></p>
<p>184 | Chapter 6: Data Loading, Storage, and File Formats</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Out[96]: <br/>&lt;<b>class</b> '<b>pandas</b>.io.pytables.HDFStore'&gt;<br/>File path: mydata.h5<br/>/obj1                frame        (shape-&gt;[100,1])                               <br/>        <br/>/obj1_col            series       (shape-&gt;[100])                                 <br/>        <br/>/obj2                frame_table  (typ-&gt;appendable,nrows-&gt;100,ncols-&gt;1,indexers-&gt;<br/>[index])<br/>/obj3                frame_table  (typ-&gt;appendable,nrows-&gt;100,ncols-&gt;1,indexers-&gt;<br/>[index])<br/></p>
<p>Objects contained in the HDF5 file can then be retrieved with the same dict-like API:<br/>In [97]: store['obj1']<br/>Out[97]: <br/>           a<br/>0  -0.204708<br/>1   0.478943<br/>2  -0.519439<br/>3  -0.555730<br/>4   1.965781<br/>..       ...<br/>95  0.795253<br/>96  0.118110<br/>97 -0.748532<br/>98  0.584970<br/>99  0.152677<br/>[100 rows x 1 columns]<br/></p>
<p>HDFStore supports two storage schemas, 'fixed' and 'table'. The latter is generally<br/>slower, but it supports query operations using a special syntax:<br/></p>
<p>In [98]: store.put('obj2', frame, format='table')<br/></p>
<p>In [99]: store.select('obj2', where=['index &gt;= 10 and index &lt;= 15'])<br/>Out[99]: <br/>           a<br/>10  1.007189<br/>11 -1.296221<br/>12  0.274992<br/>13  0.228913<br/>14  1.352917<br/>15  0.886429<br/></p>
<p>In [100]: store.close()<br/></p>
<p>The put is an explicit version of the store['obj2'] = frame method but allows us to<br/>set other options like the storage format.<br/>The pandas.read_hdf function gives you a shortcut to these tools:<br/></p>
<p>In [101]: frame.to_hdf('mydata.h5', 'obj3', format='table')<br/></p>
<p>6.2 Binary Data Formats | 185</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In [102]: pd.read_hdf('mydata.h5', 'obj3', where=['index &lt; 5'])<br/>Out[102]: <br/>          a<br/>0 -0.204708<br/>1  0.478943<br/>2 -0.519439<br/>3 -0.555730<br/>4  1.965781<br/></p>
<p>If you are processing data that is stored on remote servers, like<br/>Amazon S3 or HDFS, using a different binary format designed for<br/>distributed storage like Apache Parquet may be more suitable.<br/>Python for Parquet and other such storage formats is still develop&#8208;<br/>ing, so I do not write about them in this book.<br/></p>
<p>If you work with large quantities of data locally, I would encourage you to explore<br/>PyTables and h5py to see how they can suit your needs. Since many data analysis<br/>problems are I/O-bound (rather than CPU-bound), using a tool like HDF5 can mas&#8208;<br/>sively accelerate your applications.<br/></p>
<p>HDF5 is <i>not</i> a database. It is best suited for write-once, read-many<br/>datasets. While data can be added to a file at any time, if multiple<br/>writers do so simultaneously, the file can become corrupted.<br/></p>
<p>Reading Microsoft Excel Files<br/>pandas also supports reading tabular data stored in Excel 2003 (and higher) files<br/>using either the ExcelFile class or pandas.read_excel function. Internally these<br/>tools use the add-on packages xlrd and openpyxl to read XLS and XLSX files, respec&#8208;<br/>tively. You may need to install these manually with pip or conda.<br/>To use ExcelFile, create an instance by passing a path to an xls or xlsx file:<br/></p>
<p>In [104]: xlsx = pd.ExcelFile('examples/ex1.xlsx')<br/></p>
<p>Data stored in a sheet can then be read into DataFrame with parse:<br/>In [105]: pd.read_excel(xlsx, 'Sheet1')<br/>Out[105]: <br/>   a   b   c   d message<br/>0  1   2   3   4   hello<br/>1  5   6   7   8   world<br/>2  9  10  11  12     foo<br/></p>
<p>If you are reading multiple sheets in a file, then it is faster to create the ExcelFile,<br/>but you can also simply pass the filename to pandas.read_excel:<br/></p>
<p>186 | Chapter 6: Data Loading, Storage, and File Formats</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In [106]: frame = pd.read_excel('examples/ex1.xlsx', 'Sheet1')<br/></p>
<p>In [107]: frame<br/>Out[107]: <br/>   a   b   c   d message<br/>0  1   2   3   4   hello<br/>1  5   6   7   8   world<br/>2  9  10  11  12     foo<br/></p>
<p>To write pandas data to Excel format, you must first create an ExcelWriter, then<br/>write data to it using pandas objects&#8217; to_excel method:<br/></p>
<p>In [108]: writer = pd.ExcelWriter('examples/ex2.xlsx')<br/></p>
<p>In [109]: frame.to_excel(writer, 'Sheet1')<br/></p>
<p>In [110]: writer.save()<br/></p>
<p>You can also pass a file path to to_excel and avoid the ExcelWriter:<br/>In [111]: frame.to_excel('examples/ex2.xlsx')<br/></p>
<p>6.3 Interacting with Web APIs<br/>Many websites have public APIs providing data feeds via JSON or some other format.<br/>There are a number of ways to access these APIs from Python; one easy-to-use<br/>method that I recommend is the requests package.<br/>To find the last 30 GitHub issues for pandas on GitHub, we can make a GET HTTP<br/>request using the add-on requests library:<br/></p>
<p>In [113]: <b>import</b> <b>requests<br/></b></p>
<p>In [114]: url = 'https://api.github.com/repos/pandas-dev/pandas/issues'<br/></p>
<p>In [115]: resp = requests.get(url)<br/></p>
<p>In [116]: resp<br/>Out[116]: &lt;Response [200]&gt;<br/></p>
<p>The Response object&#8217;s json method will return a dictionary containing JSON parsed<br/>into native Python objects:<br/></p>
<p>In [117]: data = resp.json()<br/></p>
<p>In [118]: data[0]['title']<br/>Out[118]: 'Period does not round down for frequencies less that 1 hour'<br/></p>
<p>Each element in data is a dictionary containing all of the data found on a GitHub<br/>issue page (except for the comments). We can pass data directly to DataFrame and<br/>extract fields of interest:<br/></p>
<p>6.3 Interacting with Web APIs | 187</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In [119]: issues = pd.DataFrame(data, columns=['number', 'title',<br/>   .....:                                      'labels', 'state'])<br/></p>
<p>In [120]: issues<br/>Out[120]: <br/>    number                                              title  \<br/>0    17666  Period does <b>not</b> round down <b>for</b> frequencies les...   <br/>1    17665           DOC: improve docstring of function where   <br/>2    17664               COMPAT: skip 32-bit test on int repr   <br/>3    17662                          implement Delegator <b>class</b>   <br/>4    17654  <b>BUG</b>: Fix series rename called <b>with</b> str alterin...   <br/>..     ...                                                ...   <br/>25   17603  BUG: Correctly localize naive datetime strings...   <br/>26   17599                     core.dtypes.generic --&gt; cython   <br/>27   17596   Merge cdate_range functionality into bdate_range   <br/>28   17587  Time Grouper bug fix when applied <b>for</b> list gro...   <br/>29   17583  BUG: fix tz-aware DatetimeIndex + TimedeltaInd...   <br/>                                               labels state  <br/>0                                                  []  open  <br/>1   [{'id': 134699, 'url': 'https://api.github.com...  open  <br/>2   [{'id': 563047854, 'url': 'https://api.github....  open  <br/>3                                                  []  open  <br/>4   [{'id': 76811, 'url': 'https://api.github.com/...  open  <br/>..                                                ...   ...  <br/>25  [{'id': 76811, 'url': 'https://api.github.com/...  open  <br/>26  [{'id': 49094459, 'url': 'https://api.github.c...  open  <br/>27  [{'id': 35818298, 'url': 'https://api.github.c...  open  <br/>28  [{'id': 233160, 'url': 'https://api.github.com...  open  <br/>29  [{'id': 76811, 'url': 'https://api.github.com/...  open  <br/>[30 rows x 4 columns]<br/></p>
<p>With a bit of elbow grease, you can create some higher-level interfaces to common<br/>web APIs that return DataFrame objects for easy analysis.<br/></p>
<p>6.4 Interacting with Databases<br/>In a business setting, most data may not be stored in text or Excel files. SQL-based<br/>relational databases (such as SQL Server, PostgreSQL, and MySQL) are in wide use,<br/>and many alternative databases have become quite popular. The choice of database is<br/>usually dependent on the performance, data integrity, and scalability needs of an<br/>application.<br/>Loading data from SQL into a DataFrame is fairly straightforward, and pandas has<br/>some functions to simplify the process. As an example, I&#8217;ll create a SQLite database<br/>using Python&#8217;s built-in sqlite3 driver:<br/></p>
<p>In [121]: <b>import</b> <b>sqlite3<br/></b></p>
<p>In [122]: query = &quot;&quot;&quot;<br/>   .....: CREATE TABLE test<br/></p>
<p>188 | Chapter 6: Data Loading, Storage, and File Formats</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>   .....: (a VARCHAR(20), b VARCHAR(20),<br/>   .....:  c REAL,        d INTEGER<br/>   .....: );&quot;&quot;&quot;<br/></p>
<p>In [123]: con = sqlite3.connect('mydata.sqlite')<br/></p>
<p>In [124]: con.execute(query)<br/>Out[124]: &lt;sqlite3.Cursor at 0x7f6b12a50f10&gt;<br/></p>
<p>In [125]: con.commit()<br/></p>
<p>Then, insert a few rows of data:<br/>In [126]: data = [('Atlanta', 'Georgia', 1.25, 6),<br/>   .....:         ('Tallahassee', 'Florida', 2.6, 3),<br/>   .....:         ('Sacramento', 'California', 1.7, 5)]<br/></p>
<p>In [127]: stmt = &quot;INSERT INTO test VALUES(?, ?, ?, ?)&quot;<br/></p>
<p>In [128]: con.executemany(stmt, data)<br/>Out[128]: &lt;sqlite3.Cursor at 0x7f6b15c66ce0&gt;<br/></p>
<p>In [129]: con.commit()<br/></p>
<p>Most Python SQL drivers (PyODBC, psycopg2, MySQLdb, pymssql, etc.) return a list<br/>of tuples when selecting data from a table:<br/></p>
<p>In [130]: cursor = con.execute('select * from test')<br/></p>
<p>In [131]: rows = cursor.fetchall()<br/></p>
<p>In [132]: rows<br/>Out[132]: <br/>[('Atlanta', 'Georgia', 1.25, 6),<br/> ('Tallahassee', 'Florida', 2.6, 3),<br/> ('Sacramento', 'California', 1.7, 5)]<br/></p>
<p>You can pass the list of tuples to the DataFrame constructor, but you also need the<br/>column names, contained in the cursor&#8217;s description attribute:<br/></p>
<p>In [133]: cursor.description<br/>Out[133]: <br/>(('a', None, None, None, None, None, None),<br/> ('b', None, None, None, None, None, None),<br/> ('c', None, None, None, None, None, None),<br/> ('d', None, None, None, None, None, None))<br/></p>
<p>In [134]: pd.DataFrame(rows, columns=[x[0] <b>for</b> x <b>in</b> cursor.description])<br/>Out[134]: <br/>             a           b     c  d<br/>0      Atlanta     Georgia  1.25  6<br/>1  Tallahassee     Florida  2.60  3<br/>2   Sacramento  California  1.70  5<br/></p>
<p>6.4 Interacting with Databases | 189</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>This is quite a bit of munging that you&#8217;d rather not repeat each time you query the<br/>database. The SQLAlchemy project is a popular Python SQL toolkit that abstracts<br/>away many of the common differences between SQL databases. pandas has a<br/>read_sql function that enables you to read data easily from a general SQLAlchemy<br/>connection. Here, we&#8217;ll connect to the same SQLite database with SQLAlchemy and<br/>read data from the table created before:<br/></p>
<p>In [135]: <b>import</b> <b>sqlalchemy</b> <b>as</b> <b>sqla<br/></b></p>
<p>In [136]: db = sqla.create_engine('sqlite:///mydata.sqlite')<br/></p>
<p>In [137]: pd.read_sql('select * from test', db)<br/>Out[137]: <br/>             a           b     c  d<br/>0      Atlanta     Georgia  1.25  6<br/>1  Tallahassee     Florida  2.60  3<br/>2   Sacramento  California  1.70  5<br/></p>
<p>6.5 Conclusion<br/>Getting access to data is frequently the first step in the data analysis process. We have<br/>looked at a number of useful tools in this chapter that should help you get started. In<br/>the upcoming chapters we will dig deeper into data wrangling, data visualization,<br/>time series analysis, and other topics.<br/></p>
<p>190 | Chapter 6: Data Loading, Storage, and File Formats</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>CHAPTER 7<br/>Data Cleaning and Preparation<br/></p>
<p>During the course of doing data analysis and modeling, a significant amount of time<br/>is spent on data preparation: loading, cleaning, transforming, and rearranging. Such<br/>tasks are often reported to take up 80% or more of an analyst&#8217;s time. Sometimes the<br/>way that data is stored in files or databases is not in the right format for a particular<br/>task. Many researchers choose to do ad hoc processing of data from one form to<br/>another using a general-purpose programming language, like Python, Perl, R, or Java,<br/>or Unix text-processing tools like sed or awk. Fortunately, pandas, along with the<br/>built-in Python language features, provides you with a high-level, flexible, and fast set<br/>of tools to enable you to manipulate data into the right form.<br/>If you identify a type of data manipulation that isn&#8217;t anywhere in this book or else&#8208;<br/>where in the pandas library, feel free to share your use case on one of the Python<br/>mailing lists or on the pandas GitHub site. Indeed, much of the design and imple&#8208;<br/>mentation of pandas has been driven by the needs of real-world applications.<br/>In this chapter I discuss tools for missing data, duplicate data, string manipulation,<br/>and some other analytical data transformations. In the next chapter, I focus on com&#8208;<br/>bining and rearranging datasets in various ways.<br/></p>
<p>7.1 Handling Missing Data<br/>Missing data occurs commonly in many data analysis applications. One of the goals<br/>of pandas is to make working with missing data as painless as possible. For example,<br/>all of the descriptive statistics on pandas objects exclude missing data by default.<br/>The way that missing data is represented in pandas objects is somewhat imperfect,<br/>but it is functional for a lot of users. For numeric data, pandas uses the floating-point <br/>value NaN (Not a Number) to represent missing data. We call this a <i>sentinel value</i> that<br/>can be easily detected:<br/></p>
<p>191</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In [10]: string_data = pd.Series(['aardvark', 'artichoke', np.nan, 'avocado'])<br/></p>
<p>In [11]: string_data<br/>Out[11]: <br/>0     aardvark<br/>1    artichoke<br/>2          NaN<br/>3      avocado<br/>dtype: object<br/></p>
<p>In [12]: string_data.isnull()<br/>Out[12]: <br/>0    False<br/>1    False<br/>2     True<br/>3    False<br/>dtype: bool<br/></p>
<p>In pandas, we&#8217;ve adopted a convention used in the R programming language by refer&#8208;<br/>ring to missing data as NA, which stands for <i>not available</i>. In statistics applications, <br/>NA data may either be data that does not exist or that exists but was not observed<br/>(through problems with data collection, for example). When cleaning up data for<br/>analysis, it is often important to do analysis on the missing data itself to identify data<br/>collection problems or potential biases in the data caused by missing data.<br/>The built-in Python None value is also treated as NA in object arrays:<br/></p>
<p>In [13]: string_data[0] = None<br/></p>
<p>In [14]: string_data.isnull()<br/>Out[14]: <br/>0     True<br/>1    False<br/>2     True<br/>3    False<br/>dtype: bool<br/></p>
<p>There is work ongoing in the pandas project to improve the internal details of how<br/>missing data is handled, but the user API functions, like pandas.isnull, abstract <br/>away many of the annoying details. See Table 7-1 for a list of some functions related<br/>to missing data handling.<br/><i>Table 7-1. NA handling methods<br/></i></p>
<p>Argument Description<br/>dropna Filter axis labels based on whether values for each label have missing data, with varying thresholds for how<br/></p>
<p>much missing data to tolerate.<br/>fillna Fill in missing data with some value or using an interpolation method such as 'ffill' or 'bfill'.<br/>isnull Return boolean values indicating which values are missing/NA.<br/>notnull Negation of isnull.<br/></p>
<p>192 | Chapter 7: Data Cleaning and Preparation</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Filtering Out Missing Data<br/>There are a few ways to filter out missing data. While you always have the option to<br/>do it by hand using pandas.isnull and boolean indexing, the dropna can be helpful.<br/>On a Series, it returns the Series with only the non-null data and index values:<br/></p>
<p>In [15]: <b>from</b> <b>numpy</b> <b>import</b> nan <b>as</b> NA<br/></p>
<p>In [16]: data = pd.Series([1, NA, 3.5, NA, 7])<br/></p>
<p>In [17]: data.dropna()<br/>Out[17]: <br/>0    1.0<br/>2    3.5<br/>4    7.0<br/>dtype: float64<br/></p>
<p>This is equivalent to:<br/>In [18]: data[data.notnull()]<br/>Out[18]: <br/>0    1.0<br/>2    3.5<br/>4    7.0<br/>dtype: float64<br/></p>
<p>With DataFrame objects, things are a bit more complex. You may want to drop rows<br/>or columns that are all NA or only those containing any NAs. dropna by default drops<br/>any row containing a missing value:<br/></p>
<p>In [19]: data = pd.DataFrame([[1., 6.5, 3.], [1., NA, NA],<br/>   ....:                      [NA, NA, NA], [NA, 6.5, 3.]])<br/></p>
<p>In [20]: cleaned = data.dropna()<br/></p>
<p>In [21]: data<br/>Out[21]: <br/>     0    1    2<br/>0  1.0  6.5  3.0<br/>1  1.0  NaN  NaN<br/>2  NaN  NaN  NaN<br/>3  NaN  6.5  3.0<br/></p>
<p>In [22]: cleaned<br/>Out[22]: <br/>     0    1    2<br/>0  1.0  6.5  3.0<br/></p>
<p>Passing how='all' will only drop rows that are all NA:<br/>In [23]: data.dropna(how='all')<br/>Out[23]: <br/>     0    1    2<br/></p>
<p>7.1 Handling Missing Data | 193</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>0  1.0  6.5  3.0<br/>1  1.0  NaN  NaN<br/>3  NaN  6.5  3.0<br/></p>
<p>To drop columns in the same way, pass axis=1:<br/>In [24]: data[4] = NA<br/></p>
<p>In [25]: data<br/>Out[25]: <br/>     0    1    2   4<br/>0  1.0  6.5  3.0 NaN<br/>1  1.0  NaN  NaN NaN<br/>2  NaN  NaN  NaN NaN<br/>3  NaN  6.5  3.0 NaN<br/></p>
<p>In [26]: data.dropna(axis=1, how='all')<br/>Out[26]: <br/>     0    1    2<br/>0  1.0  6.5  3.0<br/>1  1.0  NaN  NaN<br/>2  NaN  NaN  NaN<br/>3  NaN  6.5  3.0<br/></p>
<p>A related way to filter out DataFrame rows tends to concern time series data. Suppose<br/>you want to keep only rows containing a certain number of observations. You can<br/>indicate this with the thresh argument:<br/></p>
<p>In [27]: df = pd.DataFrame(np.random.randn(7, 3))<br/></p>
<p>In [28]: df.iloc[:4, 1] = NA<br/></p>
<p>In [29]: df.iloc[:2, 2] = NA<br/></p>
<p>In [30]: df<br/>Out[30]: <br/>          0         1         2<br/>0 -0.204708       NaN       NaN<br/>1 -0.555730       NaN       NaN<br/>2  0.092908       NaN  0.769023<br/>3  1.246435       NaN -1.296221<br/>4  0.274992  0.228913  1.352917<br/>5  0.886429 -2.001637 -0.371843<br/>6  1.669025 -0.438570 -0.539741<br/></p>
<p>In [31]: df.dropna()<br/>Out[31]: <br/>          0         1         2<br/>4  0.274992  0.228913  1.352917<br/>5  0.886429 -2.001637 -0.371843<br/>6  1.669025 -0.438570 -0.539741<br/></p>
<p>In [32]: df.dropna(thresh=2)<br/></p>
<p>194 | Chapter 7: Data Cleaning and Preparation</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Out[32]: <br/>          0         1         2<br/>2  0.092908       NaN  0.769023<br/>3  1.246435       NaN -1.296221<br/>4  0.274992  0.228913  1.352917<br/>5  0.886429 -2.001637 -0.371843<br/>6  1.669025 -0.438570 -0.539741<br/></p>
<p>Filling In Missing Data<br/>Rather than filtering out missing data (and potentially discarding other data along<br/>with it), you may want to fill in the &#8220;holes&#8221; in any number of ways. For most pur&#8208;<br/>poses, the fillna method is the workhorse function to use. Calling fillna with a<br/>constant replaces missing values with that value:<br/></p>
<p>In [33]: df.fillna(0)<br/>Out[33]: <br/>          0         1         2<br/>0 -0.204708  0.000000  0.000000<br/>1 -0.555730  0.000000  0.000000<br/>2  0.092908  0.000000  0.769023<br/>3  1.246435  0.000000 -1.296221<br/>4  0.274992  0.228913  1.352917<br/>5  0.886429 -2.001637 -0.371843<br/>6  1.669025 -0.438570 -0.539741<br/></p>
<p>Calling fillna with a dict, you can use a different fill value for each column:<br/>In [34]: df.fillna({1: 0.5, 2: 0})<br/>Out[34]: <br/>          0         1         2<br/>0 -0.204708  0.500000  0.000000<br/>1 -0.555730  0.500000  0.000000<br/>2  0.092908  0.500000  0.769023<br/>3  1.246435  0.500000 -1.296221<br/>4  0.274992  0.228913  1.352917<br/>5  0.886429 -2.001637 -0.371843<br/>6  1.669025 -0.438570 -0.539741<br/></p>
<p>fillna returns a new object, but you can modify the existing object in-place:<br/>In [35]: _ = df.fillna(0, inplace=True)<br/></p>
<p>In [36]: df<br/>Out[36]: <br/>          0         1         2<br/>0 -0.204708  0.000000  0.000000<br/>1 -0.555730  0.000000  0.000000<br/>2  0.092908  0.000000  0.769023<br/>3  1.246435  0.000000 -1.296221<br/>4  0.274992  0.228913  1.352917<br/>5  0.886429 -2.001637 -0.371843<br/>6  1.669025 -0.438570 -0.539741<br/></p>
<p>7.1 Handling Missing Data | 195</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>The same interpolation methods available for reindexing can be used with fillna:<br/>In [37]: df = pd.DataFrame(np.random.randn(6, 3))<br/></p>
<p>In [38]: df.iloc[2:, 1] = NA<br/></p>
<p>In [39]: df.iloc[4:, 2] = NA<br/></p>
<p>In [40]: df<br/>Out[40]: <br/>          0         1         2<br/>0  0.476985  3.248944 -1.021228<br/>1 -0.577087  0.124121  0.302614<br/>2  0.523772       NaN  1.343810<br/>3 -0.713544       NaN -2.370232<br/>4 -1.860761       NaN       NaN<br/>5 -1.265934       NaN       NaN<br/></p>
<p>In [41]: df.fillna(method='ffill')<br/>Out[41]: <br/>          0         1         2<br/>0  0.476985  3.248944 -1.021228<br/>1 -0.577087  0.124121  0.302614<br/>2  0.523772  0.124121  1.343810<br/>3 -0.713544  0.124121 -2.370232<br/>4 -1.860761  0.124121 -2.370232<br/>5 -1.265934  0.124121 -2.370232<br/></p>
<p>In [42]: df.fillna(method='ffill', limit=2)<br/>Out[42]: <br/>          0         1         2<br/>0  0.476985  3.248944 -1.021228<br/>1 -0.577087  0.124121  0.302614<br/>2  0.523772  0.124121  1.343810<br/>3 -0.713544  0.124121 -2.370232<br/>4 -1.860761       NaN -2.370232<br/>5 -1.265934       NaN -2.370232<br/></p>
<p>With fillna you can do lots of other things with a little creativity. For example, you<br/>might pass the mean or median value of a Series:<br/></p>
<p>In [43]: data = pd.Series([1., NA, 3.5, NA, 7])<br/></p>
<p>In [44]: data.fillna(data.mean())<br/>Out[44]: <br/>0    1.000000<br/>1    3.833333<br/>2    3.500000<br/>3    3.833333<br/>4    7.000000<br/>dtype: float64<br/></p>
<p>See Table 7-2 for a reference on fillna.<br/></p>
<p>196 | Chapter 7: Data Cleaning and Preparation</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><i>Table 7-2. fillna function arguments<br/></i>Argument Description<br/>value Scalar value or dict-like object to use to fill missing values<br/>method Interpolation; by default 'ffill' if function called with no other arguments<br/>axis Axis to fill on; default axis=0<br/>inplace Modify the calling object without producing a copy<br/>limit For forward and backward filling, maximum number of consecutive periods to fill<br/></p>
<p>7.2 Data Transformation<br/>So far in this chapter we&#8217;ve been concerned with rearranging data. Filtering, cleaning,<br/>and other transformations are another class of important operations.<br/></p>
<p>Removing Duplicates<br/>Duplicate rows may be found in a DataFrame for any number of reasons. Here is an<br/>example:<br/></p>
<p>In [45]: data = pd.DataFrame({'k1': ['one', 'two'] * 3 + ['two'],<br/>   ....:                      'k2': [1, 1, 2, 3, 3, 4, 4]})<br/></p>
<p>In [46]: data<br/>Out[46]: <br/>    k1  k2<br/>0  one   1<br/>1  two   1<br/>2  one   2<br/>3  two   3<br/>4  one   3<br/>5  two   4<br/>6  two   4<br/></p>
<p>The DataFrame method duplicated returns a boolean Series indicating whether each<br/>row is a duplicate (has been observed in a previous row) or not:<br/></p>
<p>In [47]: data.duplicated()<br/>Out[47]: <br/>0    False<br/>1    False<br/>2    False<br/>3    False<br/>4    False<br/>5    False<br/>6     True<br/>dtype: bool<br/></p>
<p>Relatedly, drop_duplicates returns a DataFrame where the duplicated array is<br/>False:<br/></p>
<p>7.2 Data Transformation | 197</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In [48]: data.drop_duplicates()<br/>Out[48]: <br/>    k1  k2<br/>0  one   1<br/>1  two   1<br/>2  one   2<br/>3  two   3<br/>4  one   3<br/>5  two   4<br/></p>
<p>Both of these methods by default consider all of the columns; alternatively, you can<br/>specify any subset of them to detect duplicates. Suppose we had an additional column<br/>of values and wanted to filter duplicates only based on the 'k1' column:<br/></p>
<p>In [49]: data['v1'] = range(7)<br/></p>
<p>In [50]: data.drop_duplicates(['k1'])<br/>Out[50]: <br/>    k1  k2  v1<br/>0  one   1   0<br/>1  two   1   1<br/></p>
<p>duplicated and drop_duplicates by default keep the first observed value combina&#8208;<br/>tion. Passing keep='last' will return the last one:<br/></p>
<p>In [51]: data.drop_duplicates(['k1', 'k2'], keep='last')<br/>Out[51]: <br/>    k1  k2  v1<br/>0  one   1   0<br/>1  two   1   1<br/>2  one   2   2<br/>3  two   3   3<br/>4  one   3   4<br/>6  two   4   6<br/></p>
<p>Transforming Data Using a Function or Mapping<br/>For many datasets, you may wish to perform some transformation based on the val&#8208;<br/>ues in an array, Series, or column in a DataFrame. Consider the following hypotheti&#8208;<br/>cal data collected about various kinds of meat:<br/></p>
<p>In [52]: data = pd.DataFrame({'food': ['bacon', 'pulled pork', 'bacon',<br/>   ....:                               'Pastrami', 'corned beef', 'Bacon',<br/>   ....:                               'pastrami', 'honey ham', 'nova lox'],<br/>   ....:                      'ounces': [4, 3, 12, 6, 7.5, 8, 3, 5, 6]})<br/></p>
<p>In [53]: data<br/>Out[53]: <br/>          food  ounces<br/>0        bacon     4.0<br/>1  pulled pork     3.0<br/>2        bacon    12.0<br/></p>
<p>198 | Chapter 7: Data Cleaning and Preparation</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>3     Pastrami     6.0<br/>4  corned beef     7.5<br/>5        Bacon     8.0<br/>6     pastrami     3.0<br/>7    honey ham     5.0<br/>8     nova lox     6.0<br/></p>
<p>Suppose you wanted to add a column indicating the type of animal that each food<br/>came from. Let&#8217;s write down a mapping of each distinct meat type to the kind of<br/>animal:<br/></p>
<p>meat_to_animal = {<br/>  'bacon': 'pig',<br/>  'pulled pork': 'pig',<br/>  'pastrami': 'cow',<br/>  'corned beef': 'cow',<br/>  'honey ham': 'pig',<br/>  'nova lox': 'salmon'<br/>}<br/></p>
<p>The map method on a Series accepts a function or dict-like object containing a map&#8208;<br/>ping, but here we have a small problem in that some of the meats are capitalized and<br/>others are not. Thus, we need to convert each value to lowercase using the str.lower<br/>Series method:<br/></p>
<p>In [55]: lowercased = data['food'].str.lower()<br/></p>
<p>In [56]: lowercased<br/>Out[56]: <br/>0          bacon<br/>1    pulled pork<br/>2          bacon<br/>3       pastrami<br/>4    corned beef<br/>5          bacon<br/>6       pastrami<br/>7      honey ham<br/>8       nova lox<br/>Name: food, dtype: object<br/></p>
<p>In [57]: data['animal'] = lowercased.map(meat_to_animal)<br/></p>
<p>In [58]: data<br/>Out[58]: <br/>          food  ounces  animal<br/>0        bacon     4.0     pig<br/>1  pulled pork     3.0     pig<br/>2        bacon    12.0     pig<br/>3     Pastrami     6.0     cow<br/>4  corned beef     7.5     cow<br/>5        Bacon     8.0     pig<br/>6     pastrami     3.0     cow<br/></p>
<p>7.2 Data Transformation | 199</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>7    honey ham     5.0     pig<br/>8     nova lox     6.0  salmon<br/></p>
<p>We could also have passed a function that does all the work:<br/>In [59]: data['food'].map(<b>lambda</b> x: meat_to_animal[x.lower()])<br/>Out[59]: <br/>0       pig<br/>1       pig<br/>2       pig<br/>3       cow<br/>4       cow<br/>5       pig<br/>6       cow<br/>7       pig<br/>8    salmon<br/>Name: food, dtype: object<br/></p>
<p>Using map is a convenient way to perform element-wise transformations and other<br/>data cleaning&#8211;related operations.<br/></p>
<p>Replacing Values<br/>Filling in missing data with the fillna method is a special case of more general value<br/>replacement. As you&#8217;ve already seen, map can be used to modify a subset of values in<br/>an object but replace provides a simpler and more flexible way to do so. Let&#8217;s con&#8208;<br/>sider this Series:<br/></p>
<p>In [60]: data = pd.Series([1., -999., 2., -999., -1000., 3.])<br/></p>
<p>In [61]: data<br/>Out[61]: <br/>0       1.0<br/>1    -999.0<br/>2       2.0<br/>3    -999.0<br/>4   -1000.0<br/>5       3.0<br/>dtype: float64<br/></p>
<p>The -999 values might be sentinel values for missing data. To replace these with NA<br/>values that pandas understands, we can use replace, producing a new Series (unless<br/>you pass inplace=True):<br/></p>
<p>In [62]: data.replace(-999, np.nan)<br/>Out[62]: <br/>0       1.0<br/>1       NaN<br/>2       2.0<br/>3       NaN<br/>4   -1000.0<br/></p>
<p>200 | Chapter 7: Data Cleaning and Preparation</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>5       3.0<br/>dtype: float64<br/></p>
<p>If you want to replace multiple values at once, you instead pass a list and then the<br/>substitute value:<br/></p>
<p>In [63]: data.replace([-999, -1000], np.nan)<br/>Out[63]: <br/>0    1.0<br/>1    NaN<br/>2    2.0<br/>3    NaN<br/>4    NaN<br/>5    3.0<br/>dtype: float64<br/></p>
<p>To use a different replacement for each value, pass a list of substitutes:<br/>In [64]: data.replace([-999, -1000], [np.nan, 0])<br/>Out[64]: <br/>0    1.0<br/>1    NaN<br/>2    2.0<br/>3    NaN<br/>4    0.0<br/>5    3.0<br/>dtype: float64<br/></p>
<p>The argument passed can also be a dict:<br/>In [65]: data.replace({-999: np.nan, -1000: 0})<br/>Out[65]: <br/>0    1.0<br/>1    NaN<br/>2    2.0<br/>3    NaN<br/>4    0.0<br/>5    3.0<br/>dtype: float64<br/></p>
<p>The data.replace method is distinct from data.str.replace,<br/>which performs string substitution element-wise. We look at these<br/>string methods on Series later in the chapter.<br/></p>
<p>Renaming Axis Indexes<br/>Like values in a Series, axis labels can be similarly transformed by a function or map&#8208;<br/>ping of some form to produce new, differently labeled objects. You can also modify<br/>the axes in-place without creating a new data structure. Here&#8217;s a simple example:<br/></p>
<p>7.2 Data Transformation | 201</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In [66]: data = pd.DataFrame(np.arange(12).reshape((3, 4)),<br/>   ....:                     index=['Ohio', 'Colorado', 'New York'],<br/>   ....:                     columns=['one', 'two', 'three', 'four'])<br/></p>
<p>Like a Series, the axis indexes have a map method:<br/>In [67]: transform = <b>lambda</b> x: x[:4].upper()<br/></p>
<p>In [68]: data.index.map(transform)<br/>Out[68]: Index(['OHIO', 'COLO', 'NEW '], dtype='object')<br/></p>
<p>You can assign to index, modifying the DataFrame in-place:<br/>In [69]: data.index = data.index.map(transform)<br/></p>
<p>In [70]: data<br/>Out[70]: <br/>      one  two  three  four<br/>OHIO    0    1      2     3<br/>COLO    4    5      6     7<br/>NEW     8    9     10    11<br/></p>
<p>If you want to create a transformed version of a dataset without modifying the origi&#8208;<br/>nal, a useful method is rename:<br/></p>
<p>In [71]: data.rename(index=str.title, columns=str.upper)<br/>Out[71]: <br/>      ONE  TWO  THREE  FOUR<br/>Ohio    0    1      2     3<br/>Colo    4    5      6     7<br/>New     8    9     10    11<br/></p>
<p>Notably, rename can be used in conjunction with a dict-like object providing new val&#8208;<br/>ues for a subset of the axis labels:<br/></p>
<p>In [72]: data.rename(index={'OHIO': 'INDIANA'},<br/>   ....:             columns={'three': 'peekaboo'})<br/>Out[72]: <br/>         one  two  peekaboo  four<br/>INDIANA    0    1         2     3<br/>COLO       4    5         6     7<br/>NEW        8    9        10    11<br/></p>
<p>rename saves you from the chore of copying the DataFrame manually and assigning<br/>to its index and columns attributes. Should you wish to modify a dataset in-place,<br/>pass inplace=True:<br/></p>
<p>In [73]: data.rename(index={'OHIO': 'INDIANA'}, inplace=True)<br/></p>
<p>In [74]: data<br/>Out[74]: <br/>         one  two  three  four<br/>INDIANA    0    1      2     3<br/></p>
<p>202 | Chapter 7: Data Cleaning and Preparation</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>COLO       4    5      6     7<br/>NEW        8    9     10    11<br/></p>
<p>Discretization and Binning<br/>Continuous data is often discretized or otherwise separated into &#8220;bins&#8221; for analysis.<br/>Suppose you have data about a group of people in a study, and you want to group<br/>them into discrete age buckets:<br/></p>
<p>In [75]: ages = [20, 22, 25, 27, 21, 23, 37, 31, 61, 45, 41, 32]<br/></p>
<p>Let&#8217;s divide these into bins of 18 to 25, 26 to 35, 36 to 60, and finally 61 and older. To<br/>do so, you have to use cut, a function in pandas:<br/></p>
<p>In [76]: bins = [18, 25, 35, 60, 100]<br/></p>
<p>In [77]: cats = pd.cut(ages, bins)<br/></p>
<p>In [78]: cats<br/>Out[78]: <br/>[(18, 25], (18, 25], (18, 25], (25, 35], (18, 25], ..., (25, 35], (60, 100], (35,<br/> 60], (35, 60], (25, 35]]<br/>Length: 12<br/>Categories (4, interval[int64]): [(18, 25] &lt; (25, 35] &lt; (35, 60] &lt; (60, 100]]<br/></p>
<p>The object pandas returns is a special Categorical object. The output you see<br/>describes the bins computed by pandas.cut. You can treat it like an array of strings<br/>indicating the bin name; internally it contains a categories array specifying the dis&#8208;<br/>tinct category names along with a labeling for the ages data in the codes attribute:<br/></p>
<p>In [79]: cats.codes<br/>Out[79]: array([0, 0, 0, 1, 0, 0, 2, 1, 3, 2, 2, 1], dtype=int8)<br/></p>
<p>In [80]: cats.categories<br/>Out[80]: <br/>IntervalIndex([(18, 25], (25, 35], (35, 60], (60, 100]]<br/>              closed='right',<br/>              dtype='interval[int64]')<br/></p>
<p>In [81]: pd.value_counts(cats)<br/>Out[81]: <br/>(18, 25]     5<br/>(35, 60]     3<br/>(25, 35]     3<br/>(60, 100]    1<br/>dtype: int64<br/></p>
<p>Note that pd.value_counts(cats) are the bin counts for the result of pandas.cut.<br/>Consistent with mathematical notation for intervals, a parenthesis means that the side<br/>is <i>open</i>, while the square bracket means it is <i>closed</i> (inclusive). You can change which<br/>side is closed by passing right=False:<br/></p>
<p>7.2 Data Transformation | 203</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In [82]: pd.cut(ages, [18, 26, 36, 61, 100], right=False)<br/>Out[82]: <br/>[[18, 26), [18, 26), [18, 26), [26, 36), [18, 26), ..., [26, 36), [61, 100), [36,<br/> 61), [36, 61), [26, 36)]<br/>Length: 12<br/>Categories (4, interval[int64]): [[18, 26) &lt; [26, 36) &lt; [36, 61) &lt; [61, 100)]<br/></p>
<p>You can also pass your own bin names by passing a list or array to the labels option:<br/>In [83]: group_names = ['Youth', 'YoungAdult', 'MiddleAged', 'Senior']<br/></p>
<p>In [84]: pd.cut(ages, bins, labels=group_names)<br/>Out[84]: <br/>[Youth, Youth, Youth, YoungAdult, Youth, ..., YoungAdult, Senior, MiddleAged, Mid<br/>dleAged, YoungAdult]<br/>Length: 12<br/>Categories (4, object): [Youth &lt; YoungAdult &lt; MiddleAged &lt; Senior]<br/></p>
<p>If you pass an integer number of bins to cut instead of explicit bin edges, it will com&#8208;<br/>pute equal-length bins based on the minimum and maximum values in the data.<br/>Consider the case of some uniformly distributed data chopped into fourths:<br/></p>
<p>In [85]: data = np.random.rand(20)<br/></p>
<p>In [86]: pd.cut(data, 4, precision=2)<br/>Out[86]: <br/>[(0.34, 0.55], (0.34, 0.55], (0.76, 0.97], (0.76, 0.97], (0.34, 0.55], ..., (0.34<br/>, 0.55], (0.34, 0.55], (0.55, 0.76], (0.34, 0.55], (0.12, 0.34]]<br/>Length: 20<br/>Categories (4, interval[float64]): [(0.12, 0.34] &lt; (0.34, 0.55] &lt; (0.55, 0.76] &lt; <br/>(0.76, 0.97]]<br/></p>
<p>The precision=2 option limits the decimal precision to two digits.<br/>A closely related function, qcut, bins the data based on sample quantiles. Depending<br/>on the distribution of the data, using cut will not usually result in each bin having the<br/>same number of data points. Since qcut uses sample quantiles instead, by definition<br/>you will obtain roughly equal-size bins:<br/></p>
<p>In [87]: data = np.random.randn(1000)  <i># Normally distributed<br/></i></p>
<p>In [88]: cats = pd.qcut(data, 4)  <i># Cut into quartiles<br/></i></p>
<p>In [89]: cats<br/>Out[89]: <br/>[(-0.0265, 0.62], (0.62, 3.928], (-0.68, -0.0265], (0.62, 3.928], (-0.0265, 0.62]<br/>, ..., (-0.68, -0.0265], (-0.68, -0.0265], (-2.95, -0.68], (0.62, 3.928], (-0.68,<br/> -0.0265]]<br/>Length: 1000<br/>Categories (4, interval[float64]): [(-2.95, -0.68] &lt; (-0.68, -0.0265] &lt; (-0.0265,<br/> 0.62] &lt;<br/>                                    (0.62, 3.928]]<br/></p>
<p>204 | Chapter 7: Data Cleaning and Preparation</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In [90]: pd.value_counts(cats)<br/>Out[90]: <br/>(0.62, 3.928]       250<br/>(-0.0265, 0.62]     250<br/>(-0.68, -0.0265]    250<br/>(-2.95, -0.68]      250<br/>dtype: int64<br/></p>
<p>Similar to cut you can pass your own quantiles (numbers between 0 and 1, inclusive):<br/>In [91]: pd.qcut(data, [0, 0.1, 0.5, 0.9, 1.])<br/>Out[91]: <br/>[(-0.0265, 1.286], (-0.0265, 1.286], (-1.187, -0.0265], (-0.0265, 1.286], (-0.026<br/>5, 1.286], ..., (-1.187, -0.0265], (-1.187, -0.0265], (-2.95, -1.187], (-0.0265, <br/>1.286], (-1.187, -0.0265]]<br/>Length: 1000<br/>Categories (4, interval[float64]): [(-2.95, -1.187] &lt; (-1.187, -0.0265] &lt; (-0.026<br/>5, 1.286] &lt;<br/>                                    (1.286, 3.928]]<br/></p>
<p>We&#8217;ll return to cut and qcut later in the chapter during our discussion of aggregation<br/>and group operations, as these discretization functions are especially useful for quan&#8208;<br/>tile and group analysis.<br/></p>
<p>Detecting and Filtering Outliers<br/>Filtering or transforming outliers is largely a matter of applying array operations.<br/>Consider a DataFrame with some normally distributed data:<br/></p>
<p>In [92]: data = pd.DataFrame(np.random.randn(1000, 4))<br/></p>
<p>In [93]: data.describe()<br/>Out[93]: <br/>                 0            1            2            3<br/>count  1000.000000  1000.000000  1000.000000  1000.000000<br/>mean      0.049091     0.026112    -0.002544    -0.051827<br/>std       0.996947     1.007458     0.995232     0.998311<br/>min      -3.645860    -3.184377    -3.745356    -3.428254<br/>25%      -0.599807    -0.612162    -0.687373    -0.747478<br/>50%       0.047101    -0.013609    -0.022158    -0.088274<br/>75%       0.756646     0.695298     0.699046     0.623331<br/>max       2.653656     3.525865     2.735527     3.366626<br/></p>
<p>Suppose you wanted to find values in one of the columns exceeding 3 in absolute<br/>value:<br/></p>
<p>In [94]: col = data[2]<br/></p>
<p>In [95]: col[np.abs(col) &gt; 3]<br/>Out[95]: <br/>41    -3.399312<br/>136   -3.745356<br/>Name: 2, dtype: float64<br/></p>
<p>7.2 Data Transformation | 205</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>To select all rows having a value exceeding 3 or &#8211;3, you can use the any method on a<br/>boolean DataFrame:<br/></p>
<p>In [96]: data[(np.abs(data) &gt; 3).any(1)]<br/>Out[96]: <br/>            0         1         2         3<br/>41   0.457246 -0.025907 -3.399312 -0.974657<br/>60   1.951312  3.260383  0.963301  1.201206<br/>136  0.508391 -0.196713 -3.745356 -1.520113<br/>235 -0.242459 -3.056990  1.918403 -0.578828<br/>258  0.682841  0.326045  0.425384 -3.428254<br/>322  1.179227 -3.184377  1.369891 -1.074833<br/>544 -3.548824  1.553205 -2.186301  1.277104<br/>635 -0.578093  0.193299  1.397822  3.366626<br/>782 -0.207434  3.525865  0.283070  0.544635<br/>803 -3.645860  0.255475 -0.549574 -1.907459<br/></p>
<p>Values can be set based on these criteria. Here is code to cap values outside the inter&#8208;<br/>val &#8211;3 to 3:<br/></p>
<p>In [97]: data[np.abs(data) &gt; 3] = np.sign(data) * 3<br/></p>
<p>In [98]: data.describe()<br/>Out[98]: <br/>                 0            1            2            3<br/>count  1000.000000  1000.000000  1000.000000  1000.000000<br/>mean      0.050286     0.025567    -0.001399    -0.051765<br/>std       0.992920     1.004214     0.991414     0.995761<br/>min      -3.000000    -3.000000    -3.000000    -3.000000<br/>25%      -0.599807    -0.612162    -0.687373    -0.747478<br/>50%       0.047101    -0.013609    -0.022158    -0.088274<br/>75%       0.756646     0.695298     0.699046     0.623331<br/>max       2.653656     3.000000     2.735527     3.000000<br/></p>
<p>The statement np.sign(data) produces 1 and &#8211;1 values based on whether the values<br/>in data are positive or negative:<br/></p>
<p>In [99]: np.sign(data).head()<br/>Out[99]: <br/>     0    1    2    3<br/>0 -1.0  1.0 -1.0  1.0<br/>1  1.0 -1.0  1.0 -1.0<br/>2  1.0  1.0  1.0 -1.0<br/>3 -1.0 -1.0  1.0 -1.0<br/>4 -1.0  1.0 -1.0 -1.0<br/></p>
<p>Permutation and Random Sampling<br/>Permuting (randomly reordering) a Series or the rows in a DataFrame is easy to do<br/>using the numpy.random.permutation function. Calling permutation with the length<br/>of the axis you want to permute produces an array of integers indicating the new<br/>ordering:<br/></p>
<p>206 | Chapter 7: Data Cleaning and Preparation</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In [100]: df = pd.DataFrame(np.arange(5 * 4).reshape((5, 4)))<br/></p>
<p>In [101]: sampler = np.random.permutation(5)<br/></p>
<p>In [102]: sampler<br/>Out[102]: array([3, 1, 4, 2, 0])<br/></p>
<p>That array can then be used in iloc-based indexing or the equivalent take function:<br/>In [103]: df<br/>Out[103]: <br/>    0   1   2   3<br/>0   0   1   2   3<br/>1   4   5   6   7<br/>2   8   9  10  11<br/>3  12  13  14  15<br/>4  16  17  18  19<br/></p>
<p>In [104]: df.take(sampler)<br/>Out[104]: <br/>    0   1   2   3<br/>3  12  13  14  15<br/>1   4   5   6   7<br/>4  16  17  18  19<br/>2   8   9  10  11<br/>0   0   1   2   3<br/></p>
<p>To select a random subset without replacement, you can use the sample method on<br/>Series and DataFrame:<br/></p>
<p>In [105]: df.sample(n=3)<br/>Out[105]: <br/>    0   1   2   3<br/>3  12  13  14  15<br/>4  16  17  18  19<br/>2   8   9  10  11<br/></p>
<p>To generate a sample <i>with</i> replacement (to allow repeat choices), pass replace=True<br/>to sample:<br/></p>
<p>In [106]: choices = pd.Series([5, 7, -1, 6, 4])<br/></p>
<p>In [107]: draws = choices.sample(n=10, replace=True)<br/></p>
<p>In [108]: draws<br/>Out[108]: <br/>4    4<br/>1    7<br/>4    4<br/>2   -1<br/>0    5<br/>3    6<br/>1    7<br/></p>
<p>7.2 Data Transformation | 207</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>4    4<br/>0    5<br/>4    4<br/>dtype: int64<br/></p>
<p>Computing Indicator/Dummy Variables<br/>Another type of transformation for statistical modeling or machine learning applica&#8208;<br/>tions is converting a categorical variable into a &#8220;dummy&#8221; or &#8220;indicator&#8221; matrix. If a<br/>column in a DataFrame has k distinct values, you would derive a matrix or Data&#8208;<br/>Frame with k columns containing all 1s and 0s. pandas has a get_dummies function<br/>for doing this, though devising one yourself is not difficult. Let&#8217;s return to an earlier<br/>example DataFrame:<br/></p>
<p>In [109]: df = pd.DataFrame({'key': ['b', 'b', 'a', 'c', 'a', 'b'],<br/>   .....:                    'data1': range(6)})<br/></p>
<p>In [110]: pd.get_dummies(df['key'])<br/>Out[110]: <br/>   a  b  c<br/>0  0  1  0<br/>1  0  1  0<br/>2  1  0  0<br/>3  0  0  1<br/>4  1  0  0<br/>5  0  1  0<br/></p>
<p>In some cases, you may want to add a prefix to the columns in the indicator Data&#8208;<br/>Frame, which can then be merged with the other data. get_dummies has a prefix argu&#8208;<br/>ment for doing this:<br/></p>
<p>In [111]: dummies = pd.get_dummies(df['key'], prefix='key')<br/></p>
<p>In [112]: df_with_dummy = df[['data1']].join(dummies)<br/></p>
<p>In [113]: df_with_dummy<br/>Out[113]: <br/>   data1  key_a  key_b  key_c<br/>0      0      0      1      0<br/>1      1      0      1      0<br/>2      2      1      0      0<br/>3      3      0      0      1<br/>4      4      1      0      0<br/>5      5      0      1      0<br/></p>
<p>If a row in a DataFrame belongs to multiple categories, things are a bit more compli&#8208;<br/>cated. Let&#8217;s look at the MovieLens 1M dataset, which is investigated in more detail in<br/>Chapter 14:<br/></p>
<p>208 | Chapter 7: Data Cleaning and Preparation</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In [114]: mnames = ['movie_id', 'title', 'genres']<br/></p>
<p>In [115]: movies = pd.read_table('datasets/movielens/movies.dat', sep='::',<br/>   .....:                        header=None, names=mnames)<br/></p>
<p>In [116]: movies[:10]<br/>Out[116]: <br/>   movie_id                               title                        genres<br/>0         1                    Toy Story (1995)   Animation|Children's|Comedy<br/>1         2                      Jumanji (1995)  Adventure|Children's|Fantasy<br/>2         3             Grumpier Old Men (1995)                Comedy|Romance<br/>3         4            Waiting to Exhale (1995)                  Comedy|Drama<br/>4         5  Father of the Bride Part II (1995)                        Comedy<br/>5         6                         Heat (1995)         Action|Crime|Thriller<br/>6         7                      Sabrina (1995)                Comedy|Romance<br/>7         8                 Tom <b>and</b> Huck (1995)          Adventure|Children's<br/>8         9                 Sudden Death (1995)                        Action<br/>9        10                    GoldenEye (1995)     Action|Adventure|Thriller<br/></p>
<p>Adding indicator variables for each genre requires a little bit of wrangling. First, we<br/>extract the list of unique genres in the dataset:<br/></p>
<p>In [117]: all_genres = []<br/></p>
<p>In [118]: <b>for</b> x <b>in</b> movies.genres:<br/>   .....:     all_genres.extend(x.split('|'))<br/></p>
<p>In [119]: genres = pd.unique(all_genres)<br/></p>
<p>Now we have:<br/>In [120]: genres<br/>Out[120]: <br/>array(['Animation', &quot;Children's&quot;, 'Comedy', 'Adventure', 'Fantasy',<br/>       'Romance', 'Drama', 'Action', 'Crime', 'Thriller', 'Horror',<br/>       'Sci-Fi', 'Documentary', 'War', 'Musical', 'Mystery', 'Film-Noir',<br/>       'Western'], dtype=object)<br/></p>
<p>One way to construct the indicator DataFrame is to start with a DataFrame of all<br/>zeros:<br/></p>
<p>In [121]: zero_matrix = np.zeros((len(movies), len(genres)))<br/></p>
<p>In [122]: dummies = pd.DataFrame(zero_matrix, columns=genres)<br/></p>
<p>Now, iterate through each movie and set entries in each row of dummies to 1. To do<br/>this, we use the dummies.columns to compute the column indices for each genre:<br/></p>
<p>7.2 Data Transformation | 209</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In [123]: gen = movies.genres[0]<br/></p>
<p>In [124]: gen.split('|')<br/>Out[124]: ['Animation', &quot;Children's&quot;, 'Comedy']<br/></p>
<p>In [125]: dummies.columns.get_indexer(gen.split('|'))<br/>Out[125]: array([0, 1, 2])<br/></p>
<p>Then, we can use .iloc to set values based on these indices:<br/>In [126]: <b>for</b> i, gen <b>in</b> enumerate(movies.genres):<br/>   .....:     indices = dummies.columns.get_indexer(gen.split('|'))<br/>   .....:     dummies.iloc[i, indices] = 1<br/>   .....:<br/></p>
<p>Then, as before, you can combine this with movies:<br/>In [127]: movies_windic = movies.join(dummies.add_prefix('Genre_'))<br/></p>
<p>In [128]: movies_windic.iloc[0]<br/>Out[128]: <br/>movie_id                                       1<br/>title                           Toy Story (1995)<br/>genres               Animation|Children's|Comedy<br/>Genre_Animation                                1<br/>Genre_Children's                               1<br/>Genre_Comedy                                   1<br/>Genre_Adventure                                0<br/>Genre_Fantasy                                  0<br/>Genre_Romance                                  0<br/>Genre_Drama                                    0<br/>                                ...             <br/>Genre_Crime                                    0<br/>Genre_Thriller                                 0<br/>Genre_Horror                                   0<br/>Genre_Sci-Fi                                   0<br/>Genre_Documentary                              0<br/>Genre_War                                      0<br/>Genre_Musical                                  0<br/>Genre_Mystery                                  0<br/>Genre_Film-Noir                                0<br/>Genre_Western                                  0<br/>Name: 0, Length: 21, dtype: object<br/></p>
<p>For much larger data, this method of constructing indicator vari&#8208;<br/>ables with multiple membership is not especially speedy. It would<br/>be better to write a lower-level function that writes directly to a<br/>NumPy array, and then wrap the result in a DataFrame.<br/></p>
<p>A useful recipe for statistical applications is to combine get_dummies with a discreti&#8208;<br/>zation function like cut:<br/></p>
<p>210 | Chapter 7: Data Cleaning and Preparation</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In [129]: np.random.seed(12345)<br/></p>
<p>In [130]: values = np.random.rand(10)<br/></p>
<p>In [131]: values<br/>Out[131]: <br/>array([ 0.9296,  0.3164,  0.1839,  0.2046,  0.5677,  0.5955,  0.9645,<br/>        0.6532,  0.7489,  0.6536])<br/></p>
<p>In [132]: bins = [0, 0.2, 0.4, 0.6, 0.8, 1]<br/></p>
<p>In [133]: pd.get_dummies(pd.cut(values, bins))<br/>Out[133]: <br/>   (0.0, 0.2]  (0.2, 0.4]  (0.4, 0.6]  (0.6, 0.8]  (0.8, 1.0]<br/>0           0           0           0           0           1<br/>1           0           1           0           0           0<br/>2           1           0           0           0           0<br/>3           0           1           0           0           0<br/>4           0           0           1           0           0<br/>5           0           0           1           0           0<br/>6           0           0           0           0           1<br/>7           0           0           0           1           0<br/>8           0           0           0           1           0<br/>9           0           0           0           1           0<br/></p>
<p>We set the random seed with numpy.random.seed to make the example deterministic.<br/>We will look again at pandas.get_dummies later in the book.<br/></p>
<p>7.3 String Manipulation<br/>Python has long been a popular raw data manipulation language in part due to its<br/>ease of use for string and text processing. Most text operations are made simple with<br/>the string object&#8217;s built-in methods. For more complex pattern matching and text<br/>manipulations, regular expressions may be needed. pandas adds to the mix by ena&#8208;<br/>bling you to apply string and regular expressions concisely on whole arrays of data,<br/>additionally handling the annoyance of missing data.<br/></p>
<p>String Object Methods<br/>In many string munging and scripting applications, built-in string methods are suffi&#8208;<br/>cient. As an example, a comma-separated string can be broken into pieces with<br/>split:<br/></p>
<p>In [134]: val = 'a,b,  guido'<br/></p>
<p>In [135]: val.split(',')<br/>Out[135]: ['a', 'b', '  guido']<br/></p>
<p>split is often combined with strip to trim whitespace (including line breaks):<br/></p>
<p>7.3 String Manipulation | 211</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In [136]: pieces = [x.strip() <b>for</b> x <b>in</b> val.split(',')]<br/></p>
<p>In [137]: pieces<br/>Out[137]: ['a', 'b', 'guido']<br/></p>
<p>These substrings could be concatenated together with a two-colon delimiter using<br/>addition:<br/></p>
<p>In [138]: first, second, third = pieces<br/></p>
<p>In [139]: first + '::' + second + '::' + third<br/>Out[139]: 'a::b::guido'<br/></p>
<p>But this isn&#8217;t a practical generic method. A faster and more Pythonic way is to pass a<br/>list or tuple to the join method on the string '::':<br/></p>
<p>In [140]: '::'.join(pieces)<br/>Out[140]: 'a::b::guido'<br/></p>
<p>Other methods are concerned with locating substrings. Using Python&#8217;s in keyword is<br/>the best way to detect a substring, though index and find can also be used:<br/></p>
<p>In [141]: 'guido' <b>in</b> val<br/>Out[141]: True<br/></p>
<p>In [142]: val.index(',')<br/>Out[142]: 1<br/></p>
<p>In [143]: val.find(':')<br/>Out[143]: -1<br/></p>
<p>Note the difference between find and index is that index raises an exception if the<br/>string isn&#8217;t found (versus returning &#8211;1):<br/></p>
<p>In [144]: val.index(':')<br/>---------------------------------------------------------------------------<br/><b>ValueError</b>                                Traceback (most recent call last)<br/>&lt;ipython-input-144-280f8b2856ce&gt; <b>in</b> &lt;module&gt;()<br/>----&gt; 1 val.index(':')<br/><b>ValueError</b>: substring <b>not</b> found<br/></p>
<p>Relatedly, count returns the number of occurrences of a particular substring:<br/>In [145]: val.count(',')<br/>Out[145]: 2<br/></p>
<p>replace will substitute occurrences of one pattern for another. It is commonly used<br/>to delete patterns, too, by passing an empty string:<br/></p>
<p>In [146]: val.replace(',', '::')<br/>Out[146]: 'a::b::  guido'<br/></p>
<p>In [147]: val.replace(',', '')<br/>Out[147]: 'ab  guido'<br/></p>
<p>212 | Chapter 7: Data Cleaning and Preparation</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>See Table 7-3 for a listing of some of Python&#8217;s string methods.<br/>Regular expressions can also be used with many of these operations, as you&#8217;ll see.<br/><i>Table 7-3. Python built-in string methods<br/></i></p>
<p>Argument Description<br/>count Return the number of non-overlapping occurrences of substring in the string.<br/>endswith Returns True if string ends with suffix.<br/>startswith Returns True if string starts with prefix.<br/>join Use string as delimiter for concatenating a sequence of other strings.<br/>index Return position of first character in substring if found in the string; raises ValueError if not found.<br/>find Return position of first character of first occurrence of substring in the string; like index, but returns &#8211;1<br/></p>
<p>if not found.<br/>rfind Return position of first character of last occurrence of substring in the string; returns &#8211;1 if not found.<br/>replace Replace occurrences of string with another string.<br/>strip, <br/>rstrip, <br/>lstrip<br/></p>
<p>Trim whitespace, including newlines; equivalent to x.strip() (and rstrip, lstrip, respectively)<br/>for each element.<br/></p>
<p>split Break string into list of substrings using passed delimiter.<br/>lower Convert alphabet characters to lowercase.<br/>upper Convert alphabet characters to uppercase.<br/>casefold Convert characters to lowercase, and convert any region-specific variable character combinations to a<br/></p>
<p>common comparable form.<br/>ljust, <br/>rjust<br/></p>
<p>Left justify or right justify, respectively; pad opposite side of string with spaces (or some other fill<br/>character) to return a string with a minimum width.<br/></p>
<p>Regular Expressions<br/><i>Regular expressions</i> provide a flexible way to search or match (often more complex)<br/>string patterns in text. A single expression, commonly called a <i>regex</i>, is a string<br/>formed according to the regular expression language. Python&#8217;s built-in re module is<br/>responsible for applying regular expressions to strings; I&#8217;ll give a number of examples<br/>of its use here.<br/></p>
<p>The art of writing regular expressions could be a chapter of its own<br/>and thus is outside the book&#8217;s scope. There are many excellent tuto&#8208;<br/>rials and references available on the internet and in other books.<br/></p>
<p>The re module functions fall into three categories: pattern matching, substitution,<br/>and splitting. Naturally these are all related; a regex describes a pattern to locate in the<br/>text, which can then be used for many purposes. Let&#8217;s look at a simple example:<br/></p>
<p>7.3 String Manipulation | 213</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>suppose we wanted to split a string with a variable number of whitespace characters<br/>(tabs, spaces, and newlines). The regex describing one or more whitespace characters<br/>is \s+:<br/></p>
<p>In [148]: <b>import</b> <b>re<br/></b></p>
<p>In [149]: text = &quot;foo    bar<b>\t</b> baz  <b>\t</b>qux&quot;<br/></p>
<p>In [150]: re.split('\s+', text)<br/>Out[150]: ['foo', 'bar', 'baz', 'qux']<br/></p>
<p>When you call re.split('\s+', text), the regular expression is first <i>compiled</i>, and<br/>then its split method is called on the passed text. You can compile the regex yourself <br/>with re.compile, forming a reusable regex object:<br/></p>
<p>In [151]: regex = re.compile('\s+')<br/></p>
<p>In [152]: regex.split(text)<br/>Out[152]: ['foo', 'bar', 'baz', 'qux']<br/></p>
<p>If, instead, you wanted to get a list of all patterns matching the regex, you can use the<br/>findall method:<br/></p>
<p>In [153]: regex.findall(text)<br/>Out[153]: ['    ', '<b>\t</b> ', '  <b>\t</b>']<br/></p>
<p>To avoid unwanted escaping with \ in a regular expression, use <i>raw<br/></i>string literals like r'C:\x' instead of the equivalent 'C:\\x'.<br/></p>
<p>Creating a regex object with re.compile is highly recommended if you intend to<br/>apply the same expression to many strings; doing so will save CPU cycles.<br/>match and search are closely related to findall. While findall returns all matches<br/>in a string, search returns only the first match. More rigidly, match <i>only</i> matches at<br/>the beginning of the string. As a less trivial example, let&#8217;s consider a block of text and<br/>a regular expression capable of identifying most email addresses:<br/></p>
<p>text = &quot;&quot;&quot;Dave dave@google.com<br/>Steve steve@gmail.com<br/>Rob rob@gmail.com<br/>Ryan ryan@yahoo.com<br/>&quot;&quot;&quot;<br/>pattern = r'[A-Z0-9._%+-]+@[A-Z0-9.-]+\.[A-Z]{2,4}'<br/></p>
<p><i># re.IGNORECASE makes the regex case-insensitive<br/></i>regex = re.compile(pattern, flags=re.IGNORECASE)<br/></p>
<p>Using findall on the text produces a list of the email addresses:<br/></p>
<p>214 | Chapter 7: Data Cleaning and Preparation</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In [155]: regex.findall(text)<br/>Out[155]: <br/>['dave@google.com',<br/> 'steve@gmail.com',<br/> 'rob@gmail.com',<br/> 'ryan@yahoo.com']<br/></p>
<p>search returns a special match object for the first email address in the text. For the<br/>preceding regex, the match object can only tell us the start and end position of the<br/>pattern in the string:<br/></p>
<p>In [156]: m = regex.search(text)<br/></p>
<p>In [157]: m<br/>Out[157]: &lt;_sre.SRE_Match object; span=(5, 20), match='dave@google.com'&gt;<br/></p>
<p>In [158]: text[m.start():m.end()]<br/>Out[158]: 'dave@google.com'<br/></p>
<p>regex.match returns None, as it only will match if the pattern occurs at the start of the<br/>string:<br/></p>
<p>In [159]: <b>print</b>(regex.match(text))<br/>None<br/></p>
<p>Relatedly, sub will return a new string with occurrences of the pattern replaced by the<br/>a new string:<br/></p>
<p>In [160]: <b>print</b>(regex.sub('REDACTED', text))<br/>Dave REDACTED<br/>Steve REDACTED<br/>Rob REDACTED<br/>Ryan REDACTED<br/></p>
<p>Suppose you wanted to find email addresses and simultaneously segment each<br/>address into its three components: username, domain name, and domain suffix. To<br/>do this, put parentheses around the parts of the pattern to segment:<br/></p>
<p>In [161]: pattern = r'([A-Z0-9._%+-]+)@([A-Z0-9.-]+)\.([A-Z]{2,4})'<br/></p>
<p>In [162]: regex = re.compile(pattern, flags=re.IGNORECASE)<br/></p>
<p>A match object produced by this modified regex returns a tuple of the pattern com&#8208;<br/>ponents with its groups method:<br/></p>
<p>In [163]: m = regex.match('wesm@bright.net')<br/></p>
<p>In [164]: m.groups()<br/>Out[164]: ('wesm', 'bright', 'net')<br/></p>
<p>findall returns a list of tuples when the pattern has groups:<br/>In [165]: regex.findall(text)<br/>Out[165]: <br/></p>
<p>7.3 String Manipulation | 215</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>[('dave', 'google', 'com'),<br/> ('steve', 'gmail', 'com'),<br/> ('rob', 'gmail', 'com'),<br/> ('ryan', 'yahoo', 'com')]<br/></p>
<p>sub also has access to groups in each match using special symbols like \1 and \2. The<br/>symbol \1 corresponds to the first matched group, \2 corresponds to the second, and<br/>so forth:<br/></p>
<p>In [166]: <b>print</b>(regex.sub(r'Username: \1, Domain: \2, Suffix: \3', text))<br/>Dave Username: dave, Domain: google, Suffix: com<br/>Steve Username: steve, Domain: gmail, Suffix: com<br/>Rob Username: rob, Domain: gmail, Suffix: com<br/>Ryan Username: ryan, Domain: yahoo, Suffix: com<br/></p>
<p>There is much more to regular expressions in Python, most of which is outside the<br/>book&#8217;s scope. Table 7-4 provides a brief summary.<br/><i>Table 7-4. Regular expression methods<br/></i></p>
<p>Argument Description<br/>findall Return all non-overlapping matching patterns in a string as a list<br/>finditer Like findall, but returns an iterator<br/>match Match pattern at start of string and optionally segment pattern components into groups; if the pattern<br/></p>
<p>matches, returns a match object, and otherwise None<br/>search Scan string for match to pattern; returning a match object if so; unlike match, the match can be anywhere in<br/></p>
<p>the string as opposed to only at the beginning<br/>split Break string into pieces at each occurrence of pattern<br/>sub, subn Replace all (sub) or first n occurrences (subn) of pattern in string with replacement expression; use symbols<br/></p>
<p>\1, \2, ... to refer to match group elements in the replacement string<br/></p>
<p>Vectorized String Functions in pandas<br/>Cleaning up a messy dataset for analysis often requires a lot of string munging and<br/>regularization. To complicate matters, a column containing strings will sometimes<br/>have missing data:<br/></p>
<p>In [167]: data = {'Dave': 'dave@google.com', 'Steve': 'steve@gmail.com',<br/>   .....:         'Rob': 'rob@gmail.com', 'Wes': np.nan}<br/></p>
<p>In [168]: data = pd.Series(data)<br/></p>
<p>In [169]: data<br/>Out[169]: <br/>Dave     dave@google.com<br/>Rob        rob@gmail.com<br/>Steve    steve@gmail.com<br/>Wes                  NaN<br/>dtype: object<br/></p>
<p>216 | Chapter 7: Data Cleaning and Preparation</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In [170]: data.isnull()<br/>Out[170]: <br/>Dave     False<br/>Rob      False<br/>Steve    False<br/>Wes       True<br/>dtype: bool<br/></p>
<p>You can apply string and regular expression methods can be applied (passing a<br/>lambda or other function) to each value using data.map, but it will fail on the NA<br/>(null) values. To cope with this, Series has array-oriented methods for string opera&#8208;<br/>tions that skip NA values. These are accessed through Series&#8217;s str attribute; for exam&#8208;<br/>ple, we could check whether each email address has 'gmail' in it with str.contains:<br/></p>
<p>In [171]: data.str.contains('gmail')<br/>Out[171]: <br/>Dave     False<br/>Rob       True<br/>Steve     True<br/>Wes        NaN<br/>dtype: object<br/></p>
<p>Regular expressions can be used, too, along with any re options like IGNORECASE:<br/>In [172]: pattern<br/>Out[172]: '([A-Z0-9._%+-]+)@([A-Z0-9.-]+)<b>\\</b>.([A-Z]{2,4})'<br/></p>
<p>In [173]: data.str.findall(pattern, flags=re.IGNORECASE)<br/>Out[173]: <br/>Dave     [(dave, google, com)]<br/>Rob        [(rob, gmail, com)]<br/>Steve    [(steve, gmail, com)]<br/>Wes                        NaN<br/>dtype: object<br/></p>
<p>There are a couple of ways to do vectorized element retrieval. Either use str.get or<br/>index into the str attribute:<br/></p>
<p>In [174]: matches = data.str.match(pattern, flags=re.IGNORECASE)<br/></p>
<p>In [175]: matches<br/>Out[175]: <br/>Dave     True<br/>Rob      True<br/>Steve    True<br/>Wes       NaN<br/>dtype: object<br/></p>
<p>To access elements in the embedded lists, we can pass an index to either of these<br/>functions:<br/></p>
<p>In [176]: matches.str.get(1)<br/>Out[176]: <br/></p>
<p>7.3 String Manipulation | 217</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Dave    NaN<br/>Rob     NaN<br/>Steve   NaN<br/>Wes     NaN<br/>dtype: float64<br/></p>
<p>In [177]: matches.str[0]<br/>Out[177]: <br/>Dave    NaN<br/>Rob     NaN<br/>Steve   NaN<br/>Wes     NaN<br/>dtype: float64<br/></p>
<p>You can similarly slice strings using this syntax:<br/>In [178]: data.str[:5]<br/>Out[178]: <br/>Dave     dave@<br/>Rob      rob@g<br/>Steve    steve<br/>Wes        NaN<br/>dtype: object<br/></p>
<p>See Table 7-5 for more pandas string methods.<br/><i>Table 7-5. Partial listing of vectorized string methods<br/></i></p>
<p>Method Description<br/>cat Concatenate strings element-wise with optional delimiter<br/>contains Return boolean array if each string contains pattern/regex<br/>count Count occurrences of pattern<br/>extract Use a regular expression with groups to extract one or more strings from a Series of strings; the result<br/></p>
<p>will be a DataFrame with one column per group<br/>endswith Equivalent to x.endswith(pattern) for each element<br/>startswith Equivalent to x.startswith(pattern) for each element<br/>findall Compute list of all occurrences of pattern/regex for each string<br/>get Index into each element (retrieve <i>i</i>-th element)<br/>isalnum Equivalent to built-in str.alnum<br/>isalpha Equivalent to built-in str.isalpha<br/>isdecimal Equivalent to built-in str.isdecimal<br/>isdigit Equivalent to built-in str.isdigit<br/>islower Equivalent to built-in str.islower<br/>isnumeric Equivalent to built-in str.isnumeric<br/>isupper Equivalent to built-in str.isupper<br/>join Join strings in each element of the Series with passed separator<br/>len Compute length of each string<br/>lower, upper Convert cases; equivalent to x.lower() or x.upper() for each element<br/></p>
<p>218 | Chapter 7: Data Cleaning and Preparation</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Method Description<br/>match Use re.match with the passed regular expression on each element, returning matched groups as list<br/>pad Add whitespace to left, right, or both sides of strings<br/>center Equivalent to pad(side='both')<br/>repeat Duplicate values (e.g., s.str.repeat(3) is equivalent to x * 3 for each string)<br/>replace Replace occurrences of pattern/regex with some other string<br/>slice Slice each string in the Series<br/>split Split strings on delimiter or regular expression<br/>strip Trim whitespace from both sides, including newlines<br/>rstrip Trim whitespace on right side<br/>lstrip Trim whitespace on left side<br/></p>
<p>7.4 Conclusion<br/>Effective data preparation can significantly improve productive by enabling you to<br/>spend more time analyzing data and less time getting it ready for analysis. We have<br/>explored a number of tools in this chapter, but the coverage here is by no means com&#8208;<br/>prehensive. In the next chapter, we will explore pandas&#8217;s joining and grouping func&#8208;<br/>tionality.<br/></p>
<p>7.4 Conclusion | 219</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div>
</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>CHAPTER 8<br/>Data Wrangling: Join, Combine,<br/></p>
<p>and Reshape<br/></p>
<p>In many applications, data may be spread across a number of files or databases or be<br/>arranged in a form that is not easy to analyze. This chapter focuses on tools to help<br/>combine, join, and rearrange data.<br/>First, I introduce the concept of <i>hierarchical indexing</i> in pandas, which is used exten&#8208;<br/>sively in some of these operations. I then dig into the particular data manipulations.<br/>You can see various applied usages of these tools in Chapter 14.<br/></p>
<p>8.1 Hierarchical Indexing<br/><i>Hierarchical indexing</i> is an important feature of pandas that enables you to have mul&#8208;<br/>tiple (two or more) index <i>levels</i> on an axis. Somewhat abstractly, it provides a way for<br/>you to work with higher dimensional data in a lower dimensional form. Let&#8217;s start<br/>with a simple example; create a Series with a list of lists (or arrays) as the index:<br/></p>
<p>In [9]: data = pd.Series(np.random.randn(9),<br/>   ...:                  index=[['a', 'a', 'a', 'b', 'b', 'c', 'c', 'd', 'd'],<br/>   ...:                         [1, 2, 3, 1, 3, 1, 2, 2, 3]])<br/></p>
<p>In [10]: data<br/>Out[10]: <br/>a  1   -0.204708<br/>   2    0.478943<br/>   3   -0.519439<br/>b  1   -0.555730<br/>   3    1.965781<br/>c  1    1.393406<br/>   2    0.092908<br/>d  2    0.281746<br/></p>
<p>221</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>   3    0.769023<br/>dtype: float64<br/></p>
<p>What you&#8217;re seeing is a prettified view of a Series with a MultiIndex as its index. The<br/>&#8220;gaps&#8221; in the index display mean &#8220;use the label directly above&#8221;:<br/></p>
<p>In [11]: data.index<br/>Out[11]: <br/>MultiIndex(levels=[['a', 'b', 'c', 'd'], [1, 2, 3]],<br/>           labels=[[0, 0, 0, 1, 1, 2, 2, 3, 3], [0, 1, 2, 0, 2, 0, 1, 1, 2]])<br/></p>
<p>With a hierarchically indexed object, so-called <i>partial</i> indexing is possible, enabling<br/>you to concisely select subsets of the data:<br/></p>
<p>In [12]: data['b']<br/>Out[12]: <br/>1   -0.555730<br/>3    1.965781<br/>dtype: float64<br/></p>
<p>In [13]: data['b':'c']<br/>Out[13]: <br/>b  1   -0.555730<br/>   3    1.965781<br/>c  1    1.393406<br/>   2    0.092908<br/>dtype: float64<br/></p>
<p>In [14]: data.loc[['b', 'd']]<br/>Out[14]: <br/>b  1   -0.555730<br/>   3    1.965781<br/>d  2    0.281746<br/>   3    0.769023<br/>dtype: float64<br/></p>
<p>Selection is even possible from an &#8220;inner&#8221; level:<br/>In [15]: data.loc[:, 2]<br/>Out[15]: <br/>a    0.478943<br/>c    0.092908<br/>d    0.281746<br/>dtype: float64<br/></p>
<p>Hierarchical indexing plays an important role in reshaping data and group-based<br/>operations like forming a pivot table. For example, you could rearrange the data into<br/>a DataFrame using its unstack method:<br/></p>
<p>In [16]: data.unstack()<br/>Out[16]: <br/>          1         2         3<br/>a -0.204708  0.478943 -0.519439<br/>b -0.555730       NaN  1.965781<br/></p>
<p>222 | Chapter 8: Data Wrangling: Join, Combine, and Reshape</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>c  1.393406  0.092908       NaN<br/>d       NaN  0.281746  0.769023<br/></p>
<p>The inverse operation of unstack is stack:<br/>In [17]: data.unstack().stack()<br/>Out[17]: <br/>a  1   -0.204708<br/>   2    0.478943<br/>   3   -0.519439<br/>b  1   -0.555730<br/>   3    1.965781<br/>c  1    1.393406<br/>   2    0.092908<br/>d  2    0.281746<br/>   3    0.769023<br/>dtype: float64<br/></p>
<p>stack and unstack will be explored in more detail later in this chapter.<br/>With a DataFrame, either axis can have a hierarchical index:<br/></p>
<p>In [18]: frame = pd.DataFrame(np.arange(12).reshape((4, 3)),<br/>   ....:                      index=[['a', 'a', 'b', 'b'], [1, 2, 1, 2]],<br/>   ....:                      columns=[['Ohio', 'Ohio', 'Colorado'],<br/>   ....:                               ['Green', 'Red', 'Green']])<br/></p>
<p>In [19]: frame<br/>Out[19]: <br/>     Ohio     Colorado<br/>    Green Red    Green<br/>a 1     0   1        2<br/>  2     3   4        5<br/>b 1     6   7        8<br/>  2     9  10       11<br/></p>
<p>The hierarchical levels can have names (as strings or any Python objects). If so, these<br/>will show up in the console output:<br/></p>
<p>In [20]: frame.index.names = ['key1', 'key2']<br/></p>
<p>In [21]: frame.columns.names = ['state', 'color']<br/></p>
<p>In [22]: frame<br/>Out[22]: <br/>state      Ohio     Colorado<br/>color     Green Red    Green<br/>key1 key2                   <br/>a    1        0   1        2<br/>     2        3   4        5<br/>b    1        6   7        8<br/>     2        9  10       11<br/></p>
<p>8.1 Hierarchical Indexing | 223</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Be careful to distinguish the index names 'state' and 'color'<br/>from the row labels.<br/></p>
<p>With partial column indexing you can similarly select groups of columns:<br/>In [23]: frame['Ohio']<br/>Out[23]: <br/>color      Green  Red<br/>key1 key2            <br/>a    1         0    1<br/>     2         3    4<br/>b    1         6    7<br/>     2         9   10<br/></p>
<p>A MultiIndex can be created by itself and then reused; the columns in the preceding<br/>DataFrame with level names could be created like this:<br/></p>
<p>MultiIndex.from_arrays([['Ohio', 'Ohio', 'Colorado'], ['Green', 'Red', 'Green']],<br/>                       names=['state', 'color'])<br/></p>
<p>Reordering and Sorting Levels<br/>At times you will need to rearrange the order of the levels on an axis or sort the data<br/>by the values in one specific level. The swaplevel takes two level numbers or names<br/>and returns a new object with the levels interchanged (but the data is otherwise<br/>unaltered):<br/></p>
<p>In [24]: frame.swaplevel('key1', 'key2')<br/>Out[24]: <br/>state      Ohio     Colorado<br/>color     Green Red    Green<br/>key2 key1                   <br/>1    a        0   1        2<br/>2    a        3   4        5<br/>1    b        6   7        8<br/>2    b        9  10       11<br/></p>
<p>sort_index, on the other hand, sorts the data using only the values in a single level.<br/>When swapping levels, it&#8217;s not uncommon to also use sort_index so that the result is<br/>lexicographically sorted by the indicated level:<br/></p>
<p>In [25]: frame.sort_index(level=1)<br/>Out[25]: <br/>state      Ohio     Colorado<br/>color     Green Red    Green<br/>key1 key2                   <br/>a    1        0   1        2<br/>b    1        6   7        8<br/>a    2        3   4        5<br/></p>
<p>224 | Chapter 8: Data Wrangling: Join, Combine, and Reshape</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>b    2        9  10       11<br/></p>
<p>In [26]: frame.swaplevel(0, 1).sort_index(level=0)<br/>Out[26]: <br/>state      Ohio     Colorado<br/>color     Green Red    Green<br/>key2 key1                   <br/>1    a        0   1        2<br/>     b        6   7        8<br/>2    a        3   4        5<br/>     b        9  10       11<br/></p>
<p>Data selection performance is much better on hierarchically<br/>indexed objects if the index is lexicographically sorted starting with<br/>the outermost level&#8212;that is, the result of calling<br/>sort_index(level=0) or sort_index().<br/></p>
<p>Summary Statistics by Level<br/>Many descriptive and summary statistics on DataFrame and Series have a level<br/>option in which you can specify the level you want to aggregate by on a particular<br/>axis. Consider the above DataFrame; we can aggregate by level on either the rows or<br/>columns like so:<br/></p>
<p>In [27]: frame.sum(level='key2')<br/>Out[27]: <br/>state  Ohio     Colorado<br/>color Green Red    Green<br/>key2                    <br/>1         6   8       10<br/>2        12  14       16<br/></p>
<p>In [28]: frame.sum(level='color', axis=1)<br/>Out[28]: <br/>color      Green  Red<br/>key1 key2            <br/>a    1         2    1<br/>     2         8    4<br/>b    1        14    7<br/>     2        20   10<br/></p>
<p>Under the hood, this utilizes pandas&#8217;s groupby machinery, which will be discussed in<br/>more detail later in the book.<br/></p>
<p>Indexing with a DataFrame&#8217;s columns<br/>It&#8217;s not unusual to want to use one or more columns from a DataFrame as the row<br/>index; alternatively, you may wish to move the row index into the DataFrame&#8217;s col&#8208;<br/>umns. Here&#8217;s an example DataFrame:<br/></p>
<p>8.1 Hierarchical Indexing | 225</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In [29]: frame = pd.DataFrame({'a': range(7), 'b': range(7, 0, -1),<br/>   ....:                       'c': ['one', 'one', 'one', 'two', 'two',<br/>   ....:                             'two', 'two'],<br/>   ....:                       'd': [0, 1, 2, 0, 1, 2, 3]})<br/></p>
<p>In [30]: frame<br/>Out[30]: <br/>   a  b    c  d<br/>0  0  7  one  0<br/>1  1  6  one  1<br/>2  2  5  one  2<br/>3  3  4  two  0<br/>4  4  3  two  1<br/>5  5  2  two  2<br/>6  6  1  two  3<br/></p>
<p>DataFrame&#8217;s set_index function will create a new DataFrame using one or more of<br/>its columns as the index:<br/></p>
<p>In [31]: frame2 = frame.set_index(['c', 'd'])<br/></p>
<p>In [32]: frame2<br/>Out[32]: <br/>       a  b<br/>c   d      <br/>one 0  0  7<br/>    1  1  6<br/>    2  2  5<br/>two 0  3  4<br/>    1  4  3<br/>    2  5  2<br/>    3  6  1<br/></p>
<p>By default the columns are removed from the DataFrame, though you can leave them<br/>in:<br/></p>
<p>In [33]: frame.set_index(['c', 'd'], drop=False)<br/>Out[33]: <br/>       a  b    c  d<br/>c   d              <br/>one 0  0  7  one  0<br/>    1  1  6  one  1<br/>    2  2  5  one  2<br/>two 0  3  4  two  0<br/>    1  4  3  two  1<br/>    2  5  2  two  2<br/>    3  6  1  two  3<br/></p>
<p>reset_index, on the other hand, does the opposite of set_index; the hierarchical<br/>index levels are moved into the columns:<br/></p>
<p>226 | Chapter 8: Data Wrangling: Join, Combine, and Reshape</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In [34]: frame2.reset_index()<br/>Out[34]: <br/>     c  d  a  b<br/>0  one  0  0  7<br/>1  one  1  1  6<br/>2  one  2  2  5<br/>3  two  0  3  4<br/>4  two  1  4  3<br/>5  two  2  5  2<br/>6  two  3  6  1<br/></p>
<p>8.2 Combining and Merging Datasets<br/>Data contained in pandas objects can be combined together in a number of ways:<br/></p>
<p>&#8226; pandas.merge connects rows in DataFrames based on one or more keys. This<br/>will be familiar to users of SQL or other relational databases, as it implements<br/>database <i>join</i> operations.<br/></p>
<p>&#8226; pandas.concat concatenates or &#8220;stacks&#8221; together objects along an axis.<br/>&#8226; The combine_first instance method enables splicing together overlapping data<br/></p>
<p>to fill in missing values in one object with values from another.<br/>I will address each of these and give a number of examples. They&#8217;ll be utilized in<br/>examples throughout the rest of the book.<br/></p>
<p>Database-Style DataFrame Joins<br/><i>Merge</i> or <i>join</i> operations combine datasets by linking rows using one or more <i>keys</i>.<br/>These operations are central to relational databases (e.g., SQL-based). The merge<br/>function in pandas is the main entry point for using these algorithms on your data.<br/>Let&#8217;s start with a simple example:<br/></p>
<p>In [35]: df1 = pd.DataFrame({'key': ['b', 'b', 'a', 'c', 'a', 'a', 'b'],<br/>   ....:                     'data1': range(7)})<br/></p>
<p>In [36]: df2 = pd.DataFrame({'key': ['a', 'b', 'd'],<br/>   ....:                     'data2': range(3)})<br/></p>
<p>In [37]: df1<br/>Out[37]: <br/>   data1 key<br/>0      0   b<br/>1      1   b<br/>2      2   a<br/>3      3   c<br/>4      4   a<br/>5      5   a<br/></p>
<p>8.2 Combining and Merging Datasets | 227</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>6      6   b<br/></p>
<p>In [38]: df2<br/>Out[38]: <br/>   data2 key<br/>0      0   a<br/>1      1   b<br/>2      2   d<br/></p>
<p>This is an example of a <i>many-to-one</i> join; the data in df1 has multiple rows labeled a<br/>and b, whereas df2 has only one row for each value in the key column. Calling merge<br/>with these objects we obtain:<br/></p>
<p>In [39]: pd.merge(df1, df2)<br/>Out[39]: <br/>   data1 key  data2<br/>0      0   b      1<br/>1      1   b      1<br/>2      6   b      1<br/>3      2   a      0<br/>4      4   a      0<br/>5      5   a      0<br/></p>
<p>Note that I didn&#8217;t specify which column to join on. If that information is not speci&#8208;<br/>fied, merge uses the overlapping column names as the keys. It&#8217;s a good practice to<br/>specify explicitly, though:<br/></p>
<p>In [40]: pd.merge(df1, df2, on='key')<br/>Out[40]: <br/>   data1 key  data2<br/>0      0   b      1<br/>1      1   b      1<br/>2      6   b      1<br/>3      2   a      0<br/>4      4   a      0<br/>5      5   a      0<br/></p>
<p>If the column names are different in each object, you can specify them separately:<br/>In [41]: df3 = pd.DataFrame({'lkey': ['b', 'b', 'a', 'c', 'a', 'a', 'b'],<br/>   ....:                     'data1': range(7)})<br/></p>
<p>In [42]: df4 = pd.DataFrame({'rkey': ['a', 'b', 'd'],<br/>   ....:                     'data2': range(3)})<br/></p>
<p>In [43]: pd.merge(df3, df4, left_on='lkey', right_on='rkey')<br/>Out[43]: <br/>   data1 lkey  data2 rkey<br/>0      0    b      1    b<br/>1      1    b      1    b<br/>2      6    b      1    b<br/>3      2    a      0    a<br/></p>
<p>228 | Chapter 8: Data Wrangling: Join, Combine, and Reshape</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>4      4    a      0    a<br/>5      5    a      0    a<br/></p>
<p>You may notice that the 'c' and 'd' values and associated data are missing from the<br/>result. By default merge does an 'inner' join; the keys in the result are the intersec&#8208;<br/>tion, or the common set found in both tables. Other possible options are 'left',<br/>'right', and 'outer'. The outer join takes the union of the keys, combining the<br/>effect of applying both left and right joins:<br/></p>
<p>In [44]: pd.merge(df1, df2, how='outer')<br/>Out[44]: <br/>   data1 key  data2<br/>0    0.0   b    1.0<br/>1    1.0   b    1.0<br/>2    6.0   b    1.0<br/>3    2.0   a    0.0<br/>4    4.0   a    0.0<br/>5    5.0   a    0.0<br/>6    3.0   c    NaN<br/>7    NaN   d    2.0<br/></p>
<p>See Table 8-1 for a summary of the options for how.<br/><i>Table 8-1. Different join types with how argument<br/></i></p>
<p>Option Behavior<br/>'inner' Use only the key combinations observed in both tables<br/>'left' Use all key combinations found in the left table<br/>'right' Use all key combinations found in the right table<br/>'output' Use all key combinations observed in both tables together<br/></p>
<p><i>Many-to-many</i> merges have well-defined, though not necessarily intuitive, behavior.<br/>Here&#8217;s an example:<br/></p>
<p>In [45]: df1 = pd.DataFrame({'key': ['b', 'b', 'a', 'c', 'a', 'b'],<br/>   ....:                     'data1': range(6)})<br/></p>
<p>In [46]: df2 = pd.DataFrame({'key': ['a', 'b', 'a', 'b', 'd'],<br/>   ....:                     'data2': range(5)})<br/></p>
<p>In [47]: df1<br/>Out[47]: <br/>   data1 key<br/>0      0   b<br/>1      1   b<br/>2      2   a<br/>3      3   c<br/>4      4   a<br/>5      5   b<br/></p>
<p>8.2 Combining and Merging Datasets | 229</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In [48]: df2<br/>Out[48]: <br/>   data2 key<br/>0      0   a<br/>1      1   b<br/>2      2   a<br/>3      3   b<br/>4      4   d<br/></p>
<p>In [49]: pd.merge(df1, df2, on='key', how='left')<br/>Out[49]: <br/>    data1 key  data2<br/>0       0   b    1.0<br/>1       0   b    3.0<br/>2       1   b    1.0<br/>3       1   b    3.0<br/>4       2   a    0.0<br/>5       2   a    2.0<br/>6       3   c    NaN<br/>7       4   a    0.0<br/>8       4   a    2.0<br/>9       5   b    1.0<br/>10      5   b    3.0<br/></p>
<p>Many-to-many joins form the Cartesian product of the rows. Since there were three<br/>'b' rows in the left DataFrame and two in the right one, there are six 'b' rows in the<br/>result. The join method only affects the distinct key values appearing in the result:<br/></p>
<p>In [50]: pd.merge(df1, df2, how='inner')<br/>Out[50]: <br/>   data1 key  data2<br/>0      0   b      1<br/>1      0   b      3<br/>2      1   b      1<br/>3      1   b      3<br/>4      5   b      1<br/>5      5   b      3<br/>6      2   a      0<br/>7      2   a      2<br/>8      4   a      0<br/>9      4   a      2<br/></p>
<p>To merge with multiple keys, pass a list of column names:<br/>In [51]: left = pd.DataFrame({'key1': ['foo', 'foo', 'bar'],<br/>   ....:                      'key2': ['one', 'two', 'one'],<br/>   ....:                      'lval': [1, 2, 3]})<br/></p>
<p>In [52]: right = pd.DataFrame({'key1': ['foo', 'foo', 'bar', 'bar'],<br/>   ....:                       'key2': ['one', 'one', 'one', 'two'],<br/>   ....:                       'rval': [4, 5, 6, 7]})<br/></p>
<p>In [53]: pd.merge(left, right, on=['key1', 'key2'], how='outer')<br/></p>
<p>230 | Chapter 8: Data Wrangling: Join, Combine, and Reshape</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Out[53]: <br/>  key1 key2  lval  rval<br/>0  foo  one   1.0   4.0<br/>1  foo  one   1.0   5.0<br/>2  foo  two   2.0   NaN<br/>3  bar  one   3.0   6.0<br/>4  bar  two   NaN   7.0<br/></p>
<p>To determine which key combinations will appear in the result depending on the<br/>choice of merge method, think of the multiple keys as forming an array of tuples to<br/>be used as a single join key (even though it&#8217;s not actually implemented that way).<br/></p>
<p>When you&#8217;re joining columns-on-columns, the indexes on the<br/>passed DataFrame objects are discarded.<br/></p>
<p>A last issue to consider in merge operations is the treatment of overlapping column<br/>names. While you can address the overlap manually (see the earlier section on<br/>renaming axis labels), merge has a suffixes option for specifying strings to append<br/>to overlapping names in the left and right DataFrame objects:<br/></p>
<p>In [54]: pd.merge(left, right, on='key1')<br/>Out[54]: <br/>  key1 key2_x  lval key2_y  rval<br/>0  foo    one     1    one     4<br/>1  foo    one     1    one     5<br/>2  foo    two     2    one     4<br/>3  foo    two     2    one     5<br/>4  bar    one     3    one     6<br/>5  bar    one     3    two     7<br/></p>
<p>In [55]: pd.merge(left, right, on='key1', suffixes=('_left', '_right'))<br/>Out[55]: <br/>  key1 key2_left  lval key2_right  rval<br/>0  foo       one     1        one     4<br/>1  foo       one     1        one     5<br/>2  foo       two     2        one     4<br/>3  foo       two     2        one     5<br/>4  bar       one     3        one     6<br/>5  bar       one     3        two     7<br/></p>
<p>See Table 8-2 for an argument reference on merge. Joining using the DataFrame&#8217;s row<br/>index is the subject of the next section.<br/></p>
<p>8.2 Combining and Merging Datasets | 231</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><i>Table 8-2. merge function arguments<br/></i>Argument Description<br/>left DataFrame to be merged on the left side.<br/>right DataFrame to be merged on the right side.<br/>how One of 'inner', 'outer', 'left', or 'right'; defaults to 'inner'.<br/>on Column names to join on. Must be found in both DataFrame objects. If not specified and no other join keys<br/></p>
<p>given, will use the intersection of the column names in left and right as the join keys.<br/>left_on Columns in left DataFrame to use as join keys.<br/>right_on Analogous to left_on for left DataFrame.<br/>left_index Use row index in left as its join key (or keys, if a MultiIndex).<br/>right_index Analogous to left_index.<br/>sort Sort merged data lexicographically by join keys; True by default (disable to get better performance in<br/></p>
<p>some cases on large datasets).<br/>suffixes Tuple of string values to append to column names in case of overlap; defaults to ('_x', '_y') (e.g., if<br/></p>
<p>'data' in both DataFrame objects, would appear as 'data_x' and 'data_y' in result).<br/>copy If False, avoid copying data into resulting data structure in some exceptional cases; by default always<br/></p>
<p>copies.<br/>indicator Adds a special column _merge that indicates the source of each row; values will be 'left_only',<br/></p>
<p>'right_only', or 'both' based on the origin of the joined data in each row.<br/></p>
<p>Merging on Index<br/>In some cases, the merge key(s) in a DataFrame will be found in its index. In this<br/>case, you can pass left_index=True or right_index=True (or both) to indicate that<br/>the index should be used as the merge key:<br/></p>
<p>In [56]: left1 = pd.DataFrame({'key': ['a', 'b', 'a', 'a', 'b', 'c'],<br/>   ....:                       'value': range(6)})<br/></p>
<p>In [57]: right1 = pd.DataFrame({'group_val': [3.5, 7]}, index=['a', 'b'])<br/></p>
<p>In [58]: left1<br/>Out[58]: <br/>  key  value<br/>0   a      0<br/>1   b      1<br/>2   a      2<br/>3   a      3<br/>4   b      4<br/>5   c      5<br/></p>
<p>In [59]: right1<br/>Out[59]: <br/>   group_val<br/>a        3.5<br/>b        7.0<br/></p>
<p>232 | Chapter 8: Data Wrangling: Join, Combine, and Reshape</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In [60]: pd.merge(left1, right1, left_on='key', right_index=True)<br/>Out[60]: <br/>  key  value  group_val<br/>0   a      0        3.5<br/>2   a      2        3.5<br/>3   a      3        3.5<br/>1   b      1        7.0<br/>4   b      4        7.0<br/></p>
<p>Since the default merge method is to intersect the join keys, you can instead form the<br/>union of them with an outer join:<br/></p>
<p>In [61]: pd.merge(left1, right1, left_on='key', right_index=True, how='outer')<br/>Out[61]: <br/>  key  value  group_val<br/>0   a      0        3.5<br/>2   a      2        3.5<br/>3   a      3        3.5<br/>1   b      1        7.0<br/>4   b      4        7.0<br/>5   c      5        NaN<br/></p>
<p>With hierarchically indexed data, things are more complicated, as joining on index is<br/>implicitly a multiple-key merge:<br/></p>
<p>In [62]: lefth = pd.DataFrame({'key1': ['Ohio', 'Ohio', 'Ohio',<br/>   ....:                                'Nevada', 'Nevada'],<br/>   ....:                       'key2': [2000, 2001, 2002, 2001, 2002],<br/>   ....:                       'data': np.arange(5.)})<br/></p>
<p>In [63]: righth = pd.DataFrame(np.arange(12).reshape((6, 2)),<br/>   ....:                       index=[['Nevada', 'Nevada', 'Ohio', 'Ohio',<br/>   ....:                               'Ohio', 'Ohio'],<br/>   ....:                              [2001, 2000, 2000, 2000, 2001, 2002]],<br/>   ....:                       columns=['event1', 'event2'])<br/></p>
<p>In [64]: lefth<br/>Out[64]: <br/>   data    key1  key2<br/>0   0.0    Ohio  2000<br/>1   1.0    Ohio  2001<br/>2   2.0    Ohio  2002<br/>3   3.0  Nevada  2001<br/>4   4.0  Nevada  2002<br/></p>
<p>In [65]: righth<br/>Out[65]: <br/>             event1  event2<br/>Nevada 2001       0       1<br/>       2000       2       3<br/>Ohio   2000       4       5<br/>       2000       6       7<br/></p>
<p>8.2 Combining and Merging Datasets | 233</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>       2001       8       9<br/>       2002      10      11<br/></p>
<p>In this case, you have to indicate multiple columns to merge on as a list (note the<br/>handling of duplicate index values with how='outer'):<br/></p>
<p>In [66]: pd.merge(lefth, righth, left_on=['key1', 'key2'], right_index=True)<br/>Out[66]: <br/>   data    key1  key2  event1  event2<br/>0   0.0    Ohio  2000       4       5<br/>0   0.0    Ohio  2000       6       7<br/>1   1.0    Ohio  2001       8       9<br/>2   2.0    Ohio  2002      10      11<br/>3   3.0  Nevada  2001       0       1<br/></p>
<p>In [67]: pd.merge(lefth, righth, left_on=['key1', 'key2'],<br/>   ....:          right_index=True, how='outer')<br/>Out[67]: <br/>   data    key1  key2  event1  event2<br/>0   0.0    Ohio  2000     4.0     5.0<br/>0   0.0    Ohio  2000     6.0     7.0<br/>1   1.0    Ohio  2001     8.0     9.0<br/>2   2.0    Ohio  2002    10.0    11.0<br/>3   3.0  Nevada  2001     0.0     1.0<br/>4   4.0  Nevada  2002     NaN     NaN<br/>4   NaN  Nevada  2000     2.0     3.0<br/></p>
<p>Using the indexes of both sides of the merge is also possible:<br/>In [68]: left2 = pd.DataFrame([[1., 2.], [3., 4.], [5., 6.]],<br/>   ....:                      index=['a', 'c', 'e'],<br/>   ....:                      columns=['Ohio', 'Nevada'])<br/></p>
<p>In [69]: right2 = pd.DataFrame([[7., 8.], [9., 10.], [11., 12.], [13, 14]],<br/>   ....:                       index=['b', 'c', 'd', 'e'],<br/>   ....:                       columns=['Missouri', 'Alabama'])<br/></p>
<p>In [70]: left2<br/>Out[70]: <br/>   Ohio  Nevada<br/>a   1.0     2.0<br/>c   3.0     4.0<br/>e   5.0     6.0<br/></p>
<p>In [71]: right2<br/>Out[71]: <br/>   Missouri  Alabama<br/>b       7.0      8.0<br/>c       9.0     10.0<br/>d      11.0     12.0<br/>e      13.0     14.0<br/></p>
<p>In [72]: pd.merge(left2, right2, how='outer', left_index=True, right_index=True)<br/></p>
<p>234 | Chapter 8: Data Wrangling: Join, Combine, and Reshape</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Out[72]: <br/>   Ohio  Nevada  Missouri  Alabama<br/>a   1.0     2.0       NaN      NaN<br/>b   NaN     NaN       7.0      8.0<br/>c   3.0     4.0       9.0     10.0<br/>d   NaN     NaN      11.0     12.0<br/>e   5.0     6.0      13.0     14.0<br/></p>
<p>DataFrame has a convenient join instance for merging by index. It can also be used<br/>to combine together many DataFrame objects having the same or similar indexes but<br/>non-overlapping columns. In the prior example, we could have written:<br/></p>
<p>In [73]: left2.join(right2, how='outer')<br/>Out[73]: <br/>   Ohio  Nevada  Missouri  Alabama<br/>a   1.0     2.0       NaN      NaN<br/>b   NaN     NaN       7.0      8.0<br/>c   3.0     4.0       9.0     10.0<br/>d   NaN     NaN      11.0     12.0<br/>e   5.0     6.0      13.0     14.0<br/></p>
<p>In part for legacy reasons (i.e., much earlier versions of pandas), DataFrame&#8217;s join<br/>method performs a left join on the join keys, exactly preserving the left frame&#8217;s row<br/>index. It also supports joining the index of the passed DataFrame on one of the col&#8208;<br/>umns of the calling DataFrame:<br/></p>
<p>In [74]: left1.join(right1, on='key')<br/>Out[74]: <br/>  key  value  group_val<br/>0   a      0        3.5<br/>1   b      1        7.0<br/>2   a      2        3.5<br/>3   a      3        3.5<br/>4   b      4        7.0<br/>5   c      5        NaN<br/></p>
<p>Lastly, for simple index-on-index merges, you can pass a list of DataFrames to join as<br/>an alternative to using the more general concat function described in the next <br/>section:<br/></p>
<p>In [75]: another = pd.DataFrame([[7., 8.], [9., 10.], [11., 12.], [16., 17.]],<br/>   ....:                        index=['a', 'c', 'e', 'f'],<br/>   ....:                        columns=['New York', 'Oregon'])<br/></p>
<p>In [76]: another<br/>Out[76]: <br/>   New York  Oregon<br/>a       7.0     8.0<br/>c       9.0    10.0<br/>e      11.0    12.0<br/>f      16.0    17.0<br/></p>
<p>8.2 Combining and Merging Datasets | 235</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In [77]: left2.join([right2, another])<br/>Out[77]: <br/>   Ohio  Nevada  Missouri  Alabama  New York  Oregon<br/>a   1.0     2.0       NaN      NaN       7.0     8.0<br/>c   3.0     4.0       9.0     10.0       9.0    10.0<br/>e   5.0     6.0      13.0     14.0      11.0    12.0<br/></p>
<p>In [78]: left2.join([right2, another], how='outer')<br/>Out[78]: <br/>   Ohio  Nevada  Missouri  Alabama  New York  Oregon<br/>a   1.0     2.0       NaN      NaN       7.0     8.0<br/>b   NaN     NaN       7.0      8.0       NaN     NaN<br/>c   3.0     4.0       9.0     10.0       9.0    10.0<br/>d   NaN     NaN      11.0     12.0       NaN     NaN<br/>e   5.0     6.0      13.0     14.0      11.0    12.0<br/>f   NaN     NaN       NaN      NaN      16.0    17.0<br/></p>
<p>Concatenating Along an Axis<br/>Another kind of data combination operation is referred to interchangeably as concat&#8208;<br/>enation, binding, or stacking. NumPy&#8217;s concatenate function can do this with<br/>NumPy arrays:<br/></p>
<p>In [79]: arr = np.arange(12).reshape((3, 4))<br/></p>
<p>In [80]: arr<br/>Out[80]: <br/>array([[ 0,  1,  2,  3],<br/>       [ 4,  5,  6,  7],<br/>       [ 8,  9, 10, 11]])<br/></p>
<p>In [81]: np.concatenate([arr, arr], axis=1)<br/>Out[81]: <br/>array([[ 0,  1,  2,  3,  0,  1,  2,  3],<br/>       [ 4,  5,  6,  7,  4,  5,  6,  7],<br/>       [ 8,  9, 10, 11,  8,  9, 10, 11]])<br/></p>
<p>In the context of pandas objects such as Series and DataFrame, having labeled axes<br/>enable you to further generalize array concatenation. In particular, you have a num&#8208;<br/>ber of additional things to think about:<br/></p>
<p>&#8226; If the objects are indexed differently on the other axes, should we combine the<br/>distinct elements in these axes or use only the shared values (the intersection)?<br/></p>
<p>&#8226; Do the concatenated chunks of data need to be identifiable in the resulting<br/>object?<br/></p>
<p>&#8226; Does the &#8220;concatenation axis&#8221; contain data that needs to be preserved? In many<br/>cases, the default integer labels in a DataFrame are best discarded during<br/>concatenation.<br/></p>
<p>236 | Chapter 8: Data Wrangling: Join, Combine, and Reshape</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>The concat function in pandas provides a consistent way to address each of these<br/>concerns. I&#8217;ll give a number of examples to illustrate how it works. Suppose we have<br/>three Series with no index overlap:<br/></p>
<p>In [82]: s1 = pd.Series([0, 1], index=['a', 'b'])<br/></p>
<p>In [83]: s2 = pd.Series([2, 3, 4], index=['c', 'd', 'e'])<br/></p>
<p>In [84]: s3 = pd.Series([5, 6], index=['f', 'g'])<br/></p>
<p>Calling concat with these objects in a list glues together the values and indexes:<br/>In [85]: pd.concat([s1, s2, s3])<br/>Out[85]: <br/>a    0<br/>b    1<br/>c    2<br/>d    3<br/>e    4<br/>f    5<br/>g    6<br/>dtype: int64<br/></p>
<p>By default concat works along axis=0, producing another Series. If you pass axis=1,<br/>the result will instead be a DataFrame (axis=1 is the columns):<br/></p>
<p>In [86]: pd.concat([s1, s2, s3], axis=1)<br/>Out[86]: <br/>     0    1    2<br/>a  0.0  NaN  NaN<br/>b  1.0  NaN  NaN<br/>c  NaN  2.0  NaN<br/>d  NaN  3.0  NaN<br/>e  NaN  4.0  NaN<br/>f  NaN  NaN  5.0<br/>g  NaN  NaN  6.0<br/></p>
<p>In this case there is no overlap on the other axis, which as you can see is the sorted<br/>union (the 'outer' join) of the indexes. You can instead intersect them by passing<br/>join='inner':<br/></p>
<p>In [87]: s4 = pd.concat([s1, s3])<br/></p>
<p>In [88]: s4<br/>Out[88]: <br/>a    0<br/>b    1<br/>f    5<br/>g    6<br/>dtype: int64<br/></p>
<p>In [89]: pd.concat([s1, s4], axis=1)<br/>Out[89]: <br/></p>
<p>8.2 Combining and Merging Datasets | 237</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>     0  1<br/>a  0.0  0<br/>b  1.0  1<br/>f  NaN  5<br/>g  NaN  6<br/></p>
<p>In [90]: pd.concat([s1, s4], axis=1, join='inner')<br/>Out[90]: <br/>   0  1<br/>a  0  0<br/>b  1  1<br/></p>
<p>In this last example, the 'f' and 'g' labels disappeared because of the join='inner'<br/>option.<br/>You can even specify the axes to be used on the other axes with join_axes:<br/></p>
<p>In [91]: pd.concat([s1, s4], axis=1, join_axes=[['a', 'c', 'b', 'e']])<br/>Out[91]: <br/>     0    1<br/>a  0.0  0.0<br/>c  NaN  NaN<br/>b  1.0  1.0<br/>e  NaN  NaN<br/></p>
<p>A potential issue is that the concatenated pieces are not identifiable in the result. Sup&#8208;<br/>pose instead you wanted to create a hierarchical index on the concatenation axis. To<br/>do this, use the keys argument:<br/></p>
<p>In [92]: result = pd.concat([s1, s1, s3], keys=['one', 'two', 'three'])<br/></p>
<p>In [93]: result<br/>Out[93]: <br/>one    a    0<br/>       b    1<br/>two    a    0<br/>       b    1<br/>three  f    5<br/>       g    6<br/>dtype: int64<br/></p>
<p>In [94]: result.unstack()<br/>Out[94]: <br/>         a    b    f    g<br/>one    0.0  1.0  NaN  NaN<br/>two    0.0  1.0  NaN  NaN<br/>three  NaN  NaN  5.0  6.0<br/></p>
<p>In the case of combining Series along axis=1, the keys become the DataFrame col&#8208;<br/>umn headers:<br/></p>
<p>In [95]: pd.concat([s1, s2, s3], axis=1, keys=['one', 'two', 'three'])<br/>Out[95]: <br/></p>
<p>238 | Chapter 8: Data Wrangling: Join, Combine, and Reshape</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>   one  two  three<br/>a  0.0  NaN    NaN<br/>b  1.0  NaN    NaN<br/>c  NaN  2.0    NaN<br/>d  NaN  3.0    NaN<br/>e  NaN  4.0    NaN<br/>f  NaN  NaN    5.0<br/>g  NaN  NaN    6.0<br/></p>
<p>The same logic extends to DataFrame objects:<br/>In [96]: df1 = pd.DataFrame(np.arange(6).reshape(3, 2), index=['a', 'b', 'c'],<br/>   ....:                    columns=['one', 'two'])<br/></p>
<p>In [97]: df2 = pd.DataFrame(5 + np.arange(4).reshape(2, 2), index=['a', 'c'],<br/>   ....:                    columns=['three', 'four'])<br/></p>
<p>In [98]: df1<br/>Out[98]: <br/>   one  two<br/>a    0    1<br/>b    2    3<br/>c    4    5<br/></p>
<p>In [99]: df2<br/>Out[99]: <br/>   three  four<br/>a      5     6<br/>c      7     8<br/></p>
<p>In [100]: pd.concat([df1, df2], axis=1, keys=['level1', 'level2'])<br/>Out[100]: <br/>  level1     level2     <br/>     one two  three four<br/>a      0   1    5.0  6.0<br/>b      2   3    NaN  NaN<br/>c      4   5    7.0  8.0<br/></p>
<p>If you pass a dict of objects instead of a list, the dict&#8217;s keys will be used for the keys<br/>option:<br/></p>
<p>In [101]: pd.concat({'level1': df1, 'level2': df2}, axis=1)<br/>Out[101]: <br/>  level1     level2     <br/>     one two  three four<br/>a      0   1    5.0  6.0<br/>b      2   3    NaN  NaN<br/>c      4   5    7.0  8.0<br/></p>
<p>There are additional arguments governing how the hierarchical index is created (see<br/>Table 8-3). For example, we can name the created axis levels with the names<br/>argument:<br/></p>
<p>8.2 Combining and Merging Datasets | 239</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In [102]: pd.concat([df1, df2], axis=1, keys=['level1', 'level2'],<br/>   .....:           names=['upper', 'lower'])<br/>Out[102]: <br/>upper level1     level2     <br/>lower    one two  three four<br/>a          0   1    5.0  6.0<br/>b          2   3    NaN  NaN<br/>c          4   5    7.0  8.0<br/></p>
<p>A last consideration concerns DataFrames in which the row index does not contain<br/>any relevant data:<br/></p>
<p>In [103]: df1 = pd.DataFrame(np.random.randn(3, 4), columns=['a', 'b', 'c', 'd'])<br/></p>
<p>In [104]: df2 = pd.DataFrame(np.random.randn(2, 3), columns=['b', 'd', 'a'])<br/></p>
<p>In [105]: df1<br/>Out[105]: <br/>          a         b         c         d<br/>0  1.246435  1.007189 -1.296221  0.274992<br/>1  0.228913  1.352917  0.886429 -2.001637<br/>2 -0.371843  1.669025 -0.438570 -0.539741<br/></p>
<p>In [106]: df2<br/>Out[106]: <br/>          b         d         a<br/>0  0.476985  3.248944 -1.021228<br/>1 -0.577087  0.124121  0.302614<br/></p>
<p>In this case, you can pass ignore_index=True:<br/>In [107]: pd.concat([df1, df2], ignore_index=True)<br/>Out[107]: <br/>          a         b         c         d<br/>0  1.246435  1.007189 -1.296221  0.274992<br/>1  0.228913  1.352917  0.886429 -2.001637<br/>2 -0.371843  1.669025 -0.438570 -0.539741<br/>3 -1.021228  0.476985       NaN  3.248944<br/>4  0.302614 -0.577087       NaN  0.124121<br/></p>
<p><i>Table 8-3. concat function arguments<br/></i>Argument Description<br/>objs List or dict of pandas objects to be concatenated; this is the only required argument<br/>axis Axis to concatenate along; defaults to 0 (along rows)<br/>join Either 'inner' or 'outer' ('outer' by default); whether to intersection (inner) or union<br/></p>
<p>(outer) together indexes along the other axes<br/>join_axes Specific indexes to use for the other n&#8211;1 axes instead of performing union/intersection logic<br/>keys Values to associate with objects being concatenated, forming a hierarchical index along the<br/></p>
<p>concatenation axis; can either be a list or array of arbitrary values, an array of tuples, or a list of<br/>arrays (if multiple-level arrays passed in levels)<br/></p>
<p>240 | Chapter 8: Data Wrangling: Join, Combine, and Reshape</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Argument Description<br/>levels Specific indexes to use as hierarchical index level or levels if keys passed<br/>names Names for created hierarchical levels if keys and/or levels passed<br/>verify_integrity Check new axis in concatenated object for duplicates and raise exception if so; by default (False)<br/></p>
<p>allows duplicates<br/>ignore_index Do not preserve indexes along concatenation axis, instead producing a new<br/></p>
<p>range(total_length) index<br/></p>
<p>Combining Data with Overlap<br/>There is another data combination situation that can&#8217;t be expressed as either a merge<br/>or concatenation operation. You may have two datasets whose indexes overlap in full<br/>or part. As a motivating example, consider NumPy&#8217;s where function, which performs<br/>the array-oriented equivalent of an if-else expression:<br/></p>
<p>In [108]: a = pd.Series([np.nan, 2.5, np.nan, 3.5, 4.5, np.nan],<br/>   .....:               index=['f', 'e', 'd', 'c', 'b', 'a'])<br/></p>
<p>In [109]: b = pd.Series(np.arange(len(a), dtype=np.float64),<br/>   .....:               index=['f', 'e', 'd', 'c', 'b', 'a'])<br/></p>
<p>In [110]: b[-1] = np.nan<br/></p>
<p>In [111]: a<br/>Out[111]: <br/>f    NaN<br/>e    2.5<br/>d    NaN<br/>c    3.5<br/>b    4.5<br/>a    NaN<br/>dtype: float64<br/></p>
<p>In [112]: b<br/>Out[112]: <br/>f    0.0<br/>e    1.0<br/>d    2.0<br/>c    3.0<br/>b    4.0<br/>a    NaN<br/>dtype: float64<br/></p>
<p>In [113]: np.where(pd.isnull(a), b, a)<br/>Out[113]: array([ 0. ,  2.5,  2. ,  3.5,  4.5,  nan])<br/></p>
<p>Series has a combine_first method, which performs the equivalent of this operation<br/>along with pandas&#8217;s usual data alignment logic:<br/></p>
<p>8.2 Combining and Merging Datasets | 241</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In [114]: b[:-2].combine_first(a[2:])<br/>Out[114]: <br/>a    NaN<br/>b    4.5<br/>c    3.0<br/>d    2.0<br/>e    1.0<br/>f    0.0<br/>dtype: float64<br/></p>
<p>With DataFrames, combine_first does the same thing column by column, so you<br/>can think of it as &#8220;patching&#8221; missing data in the calling object with data from the<br/>object you pass:<br/></p>
<p>In [115]: df1 = pd.DataFrame({'a': [1., np.nan, 5., np.nan],<br/>   .....:                     'b': [np.nan, 2., np.nan, 6.],<br/>   .....:                     'c': range(2, 18, 4)})<br/></p>
<p>In [116]: df2 = pd.DataFrame({'a': [5., 4., np.nan, 3., 7.],<br/>   .....:                     'b': [np.nan, 3., 4., 6., 8.]})<br/></p>
<p>In [117]: df1<br/>Out[117]: <br/>     a    b   c<br/>0  1.0  NaN   2<br/>1  NaN  2.0   6<br/>2  5.0  NaN  10<br/>3  NaN  6.0  14<br/></p>
<p>In [118]: df2<br/>Out[118]: <br/>     a    b<br/>0  5.0  NaN<br/>1  4.0  3.0<br/>2  NaN  4.0<br/>3  3.0  6.0<br/>4  7.0  8.0<br/></p>
<p>In [119]: df1.combine_first(df2)<br/>Out[119]: <br/>     a    b     c<br/>0  1.0  NaN   2.0<br/>1  4.0  2.0   6.0<br/>2  5.0  4.0  10.0<br/>3  3.0  6.0  14.0<br/>4  7.0  8.0   NaN<br/></p>
<p>8.3 Reshaping and Pivoting<br/>There are a number of basic operations for rearranging tabular data. These are alter&#8208;<br/>natingly referred to as <i>reshape</i> or <i>pivot</i> operations.<br/></p>
<p>242 | Chapter 8: Data Wrangling: Join, Combine, and Reshape</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Reshaping with Hierarchical Indexing<br/>Hierarchical indexing provides a consistent way to rearrange data in a DataFrame.<br/>There are two primary actions:<br/>stack<br/></p>
<p>This &#8220;rotates&#8221; or pivots from the columns in the data to the rows<br/>unstack<br/></p>
<p>This pivots from the rows into the columns<br/>I&#8217;ll illustrate these operations through a series of examples. Consider a small Data&#8208;<br/>Frame with string arrays as row and column indexes:<br/></p>
<p>In [120]: data = pd.DataFrame(np.arange(6).reshape((2, 3)),<br/>   .....:                     index=pd.Index(['Ohio', 'Colorado'], name='state'),<br/>   .....:                     columns=pd.Index(['one', 'two', 'three'],<br/>   .....:                     name='number'))<br/></p>
<p>In [121]: data<br/>Out[121]: <br/>number    one  two  three<br/>state                    <br/>Ohio        0    1      2<br/>Colorado    3    4      5<br/></p>
<p>Using the stack method on this data pivots the columns into the rows, producing a<br/>Series:<br/></p>
<p>In [122]: result = data.stack()<br/></p>
<p>In [123]: result<br/>Out[123]: <br/>state     number<br/>Ohio      one       0<br/>          two       1<br/>          three     2<br/>Colorado  one       3<br/>          two       4<br/>          three     5<br/>dtype: int64<br/></p>
<p>From a hierarchically indexed Series, you can rearrange the data back into a Data&#8208;<br/>Frame with unstack:<br/></p>
<p>In [124]: result.unstack()<br/>Out[124]: <br/>number    one  two  three<br/>state                    <br/>Ohio        0    1      2<br/>Colorado    3    4      5<br/></p>
<p>8.3 Reshaping and Pivoting | 243</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>By default the innermost level is unstacked (same with stack). You can unstack a dif&#8208;<br/>ferent level by passing a level number or name:<br/></p>
<p>In [125]: result.unstack(0)<br/>Out[125]: <br/>state   Ohio  Colorado<br/>number                <br/>one        0         3<br/>two        1         4<br/>three      2         5<br/></p>
<p>In [126]: result.unstack('state')<br/>Out[126]: <br/>state   Ohio  Colorado<br/>number                <br/>one        0         3<br/>two        1         4<br/>three      2         5<br/></p>
<p>Unstacking might introduce missing data if all of the values in the level aren&#8217;t found<br/>in each of the subgroups:<br/></p>
<p>In [127]: s1 = pd.Series([0, 1, 2, 3], index=['a', 'b', 'c', 'd'])<br/></p>
<p>In [128]: s2 = pd.Series([4, 5, 6], index=['c', 'd', 'e'])<br/></p>
<p>In [129]: data2 = pd.concat([s1, s2], keys=['one', 'two'])<br/></p>
<p>In [130]: data2<br/>Out[130]: <br/>one  a    0<br/>     b    1<br/>     c    2<br/>     d    3<br/>two  c    4<br/>     d    5<br/>     e    6<br/>dtype: int64<br/></p>
<p>In [131]: data2.unstack()<br/>Out[131]: <br/>       a    b    c    d    e<br/>one  0.0  1.0  2.0  3.0  NaN<br/>two  NaN  NaN  4.0  5.0  6.0<br/></p>
<p>Stacking filters out missing data by default, so the operation is more easily invertible:<br/>In [132]: data2.unstack()<br/>Out[132]: <br/>       a    b    c    d    e<br/>one  0.0  1.0  2.0  3.0  NaN<br/>two  NaN  NaN  4.0  5.0  6.0<br/></p>
<p>244 | Chapter 8: Data Wrangling: Join, Combine, and Reshape</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In [133]: data2.unstack().stack()<br/>Out[133]: <br/>one  a    0.0<br/>     b    1.0<br/>     c    2.0<br/>     d    3.0<br/>two  c    4.0<br/>     d    5.0<br/>     e    6.0<br/>dtype: float64<br/></p>
<p>In [134]: data2.unstack().stack(dropna=False)<br/>Out[134]: <br/>one  a    0.0<br/>     b    1.0<br/>     c    2.0<br/>     d    3.0<br/>     e    NaN<br/>two  a    NaN<br/>     b    NaN<br/>     c    4.0<br/>     d    5.0<br/>     e    6.0<br/>dtype: float64<br/></p>
<p>When you unstack in a DataFrame, the level unstacked becomes the lowest level in<br/>the result:<br/></p>
<p>In [135]: df = pd.DataFrame({'left': result, 'right': result + 5},<br/>   .....:                   columns=pd.Index(['left', 'right'], name='side'))<br/></p>
<p>In [136]: df<br/>Out[136]: <br/>side             left  right<br/>state    number             <br/>Ohio     one        0      5<br/>         two        1      6<br/>         three      2      7<br/>Colorado one        3      8<br/>         two        4      9<br/>         three      5     10<br/></p>
<p>In [137]: df.unstack('state')<br/>Out[137]: <br/>side   left          right         <br/>state  Ohio Colorado  Ohio Colorado<br/>number                             <br/>one       0        3     5        8<br/>two       1        4     6        9<br/>three     2        5     7       10<br/></p>
<p>When calling stack, we can indicate the name of the axis to stack:<br/></p>
<p>8.3 Reshaping and Pivoting | 245</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In [138]: df.unstack('state').stack('side')<br/>Out[138]: <br/>state         Colorado  Ohio<br/>number side                 <br/>one    left          3     0<br/>       right         8     5<br/>two    left          4     1<br/>       right         9     6<br/>three  left          5     2<br/>       right        10     7<br/></p>
<p>Pivoting &#8220;Long&#8221; to &#8220;Wide&#8221; Format<br/>A common way to store multiple time series in databases and CSV is in so-called <i>long<br/></i>or <i>stacked</i> format. Let&#8217;s load some example data and do a small amount of time series<br/>wrangling and other data cleaning:<br/></p>
<p>In [139]: data = pd.read_csv('examples/macrodata.csv')<br/></p>
<p>In [140]: data.head()<br/>Out[140]: <br/>     year  quarter   realgdp  realcons  realinv  realgovt  realdpi    cpi  \<br/>0  1959.0      1.0  2710.349    1707.4  286.898   470.045   1886.9  28.98   <br/>1  1959.0      2.0  2778.801    1733.7  310.859   481.301   1919.7  29.15   <br/>2  1959.0      3.0  2775.488    1751.8  289.226   491.260   1916.4  29.35   <br/>3  1959.0      4.0  2785.204    1753.7  299.356   484.052   1931.3  29.37   <br/>4  1960.0      1.0  2847.699    1770.5  331.722   462.199   1955.5  29.54   <br/>      m1  tbilrate  unemp      pop  infl  realint  <br/>0  139.7      2.82    5.8  177.146  0.00     0.00  <br/>1  141.7      3.08    5.1  177.830  2.34     0.74  <br/>2  140.5      3.82    5.3  178.657  2.74     1.09  <br/>3  140.0      4.33    5.6  179.386  0.27     4.06  <br/>4  139.6      3.50    5.2  180.007  2.31     1.19  <br/></p>
<p>In [141]: periods = pd.PeriodIndex(year=data.year, quarter=data.quarter,<br/>   .....:                          name='date')<br/></p>
<p>In [142]: columns = pd.Index(['realgdp', 'infl', 'unemp'], name='item')<br/></p>
<p>In [143]: data = data.reindex(columns=columns)<br/></p>
<p>In [144]: data.index = periods.to_timestamp('D', 'end')<br/></p>
<p>In [145]: ldata = data.stack().reset_index().rename(columns={0: 'value'})<br/></p>
<p>We will look at PeriodIndex a bit more closely in Chapter 11. In short, it combines<br/>the year and quarter columns to create a kind of time interval type.<br/>Now, ldata looks like:<br/></p>
<p>In [146]: ldata[:10]<br/>Out[146]: <br/></p>
<p>246 | Chapter 8: Data Wrangling: Join, Combine, and Reshape</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>        date     item     value<br/>0 1959-03-31  realgdp  2710.349<br/>1 1959-03-31     infl     0.000<br/>2 1959-03-31    unemp     5.800<br/>3 1959-06-30  realgdp  2778.801<br/>4 1959-06-30     infl     2.340<br/>5 1959-06-30    unemp     5.100<br/>6 1959-09-30  realgdp  2775.488<br/>7 1959-09-30     infl     2.740<br/>8 1959-09-30    unemp     5.300<br/>9 1959-12-31  realgdp  2785.204<br/></p>
<p>This is the so-called <i>long</i> format for multiple time series, or other observational data<br/>with two or more keys (here, our keys are date and item). Each row in the table repre&#8208;<br/>sents a single observation.<br/>Data is frequently stored this way in relational databases like MySQL, as a fixed<br/>schema (column names and data types) allows the number of distinct values in the<br/>item column to change as data is added to the table. In the previous example, date<br/>and item would usually be the primary keys (in relational database parlance), offering<br/>both relational integrity and easier joins. In some cases, the data may be more diffi&#8208;<br/>cult to work with in this format; you might prefer to have a DataFrame containing<br/>one column per distinct item value indexed by timestamps in the date column. Data&#8208;<br/>Frame&#8217;s pivot method performs exactly this transformation:<br/></p>
<p>In [147]: pivoted = ldata.pivot('date', 'item', 'value')<br/></p>
<p>In [148]: pivoted<br/>Out[148]: <br/>item        infl    realgdp  unemp<br/>date                              <br/>1959-03-31  0.00   2710.349    5.8<br/>1959-06-30  2.34   2778.801    5.1<br/>1959-09-30  2.74   2775.488    5.3<br/>1959-12-31  0.27   2785.204    5.6<br/>1960-03-31  2.31   2847.699    5.2<br/>1960-06-30  0.14   2834.390    5.2<br/>1960-09-30  2.70   2839.022    5.6<br/>1960-12-31  1.21   2802.616    6.3<br/>1961-03-31 -0.40   2819.264    6.8<br/>1961-06-30  1.47   2872.005    7.0<br/>...          ...        ...    ...<br/>2007-06-30  2.75  13203.977    4.5<br/>2007-09-30  3.45  13321.109    4.7<br/>2007-12-31  6.38  13391.249    4.8<br/>2008-03-31  2.82  13366.865    4.9<br/>2008-06-30  8.53  13415.266    5.4<br/>2008-09-30 -3.16  13324.600    6.0<br/>2008-12-31 -8.79  13141.920    6.9<br/>2009-03-31  0.94  12925.410    8.1<br/>2009-06-30  3.37  12901.504    9.2<br/></p>
<p>8.3 Reshaping and Pivoting | 247</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>2009-09-30  3.56  12990.341    9.6<br/>[203 rows x 3 columns]<br/></p>
<p>The first two values passed are the columns to be used respectively as the row and<br/>column index, then finally an optional value column to fill the DataFrame. Suppose<br/>you had two value columns that you wanted to reshape simultaneously:<br/></p>
<p>In [149]: ldata['value2'] = np.random.randn(len(ldata))<br/></p>
<p>In [150]: ldata[:10]<br/>Out[150]: <br/>        date     item     value    value2<br/>0 1959-03-31  realgdp  2710.349  0.523772<br/>1 1959-03-31     infl     0.000  0.000940<br/>2 1959-03-31    unemp     5.800  1.343810<br/>3 1959-06-30  realgdp  2778.801 -0.713544<br/>4 1959-06-30     infl     2.340 -0.831154<br/>5 1959-06-30    unemp     5.100 -2.370232<br/>6 1959-09-30  realgdp  2775.488 -1.860761<br/>7 1959-09-30     infl     2.740 -0.860757<br/>8 1959-09-30    unemp     5.300  0.560145<br/>9 1959-12-31  realgdp  2785.204 -1.265934<br/></p>
<p>By omitting the last argument, you obtain a DataFrame with hierarchical columns:<br/>In [151]: pivoted = ldata.pivot('date', 'item')<br/></p>
<p>In [152]: pivoted[:5]<br/>Out[152]: <br/>           value                    value2                    <br/>item        infl   realgdp unemp      infl   realgdp     unemp<br/>date                                                          <br/>1959-03-31  0.00  2710.349   5.8  0.000940  0.523772  1.343810<br/>1959-06-30  2.34  2778.801   5.1 -0.831154 -0.713544 -2.370232<br/>1959-09-30  2.74  2775.488   5.3 -0.860757 -1.860761  0.560145<br/>1959-12-31  0.27  2785.204   5.6  0.119827 -1.265934 -1.063512<br/>1960-03-31  2.31  2847.699   5.2 -2.359419  0.332883 -0.199543<br/></p>
<p>In [153]: pivoted['value'][:5]<br/>Out[153]: <br/>item        infl   realgdp  unemp<br/>date                             <br/>1959-03-31  0.00  2710.349    5.8<br/>1959-06-30  2.34  2778.801    5.1<br/>1959-09-30  2.74  2775.488    5.3<br/>1959-12-31  0.27  2785.204    5.6<br/>1960-03-31  2.31  2847.699    5.2<br/></p>
<p>Note that pivot is equivalent to creating a hierarchical index using set_index fol&#8208;<br/>lowed by a call to unstack:<br/></p>
<p>248 | Chapter 8: Data Wrangling: Join, Combine, and Reshape</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In [154]: unstacked = ldata.set_index(['date', 'item']).unstack('item')<br/></p>
<p>In [155]: unstacked[:7]<br/>Out[155]: <br/>           value                    value2                    <br/>item        infl   realgdp unemp      infl   realgdp     unemp<br/>date                                                          <br/>1959-03-31  0.00  2710.349   5.8  0.000940  0.523772  1.343810<br/>1959-06-30  2.34  2778.801   5.1 -0.831154 -0.713544 -2.370232<br/>1959-09-30  2.74  2775.488   5.3 -0.860757 -1.860761  0.560145<br/>1959-12-31  0.27  2785.204   5.6  0.119827 -1.265934 -1.063512<br/>1960-03-31  2.31  2847.699   5.2 -2.359419  0.332883 -0.199543<br/>1960-06-30  0.14  2834.390   5.2 -0.970736 -1.541996 -1.307030<br/>1960-09-30  2.70  2839.022   5.6  0.377984  0.286350 -0.753887<br/></p>
<p>Pivoting &#8220;Wide&#8221; to &#8220;Long&#8221; Format<br/>An inverse operation to pivot for DataFrames is pandas.melt. Rather than trans&#8208;<br/>forming one column into many in a new DataFrame, it merges multiple columns into<br/>one, producing a DataFrame that is longer than the input. Let&#8217;s look at an example:<br/></p>
<p>In [157]: df = pd.DataFrame({'key': ['foo', 'bar', 'baz'],<br/>   .....:                    'A': [1, 2, 3],<br/>   .....:                    'B': [4, 5, 6],<br/>   .....:                    'C': [7, 8, 9]})<br/></p>
<p>In [158]: df<br/>Out[158]: <br/>   A  B  C  key<br/>0  1  4  7  foo<br/>1  2  5  8  bar<br/>2  3  6  9  baz<br/></p>
<p>The 'key' column may be a group indicator, and the other columns are data values.<br/>When using pandas.melt, we must indicate which columns (if any) are group indica&#8208;<br/>tors. Let&#8217;s use 'key' as the only group indicator here:<br/></p>
<p>In [159]: melted = pd.melt(df, ['key'])<br/></p>
<p>In [160]: melted<br/>Out[160]: <br/>   key variable  value<br/>0  foo        A      1<br/>1  bar        A      2<br/>2  baz        A      3<br/>3  foo        B      4<br/>4  bar        B      5<br/>5  baz        B      6<br/>6  foo        C      7<br/>7  bar        C      8<br/>8  baz        C      9<br/></p>
<p>8.3 Reshaping and Pivoting | 249</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Using pivot, we can reshape back to the original layout:<br/>In [161]: reshaped = melted.pivot('key', 'variable', 'value')<br/></p>
<p>In [162]: reshaped<br/>Out[162]: <br/>variable  A  B  C<br/>key              <br/>bar       2  5  8<br/>baz       3  6  9<br/>foo       1  4  7<br/></p>
<p>Since the result of pivot creates an index from the column used as the row labels, we<br/>may want to use reset_index to move the data back into a column:<br/></p>
<p>In [163]: reshaped.reset_index()<br/>Out[163]: <br/>variable  key  A  B  C<br/>0         bar  2  5  8<br/>1         baz  3  6  9<br/>2         foo  1  4  7<br/></p>
<p>You can also specify a subset of columns to use as value columns:<br/>In [164]: pd.melt(df, id_vars=['key'], value_vars=['A', 'B'])<br/>Out[164]: <br/>   key variable  value<br/>0  foo        A      1<br/>1  bar        A      2<br/>2  baz        A      3<br/>3  foo        B      4<br/>4  bar        B      5<br/>5  baz        B      6<br/></p>
<p>pandas.melt can be used without any group identifiers, too:<br/>In [165]: pd.melt(df, value_vars=['A', 'B', 'C'])<br/>Out[165]: <br/>  variable  value<br/>0        A      1<br/>1        A      2<br/>2        A      3<br/>3        B      4<br/>4        B      5<br/>5        B      6<br/>6        C      7<br/>7        C      8<br/>8        C      9<br/></p>
<p>In [166]: pd.melt(df, value_vars=['key', 'A', 'B'])<br/>Out[166]: <br/>  variable value<br/>0      key   foo<br/>1      key   bar<br/></p>
<p>250 | Chapter 8: Data Wrangling: Join, Combine, and Reshape</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>2      key   baz<br/>3        A     1<br/>4        A     2<br/>5        A     3<br/>6        B     4<br/>7        B     5<br/>8        B     6<br/></p>
<p>8.4 Conclusion<br/>Now that you have some pandas basics for data import, cleaning, and reorganization<br/>under your belt, we are ready to move on to data visualization with matplotlib. We<br/>will return to pandas later in the book when we discuss more advanced analytics.<br/></p>
<p>8.4 Conclusion | 251</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div>
</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>CHAPTER 9<br/>Plotting and Visualization<br/></p>
<p>Making informative visualizations (sometimes called <i>plots</i>) is one of the most impor&#8208;<br/>tant tasks in data analysis. It may be a part of the exploratory process&#8212;for example,<br/>to help identify outliers or needed data transformations, or as a way of generating<br/>ideas for models. For others, building an interactive visualization for the web may be<br/>the end goal. Python has many add-on libraries for making static or dynamic visuali&#8208;<br/>zations, but I&#8217;ll be mainly focused on matplotlib and libraries that build on top of it.<br/>matplotlib is a desktop plotting package designed for creating (mostly two-<br/>dimensional) publication-quality plots. The project was started by John Hunter in<br/>2002 to enable a MATLAB-like plotting interface in Python. The matplotlib and IPy&#8208;<br/>thon communities have collaborated to simplify interactive plotting from the IPython<br/>shell (and now, Jupyter notebook). matplotlib supports various GUI backends on all<br/>operating systems and additionally can export visualizations to all of the common<br/>vector and raster graphics formats (PDF, SVG, JPG, PNG, BMP, GIF, etc.). With the<br/>exception of a few diagrams, nearly all of the graphics in this book were produced<br/>using matplotlib.<br/>Over time, matplotlib has spawned a number of add-on toolkits for data visualization<br/>that use matplotlib for their underlying plotting. One of these is seaborn, which we<br/>explore later in this chapter.<br/>The simplest way to follow the code examples in the chapter is to use interactive plot&#8208;<br/>ting in the Jupyter notebook. To set this up, execute the following statement in a<br/>Jupyter notebook:<br/></p>
<p>%matplotlib notebook<br/></p>
<p>9.1 A Brief matplotlib API Primer<br/>With matplotlib, we use the following import convention:<br/></p>
<p>253</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In [11]: <b>import</b> <b>matplotlib.pyplot</b> <b>as</b> <b>plt<br/></b></p>
<p>After running %matplotlib notebook in Jupyter (or simply %matplotlib in IPy&#8208;<br/>thon), we can try creating a simple plot. If everything is set up right, a line plot like<br/>Figure 9-1 should appear:<br/></p>
<p>In [12]: <b>import</b> <b>numpy</b> <b>as</b> <b>np<br/></b></p>
<p>In [13]: data = np.arange(10)<br/></p>
<p>In [14]: data<br/>Out[14]: array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])<br/></p>
<p>In [15]: plt.plot(data)<br/></p>
<p><i>Figure 9-1. Simple line plot<br/></i>While libraries like seaborn and pandas&#8217;s built-in plotting functions will deal with<br/>many of the mundane details of making plots, should you wish to customize them<br/>beyond the function options provided, you will need to learn a bit about the matplot&#8208;<br/>lib API.<br/></p>
<p>There is not enough room in the book to give a comprehensive<br/>treatment to the breadth and depth of functionality in matplotlib. It<br/>should be enough to teach you the ropes to get up and running.<br/>The matplotlib gallery and documentation are the best resource for<br/>learning advanced features.<br/></p>
<p>254 | Chapter 9: Plotting and Visualization</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Figures and Subplots<br/>Plots in matplotlib reside within a Figure object. You can create a new figure with<br/>plt.figure:<br/></p>
<p>In [16]: fig = plt.figure()<br/></p>
<p>In IPython, an empty plot window will appear, but in Jupyter nothing will be shown<br/>until we use a few more commands. plt.figure has a number of options; notably,<br/>figsize will guarantee the figure has a certain size and aspect ratio if saved to disk.<br/>You can&#8217;t make a plot with a blank figure. You have to create one or more subplots<br/>using add_subplot:<br/></p>
<p>In [17]: ax1 = fig.add_subplot(2, 2, 1)<br/></p>
<p>This means that the figure should be 2 &#215; 2 (so up to four plots in total), and we&#8217;re<br/>selecting the first of four subplots (numbered from 1). If you create the next two sub&#8208;<br/>plots, you&#8217;ll end up with a visualization that looks like Figure 9-2:<br/></p>
<p>In [18]: ax2 = fig.add_subplot(2, 2, 2)<br/></p>
<p>In [19]: ax3 = fig.add_subplot(2, 2, 3)<br/></p>
<p><i>Figure 9-2. An empty matplotlib figure with three subplots<br/></i></p>
<p>9.1 A Brief matplotlib API Primer | 255</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>One nuance of using Jupyter notebooks is that plots are reset after<br/>each cell is evaluated, so for more complex plots you must put all of<br/>the plotting commands in a single notebook cell.<br/></p>
<p>Here we run all of these commands in the same cell:<br/>fig = plt.figure()<br/>ax1 = fig.add_subplot(2, 2, 1)<br/>ax2 = fig.add_subplot(2, 2, 2)<br/>ax3 = fig.add_subplot(2, 2, 3)<br/></p>
<p>When you issue a plotting command like plt.plot([1.5, 3.5, -2, 1.6]), mat&#8208;<br/>plotlib draws on the last figure and subplot used (creating one if necessary), thus hid&#8208;<br/>ing the figure and subplot creation. So if we add the following command, you&#8217;ll get<br/>something like Figure 9-3:<br/></p>
<p>In [20]: plt.plot(np.random.randn(50).cumsum(), 'k--')<br/></p>
<p><i>Figure 9-3. Data visualization after single plot<br/></i></p>
<p>The 'k--' is a <i>style</i> option instructing matplotlib to plot a black dashed line. The<br/>objects returned by fig.add_subplot here are AxesSubplot objects, on which you<br/>can directly plot on the other empty subplots by calling each one&#8217;s instance method<br/>(see Figure 9-4):<br/></p>
<p>256 | Chapter 9: Plotting and Visualization</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In [21]: _ = ax1.hist(np.random.randn(100), bins=20, color='k', alpha=0.3)<br/></p>
<p>In [22]: ax2.scatter(np.arange(30), np.arange(30) + 3 * np.random.randn(30))<br/></p>
<p><i>Figure 9-4. Data visualization after additional plots<br/></i>You can find a comprehensive catalog of plot types in the matplotlib documentation.<br/>Creating a figure with a grid of subplots is a very common task, so matplotlib<br/>includes a convenience method, plt.subplots, that creates a new figure and returns<br/>a NumPy array containing the created subplot objects:<br/></p>
<p>In [24]: fig, axes = plt.subplots(2, 3)<br/></p>
<p>In [25]: axes<br/>Out[25]: <br/>array([[&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7fb626374048&gt;,<br/>        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7fb62625db00&gt;,<br/>        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7fb6262f6c88&gt;],<br/>       [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7fb6261a36a0&gt;,<br/>        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7fb626181860&gt;,<br/>        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7fb6260fd4e0&gt;]], dtype<br/>=object)<br/></p>
<p>This is very useful, as the axes array can be easily indexed like a two-dimensional<br/>array; for example, axes[0, 1]. You can also indicate that subplots should have the<br/>same x- or y-axis using sharex and sharey, respectively. This is especially useful<br/>when you&#8217;re comparing data on the same scale; otherwise, matplotlib autoscales plot<br/>limits independently. See Table 9-1 for more on this method.<br/></p>
<p>9.1 A Brief matplotlib API Primer | 257</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><i>Table 9-1. pyplot.subplots options<br/></i>Argument Description<br/>nrows Number of rows of subplots<br/>ncols Number of columns of subplots<br/>sharex All subplots should use the same x-axis ticks (adjusting the xlim will affect all subplots)<br/>sharey All subplots should use the same y-axis ticks (adjusting the ylim will affect all subplots)<br/>subplot_kw Dict of keywords passed to add_subplot call used to create each subplot<br/>**fig_kw Additional keywords to subplots are used when creating the figure, such as plt.subplots(2, 2, <br/></p>
<p>figsize=(8, 6))<br/></p>
<p>Adjusting the spacing around subplots<br/>By default matplotlib leaves a certain amount of padding around the outside of the<br/>subplots and spacing between subplots. This spacing is all specified relative to the<br/>height and width of the plot, so that if you resize the plot either programmatically or<br/>manually using the GUI window, the plot will dynamically adjust itself. You can<br/>change the spacing using the subplots_adjust method on Figure objects, also avail&#8208;<br/>able as a top-level function:<br/></p>
<p>subplots_adjust(left=None, bottom=None, right=None, top=None,<br/>                wspace=None, hspace=None)<br/></p>
<p>wspace and hspace controls the percent of the figure width and figure height, respec&#8208;<br/>tively, to use as spacing between subplots. Here is a small example where I shrink the<br/>spacing all the way to zero (see Figure 9-5):<br/></p>
<p>fig, axes = plt.subplots(2, 2, sharex=True, sharey=True)<br/><b>for</b> i <b>in</b> range(2):<br/>    <b>for</b> j <b>in</b> range(2):<br/>        axes[i, j].hist(np.random.randn(500), bins=50, color='k', alpha=0.5)<br/>plt.subplots_adjust(wspace=0, hspace=0)<br/></p>
<p>258 | Chapter 9: Plotting and Visualization</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><i>Figure 9-5. Data visualization with no inter-subplot spacing<br/></i>You may notice that the axis labels overlap. matplotlib doesn&#8217;t check whether the<br/>labels overlap, so in a case like this you would need to fix the labels yourself by speci&#8208;<br/>fying explicit tick locations and tick labels (we&#8217;ll look at how to do this in the follow&#8208;<br/>ing sections).<br/></p>
<p>Colors, Markers, and Line Styles<br/>Matplotlib&#8217;s main plot function accepts arrays of x and y coordinates and optionally a<br/>string abbreviation indicating color and line style. For example, to plot x versus y<br/>with green dashes, you would execute:<br/></p>
<p>ax.plot(x, y, 'g--')<br/></p>
<p>This way of specifying both color and line style in a string is provided as a conve&#8208;<br/>nience; in practice if you were creating plots programmatically you might prefer not<br/>to have to munge strings together to create plots with the desired style. The same plot<br/>could also have been expressed more explicitly as:<br/></p>
<p>ax.plot(x, y, linestyle='--', color='g')<br/></p>
<p>There are a number of color abbreviations provided for commonly used colors, but<br/>you can use any color on the spectrum by specifying its hex code (e.g., '#CECECE').<br/>You can see the full set of line styles by looking at the docstring for plot (use plot? in<br/>IPython or Jupyter).<br/></p>
<p>9.1 A Brief matplotlib API Primer | 259</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Line plots can additionally have <i>markers</i> to highlight the actual data points. Since<br/>matplotlib creates a continuous line plot, interpolating between points, it can occa&#8208;<br/>sionally be unclear where the points lie. The marker can be part of the style string,<br/>which must have color followed by marker type and line style (see Figure 9-6):<br/></p>
<p>In [30]: <b>from</b> <b>numpy.random</b> <b>import</b> randn<br/></p>
<p>In [31]: plt.plot(randn(30).cumsum(), 'ko--')<br/></p>
<p><i>Figure 9-6. Line plot with markers<br/></i>This could also have been written more explicitly as:<br/></p>
<p>plot(randn(30).cumsum(), color='k', linestyle='dashed', marker='o')<br/></p>
<p>For line plots, you will notice that subsequent points are linearly interpolated by<br/>default. This can be altered with the drawstyle option (Figure 9-7):<br/></p>
<p>In [33]: data = np.random.randn(30).cumsum()<br/></p>
<p>In [34]: plt.plot(data, 'k--', label='Default')<br/>Out[34]: [&lt;matplotlib.lines.Line2D at 0x7fb624d86160&gt;]<br/></p>
<p>In [35]: plt.plot(data, 'k-', drawstyle='steps-post', label='steps-post')<br/>Out[35]: [&lt;matplotlib.lines.Line2D at 0x7fb624d869e8&gt;]<br/></p>
<p>In [36]: plt.legend(loc='best')<br/></p>
<p>260 | Chapter 9: Plotting and Visualization</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><i>Figure 9-7. Line plot with different drawstyle options<br/></i></p>
<p>You may notice output like &lt;matplotlib.lines.Line2D at ...&gt; when you run this.<br/>matplotlib returns objects that reference the plot subcomponent that was just added.<br/>A lot of the time you can safely ignore this output. Here, since we passed the label<br/>arguments to plot, we are able to create a plot legend to identify each line using<br/>plt.legend.<br/></p>
<p>You must call plt.legend (or ax.legend, if you have a reference to<br/>the axes) to create the legend, whether or not you passed the label<br/>options when plotting the data.<br/></p>
<p>Ticks, Labels, and Legends<br/>For most kinds of plot decorations, there are two main ways to do things: using the <br/>procedural pyplot interface (i.e., matplotlib.pyplot) and the more object-oriented<br/>native matplotlib API.<br/>The pyplot interface, designed for interactive use, consists of methods like xlim,<br/>xticks, and xticklabels. These control the plot range, tick locations, and tick labels,<br/>respectively. They can be used in two ways:<br/></p>
<p>9.1 A Brief matplotlib API Primer | 261</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>&#8226; Called with no arguments returns the current parameter value (e.g., plt.xlim()<br/>returns the current x-axis plotting range)<br/></p>
<p>&#8226; Called with parameters sets the parameter value (e.g., plt.xlim([0, 10]), sets<br/>the x-axis range to 0 to 10)<br/></p>
<p>All such methods act on the active or most recently created AxesSubplot. Each of<br/>them corresponds to two methods on the subplot object itself; in the case of xlim<br/>these are ax.get_xlim and ax.set_xlim. I prefer to use the subplot instance methods<br/>myself in the interest of being explicit (and especially when working with multiple<br/>subplots), but you can certainly use whichever you find more convenient.<br/></p>
<p>Setting the title, axis labels, ticks, and ticklabels<br/>To illustrate customizing the axes, I&#8217;ll create a simple figure and plot of a random<br/>walk (see Figure 9-8):<br/></p>
<p>In [37]: fig = plt.figure()<br/></p>
<p>In [38]: ax = fig.add_subplot(1, 1, 1)<br/></p>
<p>In [39]: ax.plot(np.random.randn(1000).cumsum())<br/></p>
<p><i>Figure 9-8. Simple plot for illustrating xticks (with label)<br/></i></p>
<p>To change the x-axis ticks, it&#8217;s easiest to use set_xticks and set_xticklabels. The<br/>former instructs matplotlib where to place the ticks along the data range; by default<br/></p>
<p>262 | Chapter 9: Plotting and Visualization</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>these locations will also be the labels. But we can set any other values as the labels<br/>using set_xticklabels:<br/></p>
<p>In [40]: ticks = ax.set_xticks([0, 250, 500, 750, 1000])<br/></p>
<p>In [41]: labels = ax.set_xticklabels(['one', 'two', 'three', 'four', 'five'],<br/>   ....:                             rotation=30, fontsize='small')<br/></p>
<p>The rotation option sets the x tick labels at a 30-degree rotation. Lastly, set_xlabel<br/>gives a name to the x-axis and set_title the subplot title (see Figure 9-9 for the<br/>resulting figure):<br/></p>
<p>In [42]: ax.set_title('My first matplotlib plot')<br/>Out[42]: &lt;matplotlib.text.Text at 0x7fb624d055f8&gt;<br/></p>
<p>In [43]: ax.set_xlabel('Stages')<br/></p>
<p><i>Figure 9-9. Simple plot for illustrating xticks<br/></i></p>
<p>Modifying the y-axis consists of the same process, substituting y for x in the above.<br/>The axes class has a set method that allows batch setting of plot properties. From the<br/>prior example, we could also have written:<br/></p>
<p>props = {<br/>    'title': 'My first matplotlib plot',<br/>    'xlabel': 'Stages'<br/>}<br/>ax.set(**props)<br/></p>
<p>9.1 A Brief matplotlib API Primer | 263</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Adding legends<br/>Legends are another critical element for identifying plot elements. There are a couple<br/>of ways to add one. The easiest is to pass the label argument when adding each piece<br/>of the plot:<br/></p>
<p>In [44]: <b>from</b> <b>numpy.random</b> <b>import</b> randn<br/></p>
<p>In [45]: fig = plt.figure(); ax = fig.add_subplot(1, 1, 1)<br/></p>
<p>In [46]: ax.plot(randn(1000).cumsum(), 'k', label='one')<br/>Out[46]: [&lt;matplotlib.lines.Line2D at 0x7fb624bdf860&gt;]<br/></p>
<p>In [47]: ax.plot(randn(1000).cumsum(), 'k--', label='two')<br/>Out[47]: [&lt;matplotlib.lines.Line2D at 0x7fb624be90f0&gt;]<br/></p>
<p>In [48]: ax.plot(randn(1000).cumsum(), 'k.', label='three')<br/>Out[48]: [&lt;matplotlib.lines.Line2D at 0x7fb624be9160&gt;]<br/></p>
<p>Once you&#8217;ve done this, you can either call ax.legend() or plt.legend() to automat&#8208;<br/>ically create a legend. The resulting plot is in Figure 9-10:<br/></p>
<p>In [49]: ax.legend(loc='best')<br/></p>
<p><i>Figure 9-10. Simple plot with three lines and legend<br/></i></p>
<p>The legend method has several other choices for the location loc argument. See the<br/>docstring (with ax.legend?) for more information.<br/></p>
<p>264 | Chapter 9: Plotting and Visualization</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>The loc tells matplotlib where to place the plot. If you aren&#8217;t picky, 'best' is a good<br/>option, as it will choose a location that is most out of the way. To exclude one or more<br/>elements from the legend, pass no label or label='_nolegend_'.<br/></p>
<p>Annotations and Drawing on a Subplot<br/>In addition to the standard plot types, you may wish to draw your own plot annota&#8208;<br/>tions, which could consist of text, arrows, or other shapes. You can add annotations<br/>and text using the text, arrow, and annotate functions. text draws text at given<br/>coordinates (x, y) on the plot with optional custom styling:<br/></p>
<p>ax.text(x, y, 'Hello world!',<br/>        family='monospace', fontsize=10)<br/></p>
<p>Annotations can draw both text and arrows arranged appropriately. As an example,<br/>let&#8217;s plot the closing S&amp;P 500 index price since 2007 (obtained from Yahoo! Finance)<br/>and annotate it with some of the important dates from the 2008&#8211;2009 financial crisis.<br/>You can most easily reproduce this code example in a single cell in a Jupyter note&#8208;<br/>book. See Figure 9-11 for the result:<br/></p>
<p><b>from</b> <b>datetime</b> <b>import</b> datetime<br/></p>
<p>fig = plt.figure()<br/>ax = fig.add_subplot(1, 1, 1)<br/></p>
<p>data = pd.read_csv('examples/spx.csv', index_col=0, parse_dates=True)<br/>spx = data['SPX']<br/></p>
<p>spx.plot(ax=ax, style='k-')<br/></p>
<p>crisis_data = [<br/>    (datetime(2007, 10, 11), 'Peak of bull market'),<br/>    (datetime(2008, 3, 12), 'Bear Stearns Fails'),<br/>    (datetime(2008, 9, 15), 'Lehman Bankruptcy')<br/>]<br/></p>
<p><b>for</b> date, label <b>in</b> crisis_data:<br/>    ax.annotate(label, xy=(date, spx.asof(date) + 75),<br/>                xytext=(date, spx.asof(date) + 225),<br/>                arrowprops=dict(facecolor='black', headwidth=4, width=2,<br/>                                headlength=4),<br/>                horizontalalignment='left', verticalalignment='top')<br/></p>
<p><i># Zoom in on 2007-2010<br/></i>ax.set_xlim(['1/1/2007', '1/1/2011'])<br/>ax.set_ylim([600, 1800])<br/></p>
<p>ax.set_title('Important dates in the 2008-2009 financial crisis')<br/></p>
<p>9.1 A Brief matplotlib API Primer | 265</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><i>Figure 9-11. Important dates in the 2008&#8211;2009 financial crisis<br/></i></p>
<p>There are a couple of important points to highlight in this plot: the ax.annotate<br/>method can draw labels at the indicated x and y coordinates. We use the set_xlim<br/>and set_ylim methods to manually set the start and end boundaries for the plot<br/>rather than using matplotlib&#8217;s default. Lastly, ax.set_title adds a main title to the<br/>plot.<br/>See the online matplotlib gallery for many more annotation examples to learn from.<br/>Drawing shapes requires some more care. matplotlib has objects that represent many<br/>common shapes, referred to as <i>patches</i>. Some of these, like Rectangle and Circle, are<br/>found in matplotlib.pyplot, but the full set is located in matplotlib.patches.<br/>To add a shape to a plot, you create the patch object shp and add it to a subplot by<br/>calling ax.add_patch(shp) (see Figure 9-12):<br/></p>
<p>fig = plt.figure()<br/>ax = fig.add_subplot(1, 1, 1)<br/></p>
<p>rect = plt.Rectangle((0.2, 0.75), 0.4, 0.15, color='k', alpha=0.3)<br/>circ = plt.Circle((0.7, 0.2), 0.15, color='b', alpha=0.3)<br/>pgon = plt.Polygon([[0.15, 0.15], [0.35, 0.4], [0.2, 0.6]],<br/>                   color='g', alpha=0.5)<br/></p>
<p>ax.add_patch(rect)<br/>ax.add_patch(circ)<br/>ax.add_patch(pgon)<br/></p>
<p>266 | Chapter 9: Plotting and Visualization</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><i>Figure 9-12. Data visualization composed from three different patches<br/></i>If you look at the implementation of many familiar plot types, you will see that they<br/>are assembled from patches.<br/></p>
<p>Saving Plots to File<br/>You can save the active figure to file using plt.savefig. This method is equivalent to<br/>the figure object&#8217;s savefig instance method. For example, to save an SVG version of a<br/>figure, you need only type:<br/></p>
<p>plt.savefig('figpath.svg')<br/></p>
<p>The file type is inferred from the file extension. So if you used .pdf instead, you<br/>would get a PDF. There are a couple of important options that I use frequently for<br/>publishing graphics: dpi, which controls the dots-per-inch resolution, and<br/>bbox_inches, which can trim the whitespace around the actual figure. To get the<br/>same plot as a PNG with minimal whitespace around the plot and at 400 DPI, you<br/>would do:<br/></p>
<p>plt.savefig('figpath.png', dpi=400, bbox_inches='tight')<br/></p>
<p>savefig doesn&#8217;t have to write to disk; it can also write to any file-like object, such as a<br/>BytesIO:<br/></p>
<p><b>from</b> <b>io</b> <b>import</b> BytesIO<br/>buffer = BytesIO()<br/>plt.savefig(buffer)<br/>plot_data = buffer.getvalue()<br/></p>
<p>See Table 9-2 for a list of some other options for savefig.<br/></p>
<p>9.1 A Brief matplotlib API Primer | 267</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><i>Table 9-2. Figure.savefig options<br/></i>Argument Description<br/>fname String containing a filepath or a Python file-like object. The figure format is inferred from the file<br/></p>
<p>extension (e.g., .pdf for PDF or .png for PNG)<br/>dpi The figure resolution in dots per inch; defaults to 100 out of the box but can be configured<br/>facecolor, <br/>edgecolor<br/></p>
<p>The color of the figure background outside of the subplots; 'w' (white), by default<br/></p>
<p>format The explicit file format to use ('png', 'pdf', 'svg', 'ps', 'eps', ...)<br/>bbox_inches The portion of the figure to save; if 'tight' is passed, will attempt to trim the empty space around<br/></p>
<p>the figure<br/></p>
<p>matplotlib Configuration<br/>matplotlib comes configured with color schemes and defaults that are geared primar&#8208;<br/>ily toward preparing figures for publication. Fortunately, nearly all of the default<br/>behavior can be customized via an extensive set of global parameters governing figure<br/>size, subplot spacing, colors, font sizes, grid styles, and so on. One way to modify the<br/>configuration programmatically from Python is to use the rc method; for example, to<br/>set the global default figure size to be 10 &#215; 10, you could enter:<br/></p>
<p>plt.rc('figure', figsize=(10, 10))<br/></p>
<p>The first argument to rc is the component you wish to customize, such as 'figure',<br/>'axes', 'xtick', 'ytick', 'grid', 'legend', or many others. After that can follow a<br/>sequence of keyword arguments indicating the new parameters. An easy way to write<br/>down the options in your program is as a dict:<br/></p>
<p>font_options = {'family' : 'monospace',<br/>                'weight' : 'bold',<br/>                'size'   : 'small'}<br/>plt.rc('font', **font_options)<br/></p>
<p>For more extensive customization and to see a list of all the options, matplotlib comes<br/>with a configuration file <i>matplotlibrc</i> in the <i>matplotlib/mpl-data</i> directory. If you cus&#8208;<br/>tomize this file and place it in your home directory titled <i>.matplotlibrc</i>, it will be<br/>loaded each time you use matplotlib.<br/>As we&#8217;ll see in the next section, the seaborn package has several built-in plot themes<br/>or <i>styles</i> that use matplotlib&#8217;s configuration system internally.<br/></p>
<p>9.2 Plotting with pandas and seaborn<br/>matplotlib can be a fairly low-level tool. You assemble a plot from its base compo&#8208;<br/>nents: the data display (i.e., the type of plot: line, bar, box, scatter, contour, etc.), leg&#8208;<br/>end, title, tick labels, and other annotations.<br/></p>
<p>268 | Chapter 9: Plotting and Visualization</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In pandas we may have multiple columns of data, along with row and column labels.<br/>pandas itself has built-in methods that simplify creating visualizations from Data&#8208;<br/>Frame and Series objects. Another library is seaborn, a statistical graphics library cre&#8208;<br/>ated by Michael Waskom. Seaborn simplifies creating many common visualization<br/>types.<br/></p>
<p>Importing seaborn modifies the default matplotlib color schemes<br/>and plot styles to improve readability and aesthetics. Even if you do<br/>not use the seaborn API, you may prefer to import seaborn as a<br/>simple way to improve the visual aesthetics of general matplotlib<br/>plots.<br/></p>
<p>Line Plots<br/>Series and DataFrame each have a plot attribute for making some basic plot types. By<br/>default, plot() makes line plots (see Figure 9-13):<br/></p>
<p>In [60]: s = pd.Series(np.random.randn(10).cumsum(), index=np.arange(0, 100, 10))<br/></p>
<p>In [61]: s.plot()<br/></p>
<p><i>Figure 9-13. Simple Series plot<br/></i>The Series object&#8217;s index is passed to matplotlib for plotting on the x-axis, though you<br/>can disable this by passing use_index=False. The x-axis ticks and limits can be<br/>adjusted with the xticks and xlim options, and y-axis respectively with yticks and<br/></p>
<p>9.2 Plotting with pandas and seaborn | 269</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>ylim. See Table 9-3 for a full listing of plot options. I&#8217;ll comment on a few more of<br/>them throughout this section and leave the rest to you to explore.<br/>Most of pandas&#8217;s plotting methods accept an optional ax parameter, which can be a<br/>matplotlib subplot object. This gives you more flexible placement of subplots in a grid<br/>layout.<br/>DataFrame&#8217;s plot method plots each of its columns as a different line on the same<br/>subplot, creating a legend automatically (see Figure 9-14):<br/></p>
<p>In [62]: df = pd.DataFrame(np.random.randn(10, 4).cumsum(0),<br/>   ....:                   columns=['A', 'B', 'C', 'D'],<br/>   ....:                   index=np.arange(0, 100, 10))<br/></p>
<p>In [63]: df.plot()<br/></p>
<p><i>Figure 9-14. Simple DataFrame plot<br/></i></p>
<p>The plot attribute contains a &#8220;family&#8221; of methods for different plot types. For exam&#8208;<br/>ple, df.plot() is equivalent to df.plot.line(). We&#8217;ll explore some of these methods<br/>next.<br/></p>
<p>Additional keyword arguments to plot are passed through to the<br/>respective matplotlib plotting function, so you can further custom&#8208;<br/>ize these plots by learning more about the matplotlib API.<br/></p>
<p>270 | Chapter 9: Plotting and Visualization</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><i>Table 9-3. Series.plot method arguments<br/></i>Argument Description<br/>label Label for plot legend<br/>ax matplotlib subplot object to plot on; if nothing passed, uses active matplotlib subplot<br/>style Style string, like 'ko--', to be passed to matplotlib<br/>alpha The plot fill opacity (from 0 to 1)<br/>kind Can be 'area', 'bar', 'barh', 'density', 'hist', 'kde', 'line', 'pie'<br/>logy Use logarithmic scaling on the y-axis<br/>use_index Use the object index for tick labels<br/>rot Rotation of tick labels (0 through 360)<br/>xticks Values to use for x-axis ticks<br/>yticks Values to use for y-axis ticks<br/>xlim x-axis limits (e.g., [0, 10])<br/>ylim y-axis limits<br/>grid Display axis grid (on by default)<br/></p>
<p>DataFrame has a number of options allowing some flexibility with how the columns<br/>are handled; for example, whether to plot them all on the same subplot or to create<br/>separate subplots. See Table 9-4 for more on these.<br/><i>Table 9-4. DataFrame-specific plot arguments<br/></i></p>
<p>Argument Description<br/>subplots Plot each DataFrame column in a separate subplot<br/>sharex If subplots=True, share the same x-axis, linking ticks and limits<br/>sharey If subplots=True, share the same y-axis<br/>figsize Size of figure to create as tuple<br/>title Plot title as string<br/>legend Add a subplot legend (True by default)<br/>sort_columns Plot columns in alphabetical order; by default uses existing column order<br/></p>
<p>For time series plotting, see Chapter 11.<br/></p>
<p>9.2 Plotting with pandas and seaborn | 271</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Bar Plots<br/>The plot.bar() and plot.barh() make vertical and horizontal bar plots, respec&#8208;<br/>tively. In this case, the Series or DataFrame index will be used as the x (bar) or y<br/>(barh) ticks (see Figure 9-15):<br/></p>
<p>In [64]: fig, axes = plt.subplots(2, 1)<br/></p>
<p>In [65]: data = pd.Series(np.random.rand(16), index=list('abcdefghijklmnop'))<br/></p>
<p>In [66]: data.plot.bar(ax=axes[0], color='k', alpha=0.7)<br/>Out[66]: &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fb62493d470&gt;<br/></p>
<p>In [67]: data.plot.barh(ax=axes[1], color='k', alpha=0.7)<br/></p>
<p><i>Figure 9-15. Horizonal and vertical bar plot<br/></i></p>
<p>The options color='k' and alpha=0.7 set the color of the plots to black and use par&#8208;<br/>tial transparency on the filling.<br/></p>
<p>272 | Chapter 9: Plotting and Visualization</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>With a DataFrame, bar plots group the values in each row together in a group in bars,<br/>side by side, for each value. See Figure 9-16:<br/></p>
<p>In [69]: df = pd.DataFrame(np.random.rand(6, 4),<br/>   ....:                   index=['one', 'two', 'three', 'four', 'five', 'six'],<br/>   ....:                   columns=pd.Index(['A', 'B', 'C', 'D'], name='Genus'))<br/></p>
<p>In [70]: df<br/>Out[70]: <br/>Genus         A         B         C         D<br/>one    0.370670  0.602792  0.229159  0.486744<br/>two    0.420082  0.571653  0.049024  0.880592<br/>three  0.814568  0.277160  0.880316  0.431326<br/>four   0.374020  0.899420  0.460304  0.100843<br/>five   0.433270  0.125107  0.494675  0.961825<br/>six    0.601648  0.478576  0.205690  0.560547<br/></p>
<p>In [71]: df.plot.bar()<br/></p>
<p><i>Figure 9-16. DataFrame bar plot<br/></i>Note that the name &#8220;Genus&#8221; on the DataFrame&#8217;s columns is used to title the legend.<br/></p>
<p>9.2 Plotting with pandas and seaborn | 273</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>We create stacked bar plots from a DataFrame by passing stacked=True, resulting in<br/>the value in each row being stacked together (see Figure 9-17):<br/></p>
<p>In [73]: df.plot.barh(stacked=True, alpha=0.5)<br/></p>
<p><i>Figure 9-17. DataFrame stacked bar plot<br/></i></p>
<p>A useful recipe for bar plots is to visualize a Series&#8217;s value frequency <br/>using value_counts: s.value_counts().plot.bar().<br/></p>
<p>Returning to the tipping dataset used earlier in the book, suppose we wanted to make<br/>a stacked bar plot showing the percentage of data points for each party size on each<br/>day. I load the data using read_csv and make a cross-tabulation by day and party size:<br/></p>
<p>In [75]: tips = pd.read_csv('examples/tips.csv')<br/></p>
<p>In [76]: party_counts = pd.crosstab(tips['day'], tips['size'])<br/></p>
<p>In [77]: party_counts<br/>Out[77]: <br/>size  1   2   3   4  5  6<br/>day                      <br/>Fri   1  16   1   1  0  0<br/>Sat   2  53  18  13  1  0<br/>Sun   0  39  15  18  3  1<br/>Thur  1  48   4   5  1  3<br/></p>
<p>274 | Chapter 9: Plotting and Visualization</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><i># Not many 1- and 6-person parties<br/></i>In [78]: party_counts = party_counts.loc[:, 2:5]<br/></p>
<p>Then, normalize so that each row sums to 1 and make the plot (see Figure 9-18):<br/><i># Normalize to sum to 1<br/></i>In [79]: party_pcts = party_counts.div(party_counts.sum(1), axis=0)<br/></p>
<p>In [80]: party_pcts<br/>Out[80]: <br/>size         2         3         4         5<br/>day                                         <br/>Fri   0.888889  0.055556  0.055556  0.000000<br/>Sat   0.623529  0.211765  0.152941  0.011765<br/>Sun   0.520000  0.200000  0.240000  0.040000<br/>Thur  0.827586  0.068966  0.086207  0.017241<br/></p>
<p>In [81]: party_pcts.plot.bar()<br/></p>
<p><i>Figure 9-18. Fraction of parties by size on each day<br/></i>So you can see that party sizes appear to increase on the weekend in this dataset.<br/>With data that requires aggregation or summarization before making a plot, using the<br/>seaborn package can make things much simpler. Let&#8217;s look now at the tipping per&#8208;<br/>centage by day with seaborn (see Figure 9-19 for the resulting plot):<br/></p>
<p>9.2 Plotting with pandas and seaborn | 275</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In [83]: <b>import</b> <b>seaborn</b> <b>as</b> <b>sns<br/></b></p>
<p>In [84]: tips['tip_pct'] = tips['tip'] / (tips['total_bill'] - tips['tip'])<br/></p>
<p>In [85]: tips.head()<br/>Out[85]: <br/>   total_bill   tip smoker  day    time  size   tip_pct<br/>0       16.99  1.01     No  Sun  Dinner     2  0.063204<br/>1       10.34  1.66     No  Sun  Dinner     3  0.191244<br/>2       21.01  3.50     No  Sun  Dinner     3  0.199886<br/>3       23.68  3.31     No  Sun  Dinner     2  0.162494<br/>4       24.59  3.61     No  Sun  Dinner     4  0.172069<br/></p>
<p>In [86]: sns.barplot(x='tip_pct', y='day', data=tips, orient='h')<br/></p>
<p><i>Figure 9-19. Tipping percentage by day with error bars<br/></i></p>
<p>Plotting functions in seaborn take a data argument, which can be a pandas Data&#8208;<br/>Frame. The other arguments refer to column names. Because there are multiple<br/>observations for each value in the day, the bars are the average value of tip_pct. The<br/>black lines drawn on the bars represent the 95% confidence interval (this can be con&#8208;<br/>figured through optional arguments).<br/></p>
<p>276 | Chapter 9: Plotting and Visualization</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>seaborn.barplot has a hue option that enables us to split by an additional categorical<br/>value (Figure 9-20):<br/></p>
<p>In [88]: sns.barplot(x='tip_pct', y='day', hue='time', data=tips, orient='h')<br/></p>
<p><i>Figure 9-20. Tipping percentage by day and time<br/></i>Notice that seaborn has automatically changed the aesthetics of plots: the default<br/>color palette, plot background, and grid line colors. You can switch between different<br/>plot appearances using seaborn.set:<br/></p>
<p>In [90]: sns.set(style=&quot;whitegrid&quot;)<br/></p>
<p>Histograms and Density Plots<br/>A histogram is a kind of bar plot that gives a discretized display of value frequency.<br/>The data points are split into discrete, evenly spaced bins, and the number of data<br/>points in each bin is plotted. Using the tipping data from before, we can make a histo&#8208;<br/>gram of tip percentages of the total bill using the plot.hist method on the Series<br/>(see Figure 9-21):<br/></p>
<p>In [92]: tips['tip_pct'].plot.hist(bins=50)<br/></p>
<p>9.2 Plotting with pandas and seaborn | 277</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><i>Figure 9-21. Histogram of tip percentages<br/></i>A related plot type is a <i>density plot</i>, which is formed by computing an estimate of a<br/>continuous probability distribution that might have generated the observed data. The<br/>usual procedure is to approximate this distribution as a mixture of &#8220;kernels&#8221;&#8212;that is,<br/>simpler distributions like the normal distribution. Thus, density plots are also known<br/>as kernel density estimate (KDE) plots. Using plot.kde makes a density plot using<br/>the conventional mixture-of-normals estimate (see Figure 9-22):<br/></p>
<p>In [94]: tips['tip_pct'].plot.density()<br/></p>
<p>278 | Chapter 9: Plotting and Visualization</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><i>Figure 9-22. Density plot of tip percentages<br/></i></p>
<p>Seaborn makes histograms and density plots even easier through its distplot<br/>method, which can plot both a histogram and a continuous density estimate simulta&#8208;<br/>neously. As an example, consider a bimodal distribution consisting of draws from<br/>two different standard normal distributions (see Figure 9-23):<br/></p>
<p>In [96]: comp1 = np.random.normal(0, 1, size=200)<br/></p>
<p>In [97]: comp2 = np.random.normal(10, 2, size=200)<br/></p>
<p>In [98]: values = pd.Series(np.concatenate([comp1, comp2]))<br/></p>
<p>In [99]: sns.distplot(values, bins=100, color='k')<br/></p>
<p>9.2 Plotting with pandas and seaborn | 279</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><i>Figure 9-23. Normalized histogram of normal mixture with density estimate<br/></i></p>
<p>Scatter or Point Plots<br/>Point plots or scatter plots can be a useful way of examining the relationship between<br/>two one-dimensional data series. For example, here we load the macrodata dataset<br/>from the statsmodels project, select a few variables, then compute log differences:<br/></p>
<p>In [100]: macro = pd.read_csv('examples/macrodata.csv')<br/></p>
<p>In [101]: data = macro[['cpi', 'm1', 'tbilrate', 'unemp']]<br/></p>
<p>In [102]: trans_data = np.log(data).diff().dropna()<br/></p>
<p>In [103]: trans_data[-5:]<br/>Out[103]: <br/>          cpi        m1  tbilrate     unemp<br/>198 -0.007904  0.045361 -0.396881  0.105361<br/>199 -0.021979  0.066753 -2.277267  0.139762<br/>200  0.002340  0.010286  0.606136  0.160343<br/>201  0.008419  0.037461 -0.200671  0.127339<br/>202  0.008894  0.012202 -0.405465  0.042560<br/></p>
<p>280 | Chapter 9: Plotting and Visualization</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>We can then use seaborn&#8217;s regplot method, which makes a scatter plot and fits a lin&#8208;<br/>ear regression line (see Figure 9-24):<br/></p>
<p>In [105]: sns.regplot('m1', 'unemp', data=trans_data)<br/>Out[105]: &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fb613720be0&gt;<br/></p>
<p>In [106]: plt.title('Changes in log %s versus log %s' % ('m1', 'unemp'))<br/></p>
<p><i>Figure 9-24. A seaborn regression/scatter plot<br/></i>In exploratory data analysis it&#8217;s helpful to be able to look at all the scatter plots among<br/>a group of variables; this is known as a <i>pairs</i> plot or <i>scatter plot matrix</i>. Making such a<br/>plot from scratch is a bit of work, so seaborn has a convenient pairplot function,<br/>which supports placing histograms or density estimates of each variable along the<br/>diagonal (see Figure 9-25 for the resulting plot):<br/></p>
<p>In [107]: sns.pairplot(trans_data, diag_kind='kde', plot_kws={'alpha': 0.2})<br/></p>
<p>9.2 Plotting with pandas and seaborn | 281</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><i>Figure 9-25. Pair plot matrix of statsmodels macro data<br/></i></p>
<p>You may notice the plot_kws argument. This enables us to pass down configuration<br/>options to the individual plotting calls on the off-diagonal elements. Check out the<br/>seaborn.pairplot docstring for more granular configuration options.<br/></p>
<p>282 | Chapter 9: Plotting and Visualization</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Facet Grids and Categorical Data<br/>What about datasets where we have additional grouping dimensions? One way to vis&#8208;<br/>ualize data with many categorical variables is to use a <i>facet grid</i>. Seaborn has a useful<br/>built-in function factorplot that simplifies making many kinds of faceted plots (see<br/>Figure 9-26 for the resulting plot):<br/></p>
<p>In [108]: sns.factorplot(x='day', y='tip_pct', hue='time', col='smoker',<br/>   .....:                kind='bar', data=tips[tips.tip_pct &lt; 1])<br/></p>
<p><i>Figure 9-26. Tipping percentage by day/time/smoker<br/></i></p>
<p>Instead of grouping by 'time' by different bar colors within a facet, we can also<br/>expand the facet grid by adding one row per time value (Figure 9-27):<br/></p>
<p>In [109]: sns.factorplot(x='day', y='tip_pct', row='time',<br/>   .....:                col='smoker',<br/>   .....:                kind='bar', data=tips[tips.tip_pct &lt; 1])<br/></p>
<p>9.2 Plotting with pandas and seaborn | 283</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><i>Figure 9-27. tip_pct by day; facet by time/smoker<br/></i></p>
<p>factorplot supports other plot types that may be useful depending on what you are<br/>trying to display. For example, box plots (which show the median, quartiles, and out&#8208;<br/>liers) can be an effective visualization type (Figure 9-28):<br/></p>
<p>In [110]: sns.factorplot(x='tip_pct', y='day', kind='box',<br/>   .....:                data=tips[tips.tip_pct &lt; 0.5])<br/></p>
<p>284 | Chapter 9: Plotting and Visualization</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><i>Figure 9-28. Box plot of tip_pct by day<br/></i></p>
<p>You can create your own facet grid plots using the more general seaborn.FacetGrid<br/>class. See the seaborn documentation for more.<br/></p>
<p>9.3 Other Python Visualization Tools<br/>As is common with open source, there are a plethora of options for creating graphics<br/>in Python (too many to list). Since 2010, much development effort has been focused<br/>on creating interactive graphics for publication on the web. With tools like Bokeh and<br/>Plotly, it&#8217;s now possible to specify dynamic, interactive graphics in Python that are<br/>destined for a web browser.<br/>For creating static graphics for print or web, I recommend defaulting to matplotlib<br/>and add-on libraries like pandas and seaborn for your needs. For other data visualiza&#8208;<br/>tion requirements, it may be useful to learn one of the other available tools out there.<br/>I encourage you to explore the ecosystem as it continues to involve and innovate into<br/>the future.<br/></p>
<p>9.3 Other Python Visualization Tools | 285</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>9.4 Conclusion<br/>The goal of this chapter was to get your feet wet with some basic data visualization<br/>using pandas, matplotlib, and seaborn. If visually communicating the results of data<br/>analysis is important in your work, I encourage you to seek out resources to learn<br/>more about effective data visualization. It is an active field of research and you can<br/>practice with many excellent learning resources available online and in print form.<br/>In the next chapter, we turn our attention to data aggregation and group operations<br/>with pandas.<br/></p>
<p>286 | Chapter 9: Plotting and Visualization</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>CHAPTER 10<br/>Data Aggregation and Group Operations<br/></p>
<p>Categorizing a dataset and applying a function to each group, whether an aggregation<br/>or transformation, is often a critical component of a data analysis workflow. After<br/>loading, merging, and preparing a dataset, you may need to compute group statistics<br/>or possibly <i>pivot tables</i> for reporting or visualization purposes. pandas provides a<br/>flexible groupby interface, enabling you to slice, dice, and summarize datasets in a<br/>natural way.<br/>One reason for the popularity of relational databases and SQL (which stands for<br/>&#8220;structured query language&#8221;) is the ease with which data can be joined, filtered, trans&#8208;<br/>formed, and aggregated. However, query languages like SQL are somewhat con&#8208;<br/>strained in the kinds of group operations that can be performed. As you will see, with<br/>the expressiveness of Python and pandas, we can perform quite complex group oper&#8208;<br/>ations by utilizing any function that accepts a pandas object or NumPy array. In this<br/>chapter, you will learn how to:<br/></p>
<p>&#8226; Split a pandas object into pieces using one or more keys (in the form of func&#8208;<br/>tions, arrays, or DataFrame column names)<br/></p>
<p>&#8226; Calculate group summary statistics, like count, mean, or standard deviation, or a<br/>user-defined function<br/></p>
<p>&#8226; Apply within-group transformations or other manipulations, like normalization,<br/>linear regression, rank, or subset selection<br/></p>
<p>&#8226; Compute pivot tables and cross-tabulations<br/>&#8226; Perform quantile analysis and other statistical group analyses<br/></p>
<p>287</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Aggregation of time series data, a special use case of groupby, is<br/>referred to as <i>resampling</i> in this book and will receive separate<br/>treatment in Chapter 11.<br/></p>
<p>10.1 GroupBy Mechanics<br/>Hadley Wickham, an author of many popular packages for the R programming lan&#8208;<br/>guage, coined the term <i>split-apply-combine</i> for describing group operations. In the<br/>first stage of the process, data contained in a pandas object, whether a Series, Data&#8208;<br/>Frame, or otherwise, is <i>split</i> into groups based on one or more <i>keys</i> that you provide.<br/>The splitting is performed on a particular axis of an object. For example, a DataFrame<br/>can be grouped on its rows (axis=0) or its columns (axis=1). Once this is done, a<br/>function is <i>applied</i> to each group, producing a new value. Finally, the results of all<br/>those function applications are <i>combined</i> into a result object. The form of the result&#8208;<br/>ing object will usually depend on what&#8217;s being done to the data. See Figure 10-1 for a<br/>mockup of a simple group aggregation.<br/></p>
<p><i>Figure 10-1. Illustration of a group aggregation<br/></i>Each grouping key can take many forms, and the keys do not have to be all of the<br/>same type:<br/></p>
<p>&#8226; A list or array of values that is the same length as the axis being grouped<br/>&#8226; A value indicating a column name in a DataFrame<br/></p>
<p>288 | Chapter 10: Data Aggregation and Group Operations</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>&#8226; A dict or Series giving a correspondence between the values on the axis being<br/>grouped and the group names<br/></p>
<p>&#8226; A function to be invoked on the axis index or the individual labels in the index<br/>Note that the latter three methods are shortcuts for producing an array of values to be<br/>used to split up the object. Don&#8217;t worry if this all seems abstract. Throughout this<br/>chapter, I will give many examples of all these methods. To get started, here is a small<br/>tabular dataset as a DataFrame:<br/></p>
<p>In [10]: df = pd.DataFrame({'key1' : ['a', 'a', 'b', 'b', 'a'],<br/>   ....:                    'key2' : ['one', 'two', 'one', 'two', 'one'],<br/>   ....:                    'data1' : np.random.randn(5),<br/>   ....:                    'data2' : np.random.randn(5)})<br/></p>
<p>In [11]: df<br/>Out[11]: <br/>      data1     data2 key1 key2<br/>0 -0.204708  1.393406    a  one<br/>1  0.478943  0.092908    a  two<br/>2 -0.519439  0.281746    b  one<br/>3 -0.555730  0.769023    b  two<br/>4  1.965781  1.246435    a  one<br/></p>
<p>Suppose you wanted to compute the mean of the data1 column using the labels from<br/>key1. There are a number of ways to do this. One is to access data1 and call groupby<br/>with the column (a Series) at key1:<br/></p>
<p>In [12]: grouped = df['data1'].groupby(df['key1'])<br/></p>
<p>In [13]: grouped<br/>Out[13]: &lt;pandas.core.groupby.SeriesGroupBy object at 0x7faa31537390&gt;<br/></p>
<p>This grouped variable is now a <i>GroupBy</i> object. It has not actually computed anything<br/>yet except for some intermediate data about the group key df['key1']. The idea is<br/>that this object has all of the information needed to then apply some operation to<br/>each of the groups. For example, to compute group means we can call the GroupBy&#8217;s<br/>mean method:<br/></p>
<p>In [14]: grouped.mean()<br/>Out[14]: <br/>key1<br/>a    0.746672<br/>b   -0.537585<br/>Name: data1, dtype: float64<br/></p>
<p>Later, I&#8217;ll explain more about what happens when you call .mean(). The important<br/>thing here is that the data (a Series) has been aggregated according to the group key,<br/>producing a new Series that is now indexed by the unique values in the key1 column.<br/></p>
<p>10.1 GroupBy Mechanics | 289</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>The result index has the name 'key1' because the DataFrame column df['key1']<br/>did.<br/>If instead we had passed multiple arrays as a list, we&#8217;d get something different:<br/></p>
<p>In [15]: means = df['data1'].groupby([df['key1'], df['key2']]).mean()<br/></p>
<p>In [16]: means<br/>Out[16]: <br/>key1  key2<br/>a     one     0.880536<br/>      two     0.478943<br/>b     one    -0.519439<br/>      two    -0.555730<br/>Name: data1, dtype: float64<br/></p>
<p>Here we grouped the data using two keys, and the resulting Series now has a hier&#8208;<br/>archical index consisting of the unique pairs of keys observed:<br/></p>
<p>In [17]: means.unstack()<br/>Out[17]: <br/>key2       one       two<br/>key1                    <br/>a     0.880536  0.478943<br/>b    -0.519439 -0.555730<br/></p>
<p>In this example, the group keys are all Series, though they could be any arrays of the<br/>right length:<br/></p>
<p>In [18]: states = np.array(['Ohio', 'California', 'California', 'Ohio', 'Ohio'])<br/></p>
<p>In [19]: years = np.array([2005, 2005, 2006, 2005, 2006])<br/></p>
<p>In [20]: df['data1'].groupby([states, years]).mean()<br/>Out[20]: <br/>California  2005    0.478943<br/>            2006   -0.519439<br/>Ohio        2005   -0.380219<br/>            2006    1.965781<br/>Name: data1, dtype: float64<br/></p>
<p>Frequently the grouping information is found in the same DataFrame as the data you<br/>want to work on. In that case, you can pass column names (whether those are strings,<br/>numbers, or other Python objects) as the group keys:<br/></p>
<p>In [21]: df.groupby('key1').mean()<br/>Out[21]: <br/>         data1     data2<br/>key1                    <br/>a     0.746672  0.910916<br/>b    -0.537585  0.525384<br/></p>
<p>In [22]: df.groupby(['key1', 'key2']).mean()<br/></p>
<p>290 | Chapter 10: Data Aggregation and Group Operations</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Out[22]: <br/>              data1     data2<br/>key1 key2                    <br/>a    one   0.880536  1.319920<br/>     two   0.478943  0.092908<br/>b    one  -0.519439  0.281746<br/>     two  -0.555730  0.769023<br/></p>
<p>You may have noticed in the first case df.groupby('key1').mean() that there is no<br/>key2 column in the result. Because df['key2'] is not numeric data, it is said to be a<br/><i>nuisance column</i>, which is therefore excluded from the result. By default, all of the<br/>numeric columns are aggregated, though it is possible to filter down to a subset, as<br/>you&#8217;ll see soon.<br/>Regardless of the objective in using groupby, a generally useful GroupBy method is<br/>size, which returns a Series containing group sizes:<br/></p>
<p>In [23]: df.groupby(['key1', 'key2']).size()<br/>Out[23]: <br/>key1  key2<br/>a     one     2<br/>      two     1<br/>b     one     1<br/>      two     1<br/>dtype: int64<br/></p>
<p>Take note that any missing values in a group key will be excluded from the result.<br/></p>
<p>Iterating Over Groups<br/>The GroupBy object supports iteration, generating a sequence of 2-tuples containing<br/>the group name along with the chunk of data. Consider the following:<br/></p>
<p>In [24]: <b>for</b> name, group <b>in</b> df.groupby('key1'):<br/>   ....:     <b>print</b>(name)<br/>   ....:     <b>print</b>(group)<br/>   ....:<br/>a<br/>      data1     data2 key1 key2<br/>0 -0.204708  1.393406    a  one<br/>1  0.478943  0.092908    a  two<br/>4  1.965781  1.246435    a  one<br/>b<br/>      data1     data2 key1 key2<br/>2 -0.519439  0.281746    b  one<br/>3 -0.555730  0.769023    b  two<br/></p>
<p>In the case of multiple keys, the first element in the tuple will be a tuple of key values:<br/>In [25]: <b>for</b> (k1, k2), group <b>in</b> df.groupby(['key1', 'key2']):<br/>   ....:     <b>print</b>((k1, k2))<br/>   ....:     <b>print</b>(group)<br/></p>
<p>10.1 GroupBy Mechanics | 291</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>   ....:<br/>('a', 'one')<br/>      data1     data2 key1 key2<br/>0 -0.204708  1.393406    a  one<br/>4  1.965781  1.246435    a  one<br/>('a', 'two')<br/>      data1     data2 key1 key2<br/>1  0.478943  0.092908    a  two<br/>('b', 'one')<br/>      data1     data2 key1 key2<br/>2 -0.519439  0.281746    b  one<br/>('b', 'two')<br/>     data1     data2 key1 key2<br/>3 -0.55573  0.769023    b  two<br/></p>
<p>Of course, you can choose to do whatever you want with the pieces of data. A recipe<br/>you may find useful is computing a dict of the data pieces as a one-liner:<br/></p>
<p>In [26]: pieces = dict(list(df.groupby('key1')))<br/></p>
<p>In [27]: pieces['b']<br/>Out[27]: <br/>      data1     data2 key1 key2<br/>2 -0.519439  0.281746    b  one<br/>3 -0.555730  0.769023    b  two<br/></p>
<p>By default groupby groups on axis=0, but you can group on any of the other axes.<br/>For example, we could group the columns of our example df here by dtype like so:<br/></p>
<p>In [28]: df.dtypes<br/>Out[28]: <br/>data1    float64<br/>data2    float64<br/>key1      object<br/>key2      object<br/>dtype: object<br/></p>
<p>In [29]: grouped = df.groupby(df.dtypes, axis=1)<br/></p>
<p>We can print out the groups like so:<br/>In [30]: <b>for</b> dtype, group <b>in</b> grouped:<br/>   ....:     <b>print</b>(dtype)<br/>   ....:     <b>print</b>(group)<br/>   ....:<br/>float64<br/>      data1     data2<br/>0 -0.204708  1.393406<br/>1  0.478943  0.092908<br/>2 -0.519439  0.281746<br/>3 -0.555730  0.769023<br/>4  1.965781  1.246435<br/>object<br/>  key1 key2<br/></p>
<p>292 | Chapter 10: Data Aggregation and Group Operations</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>0    a  one<br/>1    a  two<br/>2    b  one<br/>3    b  two<br/>4    a  one<br/></p>
<p>Selecting a Column or Subset of Columns<br/>Indexing a GroupBy object created from a DataFrame with a column name or array<br/>of column names has the effect of column subsetting for aggregation. This means<br/>that:<br/></p>
<p>df.groupby('key1')['data1']<br/>df.groupby('key1')[['data2']]<br/></p>
<p>are syntactic sugar for:<br/>df['data1'].groupby(df['key1'])<br/>df[['data2']].groupby(df['key1'])<br/></p>
<p>Especially for large datasets, it may be desirable to aggregate only a few columns. For<br/>example, in the preceding dataset, to compute means for just the data2 column and<br/>get the result as a DataFrame, we could write:<br/></p>
<p>In [31]: df.groupby(['key1', 'key2'])[['data2']].mean()<br/>Out[31]: <br/>              data2<br/>key1 key2          <br/>a    one   1.319920<br/>     two   0.092908<br/>b    one   0.281746<br/>     two   0.769023<br/></p>
<p>The object returned by this indexing operation is a grouped DataFrame if a list or<br/>array is passed or a grouped Series if only a single column name is passed as a scalar:<br/></p>
<p>In [32]: s_grouped = df.groupby(['key1', 'key2'])['data2']<br/></p>
<p>In [33]: s_grouped<br/>Out[33]: &lt;pandas.core.groupby.SeriesGroupBy object at 0x7faa30c78da0&gt;<br/></p>
<p>In [34]: s_grouped.mean()<br/>Out[34]: <br/>key1  key2<br/>a     one     1.319920<br/>      two     0.092908<br/>b     one     0.281746<br/>      two     0.769023<br/>Name: data2, dtype: float64<br/></p>
<p>10.1 GroupBy Mechanics | 293</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Grouping with Dicts and Series<br/>Grouping information may exist in a form other than an array. Let&#8217;s consider another<br/>example DataFrame:<br/></p>
<p>In [35]: people = pd.DataFrame(np.random.randn(5, 5),<br/>   ....:                       columns=['a', 'b', 'c', 'd', 'e'],<br/>   ....:                       index=['Joe', 'Steve', 'Wes', 'Jim', 'Travis'])<br/></p>
<p>In [36]: people.iloc[2:3, [1, 2]] = np.nan <i># Add a few NA values<br/></i></p>
<p>In [37]: people<br/>Out[37]: <br/>               a         b         c         d         e<br/>Joe     1.007189 -1.296221  0.274992  0.228913  1.352917<br/>Steve   0.886429 -2.001637 -0.371843  1.669025 -0.438570<br/>Wes    -0.539741       NaN       NaN -1.021228 -0.577087<br/>Jim     0.124121  0.302614  0.523772  0.000940  1.343810<br/>Travis -0.713544 -0.831154 -2.370232 -1.860761 -0.860757<br/></p>
<p>Now, suppose I have a group correspondence for the columns and want to sum<br/>together the columns by group:<br/></p>
<p>In [38]: mapping = {'a': 'red', 'b': 'red', 'c': 'blue',<br/>   ....:            'd': 'blue', 'e': 'red', 'f' : 'orange'}<br/></p>
<p>Now, you could construct an array from this dict to pass to groupby, but instead we<br/>can just pass the dict (I included the key 'f' to highlight that unused grouping keys<br/>are OK):<br/></p>
<p>In [39]: by_column = people.groupby(mapping, axis=1)<br/></p>
<p>In [40]: by_column.sum()<br/>Out[40]: <br/>            blue       red<br/>Joe     0.503905  1.063885<br/>Steve   1.297183 -1.553778<br/>Wes    -1.021228 -1.116829<br/>Jim     0.524712  1.770545<br/>Travis -4.230992 -2.405455<br/></p>
<p>The same functionality holds for Series, which can be viewed as a fixed-size mapping:<br/>In [41]: map_series = pd.Series(mapping)<br/></p>
<p>In [42]: map_series<br/>Out[42]: <br/>a       red<br/>b       red<br/>c      blue<br/>d      blue<br/>e       red<br/>f    orange<br/></p>
<p>294 | Chapter 10: Data Aggregation and Group Operations</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>dtype: object<br/></p>
<p>In [43]: people.groupby(map_series, axis=1).count()<br/>Out[43]: <br/>        blue  red<br/>Joe        2    3<br/>Steve      2    3<br/>Wes        1    2<br/>Jim        2    3<br/>Travis     2    3<br/></p>
<p>Grouping with Functions<br/>Using Python functions is a more generic way of defining a group mapping compared<br/>with a dict or Series. Any function passed as a group key will be called once per index<br/>value, with the return values being used as the group names. More concretely, con&#8208;<br/>sider the example DataFrame from the previous section, which has people&#8217;s first<br/>names as index values. Suppose you wanted to group by the length of the names;<br/>while you could compute an array of string lengths, it&#8217;s simpler to just pass the len<br/>function:<br/></p>
<p>In [44]: people.groupby(len).sum()<br/>Out[44]: <br/>          a         b         c         d         e<br/>3  0.591569 -0.993608  0.798764 -0.791374  2.119639<br/>5  0.886429 -2.001637 -0.371843  1.669025 -0.438570<br/>6 -0.713544 -0.831154 -2.370232 -1.860761 -0.860757<br/></p>
<p>Mixing functions with arrays, dicts, or Series is not a problem as everything gets con&#8208;<br/>verted to arrays internally:<br/></p>
<p>In [45]: key_list = ['one', 'one', 'one', 'two', 'two']<br/></p>
<p>In [46]: people.groupby([len, key_list]).min()<br/>Out[46]: <br/>              a         b         c         d         e<br/>3 one -0.539741 -1.296221  0.274992 -1.021228 -0.577087<br/>  two  0.124121  0.302614  0.523772  0.000940  1.343810<br/>5 one  0.886429 -2.001637 -0.371843  1.669025 -0.438570<br/>6 two -0.713544 -0.831154 -2.370232 -1.860761 -0.860757<br/></p>
<p>Grouping by Index Levels<br/>A final convenience for hierarchically indexed datasets is the ability to aggregate<br/>using one of the levels of an axis index. Let&#8217;s look at an example:<br/></p>
<p>In [47]: columns = pd.MultiIndex.from_arrays([['US', 'US', 'US', 'JP', 'JP'],<br/>   ....:                                     [1, 3, 5, 1, 3]],<br/>   ....:                                     names=['cty', 'tenor'])<br/></p>
<p>In [48]: hier_df = pd.DataFrame(np.random.randn(4, 5), columns=columns)<br/></p>
<p>10.1 GroupBy Mechanics | 295</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In [49]: hier_df<br/>Out[49]: <br/>cty          US                            JP          <br/>tenor         1         3         5         1         3<br/>0      0.560145 -1.265934  0.119827 -1.063512  0.332883<br/>1     -2.359419 -0.199543 -1.541996 -0.970736 -1.307030<br/>2      0.286350  0.377984 -0.753887  0.331286  1.349742<br/>3      0.069877  0.246674 -0.011862  1.004812  1.327195<br/></p>
<p>To group by level, pass the level number or name using the level keyword:<br/>In [50]: hier_df.groupby(level='cty', axis=1).count()<br/>Out[50]: <br/>cty  JP  US<br/>0     2   3<br/>1     2   3<br/>2     2   3<br/>3     2   3<br/></p>
<p>10.2 Data Aggregation<br/>Aggregations refer to any data transformation that produces scalar values from<br/>arrays. The preceding examples have used several of them, including mean, count,<br/>min, and sum. You may wonder what is going on when you invoke mean() on a<br/>GroupBy object. Many common aggregations, such as those found in Table 10-1,<br/>have optimized implementations. However, you are not limited to only this set of<br/>methods.<br/><i>Table 10-1. Optimized groupby methods<br/></i></p>
<p>Function name Description<br/>count Number of non-NA values in the group<br/>sum Sum of non-NA values<br/>mean Mean of non-NA values<br/>median Arithmetic median of non-NA values<br/>std, var Unbiased (n &#8211; 1 denominator) standard deviation and variance<br/>min, max Minimum and maximum of non-NA values<br/>prod Product of non-NA values<br/>first, last First and last non-NA values<br/></p>
<p>You can use aggregations of your own devising and additionally call any method that<br/>is also defined on the grouped object. For example, you might recall that quantile<br/>computes sample quantiles of a Series or a DataFrame&#8217;s columns.<br/>While quantile is not explicitly implemented for GroupBy, it is a Series method and<br/>thus available for use. Internally, GroupBy efficiently slices up the Series, calls<br/></p>
<p>296 | Chapter 10: Data Aggregation and Group Operations</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>piece.quantile(0.9) for each piece, and then assembles those results together into<br/>the result object:<br/></p>
<p>In [51]: df<br/>Out[51]: <br/>      data1     data2 key1 key2<br/>0 -0.204708  1.393406    a  one<br/>1  0.478943  0.092908    a  two<br/>2 -0.519439  0.281746    b  one<br/>3 -0.555730  0.769023    b  two<br/>4  1.965781  1.246435    a  one<br/></p>
<p>In [52]: grouped = df.groupby('key1')<br/></p>
<p>In [53]: grouped['data1'].quantile(0.9)<br/>Out[53]: <br/>key1<br/>a    1.668413<br/>b   -0.523068<br/>Name: data1, dtype: float64<br/></p>
<p>To use your own aggregation functions, pass any function that aggregates an array to<br/>the aggregate or agg method:<br/></p>
<p>In [54]: <b>def</b> peak_to_peak(arr):<br/>   ....:     <b>return</b> arr.max() - arr.min()<br/></p>
<p>In [55]: grouped.agg(peak_to_peak)<br/>Out[55]: <br/>         data1     data2<br/>key1                    <br/>a     2.170488  1.300498<br/>b     0.036292  0.487276<br/></p>
<p>You may notice that some methods like describe also work, even though they are not<br/>aggregations, strictly speaking:<br/></p>
<p>In [56]: grouped.describe()<br/>Out[56]: <br/>     data1                                                              \<br/>     count      mean       std       min       25%       50%       75%   <br/>key1                                                                     <br/>a      3.0  0.746672  1.109736 -0.204708  0.137118  0.478943  1.222362   <br/>b      2.0 -0.537585  0.025662 -0.555730 -0.546657 -0.537585 -0.528512   <br/>               data2                                                    \<br/>           max count      mean       std       min       25%       50%   <br/>key1                                                                     <br/>a     1.965781   3.0  0.910916  0.712217  0.092908  0.669671  1.246435   <br/>b    -0.519439   2.0  0.525384  0.344556  0.281746  0.403565  0.525384   <br/>                          <br/>           75%       max  <br/>key1                      <br/></p>
<p>10.2 Data Aggregation | 297</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>a     1.319920  1.393406  <br/>b     0.647203  0.769023  <br/></p>
<p>I will explain in more detail what has happened here in Section 10.3, &#8220;Apply: General<br/>split-apply-combine,&#8221; on page 302.<br/></p>
<p>Custom aggregation functions are generally much slower than the<br/>optimized functions found in Table 10-1. This is because there is<br/>some extra overhead (function calls, data rearrangement) in con&#8208;<br/>structing the intermediate group data chunks.<br/></p>
<p>Column-Wise and Multiple Function Application<br/>Let&#8217;s return to the tipping dataset from earlier examples. After loading it with<br/>read_csv, we add a tipping percentage column tip_pct:<br/></p>
<p>In [57]: tips = pd.read_csv('examples/tips.csv')<br/></p>
<p><i># Add tip percentage of total bill<br/></i>In [58]: tips['tip_pct'] = tips['tip'] / tips['total_bill']<br/></p>
<p>In [59]: tips[:6]<br/>Out[59]: <br/>   total_bill   tip smoker  day    time  size   tip_pct<br/>0       16.99  1.01     No  Sun  Dinner     2  0.059447<br/>1       10.34  1.66     No  Sun  Dinner     3  0.160542<br/>2       21.01  3.50     No  Sun  Dinner     3  0.166587<br/>3       23.68  3.31     No  Sun  Dinner     2  0.139780<br/>4       24.59  3.61     No  Sun  Dinner     4  0.146808<br/>5       25.29  4.71     No  Sun  Dinner     4  0.186240<br/></p>
<p>As you&#8217;ve already seen, aggregating a Series or all of the columns of a DataFrame is a<br/>matter of using aggregate with the desired function or calling a method like mean or<br/>std. However, you may want to aggregate using a different function depending on the<br/>column, or multiple functions at once. Fortunately, this is possible to do, which I&#8217;ll<br/>illustrate through a number of examples. First, I&#8217;ll group the tips by day and smoker:<br/></p>
<p>In [60]: grouped = tips.groupby(['day', 'smoker'])<br/></p>
<p>Note that for descriptive statistics like those in Table 10-1, you can pass the name of<br/>the function as a string:<br/></p>
<p>In [61]: grouped_pct = grouped['tip_pct']<br/></p>
<p>In [62]: grouped_pct.agg('mean')<br/>Out[62]: <br/>day   smoker<br/>Fri   No        0.151650<br/>      Yes       0.174783<br/>Sat   No        0.158048<br/></p>
<p>298 | Chapter 10: Data Aggregation and Group Operations</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>      Yes       0.147906<br/>Sun   No        0.160113<br/>      Yes       0.187250<br/>Thur  No        0.160298<br/>      Yes       0.163863<br/>Name: tip_pct, dtype: float64<br/></p>
<p>If you pass a list of functions or function names instead, you get back a DataFrame<br/>with column names taken from the functions:<br/></p>
<p>In [63]: grouped_pct.agg(['mean', 'std', peak_to_peak])<br/>Out[63]: <br/>                 mean       std  peak_to_peak<br/>day  smoker                                  <br/>Fri  No      0.151650  0.028123      0.067349<br/>     Yes     0.174783  0.051293      0.159925<br/>Sat  No      0.158048  0.039767      0.235193<br/>     Yes     0.147906  0.061375      0.290095<br/>Sun  No      0.160113  0.042347      0.193226<br/>     Yes     0.187250  0.154134      0.644685<br/>Thur No      0.160298  0.038774      0.193350<br/>     Yes     0.163863  0.039389      0.151240<br/></p>
<p>Here we passed a list of aggregation functions to agg to evaluate indepedently on the<br/>data groups.<br/>You don&#8217;t need to accept the names that GroupBy gives to the columns; notably,<br/>lambda functions have the name '&lt;lambda&gt;', which makes them hard to identify<br/>(you can see for yourself by looking at a function&#8217;s __name__ attribute). Thus, if you<br/>pass a list of (name, function) tuples, the first element of each tuple will be used as<br/>the DataFrame column names (you can think of a list of 2-tuples as an ordered<br/>mapping):<br/></p>
<p>In [64]: grouped_pct.agg([('foo', 'mean'), ('bar', np.std)])<br/>Out[64]: <br/>                  foo       bar<br/>day  smoker                    <br/>Fri  No      0.151650  0.028123<br/>     Yes     0.174783  0.051293<br/>Sat  No      0.158048  0.039767<br/>     Yes     0.147906  0.061375<br/>Sun  No      0.160113  0.042347<br/>     Yes     0.187250  0.154134<br/>Thur No      0.160298  0.038774<br/>     Yes     0.163863  0.039389<br/></p>
<p>With a DataFrame you have more options, as you can specify a list of functions to<br/>apply to all of the columns or different functions per column. To start, suppose we<br/>wanted to compute the same three statistics for the tip_pct and total_bill<br/>columns:<br/></p>
<p>10.2 Data Aggregation | 299</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In [65]: functions = ['count', 'mean', 'max']<br/></p>
<p>In [66]: result = grouped['tip_pct', 'total_bill'].agg(functions)<br/></p>
<p>In [67]: result<br/>Out[67]: <br/>            tip_pct                     total_bill                  <br/>              count      mean       max      count       mean    max<br/>day  smoker                                                         <br/>Fri  No           4  0.151650  0.187735          4  18.420000  22.75<br/>     Yes         15  0.174783  0.263480         15  16.813333  40.17<br/>Sat  No          45  0.158048  0.291990         45  19.661778  48.33<br/>     Yes         42  0.147906  0.325733         42  21.276667  50.81<br/>Sun  No          57  0.160113  0.252672         57  20.506667  48.17<br/>     Yes         19  0.187250  0.710345         19  24.120000  45.35<br/>Thur No          45  0.160298  0.266312         45  17.113111  41.19<br/>     Yes         17  0.163863  0.241255         17  19.190588  43.11<br/></p>
<p>As you can see, the resulting DataFrame has hierarchical columns, the same as you<br/>would get aggregating each column separately and using concat to glue the results<br/>together using the column names as the keys argument:<br/></p>
<p>In [68]: result['tip_pct']<br/>Out[68]: <br/>             count      mean       max<br/>day  smoker                           <br/>Fri  No          4  0.151650  0.187735<br/>     Yes        15  0.174783  0.263480<br/>Sat  No         45  0.158048  0.291990<br/>     Yes        42  0.147906  0.325733<br/>Sun  No         57  0.160113  0.252672<br/>     Yes        19  0.187250  0.710345<br/>Thur No         45  0.160298  0.266312<br/>     Yes        17  0.163863  0.241255<br/></p>
<p>As before, a list of tuples with custom names can be passed:<br/>In [69]: ftuples = [('Durchschnitt', 'mean'), ('Abweichung', np.var)]<br/></p>
<p>In [70]: grouped['tip_pct', 'total_bill'].agg(ftuples)<br/>Out[70]: <br/>                 tip_pct              total_bill            <br/>            Durchschnitt Abweichung Durchschnitt  Abweichung<br/>day  smoker                                                 <br/>Fri  No         0.151650   0.000791    18.420000   25.596333<br/>     Yes        0.174783   0.002631    16.813333   82.562438<br/>Sat  No         0.158048   0.001581    19.661778   79.908965<br/>     Yes        0.147906   0.003767    21.276667  101.387535<br/>Sun  No         0.160113   0.001793    20.506667   66.099980<br/>     Yes        0.187250   0.023757    24.120000  109.046044<br/>Thur No         0.160298   0.001503    17.113111   59.625081<br/>     Yes        0.163863   0.001551    19.190588   69.808518<br/></p>
<p>300 | Chapter 10: Data Aggregation and Group Operations</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Now, suppose you wanted to apply potentially different functions to one or more of<br/>the columns. To do this, pass a dict to agg that contains a mapping of column names<br/>to any of the function specifications listed so far:<br/></p>
<p>In [71]: grouped.agg({'tip' : np.max, 'size' : 'sum'})<br/>Out[71]: <br/>               tip  size<br/>day  smoker             <br/>Fri  No       3.50     9<br/>     Yes      4.73    31<br/>Sat  No       9.00   115<br/>     Yes     10.00   104<br/>Sun  No       6.00   167<br/>     Yes      6.50    49<br/>Thur No       6.70   112<br/>     Yes      5.00    40<br/></p>
<p>In [72]: grouped.agg({'tip_pct' : ['min', 'max', 'mean', 'std'],<br/>   ....:              'size' : 'sum'})<br/>Out[72]: <br/>              tip_pct                               size<br/>                  min       max      mean       std  sum<br/>day  smoker                                             <br/>Fri  No      0.120385  0.187735  0.151650  0.028123    9<br/>     Yes     0.103555  0.263480  0.174783  0.051293   31<br/>Sat  No      0.056797  0.291990  0.158048  0.039767  115<br/>     Yes     0.035638  0.325733  0.147906  0.061375  104<br/>Sun  No      0.059447  0.252672  0.160113  0.042347  167<br/>     Yes     0.065660  0.710345  0.187250  0.154134   49<br/>Thur No      0.072961  0.266312  0.160298  0.038774  112<br/>     Yes     0.090014  0.241255  0.163863  0.039389   40<br/></p>
<p>A DataFrame will have hierarchical columns only if multiple functions are applied to<br/>at least one column.<br/></p>
<p>Returning Aggregated Data Without Row Indexes<br/>In all of the examples up until now, the aggregated data comes back with an index,<br/>potentially hierarchical, composed from the unique group key combinations. Since<br/>this isn&#8217;t always desirable, you can disable this behavior in most cases by passing<br/>as_index=False to groupby:<br/></p>
<p>In [73]: tips.groupby(['day', 'smoker'], as_index=False).mean()<br/>Out[73]: <br/>    day smoker  total_bill       tip      size   tip_pct<br/>0   Fri     No   18.420000  2.812500  2.250000  0.151650<br/>1   Fri    Yes   16.813333  2.714000  2.066667  0.174783<br/>2   Sat     No   19.661778  3.102889  2.555556  0.158048<br/>3   Sat    Yes   21.276667  2.875476  2.476190  0.147906<br/>4   Sun     No   20.506667  3.167895  2.929825  0.160113<br/>5   Sun    Yes   24.120000  3.516842  2.578947  0.187250<br/></p>
<p>10.2 Data Aggregation | 301</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>6  Thur     No   17.113111  2.673778  2.488889  0.160298<br/>7  Thur    Yes   19.190588  3.030000  2.352941  0.163863<br/></p>
<p>Of course, it&#8217;s always possible to obtain the result in this format by calling<br/>reset_index on the result. Using the as_index=False method avoids some unneces&#8208;<br/>sary computations.<br/></p>
<p>10.3 Apply: General split-apply-combine<br/>The most general-purpose GroupBy method is apply, which is the subject of the rest<br/>of this section. As illustrated in Figure 10-2, apply splits the object being manipulated<br/>into pieces, invokes the passed function on each piece, and then attempts to concate&#8208;<br/>nate the pieces together.<br/></p>
<p><i>Figure 10-2. Illustration of a group aggregation<br/></i>Returning to the tipping dataset from before, suppose you wanted to select the top<br/>five tip_pct values by group. First, write a function that selects the rows with the<br/>largest values in a particular column:<br/></p>
<p>In [74]: <b>def</b> top(df, n=5, column='tip_pct'):<br/>   ....:     <b>return</b> df.sort_values(by=column)[-n:]<br/></p>
<p>In [75]: top(tips, n=6)<br/>Out[75]: <br/>     total_bill   tip smoker  day    time  size   tip_pct<br/>109       14.31  4.00    Yes  Sat  Dinner     2  0.279525<br/>183       23.17  6.50    Yes  Sun  Dinner     4  0.280535<br/>232       11.61  3.39     No  Sat  Dinner     2  0.291990<br/></p>
<p>302 | Chapter 10: Data Aggregation and Group Operations</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>67         3.07  1.00    Yes  Sat  Dinner     1  0.325733<br/>178        9.60  4.00    Yes  Sun  Dinner     2  0.416667<br/>172        7.25  5.15    Yes  Sun  Dinner     2  0.710345<br/></p>
<p>Now, if we group by smoker, say, and call apply with this function, we get the<br/>following:<br/></p>
<p>In [76]: tips.groupby('smoker').apply(top)<br/>Out[76]: <br/>            total_bill   tip smoker   day    time  size   tip_pct<br/>smoker                                                           <br/>No     88        24.71  5.85     No  Thur   Lunch     2  0.236746<br/>       185       20.69  5.00     No   Sun  Dinner     5  0.241663<br/>       51        10.29  2.60     No   Sun  Dinner     2  0.252672<br/>       149        7.51  2.00     No  Thur   Lunch     2  0.266312<br/>       232       11.61  3.39     No   Sat  Dinner     2  0.291990<br/>Yes    109       14.31  4.00    Yes   Sat  Dinner     2  0.279525<br/>       183       23.17  6.50    Yes   Sun  Dinner     4  0.280535<br/>       67         3.07  1.00    Yes   Sat  Dinner     1  0.325733<br/>       178        9.60  4.00    Yes   Sun  Dinner     2  0.416667<br/>       172        7.25  5.15    Yes   Sun  Dinner     2  0.710345<br/></p>
<p>What has happened here? The top function is called on each row group from the<br/>DataFrame, and then the results are glued together using pandas.concat, labeling the<br/>pieces with the group names. The result therefore has a hierarchical index whose<br/>inner level contains index values from the original DataFrame.<br/>If you pass a function to apply that takes other arguments or keywords, you can pass<br/>these after the function:<br/></p>
<p>In [77]: tips.groupby(['smoker', 'day']).apply(top, n=1, column='total_bill')<br/>Out[77]: <br/>                 total_bill    tip smoker   day    time  size   tip_pct<br/>smoker day                                                             <br/>No     Fri  94        22.75   3.25     No   Fri  Dinner     2  0.142857<br/>       Sat  212       48.33   9.00     No   Sat  Dinner     4  0.186220<br/>       Sun  156       48.17   5.00     No   Sun  Dinner     6  0.103799<br/>       Thur 142       41.19   5.00     No  Thur   Lunch     5  0.121389<br/>Yes    Fri  95        40.17   4.73    Yes   Fri  Dinner     4  0.117750<br/>       Sat  170       50.81  10.00    Yes   Sat  Dinner     3  0.196812<br/>       Sun  182       45.35   3.50    Yes   Sun  Dinner     3  0.077178<br/>       Thur 197       43.11   5.00    Yes  Thur   Lunch     4  0.115982<br/></p>
<p>Beyond these basic usage mechanics, getting the most out of apply<br/>may require some creativity. What occurs inside the function<br/>passed is up to you; it only needs to return a pandas object or a<br/>scalar value. The rest of this chapter will mainly consist of examples<br/>showing you how to solve various problems using groupby.<br/></p>
<p>10.3 Apply: General split-apply-combine | 303</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>You may recall that I earlier called describe on a GroupBy object:<br/>In [78]: result = tips.groupby('smoker')['tip_pct'].describe()<br/></p>
<p>In [79]: result<br/>Out[79]: <br/>        count      mean       std       min       25%       50%       75%  \<br/>smoker                                                                      <br/>No      151.0  0.159328  0.039910  0.056797  0.136906  0.155625  0.185014   <br/>Yes      93.0  0.163196  0.085119  0.035638  0.106771  0.153846  0.195059   <br/>             max  <br/>smoker            <br/>No      0.291990  <br/>Yes     0.710345  <br/></p>
<p>In [80]: result.unstack('smoker')<br/>Out[80]: <br/>       smoker<br/>count  No        151.000000<br/>       Yes        93.000000<br/>mean   No          0.159328<br/>       Yes         0.163196<br/>std    No          0.039910<br/>       Yes         0.085119<br/>min    No          0.056797<br/>       Yes         0.035638<br/>25%    No          0.136906<br/>       Yes         0.106771<br/>50%    No          0.155625<br/>       Yes         0.153846<br/>75%    No          0.185014<br/>       Yes         0.195059<br/>max    No          0.291990<br/>       Yes         0.710345<br/>dtype: float64<br/></p>
<p>Inside GroupBy, when you invoke a method like describe, it is actually just a short&#8208;<br/>cut for:<br/></p>
<p>f = <b>lambda</b> x: x.describe()<br/>grouped.apply(f)<br/></p>
<p>Suppressing the Group Keys<br/>In the preceding examples, you see that the resulting object has a hierarchical index<br/>formed from the group keys along with the indexes of each piece of the original<br/>object. You can disable this by passing group_keys=False to groupby:<br/></p>
<p>304 | Chapter 10: Data Aggregation and Group Operations</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In [81]: tips.groupby('smoker', group_keys=False).apply(top)<br/>Out[81]: <br/>     total_bill   tip smoker   day    time  size   tip_pct<br/>88        24.71  5.85     No  Thur   Lunch     2  0.236746<br/>185       20.69  5.00     No   Sun  Dinner     5  0.241663<br/>51        10.29  2.60     No   Sun  Dinner     2  0.252672<br/>149        7.51  2.00     No  Thur   Lunch     2  0.266312<br/>232       11.61  3.39     No   Sat  Dinner     2  0.291990<br/>109       14.31  4.00    Yes   Sat  Dinner     2  0.279525<br/>183       23.17  6.50    Yes   Sun  Dinner     4  0.280535<br/>67         3.07  1.00    Yes   Sat  Dinner     1  0.325733<br/>178        9.60  4.00    Yes   Sun  Dinner     2  0.416667<br/>172        7.25  5.15    Yes   Sun  Dinner     2  0.710345<br/></p>
<p>Quantile and Bucket Analysis<br/>As you may recall from Chapter 8, pandas has some tools, in particular cut and qcut,<br/>for slicing data up into buckets with bins of your choosing or by sample quantiles.<br/>Combining these functions with groupby makes it convenient to perform bucket or<br/>quantile analysis on a dataset. Consider a simple random dataset and an equal-length<br/>bucket categorization using cut:<br/></p>
<p>In [82]: frame = pd.DataFrame({'data1': np.random.randn(1000),<br/>   ....:                       'data2': np.random.randn(1000)})<br/></p>
<p>In [83]: quartiles = pd.cut(frame.data1, 4)<br/></p>
<p>In [84]: quartiles[:10]<br/>Out[84]: <br/>0     (-1.23, 0.489]<br/>1    (-2.956, -1.23]<br/>2     (-1.23, 0.489]<br/>3     (0.489, 2.208]<br/>4     (-1.23, 0.489]<br/>5     (0.489, 2.208]<br/>6     (-1.23, 0.489]<br/>7     (-1.23, 0.489]<br/>8     (0.489, 2.208]<br/>9     (0.489, 2.208]<br/>Name: data1, dtype: category<br/>Categories (4, interval[float64]): [(-2.956, -1.23] &lt; (-1.23, 0.489] &lt; (0.489, 2.<br/>208] &lt; (2.208, 3.928]]<br/></p>
<p>The Categorical object returned by cut can be passed directly to groupby. So we<br/>could compute a set of statistics for the data2 column like so:<br/></p>
<p>In [85]: <b>def</b> get_stats(group):<br/>   ....:     <b>return</b> {'min': group.min(), 'max': group.max(),<br/>   ....:             'count': group.count(), 'mean': group.mean()}<br/></p>
<p>In [86]: grouped = frame.data2.groupby(quartiles)<br/></p>
<p>10.3 Apply: General split-apply-combine | 305</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In [87]: grouped.apply(get_stats).unstack()<br/>Out[87]: <br/>                 count       max      mean       min<br/>data1                                               <br/>(-2.956, -1.23]   95.0  1.670835 -0.039521 -3.399312<br/>(-1.23, 0.489]   598.0  3.260383 -0.002051 -2.989741<br/>(0.489, 2.208]   297.0  2.954439  0.081822 -3.745356<br/>(2.208, 3.928]    10.0  1.765640  0.024750 -1.929776<br/></p>
<p>These were equal-length buckets; to compute equal-size buckets based on sample<br/>quantiles, use qcut. I&#8217;ll pass labels=False to just get quantile numbers:<br/></p>
<p><i># Return quantile numbers<br/></i>In [88]: grouping = pd.qcut(frame.data1, 10, labels=False)<br/></p>
<p>In [89]: grouped = frame.data2.groupby(grouping)<br/></p>
<p>In [90]: grouped.apply(get_stats).unstack()<br/>Out[90]: <br/>       count       max      mean       min<br/>data1                                     <br/>0      100.0  1.670835 -0.049902 -3.399312<br/>1      100.0  2.628441  0.030989 -1.950098<br/>2      100.0  2.527939 -0.067179 -2.925113<br/>3      100.0  3.260383  0.065713 -2.315555<br/>4      100.0  2.074345 -0.111653 -2.047939<br/>5      100.0  2.184810  0.052130 -2.989741<br/>6      100.0  2.458842 -0.021489 -2.223506<br/>7      100.0  2.954439 -0.026459 -3.056990<br/>8      100.0  2.735527  0.103406 -3.745356<br/>9      100.0  2.377020  0.220122 -2.064111<br/></p>
<p>We will take a closer look at pandas&#8217;s Categorical type in Chapter 12.<br/></p>
<p>Example: Filling Missing Values with Group-Specific Values<br/>When cleaning up missing data, in some cases you will replace data observations<br/>using dropna, but in others you may want to impute (fill in) the null (NA) values<br/>using a fixed value or some value derived from the data. fillna is the right tool to<br/>use; for example, here I fill in NA values with the mean:<br/></p>
<p>In [91]: s = pd.Series(np.random.randn(6))<br/></p>
<p>In [92]: s[::2] = np.nan<br/></p>
<p>In [93]: s<br/>Out[93]: <br/>0         NaN<br/>1   -0.125921<br/>2         NaN<br/>3   -0.884475<br/></p>
<p>306 | Chapter 10: Data Aggregation and Group Operations</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>4         NaN<br/>5    0.227290<br/>dtype: float64<br/></p>
<p>In [94]: s.fillna(s.mean())<br/>Out[94]: <br/>0   -0.261035<br/>1   -0.125921<br/>2   -0.261035<br/>3   -0.884475<br/>4   -0.261035<br/>5    0.227290<br/>dtype: float64<br/></p>
<p>Suppose you need the fill value to vary by group. One way to do this is to group the<br/>data and use apply with a function that calls fillna on each data chunk. Here is<br/>some sample data on US states divided into eastern and western regions:<br/></p>
<p>In [95]: states = ['Ohio', 'New York', 'Vermont', 'Florida',<br/>   ....:           'Oregon', 'Nevada', 'California', 'Idaho']<br/></p>
<p>In [96]: group_key = ['East'] * 4 + ['West'] * 4<br/></p>
<p>In [97]: data = pd.Series(np.random.randn(8), index=states)<br/></p>
<p>In [98]: data<br/>Out[98]: <br/>Ohio          0.922264<br/>New York     -2.153545<br/>Vermont      -0.365757<br/>Florida      -0.375842<br/>Oregon        0.329939<br/>Nevada        0.981994<br/>California    1.105913<br/>Idaho        -1.613716<br/>dtype: float64<br/></p>
<p>Note that the syntax ['East'] * 4 produces a list containing four copies of the ele&#8208;<br/>ments in ['East']. Adding lists together concatenates them.<br/>Let&#8217;s set some values in the data to be missing:<br/></p>
<p>In [99]: data[['Vermont', 'Nevada', 'Idaho']] = np.nan<br/></p>
<p>In [100]: data<br/>Out[100]: <br/>Ohio          0.922264<br/>New York     -2.153545<br/>Vermont            NaN<br/>Florida      -0.375842<br/>Oregon        0.329939<br/>Nevada             NaN<br/>California    1.105913<br/></p>
<p>10.3 Apply: General split-apply-combine | 307</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Idaho              NaN<br/>dtype: float64<br/></p>
<p>In [101]: data.groupby(group_key).mean()<br/>Out[101]: <br/>East   -0.535707<br/>West    0.717926<br/>dtype: float64<br/></p>
<p>We can fill the NA values using the group means like so:<br/>In [102]: fill_mean = <b>lambda</b> g: g.fillna(g.mean())<br/></p>
<p>In [103]: data.groupby(group_key).apply(fill_mean)<br/>Out[103]: <br/>Ohio          0.922264<br/>New York     -2.153545<br/>Vermont      -0.535707<br/>Florida      -0.375842<br/>Oregon        0.329939<br/>Nevada        0.717926<br/>California    1.105913<br/>Idaho         0.717926<br/>dtype: float64<br/></p>
<p>In another case, you might have predefined fill values in your code that vary by<br/>group. Since the groups have a name attribute set internally, we can use that:<br/></p>
<p>In [104]: fill_values = {'East': 0.5, 'West': -1}<br/></p>
<p>In [105]: fill_func = <b>lambda</b> g: g.fillna(fill_values[g.name])<br/></p>
<p>In [106]: data.groupby(group_key).apply(fill_func)<br/>Out[106]: <br/>Ohio          0.922264<br/>New York     -2.153545<br/>Vermont       0.500000<br/>Florida      -0.375842<br/>Oregon        0.329939<br/>Nevada       -1.000000<br/>California    1.105913<br/>Idaho        -1.000000<br/>dtype: float64<br/></p>
<p>Example: Random Sampling and Permutation<br/>Suppose you wanted to draw a random sample (with or without replacement) from a<br/>large dataset for Monte Carlo simulation purposes or some other application. There<br/>are a number of ways to perform the &#8220;draws&#8221;; here we use the sample method for<br/>Series.<br/>To demonstrate, here&#8217;s a way to construct a deck of English-style playing cards:<br/></p>
<p>308 | Chapter 10: Data Aggregation and Group Operations</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><i># Hearts, Spades, Clubs, Diamonds<br/></i>suits = ['H', 'S', 'C', 'D']<br/>card_val = (list(range(1, 11)) + [10] * 3) * 4<br/>base_names = ['A'] + list(range(2, 11)) + ['J', 'K', 'Q']<br/>cards = []<br/><b>for</b> suit <b>in</b> ['H', 'S', 'C', 'D']:<br/>    cards.extend(str(num) + suit <b>for</b> num <b>in</b> base_names)<br/></p>
<p>deck = pd.Series(card_val, index=cards)<br/></p>
<p>So now we have a Series of length 52 whose index contains card names and values are<br/>the ones used in Blackjack and other games (to keep things simple, I just let the ace<br/>'A' be 1):<br/></p>
<p>In [108]: deck[:13]<br/>Out[108]: <br/>AH      1<br/>2H      2<br/>3H      3<br/>4H      4<br/>5H      5<br/>6H      6<br/>7H      7<br/>8H      8<br/>9H      9<br/>10H    10<br/>JH     10<br/>KH     10<br/>QH     10<br/>dtype: int64<br/></p>
<p>Now, based on what I said before, drawing a hand of five cards from the deck could<br/>be written as:<br/></p>
<p>In [109]: <b>def</b> draw(deck, n=5):<br/>   .....:     <b>return</b> deck.sample(n)<br/></p>
<p>In [110]: draw(deck)<br/>Out[110]: <br/>AD     1<br/>8C     8<br/>5H     5<br/>KC    10<br/>2C     2<br/>dtype: int64<br/></p>
<p>Suppose you wanted two random cards from each suit. Because the suit is the last<br/>character of each card name, we can group based on this and use apply:<br/></p>
<p>In [111]: get_suit = <b>lambda</b> card: card[-1] <i># last letter is suit<br/></i></p>
<p>In [112]: deck.groupby(get_suit).apply(draw, n=2)<br/>Out[112]: <br/></p>
<p>10.3 Apply: General split-apply-combine | 309</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>C  2C     2<br/>   3C     3<br/>D  KD    10<br/>   8D     8<br/>H  KH    10<br/>   3H     3<br/>S  2S     2<br/>   4S     4<br/>dtype: int64<br/></p>
<p>Alternatively, we could write:<br/>In [113]: deck.groupby(get_suit, group_keys=False).apply(draw, n=2)<br/>Out[113]: <br/>KC    10<br/>JC    10<br/>AD     1<br/>5D     5<br/>5H     5<br/>6H     6<br/>7S     7<br/>KS    10<br/>dtype: int64<br/></p>
<p>Example: Group Weighted Average and Correlation<br/>Under the split-apply-combine paradigm of groupby, operations between columns in<br/>a DataFrame or two Series, such as a group weighted average, are possible. As an<br/>example, take this dataset containing group keys, values, and some weights:<br/></p>
<p>In [114]: df = pd.DataFrame({'category': ['a', 'a', 'a', 'a',<br/>   .....:                                 'b', 'b', 'b', 'b'],<br/>   .....:                    'data': np.random.randn(8),<br/>   .....:                    'weights': np.random.rand(8)})<br/></p>
<p>In [115]: df<br/>Out[115]: <br/>  category      data   weights<br/>0        a  1.561587  0.957515<br/>1        a  1.219984  0.347267<br/>2        a -0.482239  0.581362<br/>3        a  0.315667  0.217091<br/>4        b -0.047852  0.894406<br/>5        b -0.454145  0.918564<br/>6        b -0.556774  0.277825<br/>7        b  0.253321  0.955905<br/></p>
<p>The group weighted average by category would then be:<br/>In [116]: grouped = df.groupby('category')<br/></p>
<p>In [117]: get_wavg = <b>lambda</b> g: np.average(g['data'], weights=g['weights'])<br/></p>
<p>310 | Chapter 10: Data Aggregation and Group Operations</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In [118]: grouped.apply(get_wavg)<br/>Out[118]: <br/>category<br/>a    0.811643<br/>b   -0.122262<br/>dtype: float64<br/></p>
<p>As another example, consider a financial dataset originally obtained from Yahoo!<br/>Finance containing end-of-day prices for a few stocks and the S&amp;P 500 index (the SPX<br/>symbol):<br/></p>
<p>In [119]: close_px = pd.read_csv('examples/stock_px_2.csv', parse_dates=True,<br/>   .....:                        index_col=0)<br/></p>
<p>In [120]: close_px.info()<br/>&lt;<b>class</b> '<b>pandas</b>.core.frame.DataFrame'&gt;<br/>DatetimeIndex: 2214 entries, 2003-01-02 to 2011-10-14<br/>Data columns (total 4 columns):<br/>AAPL    2214 non-null float64<br/>MSFT    2214 non-null float64<br/>XOM     2214 non-null float64<br/>SPX     2214 non-null float64<br/>dtypes: float64(4)<br/>memory usage: 86.5 KB<br/></p>
<p>In [121]: close_px[-4:]<br/>Out[121]: <br/>              AAPL   MSFT    XOM      SPX<br/>2011-10-11  400.29  27.00  76.27  1195.54<br/>2011-10-12  402.19  26.96  77.16  1207.25<br/>2011-10-13  408.43  27.18  76.37  1203.66<br/>2011-10-14  422.00  27.27  78.11  1224.58<br/></p>
<p>One task of interest might be to compute a DataFrame consisting of the yearly corre&#8208;<br/>lations of daily returns (computed from percent changes) with SPX. As one way to do<br/>this, we first create a function that computes the pairwise correlation of each column<br/>with the 'SPX' column:<br/></p>
<p>In [122]: spx_corr = <b>lambda</b> x: x.corrwith(x['SPX'])<br/></p>
<p>Next, we compute percent change on close_px using pct_change:<br/>In [123]: rets = close_px.pct_change().dropna()<br/></p>
<p>Lastly, we group these percent changes by year, which can be extracted from each row<br/>label with a one-line function that returns the year attribute of each datetime label:<br/></p>
<p>In [124]: get_year = <b>lambda</b> x: x.year<br/></p>
<p>In [125]: by_year = rets.groupby(get_year)<br/></p>
<p>In [126]: by_year.apply(spx_corr)<br/>Out[126]: <br/></p>
<p>10.3 Apply: General split-apply-combine | 311</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>          AAPL      MSFT       XOM  SPX<br/>2003  0.541124  0.745174  0.661265  1.0<br/>2004  0.374283  0.588531  0.557742  1.0<br/>2005  0.467540  0.562374  0.631010  1.0<br/>2006  0.428267  0.406126  0.518514  1.0<br/>2007  0.508118  0.658770  0.786264  1.0<br/>2008  0.681434  0.804626  0.828303  1.0<br/>2009  0.707103  0.654902  0.797921  1.0<br/>2010  0.710105  0.730118  0.839057  1.0<br/>2011  0.691931  0.800996  0.859975  1.0<br/></p>
<p>You could also compute inter-column correlations. Here we compute the annual cor&#8208;<br/>relation between Apple and Microsoft:<br/></p>
<p>In [127]: by_year.apply(<b>lambda</b> g: g['AAPL'].corr(g['MSFT']))<br/>Out[127]: <br/>2003    0.480868<br/>2004    0.259024<br/>2005    0.300093<br/>2006    0.161735<br/>2007    0.417738<br/>2008    0.611901<br/>2009    0.432738<br/>2010    0.571946<br/>2011    0.581987<br/>dtype: float64<br/></p>
<p>Example: Group-Wise Linear Regression<br/>In the same theme as the previous example, you can use groupby to perform more<br/>complex group-wise statistical analysis, as long as the function returns a pandas<br/>object or scalar value. For example, I can define the following regress function<br/>(using the statsmodels econometrics library), which executes an ordinary least<br/>squares (OLS) regression on each chunk of data:<br/></p>
<p><b>import</b> <b>statsmodels.api</b> <b>as</b> <b>sm<br/>def</b> regress(data, yvar, xvars):<br/>    Y = data[yvar]<br/>    X = data[xvars]<br/>    X['intercept'] = 1.<br/>    result = sm.OLS(Y, X).fit()<br/>    <b>return</b> result.params<br/></p>
<p>Now, to run a yearly linear regression of AAPL on SPX returns, execute:<br/>In [129]: by_year.apply(regress, 'AAPL', ['SPX'])<br/>Out[129]: <br/>           SPX  intercept<br/>2003  1.195406   0.000710<br/>2004  1.363463   0.004201<br/>2005  1.766415   0.003246<br/>2006  1.645496   0.000080<br/></p>
<p>312 | Chapter 10: Data Aggregation and Group Operations</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>2007  1.198761   0.003438<br/>2008  0.968016  -0.001110<br/>2009  0.879103   0.002954<br/>2010  1.052608   0.001261<br/>2011  0.806605   0.001514<br/></p>
<p>10.4 Pivot Tables and Cross-Tabulation<br/>A <i>pivot table</i> is a data summarization tool frequently found in spreadsheet programs<br/>and other data analysis software. It aggregates a table of data by one or more keys,<br/>arranging the data in a rectangle with some of the group keys along the rows and<br/>some along the columns. Pivot tables in Python with pandas are made possible<br/>through the groupby facility described in this chapter combined with reshape opera&#8208;<br/>tions utilizing hierarchical indexing. DataFrame has a pivot_table method, and<br/>there is also a top-level pandas.pivot_table function. In addition to providing a<br/>convenience interface to groupby, pivot_table can add partial totals, also known as<br/><i>margins</i>.<br/>Returning to the tipping dataset, suppose you wanted to compute a table of group<br/>means (the default pivot_table aggregation type) arranged by day and smoker on<br/>the rows:<br/></p>
<p>In [130]: tips.pivot_table(index=['day', 'smoker'])<br/>Out[130]: <br/>                 size       tip   tip_pct  total_bill<br/>day  smoker                                          <br/>Fri  No      2.250000  2.812500  0.151650   18.420000<br/>     Yes     2.066667  2.714000  0.174783   16.813333<br/>Sat  No      2.555556  3.102889  0.158048   19.661778<br/>     Yes     2.476190  2.875476  0.147906   21.276667<br/>Sun  No      2.929825  3.167895  0.160113   20.506667<br/>     Yes     2.578947  3.516842  0.187250   24.120000<br/>Thur No      2.488889  2.673778  0.160298   17.113111<br/>     Yes     2.352941  3.030000  0.163863   19.190588<br/></p>
<p>This could have been produced with groupby directly. Now, suppose we want to<br/>aggregate only tip_pct and size, and additionally group by time. I&#8217;ll put smoker in<br/>the table columns and day in the rows:<br/></p>
<p>In [131]: tips.pivot_table(['tip_pct', 'size'], index=['time', 'day'],<br/>   .....:                  columns='smoker')<br/>Out[131]: <br/>                 size             tip_pct          <br/>smoker             No       Yes        No       Yes<br/>time   day                                         <br/>Dinner Fri   2.000000  2.222222  0.139622  0.165347<br/>       Sat   2.555556  2.476190  0.158048  0.147906<br/>       Sun   2.929825  2.578947  0.160113  0.187250<br/>       Thur  2.000000       NaN  0.159744       NaN<br/></p>
<p>10.4 Pivot Tables and Cross-Tabulation | 313</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Lunch  Fri   3.000000  1.833333  0.187735  0.188937<br/>       Thur  2.500000  2.352941  0.160311  0.163863<br/></p>
<p>We could augment this table to include partial totals by passing margins=True. This<br/>has the effect of adding All row and column labels, with corresponding values being<br/>the group statistics for all the data within a single tier:<br/></p>
<p>In [132]: tips.pivot_table(['tip_pct', 'size'], index=['time', 'day'],<br/>   .....:                  columns='smoker', margins=True)<br/>Out[132]: <br/>                 size                       tip_pct                    <br/>smoker             No       Yes       All        No       Yes       All<br/>time   day                                                             <br/>Dinner Fri   2.000000  2.222222  2.166667  0.139622  0.165347  0.158916<br/>       Sat   2.555556  2.476190  2.517241  0.158048  0.147906  0.153152<br/>       Sun   2.929825  2.578947  2.842105  0.160113  0.187250  0.166897<br/>       Thur  2.000000       NaN  2.000000  0.159744       NaN  0.159744<br/>Lunch  Fri   3.000000  1.833333  2.000000  0.187735  0.188937  0.188765<br/>       Thur  2.500000  2.352941  2.459016  0.160311  0.163863  0.161301<br/>All          2.668874  2.408602  2.569672  0.159328  0.163196  0.160803<br/></p>
<p>Here, the All values are means without taking into account smoker versus non-<br/>smoker (the All columns) or any of the two levels of grouping on the rows (the All<br/>row).<br/>To use a different aggregation function, pass it to aggfunc. For example, 'count' or<br/>len will give you a cross-tabulation (count or frequency) of group sizes:<br/></p>
<p>In [133]: tips.pivot_table('tip_pct', index=['time', 'smoker'], columns='day',<br/>   .....:                  aggfunc=len, margins=True)<br/>Out[133]: <br/>day             Fri   Sat   Sun  Thur    All<br/>time   smoker                               <br/>Dinner No       3.0  45.0  57.0   1.0  106.0<br/>       Yes      9.0  42.0  19.0   NaN   70.0<br/>Lunch  No       1.0   NaN   NaN  44.0   45.0<br/>       Yes      6.0   NaN   NaN  17.0   23.0<br/>All            19.0  87.0  76.0  62.0  244.0<br/></p>
<p>If some combinations are empty (or otherwise NA), you may wish to pass a<br/>fill_value:<br/></p>
<p>In [134]: tips.pivot_table('tip_pct', index=['time', 'size', 'smoker'],<br/>   .....:                  columns='day', aggfunc='mean', fill_value=0)<br/>Out[134]: <br/>day                      Fri       Sat       Sun      Thur<br/>time   size smoker                                        <br/>Dinner 1    No      0.000000  0.137931  0.000000  0.000000<br/>            Yes     0.000000  0.325733  0.000000  0.000000<br/>       2    No      0.139622  0.162705  0.168859  0.159744<br/>            Yes     0.171297  0.148668  0.207893  0.000000<br/>       3    No      0.000000  0.154661  0.152663  0.000000<br/></p>
<p>314 | Chapter 10: Data Aggregation and Group Operations</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>            Yes     0.000000  0.144995  0.152660  0.000000<br/>       4    No      0.000000  0.150096  0.148143  0.000000<br/>            Yes     0.117750  0.124515  0.193370  0.000000<br/>       5    No      0.000000  0.000000  0.206928  0.000000<br/>            Yes     0.000000  0.106572  0.065660  0.000000<br/>...                      ...       ...       ...       ...<br/>Lunch  1    No      0.000000  0.000000  0.000000  0.181728<br/>            Yes     0.223776  0.000000  0.000000  0.000000<br/>       2    No      0.000000  0.000000  0.000000  0.166005<br/>            Yes     0.181969  0.000000  0.000000  0.158843<br/>       3    No      0.187735  0.000000  0.000000  0.084246<br/>            Yes     0.000000  0.000000  0.000000  0.204952<br/>       4    No      0.000000  0.000000  0.000000  0.138919<br/>            Yes     0.000000  0.000000  0.000000  0.155410<br/>       5    No      0.000000  0.000000  0.000000  0.121389<br/>       6    No      0.000000  0.000000  0.000000  0.173706<br/>[21 rows x 4 columns]<br/></p>
<p>See Table 10-2 for a summary of pivot_table methods.<br/><i>Table 10-2. pivot_table options<br/></i></p>
<p>Function name Description<br/>values Column name or names to aggregate; by default aggregates all numeric columns<br/>index Column names or other group keys to group on the rows of the resulting pivot table<br/>columns Column names or other group keys to group on the columns of the resulting pivot table<br/>aggfunc Aggregation function or list of functions ('mean' by default); can be any function valid in a groupby<br/></p>
<p>context<br/>fill_value Replace missing values in result table<br/>dropna If True, do not include columns whose entries are all NA<br/>margins Add row/column subtotals and grand total (False by default)<br/></p>
<p>Cross-Tabulations: Crosstab<br/>A cross-tabulation (or <i>crosstab</i> for short) is a special case of a pivot table that com&#8208;<br/>putes group frequencies. Here is an example:<br/></p>
<p>In [138]: data<br/>Out[138]: <br/>   Sample Nationality    Handedness<br/>0       1         USA  Right-handed<br/>1       2       Japan   Left-handed<br/>2       3         USA  Right-handed<br/>3       4       Japan  Right-handed<br/>4       5       Japan   Left-handed<br/>5       6       Japan  Right-handed<br/>6       7         USA  Right-handed<br/>7       8         USA   Left-handed<br/>8       9       Japan  Right-handed<br/>9      10         USA  Right-handed<br/></p>
<p>10.4 Pivot Tables and Cross-Tabulation | 315</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>As part of some survey analysis, we might want to summarize this data by nationality<br/>and handedness. You could use pivot_table to do this, but the pandas.crosstab<br/>function can be more convenient:<br/></p>
<p>In [139]: pd.crosstab(data.Nationality, data.Handedness, margins=True)<br/>Out[139]: <br/>Handedness   Left-handed  Right-handed  All<br/>Nationality                                <br/>Japan                  2             3    5<br/>USA                    1             4    5<br/>All                    3             7   10<br/></p>
<p>The first two arguments to crosstab can each either be an array or Series or a list of<br/>arrays. As in the tips data:<br/></p>
<p>In [140]: pd.crosstab([tips.time, tips.day], tips.smoker, margins=True)<br/>Out[140]: <br/>smoker        No  Yes  All<br/>time   day                <br/>Dinner Fri     3    9   12<br/>       Sat    45   42   87<br/>       Sun    57   19   76<br/>       Thur    1    0    1<br/>Lunch  Fri     1    6    7<br/>       Thur   44   17   61<br/>All          151   93  244<br/></p>
<p>10.5 Conclusion<br/>Mastering pandas&#8217;s data grouping tools can help both with data cleaning as well as<br/>modeling or statistical analysis work. In Chapter 14 we will look at several more<br/>example use cases for groupby on real data.<br/>In the next chapter, we turn our attention to time series data.<br/></p>
<p>316 | Chapter 10: Data Aggregation and Group Operations</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>CHAPTER 11<br/>Time Series<br/></p>
<p>Time series data is an important form of structured data in many different fields, such<br/>as finance, economics, ecology, neuroscience, and physics. Anything that is observed<br/>or measured at many points in time forms a time series. Many time series are fixed<br/><i>frequency</i>, which is to say that data points occur at regular intervals according to some<br/>rule, such as every 15 seconds, every 5 minutes, or once per month. Time series can<br/>also be <i>irregular</i> without a fixed unit of time or offset between units. How you mark<br/>and refer to time series data depends on the application, and you may have one of the<br/>following:<br/></p>
<p>&#8226; <i>Timestamps</i>, specific instants in time<br/>&#8226; Fixed <i>periods</i>, such as the month January 2007 or the full year 2010<br/>&#8226; <i>Intervals</i> of time, indicated by a start and end timestamp. Periods can be thought<br/></p>
<p>of as special cases of intervals<br/>&#8226; Experiment or elapsed time; each timestamp is a measure of time relative to a<br/></p>
<p>particular start time (e.g., the diameter of a cookie baking each second since<br/>being placed in the oven)<br/></p>
<p>In this chapter, I am mainly concerned with time series in the first three categories,<br/>though many of the techniques can be applied to experimental time series where the<br/>index may be an integer or floating-point number indicating elapsed time from the<br/>start of the experiment. The simplest and most widely used kind of time series are<br/>those indexed by timestamp.<br/></p>
<p>317</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>pandas also supports indexes based on timedeltas, which can be a<br/>useful way of representing experiment or elapsed time. We do not<br/>explore timedelta indexes in this book, but you can learn more in<br/>the pandas documentation.<br/></p>
<p>pandas provides many built-in time series tools and data algorithms. You can effi&#8208;<br/>ciently work with very large time series and easily slice and dice, aggregate, and<br/>resample irregular- and fixed-frequency time series. Some of these tools are especially<br/>useful for financial and economics applications, but you could certainly use them to<br/>analyze server log data, too.<br/></p>
<p>11.1 Date and Time Data Types and Tools<br/>The Python standard library includes data types for date and time data, as well as<br/>calendar-related functionality. The datetime, time, and calendar modules are the<br/>main places to start. The datetime.datetime type, or simply datetime, is widely<br/>used:<br/></p>
<p>In [10]: <b>from</b> <b>datetime</b> <b>import</b> datetime<br/></p>
<p>In [11]: now = datetime.now()<br/></p>
<p>In [12]: now<br/>Out[12]: datetime.datetime(2017, 9, 25, 14, 5, 52, 72973)<br/></p>
<p>In [13]: now.year, now.month, now.day<br/>Out[13]: (2017, 9, 25)<br/></p>
<p>datetime stores both the date and time down to the microsecond. timedelta repre&#8208;<br/>sents the temporal difference between two datetime objects:<br/></p>
<p>In [14]: delta = datetime(2011, 1, 7) - datetime(2008, 6, 24, 8, 15)<br/></p>
<p>In [15]: delta<br/>Out[15]: datetime.timedelta(926, 56700)<br/></p>
<p>In [16]: delta.days<br/>Out[16]: 926<br/></p>
<p>In [17]: delta.seconds<br/>Out[17]: 56700<br/></p>
<p>You can add (or subtract) a timedelta or multiple thereof to a datetime object to<br/>yield a new shifted object:<br/></p>
<p>In [18]: <b>from</b> <b>datetime</b> <b>import</b> timedelta<br/></p>
<p>In [19]: start = datetime(2011, 1, 7)<br/></p>
<p>318 | Chapter 11: Time Series</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In [20]: start + timedelta(12)<br/>Out[20]: datetime.datetime(2011, 1, 19, 0, 0)<br/></p>
<p>In [21]: start - 2 * timedelta(12)<br/>Out[21]: datetime.datetime(2010, 12, 14, 0, 0)<br/></p>
<p>Table 11-1 summarizes the data types in the datetime module. While this chapter is<br/>mainly concerned with the data types in pandas and higher-level time series manipu&#8208;<br/>lation, you may encounter the datetime-based types in many other places in Python<br/>in the wild.<br/><i>Table 11-1. Types in datetime module<br/></i></p>
<p>Type Description<br/>date Store calendar date (year, month, day) using the Gregorian calendar<br/>time Store time of day as hours, minutes, seconds, and microseconds<br/>datetime Stores both date and time<br/>timedelta Represents the difference between two datetime values (as days, seconds, and microseconds)<br/>tzinfo Base type for storing time zone information<br/></p>
<p>Converting Between String and Datetime<br/>You can format datetime objects and pandas Timestamp objects, which I&#8217;ll introduce<br/>later, as strings using str or the strftime method, passing a format specification:<br/></p>
<p>In [22]: stamp = datetime(2011, 1, 3)<br/></p>
<p>In [23]: str(stamp)<br/>Out[23]: '2011-01-03 00:00:00'<br/></p>
<p>In [24]: stamp.strftime('%Y-%m-%d')<br/>Out[24]: '2011-01-03'<br/></p>
<p>See Table 11-2 for a complete list of the format codes (reproduced from Chapter 2).<br/><i>Table 11-2. Datetime format specification (ISO C89 compatible)<br/></i></p>
<p>Type Description<br/>%Y Four-digit year<br/>%y Two-digit year<br/>%m Two-digit month [01, 12]<br/>%d Two-digit day [01, 31]<br/>%H Hour (24-hour clock) [00, 23]<br/>%I Hour (12-hour clock) [01, 12]<br/>%M Two-digit minute [00, 59]<br/>%S Second [00, 61] (seconds 60, 61 account for leap seconds)<br/>%w Weekday as integer [0 (Sunday), 6]<br/></p>
<p>11.1 Date and Time Data Types and Tools | 319</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Type Description<br/>%U Week number of the year [00, 53]; Sunday is considered the first day of the week, and days before the first Sunday of<br/></p>
<p>the year are &#8220;week 0&#8221;<br/>%W Week number of the year [00, 53]; Monday is considered the first day of the week, and days before the first Monday of<br/></p>
<p>the year are &#8220;week 0&#8221;<br/>%z UTC time zone offset as +HHMM or -HHMM; empty if time zone naive<br/>%F Shortcut for %Y-%m-%d (e.g., 2012-4-18)<br/>%D Shortcut for %m/%d/%y (e.g., 04/18/12)<br/></p>
<p>You can use these same format codes to convert strings to dates using date<br/>time.strptime:<br/></p>
<p>In [25]: value = '2011-01-03'<br/></p>
<p>In [26]: datetime.strptime(value, '%Y-%m-%d')<br/>Out[26]: datetime.datetime(2011, 1, 3, 0, 0)<br/></p>
<p>In [27]: datestrs = ['7/6/2011', '8/6/2011']<br/></p>
<p>In [28]: [datetime.strptime(x, '%m/%d/%Y') <b>for</b> x <b>in</b> datestrs]<br/>Out[28]: <br/>[datetime.datetime(2011, 7, 6, 0, 0),<br/> datetime.datetime(2011, 8, 6, 0, 0)]<br/></p>
<p>datetime.strptime is a good way to parse a date with a known format. However, it<br/>can be a bit annoying to have to write a format spec each time, especially for common<br/>date formats. In this case, you can use the parser.parse method in the third-party<br/>dateutil package (this is installed automatically when you install pandas):<br/></p>
<p>In [29]: <b>from</b> <b>dateutil.parser</b> <b>import</b> parse<br/></p>
<p>In [30]: parse('2011-01-03')<br/>Out[30]: datetime.datetime(2011, 1, 3, 0, 0)<br/></p>
<p>dateutil is capable of parsing most human-intelligible date representations:<br/>In [31]: parse('Jan 31, 1997 10:45 PM')<br/>Out[31]: datetime.datetime(1997, 1, 31, 22, 45)<br/></p>
<p>In international locales, day appearing before month is very common, so you can pass<br/>dayfirst=True to indicate this:<br/></p>
<p>In [32]: parse('6/12/2011', dayfirst=True)<br/>Out[32]: datetime.datetime(2011, 12, 6, 0, 0)<br/></p>
<p>pandas is generally oriented toward working with arrays of dates, whether used as an<br/>axis index or a column in a DataFrame. The to_datetime method parses many dif&#8208;<br/>ferent kinds of date representations. Standard date formats like ISO 8601 can be<br/>parsed very quickly:<br/></p>
<p>320 | Chapter 11: Time Series</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In [33]: datestrs = ['2011-07-06 12:00:00', '2011-08-06 00:00:00']<br/></p>
<p>In [34]: pd.to_datetime(datestrs)<br/>Out[34]: DatetimeIndex(['2011-07-06 12:00:00', '2011-08-06 00:00:00'], dtype='dat<br/>etime64[ns]', freq=None)<br/></p>
<p>It also handles values that should be considered missing (None, empty string, etc.):<br/>In [35]: idx = pd.to_datetime(datestrs + [None])<br/></p>
<p>In [36]: idx<br/>Out[36]: DatetimeIndex(['2011-07-06 12:00:00', '2011-08-06 00:00:00', 'NaT'], dty<br/>pe='datetime64[ns]', freq=None)<br/></p>
<p>In [37]: idx[2]<br/>Out[37]: NaT<br/></p>
<p>In [38]: pd.isnull(idx)<br/>Out[38]: array([False, False,  True], dtype=bool)<br/></p>
<p>NaT (Not a Time) is pandas&#8217;s null value for timestamp data.<br/></p>
<p>dateutil.parser is a useful but imperfect tool. Notably, it will rec&#8208;<br/>ognize some strings as dates that you might prefer that it didn&#8217;t&#8212;<br/>for example, '42' will be parsed as the year 2042 with today&#8217;s cal&#8208;<br/>endar date.<br/></p>
<p>datetime objects also have a number of locale-specific formatting options for systems<br/>in other countries or languages. For example, the abbreviated month names will be<br/>different on German or French systems compared with English systems. See<br/>Table 11-3 for a listing.<br/><i>Table 11-3. Locale-specific date formatting<br/></i></p>
<p>Type Description<br/>%a Abbreviated weekday name<br/>%A Full weekday name<br/>%b Abbreviated month name<br/>%B Full month name<br/>%c Full date and time (e.g., &#8216;Tue 01 May 2012 04:20:57 PM&#8217;)<br/>%p Locale equivalent of AM or PM<br/>%x Locale-appropriate formatted date (e.g., in the United States, May 1, 2012 yields &#8217;05/01/2012&#8217;)<br/>%X Locale-appropriate time (e.g., &#8217;04:24:12 PM&#8217;)<br/></p>
<p>11.1 Date and Time Data Types and Tools | 321</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>11.2 Time Series Basics<br/>A basic kind of time series object in pandas is a Series indexed by timestamps, which<br/>is often represented external to pandas as Python strings or datetime objects:<br/></p>
<p>In [39]: <b>from</b> <b>datetime</b> <b>import</b> datetime<br/></p>
<p>In [40]: dates = [datetime(2011, 1, 2), datetime(2011, 1, 5),<br/>   ....:          datetime(2011, 1, 7), datetime(2011, 1, 8),<br/>   ....:          datetime(2011, 1, 10), datetime(2011, 1, 12)]<br/></p>
<p>In [41]: ts = pd.Series(np.random.randn(6), index=dates)<br/></p>
<p>In [42]: ts<br/>Out[42]: <br/>2011-01-02   -0.204708<br/>2011-01-05    0.478943<br/>2011-01-07   -0.519439<br/>2011-01-08   -0.555730<br/>2011-01-10    1.965781<br/>2011-01-12    1.393406<br/>dtype: float64<br/></p>
<p>Under the hood, these datetime objects have been put in a DatetimeIndex:<br/>In [43]: ts.index<br/>Out[43]: <br/>DatetimeIndex(['2011-01-02', '2011-01-05', '2011-01-07', '2011-01-08',<br/>               '2011-01-10', '2011-01-12'],<br/>              dtype='datetime64[ns]', freq=None)<br/></p>
<p>Like other Series, arithmetic operations between differently indexed time series auto&#8208;<br/>matically align on the dates:<br/></p>
<p>In [44]: ts + ts[::2]<br/>Out[44]: <br/>2011-01-02   -0.409415<br/>2011-01-05         NaN<br/>2011-01-07   -1.038877<br/>2011-01-08         NaN<br/>2011-01-10    3.931561<br/>2011-01-12         NaN<br/>dtype: float64<br/></p>
<p>Recall that ts[::2] selects every second element in ts.<br/>pandas stores timestamps using NumPy&#8217;s datetime64 data type at the nanosecond<br/>resolution:<br/></p>
<p>In [45]: ts.index.dtype<br/>Out[45]: dtype('&lt;M8[ns]')<br/></p>
<p>Scalar values from a DatetimeIndex are pandas Timestamp objects:<br/></p>
<p>322 | Chapter 11: Time Series</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In [46]: stamp = ts.index[0]<br/></p>
<p>In [47]: stamp<br/>Out[47]: Timestamp('2011-01-02 00:00:00')<br/></p>
<p>A Timestamp can be substituted anywhere you would use a datetime object. Addi&#8208;<br/>tionally, it can store frequency information (if any) and understands how to do time<br/>zone conversions and other kinds of manipulations. More on both of these things<br/>later.<br/></p>
<p>Indexing, Selection, Subsetting<br/>Time series behaves like any other pandas.Series when you are indexing and select&#8208;<br/>ing data based on label:<br/></p>
<p>In [48]: stamp = ts.index[2]<br/></p>
<p>In [49]: ts[stamp]<br/>Out[49]: -0.51943871505673811<br/></p>
<p>As a convenience, you can also pass a string that is interpretable as a date:<br/>In [50]: ts['1/10/2011']<br/>Out[50]: 1.9657805725027142<br/></p>
<p>In [51]: ts['20110110']<br/>Out[51]: 1.9657805725027142<br/></p>
<p>For longer time series, a year or only a year and month can be passed to easily select<br/>slices of data:<br/></p>
<p>In [52]: longer_ts = pd.Series(np.random.randn(1000),<br/>   ....:                       index=pd.date_range('1/1/2000', periods=1000))<br/></p>
<p>In [53]: longer_ts<br/>Out[53]: <br/>2000-01-01    0.092908<br/>2000-01-02    0.281746<br/>2000-01-03    0.769023<br/>2000-01-04    1.246435<br/>2000-01-05    1.007189<br/>2000-01-06   -1.296221<br/>2000-01-07    0.274992<br/>2000-01-08    0.228913<br/>2000-01-09    1.352917<br/>2000-01-10    0.886429<br/>                ...   <br/>2002-09-17   -0.139298<br/>2002-09-18   -1.159926<br/>2002-09-19    0.618965<br/>2002-09-20    1.373890<br/>2002-09-21   -0.983505<br/></p>
<p>11.2 Time Series Basics | 323</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>2002-09-22    0.930944<br/>2002-09-23   -0.811676<br/>2002-09-24   -1.830156<br/>2002-09-25   -0.138730<br/>2002-09-26    0.334088<br/>Freq: D, Length: 1000, dtype: float64<br/></p>
<p>In [54]: longer_ts['2001']<br/>Out[54]: <br/>2001-01-01    1.599534<br/>2001-01-02    0.474071<br/>2001-01-03    0.151326<br/>2001-01-04   -0.542173<br/>2001-01-05   -0.475496<br/>2001-01-06    0.106403<br/>2001-01-07   -1.308228<br/>2001-01-08    2.173185<br/>2001-01-09    0.564561<br/>2001-01-10   -0.190481<br/>                ...   <br/>2001-12-22    0.000369<br/>2001-12-23    0.900885<br/>2001-12-24   -0.454869<br/>2001-12-25   -0.864547<br/>2001-12-26    1.129120<br/>2001-12-27    0.057874<br/>2001-12-28   -0.433739<br/>2001-12-29    0.092698<br/>2001-12-30   -1.397820<br/>2001-12-31    1.457823<br/>Freq: D, Length: 365, dtype: float64<br/></p>
<p>Here, the string '2001' is interpreted as a year and selects that time period. This also<br/>works if you specify the month:<br/></p>
<p>In [55]: longer_ts['2001-05']<br/>Out[55]: <br/>2001-05-01   -0.622547<br/>2001-05-02    0.936289<br/>2001-05-03    0.750018<br/>2001-05-04   -0.056715<br/>2001-05-05    2.300675<br/>2001-05-06    0.569497<br/>2001-05-07    1.489410<br/>2001-05-08    1.264250<br/>2001-05-09   -0.761837<br/>2001-05-10   -0.331617<br/>                ...   <br/>2001-05-22    0.503699<br/>2001-05-23   -1.387874<br/>2001-05-24    0.204851<br/>2001-05-25    0.603705<br/>2001-05-26    0.545680<br/></p>
<p>324 | Chapter 11: Time Series</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>2001-05-27    0.235477<br/>2001-05-28    0.111835<br/>2001-05-29   -1.251504<br/>2001-05-30   -2.949343<br/>2001-05-31    0.634634<br/>Freq: D, Length: 31, dtype: float64<br/></p>
<p>Slicing with datetime objects works as well:<br/>In [56]: ts[datetime(2011, 1, 7):]<br/>Out[56]: <br/>2011-01-07   -0.519439<br/>2011-01-08   -0.555730<br/>2011-01-10    1.965781<br/>2011-01-12    1.393406<br/>dtype: float64<br/></p>
<p>Because most time series data is ordered chronologically, you can slice with time&#8208;<br/>stamps not contained in a time series to perform a range query:<br/></p>
<p>In [57]: ts<br/>Out[57]: <br/>2011-01-02   -0.204708<br/>2011-01-05    0.478943<br/>2011-01-07   -0.519439<br/>2011-01-08   -0.555730<br/>2011-01-10    1.965781<br/>2011-01-12    1.393406<br/>dtype: float64<br/></p>
<p>In [58]: ts['1/6/2011':'1/11/2011']<br/>Out[58]: <br/>2011-01-07   -0.519439<br/>2011-01-08   -0.555730<br/>2011-01-10    1.965781<br/>dtype: float64<br/></p>
<p>As before, you can pass either a string date, datetime, or timestamp. Remember that<br/>slicing in this manner produces views on the source time series like slicing NumPy<br/>arrays. This means that no data is copied and modifications on the slice will be reflec&#8208;<br/>ted in the original data.<br/>There is an equivalent instance method, truncate, that slices a Series between two<br/>dates:<br/></p>
<p>In [59]: ts.truncate(after='1/9/2011')<br/>Out[59]: <br/>2011-01-02   -0.204708<br/>2011-01-05    0.478943<br/>2011-01-07   -0.519439<br/>2011-01-08   -0.555730<br/>dtype: float64<br/></p>
<p>11.2 Time Series Basics | 325</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>All of this holds true for DataFrame as well, indexing on its rows:<br/>In [60]: dates = pd.date_range('1/1/2000', periods=100, freq='W-WED')<br/></p>
<p>In [61]: long_df = pd.DataFrame(np.random.randn(100, 4),<br/>   ....:                        index=dates,<br/>   ....:                        columns=['Colorado', 'Texas',<br/>   ....:                                 'New York', 'Ohio'])<br/></p>
<p>In [62]: long_df.loc['5-2001']<br/>Out[62]: <br/>            Colorado     Texas  New York      Ohio<br/>2001-05-02 -0.006045  0.490094 -0.277186 -0.707213<br/>2001-05-09 -0.560107  2.735527  0.927335  1.513906<br/>2001-05-16  0.538600  1.273768  0.667876 -0.969206<br/>2001-05-23  1.676091 -0.817649  0.050188  1.951312<br/>2001-05-30  3.260383  0.963301  1.201206 -1.852001<br/></p>
<p>Time Series with Duplicate Indices<br/>In some applications, there may be multiple data observations falling on a particular<br/>timestamp. Here is an example:<br/></p>
<p>In [63]: dates = pd.DatetimeIndex(['1/1/2000', '1/2/2000', '1/2/2000',<br/>   ....:                           '1/2/2000', '1/3/2000'])<br/></p>
<p>In [64]: dup_ts = pd.Series(np.arange(5), index=dates)<br/></p>
<p>In [65]: dup_ts<br/>Out[65]: <br/>2000-01-01    0<br/>2000-01-02    1<br/>2000-01-02    2<br/>2000-01-02    3<br/>2000-01-03    4<br/>dtype: int64<br/></p>
<p>We can tell that the index is not unique by checking its is_unique property:<br/>In [66]: dup_ts.index.is_unique<br/>Out[66]: False<br/></p>
<p>Indexing into this time series will now either produce scalar values or slices depend&#8208;<br/>ing on whether a timestamp is duplicated:<br/></p>
<p>In [67]: dup_ts['1/3/2000']  <i># not duplicated<br/></i>Out[67]: 4<br/></p>
<p>In [68]: dup_ts['1/2/2000']  <i># duplicated<br/></i>Out[68]: <br/>2000-01-02    1<br/>2000-01-02    2<br/></p>
<p>326 | Chapter 11: Time Series</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>2000-01-02    3<br/>dtype: int64<br/></p>
<p>Suppose you wanted to aggregate the data having non-unique timestamps. One way<br/>to do this is to use groupby and pass level=0:<br/></p>
<p>In [69]: grouped = dup_ts.groupby(level=0)<br/></p>
<p>In [70]: grouped.mean()<br/>Out[70]: <br/>2000-01-01    0<br/>2000-01-02    2<br/>2000-01-03    4<br/>dtype: int64<br/></p>
<p>In [71]: grouped.count()<br/>Out[71]: <br/>2000-01-01    1<br/>2000-01-02    3<br/>2000-01-03    1<br/>dtype: int64<br/></p>
<p>11.3 Date Ranges, Frequencies, and Shifting<br/>Generic time series in pandas are assumed to be irregular; that is, they have no fixed<br/>frequency. For many applications this is sufficient. However, it&#8217;s often desirable to<br/>work relative to a fixed frequency, such as daily, monthly, or every 15 minutes, even if<br/>that means introducing missing values into a time series. Fortunately pandas has a<br/>full suite of standard time series frequencies and tools for resampling, inferring fre&#8208;<br/>quencies, and generating fixed-frequency date ranges. For example, you can convert<br/>the sample time series to be fixed daily frequency by calling resample:<br/></p>
<p>In [72]: ts<br/>Out[72]: <br/>2011-01-02   -0.204708<br/>2011-01-05    0.478943<br/>2011-01-07   -0.519439<br/>2011-01-08   -0.555730<br/>2011-01-10    1.965781<br/>2011-01-12    1.393406<br/>dtype: float64<br/></p>
<p>In [73]: resampler = ts.resample('D')<br/></p>
<p>The string 'D' is interpreted as daily frequency.<br/>Conversion between frequencies or <i>resampling</i> is a big enough topic to have its own<br/>section later (Section 11.6, &#8220;Resampling and Frequency Conversion,&#8221; on page 348).<br/>Here I&#8217;ll show you how to use the base frequencies and multiples thereof.<br/></p>
<p>11.3 Date Ranges, Frequencies, and Shifting | 327</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Generating Date Ranges<br/>While I used it previously without explanation, pandas.date_range is responsible for<br/>generating a DatetimeIndex with an indicated length according to a particular<br/>frequency:<br/></p>
<p>In [74]: index = pd.date_range('2012-04-01', '2012-06-01')<br/></p>
<p>In [75]: index<br/>Out[75]: <br/>DatetimeIndex(['2012-04-01', '2012-04-02', '2012-04-03', '2012-04-04',<br/>               '2012-04-05', '2012-04-06', '2012-04-07', '2012-04-08',<br/>               '2012-04-09', '2012-04-10', '2012-04-11', '2012-04-12',<br/>               '2012-04-13', '2012-04-14', '2012-04-15', '2012-04-16',<br/>               '2012-04-17', '2012-04-18', '2012-04-19', '2012-04-20',<br/>               '2012-04-21', '2012-04-22', '2012-04-23', '2012-04-24',<br/>               '2012-04-25', '2012-04-26', '2012-04-27', '2012-04-28',<br/>               '2012-04-29', '2012-04-30', '2012-05-01', '2012-05-02',<br/>               '2012-05-03', '2012-05-04', '2012-05-05', '2012-05-06',<br/>               '2012-05-07', '2012-05-08', '2012-05-09', '2012-05-10',<br/>               '2012-05-11', '2012-05-12', '2012-05-13', '2012-05-14',<br/>               '2012-05-15', '2012-05-16', '2012-05-17', '2012-05-18',<br/>               '2012-05-19', '2012-05-20', '2012-05-21', '2012-05-22',<br/>               '2012-05-23', '2012-05-24', '2012-05-25', '2012-05-26',<br/>               '2012-05-27', '2012-05-28', '2012-05-29', '2012-05-30',<br/>               '2012-05-31', '2012-06-01'],<br/>              dtype='datetime64[ns]', freq='D')<br/></p>
<p>By default, date_range generates daily timestamps. If you pass only a start or end<br/>date, you must pass a number of periods to generate:<br/></p>
<p>In [76]: pd.date_range(start='2012-04-01', periods=20)<br/>Out[76]: <br/>DatetimeIndex(['2012-04-01', '2012-04-02', '2012-04-03', '2012-04-04',<br/>               '2012-04-05', '2012-04-06', '2012-04-07', '2012-04-08',<br/>               '2012-04-09', '2012-04-10', '2012-04-11', '2012-04-12',<br/>               '2012-04-13', '2012-04-14', '2012-04-15', '2012-04-16',<br/>               '2012-04-17', '2012-04-18', '2012-04-19', '2012-04-20'],<br/>              dtype='datetime64[ns]', freq='D')<br/></p>
<p>In [77]: pd.date_range(end='2012-06-01', periods=20)<br/>Out[77]: <br/>DatetimeIndex(['2012-05-13', '2012-05-14', '2012-05-15', '2012-05-16',<br/>               '2012-05-17', '2012-05-18', '2012-05-19', '2012-05-20',<br/>               '2012-05-21', '2012-05-22', '2012-05-23', '2012-05-24',<br/>               '2012-05-25', '2012-05-26', '2012-05-27', '2012-05-28',<br/>               '2012-05-29', '2012-05-30', '2012-05-31', '2012-06-01'],<br/>              dtype='datetime64[ns]', freq='D')<br/></p>
<p>The start and end dates define strict boundaries for the generated date index. For<br/>example, if you wanted a date index containing the last business day of each month,<br/>you would pass the 'BM' frequency (business end of month; see more complete listing<br/></p>
<p>328 | Chapter 11: Time Series</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>of frequencies in Table 11-4) and only dates falling on or inside the date interval will<br/>be included:<br/></p>
<p>In [78]: pd.date_range('2000-01-01', '2000-12-01', freq='BM')<br/>Out[78]: <br/>DatetimeIndex(['2000-01-31', '2000-02-29', '2000-03-31', '2000-04-28',<br/>               '2000-05-31', '2000-06-30', '2000-07-31', '2000-08-31',<br/>               '2000-09-29', '2000-10-31', '2000-11-30'],<br/>              dtype='datetime64[ns]', freq='BM')<br/></p>
<p><i>Table 11-4. Base time series frequencies (not comprehensive)<br/></i>Alias Offset type Description<br/>D Day Calendar daily<br/>B BusinessDay Business daily<br/>H Hour Hourly<br/>T or min Minute Minutely<br/>S Second Secondly<br/>L or ms Milli Millisecond (1/1,000 of 1 second)<br/>U Micro Microsecond (1/1,000,000 of 1 second)<br/>M MonthEnd Last calendar day of month<br/>BM BusinessMonthEnd Last business day (weekday) of month<br/>MS MonthBegin First calendar day of month<br/>BMS BusinessMonthBegin First weekday of month<br/>W-MON, W-TUE, ... Week Weekly on given day of week (MON, TUE, WED, THU,<br/></p>
<p>FRI, SAT, or SUN)<br/>WOM-1MON, WOM-2MON, ... WeekOfMonth Generate weekly dates in the first, second, third, or<br/></p>
<p>fourth week of the month (e.g., WOM-3FRI for the<br/>third Friday of each month)<br/></p>
<p>Q-JAN, Q-FEB, ... QuarterEnd Quarterly dates anchored on last calendar day of each<br/>month, for year ending in indicated month (JAN, FEB,<br/>MAR, APR, MAY, JUN, JUL, AUG, SEP, OCT, NOV, or DEC)<br/></p>
<p>BQ-JAN, BQ-FEB, ... BusinessQuarterEnd Quarterly dates anchored on last weekday day of each<br/>month, for year ending in indicated month<br/></p>
<p>QS-JAN, QS-FEB, ... QuarterBegin Quarterly dates anchored on first calendar day of each<br/>month, for year ending in indicated month<br/></p>
<p>BQS-JAN, BQS-FEB, ... BusinessQuarterBegin Quarterly dates anchored on first weekday day of each<br/>month, for year ending in indicated month<br/></p>
<p>A-JAN, A-FEB, ... YearEnd Annual dates anchored on last calendar day of given<br/>month (JAN, FEB, MAR, APR, MAY, JUN, JUL, AUG, SEP,<br/>OCT, NOV, or DEC)<br/></p>
<p>BA-JAN, BA-FEB, ... BusinessYearEnd Annual dates anchored on last weekday of given<br/>month<br/></p>
<p>AS-JAN, AS-FEB, ... YearBegin Annual dates anchored on first day of given month<br/>BAS-JAN, BAS-FEB, ... BusinessYearBegin Annual dates anchored on first weekday of given<br/></p>
<p>month<br/></p>
<p>11.3 Date Ranges, Frequencies, and Shifting | 329</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>date_range by default preserves the time (if any) of the start or end timestamp:<br/>In [79]: pd.date_range('2012-05-02 12:56:31', periods=5)<br/>Out[79]: <br/>DatetimeIndex(['2012-05-02 12:56:31', '2012-05-03 12:56:31',<br/>               '2012-05-04 12:56:31', '2012-05-05 12:56:31',<br/>               '2012-05-06 12:56:31'],<br/>              dtype='datetime64[ns]', freq='D')<br/></p>
<p>Sometimes you will have start or end dates with time information but want to gener&#8208;<br/>ate a set of timestamps <i>normalized</i> to midnight as a convention. To do this, there is a<br/>normalize option:<br/></p>
<p>In [80]: pd.date_range('2012-05-02 12:56:31', periods=5, normalize=True)<br/>Out[80]: <br/>DatetimeIndex(['2012-05-02', '2012-05-03', '2012-05-04', '2012-05-05',<br/>               '2012-05-06'],<br/>              dtype='datetime64[ns]', freq='D')<br/></p>
<p>Frequencies and Date Offsets<br/>Frequencies in pandas are composed of a <i>base frequency</i> and a multiplier. Base fre&#8208;<br/>quencies are typically referred to by a string alias, like 'M' for monthly or 'H' for<br/>hourly. For each base frequency, there is an object defined generally referred to as a<br/><i>date offset. For example, hourly frequency can be represented with the Hour class:<br/></i></p>
<p>In [81]: <b>from</b> <b>pandas.tseries.offsets</b> <b>import</b> Hour, Minute<br/></p>
<p>In [82]: hour = Hour()<br/></p>
<p>In [83]: hour<br/>Out[83]: &lt;Hour&gt;<br/></p>
<p>You can define a multiple of an offset by passing an integer:<br/>In [84]: four_hours = Hour(4)<br/></p>
<p>In [85]: four_hours<br/>Out[85]: &lt;4 * Hours&gt;<br/></p>
<p>In most applications, you would never need to explicitly create one of these objects,<br/>instead using a string alias like 'H' or '4H'. Putting an integer before the base fre&#8208;<br/>quency creates a multiple:<br/></p>
<p>In [86]: pd.date_range('2000-01-01', '2000-01-03 23:59', freq='4h')<br/>Out[86]: <br/>DatetimeIndex(['2000-01-01 00:00:00', '2000-01-01 04:00:00',<br/>               '2000-01-01 08:00:00', '2000-01-01 12:00:00',<br/>               '2000-01-01 16:00:00', '2000-01-01 20:00:00',<br/>               '2000-01-02 00:00:00', '2000-01-02 04:00:00',<br/>               '2000-01-02 08:00:00', '2000-01-02 12:00:00',<br/>               '2000-01-02 16:00:00', '2000-01-02 20:00:00',<br/></p>
<p>330 | Chapter 11: Time Series</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>               '2000-01-03 00:00:00', '2000-01-03 04:00:00',<br/>               '2000-01-03 08:00:00', '2000-01-03 12:00:00',<br/>               '2000-01-03 16:00:00', '2000-01-03 20:00:00'],<br/>              dtype='datetime64[ns]', freq='4H')<br/></p>
<p>Many offsets can be combined together by addition:<br/>In [87]: Hour(2) + Minute(30)<br/>Out[87]: &lt;150 * Minutes&gt;<br/></p>
<p>Similarly, you can pass frequency strings, like '1h30min', that will effectively be<br/>parsed to the same expression:<br/></p>
<p>In [88]: pd.date_range('2000-01-01', periods=10, freq='1h30min')<br/>Out[88]: <br/>DatetimeIndex(['2000-01-01 00:00:00', '2000-01-01 01:30:00',<br/>               '2000-01-01 03:00:00', '2000-01-01 04:30:00',<br/>               '2000-01-01 06:00:00', '2000-01-01 07:30:00',<br/>               '2000-01-01 09:00:00', '2000-01-01 10:30:00',<br/>               '2000-01-01 12:00:00', '2000-01-01 13:30:00'],<br/>              dtype='datetime64[ns]', freq='90T')<br/></p>
<p>Some frequencies describe points in time that are not evenly spaced. For example, 'M'<br/>(calendar month end) and 'BM' (last business/weekday of month) depend on the<br/>number of days in a month and, in the latter case, whether the month ends on a<br/>weekend or not. We refer to these as <i>anchored</i> offsets.<br/>Refer back to Table 11-4 for a listing of frequency codes and date offset classes avail&#8208;<br/>able in pandas.<br/></p>
<p>Users can define their own custom frequency classes to provide<br/>date logic not available in pandas, though the full details of that are<br/>outside the scope of this book.<br/></p>
<p>Week of month dates<br/>One useful frequency class is &#8220;week of month,&#8221; starting with WOM. This enables you to<br/>get dates like the third Friday of each month:<br/></p>
<p>In [89]: rng = pd.date_range('2012-01-01', '2012-09-01', freq='WOM-3FRI')<br/></p>
<p>In [90]: list(rng)<br/>Out[90]: <br/>[Timestamp('2012-01-20 00:00:00', freq='WOM-3FRI'),<br/> Timestamp('2012-02-17 00:00:00', freq='WOM-3FRI'),<br/> Timestamp('2012-03-16 00:00:00', freq='WOM-3FRI'),<br/> Timestamp('2012-04-20 00:00:00', freq='WOM-3FRI'),<br/> Timestamp('2012-05-18 00:00:00', freq='WOM-3FRI'),<br/> Timestamp('2012-06-15 00:00:00', freq='WOM-3FRI'),<br/></p>
<p>11.3 Date Ranges, Frequencies, and Shifting | 331</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p> Timestamp('2012-07-20 00:00:00', freq='WOM-3FRI'),<br/> Timestamp('2012-08-17 00:00:00', freq='WOM-3FRI')]<br/></p>
<p>Shifting (Leading and Lagging) Data<br/>&#8220;Shifting&#8221; refers to moving data backward and forward through time. Both Series and<br/>DataFrame have a shift method for doing naive shifts forward or backward, leaving<br/>the index unmodified:<br/></p>
<p>In [91]: ts = pd.Series(np.random.randn(4),<br/>   ....:                index=pd.date_range('1/1/2000', periods=4, freq='M'))<br/></p>
<p>In [92]: ts<br/>Out[92]: <br/>2000-01-31   -0.066748<br/>2000-02-29    0.838639<br/>2000-03-31   -0.117388<br/>2000-04-30   -0.517795<br/>Freq: M, dtype: float64<br/></p>
<p>In [93]: ts.shift(2)<br/>Out[93]: <br/>2000-01-31         NaN<br/>2000-02-29         NaN<br/>2000-03-31   -0.066748<br/>2000-04-30    0.838639<br/>Freq: M, dtype: float64<br/></p>
<p>In [94]: ts.shift(-2)<br/>Out[94]: <br/>2000-01-31   -0.117388<br/>2000-02-29   -0.517795<br/>2000-03-31         NaN<br/>2000-04-30         NaN<br/>Freq: M, dtype: float64<br/></p>
<p>When we shift like this, missing data is introduced either at the start or the end of the<br/>time series.<br/>A common use of shift is computing percent changes in a time series or multiple<br/>time series as DataFrame columns. This is expressed as:<br/></p>
<p>ts / ts.shift(1) - 1<br/></p>
<p>Because naive shifts leave the index unmodified, some data is discarded. Thus if the<br/>frequency is known, it can be passed to shift to advance the timestamps instead of<br/>simply the data:<br/></p>
<p>In [95]: ts.shift(2, freq='M')<br/>Out[95]: <br/>2000-03-31   -0.066748<br/>2000-04-30    0.838639<br/></p>
<p>332 | Chapter 11: Time Series</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>2000-05-31   -0.117388<br/>2000-06-30   -0.517795<br/>Freq: M, dtype: float64<br/></p>
<p>Other frequencies can be passed, too, giving you some flexibility in how to lead and<br/>lag the data:<br/></p>
<p>In [96]: ts.shift(3, freq='D')<br/>Out[96]: <br/>2000-02-03   -0.066748<br/>2000-03-03    0.838639<br/>2000-04-03   -0.117388<br/>2000-05-03   -0.517795<br/>dtype: float64<br/></p>
<p>In [97]: ts.shift(1, freq='90T')<br/>Out[97]: <br/>2000-01-31 01:30:00   -0.066748<br/>2000-02-29 01:30:00    0.838639<br/>2000-03-31 01:30:00   -0.117388<br/>2000-04-30 01:30:00   -0.517795<br/>Freq: M, dtype: float64<br/></p>
<p>The T here stands for minutes.<br/></p>
<p>Shifting dates with offsets<br/>The pandas date offsets can also be used with datetime or Timestamp objects:<br/></p>
<p>In [98]: <b>from</b> <b>pandas.tseries.offsets</b> <b>import</b> Day, MonthEnd<br/></p>
<p>In [99]: now = datetime(2011, 11, 17)<br/></p>
<p>In [100]: now + 3 * Day()<br/>Out[100]: Timestamp('2011-11-20 00:00:00')<br/></p>
<p>If you add an anchored offset like MonthEnd, the first increment will &#8220;roll forward&#8221; a<br/>date to the next date according to the frequency rule:<br/></p>
<p>In [101]: now + MonthEnd()<br/>Out[101]: Timestamp('2011-11-30 00:00:00')<br/></p>
<p>In [102]: now + MonthEnd(2)<br/>Out[102]: Timestamp('2011-12-31 00:00:00')<br/></p>
<p>Anchored offsets can explicitly &#8220;roll&#8221; dates forward or backward by simply using their<br/>rollforward and rollback methods, respectively:<br/></p>
<p>In [103]: offset = MonthEnd()<br/></p>
<p>In [104]: offset.rollforward(now)<br/>Out[104]: Timestamp('2011-11-30 00:00:00')<br/></p>
<p>11.3 Date Ranges, Frequencies, and Shifting | 333</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In [105]: offset.rollback(now)<br/>Out[105]: Timestamp('2011-10-31 00:00:00')<br/></p>
<p>A creative use of date offsets is to use these methods with groupby:<br/>In [106]: ts = pd.Series(np.random.randn(20),<br/>   .....:                index=pd.date_range('1/15/2000', periods=20, freq='4d'))<br/></p>
<p>In [107]: ts<br/>Out[107]: <br/>2000-01-15   -0.116696<br/>2000-01-19    2.389645<br/>2000-01-23   -0.932454<br/>2000-01-27   -0.229331<br/>2000-01-31   -1.140330<br/>2000-02-04    0.439920<br/>2000-02-08   -0.823758<br/>2000-02-12   -0.520930<br/>2000-02-16    0.350282<br/>2000-02-20    0.204395<br/>2000-02-24    0.133445<br/>2000-02-28    0.327905<br/>2000-03-03    0.072153<br/>2000-03-07    0.131678<br/>2000-03-11   -1.297459<br/>2000-03-15    0.997747<br/>2000-03-19    0.870955<br/>2000-03-23   -0.991253<br/>2000-03-27    0.151699<br/>2000-03-31    1.266151<br/>Freq: 4D, dtype: float64<br/></p>
<p>In [108]: ts.groupby(offset.rollforward).mean()<br/>Out[108]: <br/>2000-01-31   -0.005833<br/>2000-02-29    0.015894<br/>2000-03-31    0.150209<br/>dtype: float64<br/></p>
<p>Of course, an easier and faster way to do this is using resample (we&#8217;ll discuss this in<br/>much more depth in Section 11.6, &#8220;Resampling and Frequency Conversion,&#8221; on page<br/>348):<br/></p>
<p>In [109]: ts.resample('M').mean()<br/>Out[109]: <br/>2000-01-31   -0.005833<br/>2000-02-29    0.015894<br/>2000-03-31    0.150209<br/>Freq: M, dtype: float64<br/></p>
<p>334 | Chapter 11: Time Series</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>11.4 Time Zone Handling<br/>Working with time zones is generally considered one of the most unpleasant parts of<br/>time series manipulation. As a result, many time series users choose to work with<br/>time series in <i>coordinated universal time</i> or <i>UTC</i>, which is the successor to Greenwich<br/>Mean Time and is the current international standard. Time zones are expressed as<br/>offsets from UTC; for example, New York is four hours behind UTC during daylight<br/>saving time and five hours behind the rest of the year.<br/>In Python, time zone information comes from the third-party pytz library (installa&#8208;<br/>ble with pip or conda), which exposes the <i>Olson database</i>, a compilation of world<br/>time zone information. This is especially important for historical data because the<br/>daylight saving time (DST) transition dates (and even UTC offsets) have been<br/>changed numerous times depending on the whims of local governments. In the Uni&#8208;<br/>ted States, the DST transition times have been changed many times since 1900!<br/>For detailed information about the pytz library, you&#8217;ll need to look at that library&#8217;s<br/>documentation. As far as this book is concerned, pandas wraps pytz&#8217;s functionality so<br/>you can ignore its API outside of the time zone names. Time zone names can be<br/>found interactively and in the docs:<br/></p>
<p>In [110]: <b>import</b> <b>pytz<br/></b></p>
<p>In [111]: pytz.common_timezones[-5:]<br/>Out[111]: ['US/Eastern', 'US/Hawaii', 'US/Mountain', 'US/Pacific', 'UTC']<br/></p>
<p>To get a time zone object from pytz, use pytz.timezone:<br/>In [112]: tz = pytz.timezone('America/New_York')<br/></p>
<p>In [113]: tz<br/>Out[113]: &lt;DstTzInfo 'America/New_York' LMT-1 day, 19:04:00 STD&gt;<br/></p>
<p>Methods in pandas will accept either time zone names or these objects.<br/></p>
<p>Time Zone Localization and Conversion<br/>By default, time series in pandas are <i>time zone naive</i>. For example, consider the fol&#8208;<br/>lowing time series:<br/></p>
<p>In [114]: rng = pd.date_range('3/9/2012 9:30', periods=6, freq='D')<br/></p>
<p>In [115]: ts = pd.Series(np.random.randn(len(rng)), index=rng)<br/></p>
<p>In [116]: ts<br/>Out[116]: <br/>2012-03-09 09:30:00   -0.202469<br/>2012-03-10 09:30:00    0.050718<br/>2012-03-11 09:30:00    0.639869<br/>2012-03-12 09:30:00    0.597594<br/></p>
<p>11.4 Time Zone Handling | 335</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>2012-03-13 09:30:00   -0.797246<br/>2012-03-14 09:30:00    0.472879<br/>Freq: D, dtype: float64<br/></p>
<p>The index&#8217;s tz field is None:<br/>In [117]: <b>print</b>(ts.index.tz)<br/>None<br/></p>
<p>Date ranges can be generated with a time zone set:<br/>In [118]: pd.date_range('3/9/2012 9:30', periods=10, freq='D', tz='UTC')<br/>Out[118]: <br/>DatetimeIndex(['2012-03-09 09:30:00+00:00', '2012-03-10 09:30:00+00:00',<br/>               '2012-03-11 09:30:00+00:00', '2012-03-12 09:30:00+00:00',<br/>               '2012-03-13 09:30:00+00:00', '2012-03-14 09:30:00+00:00',<br/>               '2012-03-15 09:30:00+00:00', '2012-03-16 09:30:00+00:00',<br/>               '2012-03-17 09:30:00+00:00', '2012-03-18 09:30:00+00:00'],<br/>              dtype='datetime64[ns, UTC]', freq='D')<br/></p>
<p>Conversion from naive to <i>localized</i> is handled by the tz_localize method:<br/>In [119]: ts<br/>Out[119]: <br/>2012-03-09 09:30:00   -0.202469<br/>2012-03-10 09:30:00    0.050718<br/>2012-03-11 09:30:00    0.639869<br/>2012-03-12 09:30:00    0.597594<br/>2012-03-13 09:30:00   -0.797246<br/>2012-03-14 09:30:00    0.472879<br/>Freq: D, dtype: float64<br/></p>
<p>In [120]: ts_utc = ts.tz_localize('UTC')<br/></p>
<p>In [121]: ts_utc<br/>Out[121]: <br/>2012-03-09 09:30:00+00:00   -0.202469<br/>2012-03-10 09:30:00+00:00    0.050718<br/>2012-03-11 09:30:00+00:00    0.639869<br/>2012-03-12 09:30:00+00:00    0.597594<br/>2012-03-13 09:30:00+00:00   -0.797246<br/>2012-03-14 09:30:00+00:00    0.472879<br/>Freq: D, dtype: float64<br/></p>
<p>In [122]: ts_utc.index<br/>Out[122]: <br/>DatetimeIndex(['2012-03-09 09:30:00+00:00', '2012-03-10 09:30:00+00:00',<br/>               '2012-03-11 09:30:00+00:00', '2012-03-12 09:30:00+00:00',<br/>               '2012-03-13 09:30:00+00:00', '2012-03-14 09:30:00+00:00'],<br/>              dtype='datetime64[ns, UTC]', freq='D')<br/></p>
<p>Once a time series has been localized to a particular time zone, it can be converted to<br/>another time zone with tz_convert:<br/></p>
<p>336 | Chapter 11: Time Series</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In [123]: ts_utc.tz_convert('America/New_York')<br/>Out[123]: <br/>2012-03-09 04:30:00-05:00   -0.202469<br/>2012-03-10 04:30:00-05:00    0.050718<br/>2012-03-11 05:30:00-04:00    0.639869<br/>2012-03-12 05:30:00-04:00    0.597594<br/>2012-03-13 05:30:00-04:00   -0.797246<br/>2012-03-14 05:30:00-04:00    0.472879<br/>Freq: D, dtype: float64<br/></p>
<p>In the case of the preceding time series, which straddles a DST transition in the Amer<br/>ica/New_York time zone, we could localize to EST and convert to, say, UTC or Berlin<br/>time:<br/></p>
<p>In [124]: ts_eastern = ts.tz_localize('America/New_York')<br/></p>
<p>In [125]: ts_eastern.tz_convert('UTC')<br/>Out[125]: <br/>2012-03-09 14:30:00+00:00   -0.202469<br/>2012-03-10 14:30:00+00:00    0.050718<br/>2012-03-11 13:30:00+00:00    0.639869<br/>2012-03-12 13:30:00+00:00    0.597594<br/>2012-03-13 13:30:00+00:00   -0.797246<br/>2012-03-14 13:30:00+00:00    0.472879<br/>Freq: D, dtype: float64<br/></p>
<p>In [126]: ts_eastern.tz_convert('Europe/Berlin')<br/>Out[126]: <br/>2012-03-09 15:30:00+01:00   -0.202469<br/>2012-03-10 15:30:00+01:00    0.050718<br/>2012-03-11 14:30:00+01:00    0.639869<br/>2012-03-12 14:30:00+01:00    0.597594<br/>2012-03-13 14:30:00+01:00   -0.797246<br/>2012-03-14 14:30:00+01:00    0.472879<br/>Freq: D, dtype: float64<br/></p>
<p>tz_localize and tz_convert are also instance methods on DatetimeIndex:<br/>In [127]: ts.index.tz_localize('Asia/Shanghai')<br/>Out[127]: <br/>DatetimeIndex(['2012-03-09 09:30:00+08:00', '2012-03-10 09:30:00+08:00',<br/>               '2012-03-11 09:30:00+08:00', '2012-03-12 09:30:00+08:00',<br/>               '2012-03-13 09:30:00+08:00', '2012-03-14 09:30:00+08:00'],<br/>              dtype='datetime64[ns, Asia/Shanghai]', freq='D')<br/></p>
<p>Localizing naive timestamps also checks for ambiguous or non-<br/>existent times around daylight saving time transitions.<br/></p>
<p>11.4 Time Zone Handling | 337</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Operations with Time Zone&#8722;Aware Timestamp Objects<br/>Similar to time series and date ranges, individual Timestamp objects similarly can be<br/>localized from naive to time zone&#8211;aware and converted from one time zone to<br/>another:<br/></p>
<p>In [128]: stamp = pd.Timestamp('2011-03-12 04:00')<br/></p>
<p>In [129]: stamp_utc = stamp.tz_localize('utc')<br/></p>
<p>In [130]: stamp_utc.tz_convert('America/New_York')<br/>Out[130]: Timestamp('2011-03-11 23:00:00-0500', tz='America/New_York')<br/></p>
<p>You can also pass a time zone when creating the Timestamp:<br/>In [131]: stamp_moscow = pd.Timestamp('2011-03-12 04:00', tz='Europe/Moscow')<br/></p>
<p>In [132]: stamp_moscow<br/>Out[132]: Timestamp('2011-03-12 04:00:00+0300', tz='Europe/Moscow')<br/></p>
<p>Time zone&#8211;aware Timestamp objects internally store a UTC timestamp value as nano&#8208;<br/>seconds since the Unix epoch (January 1, 1970); this UTC value is invariant between<br/>time zone conversions:<br/></p>
<p>In [133]: stamp_utc.value<br/>Out[133]: 1299902400000000000<br/></p>
<p>In [134]: stamp_utc.tz_convert('America/New_York').value<br/>Out[134]: 1299902400000000000<br/></p>
<p>When performing time arithmetic using pandas&#8217;s DateOffset objects, pandas<br/>respects daylight saving time transitions where possible. Here we construct time&#8208;<br/>stamps that occur right before DST transitions (forward and backward). First, 30<br/>minutes before transitioning to DST:<br/></p>
<p>In [135]: <b>from</b> <b>pandas.tseries.offsets</b> <b>import</b> Hour<br/></p>
<p>In [136]: stamp = pd.Timestamp('2012-03-12 01:30', tz='US/Eastern')<br/></p>
<p>In [137]: stamp<br/>Out[137]: Timestamp('2012-03-12 01:30:00-0400', tz='US/Eastern')<br/></p>
<p>In [138]: stamp + Hour()<br/>Out[138]: Timestamp('2012-03-12 02:30:00-0400', tz='US/Eastern')<br/></p>
<p>Then, 90 minutes before transitioning out of DST:<br/>In [139]: stamp = pd.Timestamp('2012-11-04 00:30', tz='US/Eastern')<br/></p>
<p>In [140]: stamp<br/>Out[140]: Timestamp('2012-11-04 00:30:00-0400', tz='US/Eastern')<br/></p>
<p>338 | Chapter 11: Time Series</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In [141]: stamp + 2 * Hour()<br/>Out[141]: Timestamp('2012-11-04 01:30:00-0500', tz='US/Eastern')<br/></p>
<p>Operations Between Different Time Zones<br/>If two time series with different time zones are combined, the result will be UTC.<br/>Since the timestamps are stored under the hood in UTC, this is a straightforward<br/>operation and requires no conversion to happen:<br/></p>
<p>In [142]: rng = pd.date_range('3/7/2012 9:30', periods=10, freq='B')<br/></p>
<p>In [143]: ts = pd.Series(np.random.randn(len(rng)), index=rng)<br/></p>
<p>In [144]: ts<br/>Out[144]: <br/>2012-03-07 09:30:00    0.522356<br/>2012-03-08 09:30:00   -0.546348<br/>2012-03-09 09:30:00   -0.733537<br/>2012-03-12 09:30:00    1.302736<br/>2012-03-13 09:30:00    0.022199<br/>2012-03-14 09:30:00    0.364287<br/>2012-03-15 09:30:00   -0.922839<br/>2012-03-16 09:30:00    0.312656<br/>2012-03-19 09:30:00   -1.128497<br/>2012-03-20 09:30:00   -0.333488<br/>Freq: B, dtype: float64<br/></p>
<p>In [145]: ts1 = ts[:7].tz_localize('Europe/London')<br/></p>
<p>In [146]: ts2 = ts1[2:].tz_convert('Europe/Moscow')<br/></p>
<p>In [147]: result = ts1 + ts2<br/></p>
<p>In [148]: result.index<br/>Out[148]: <br/>DatetimeIndex(['2012-03-07 09:30:00+00:00', '2012-03-08 09:30:00+00:00',<br/>               '2012-03-09 09:30:00+00:00', '2012-03-12 09:30:00+00:00',<br/>               '2012-03-13 09:30:00+00:00', '2012-03-14 09:30:00+00:00',<br/>               '2012-03-15 09:30:00+00:00'],<br/>              dtype='datetime64[ns, UTC]', freq='B')<br/></p>
<p>11.5 Periods and Period Arithmetic<br/><i>Periods</i> represent timespans, like days, months, quarters, or years. The Period class <br/>represents this data type, requiring a string or integer and a frequency from<br/>Table 11-4:<br/></p>
<p>In [149]: p = pd.Period(2007, freq='A-DEC')<br/></p>
<p>In [150]: p<br/>Out[150]: Period('2007', 'A-DEC')<br/></p>
<p>11.5 Periods and Period Arithmetic | 339</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In this case, the Period object represents the full timespan from January 1, 2007, to<br/>December 31, 2007, inclusive. Conveniently, adding and subtracting integers from<br/>periods has the effect of shifting by their frequency:<br/></p>
<p>In [151]: p + 5<br/>Out[151]: Period('2012', 'A-DEC')<br/></p>
<p>In [152]: p - 2<br/>Out[152]: Period('2005', 'A-DEC')<br/></p>
<p>If two periods have the same frequency, their difference is the number of units<br/>between them:<br/></p>
<p>In [153]: pd.Period('2014', freq='A-DEC') - p<br/>Out[153]: 7<br/></p>
<p>Regular ranges of periods can be constructed with the period_range function:<br/>In [154]: rng = pd.period_range('2000-01-01', '2000-06-30', freq='M')<br/></p>
<p>In [155]: rng<br/>Out[155]: PeriodIndex(['2000-01', '2000-02', '2000-03', '2000-04', '2000-05', '20<br/>00-06'], dtype='period[M]', freq='M')<br/></p>
<p>The PeriodIndex class stores a sequence of periods and can serve as an axis index in<br/>any pandas data structure:<br/></p>
<p>In [156]: pd.Series(np.random.randn(6), index=rng)<br/>Out[156]: <br/>2000-01   -0.514551<br/>2000-02   -0.559782<br/>2000-03   -0.783408<br/>2000-04   -1.797685<br/>2000-05   -0.172670<br/>2000-06    0.680215<br/>Freq: M, dtype: float64<br/></p>
<p>If you have an array of strings, you can also use the PeriodIndex class:<br/>In [157]: values = ['2001Q3', '2002Q2', '2003Q1']<br/></p>
<p>In [158]: index = pd.PeriodIndex(values, freq='Q-DEC')<br/></p>
<p>In [159]: index<br/>Out[159]: PeriodIndex(['2001Q3', '2002Q2', '2003Q1'], dtype='period[Q-DEC]', freq<br/>='Q-DEC')<br/></p>
<p>Period Frequency Conversion<br/>Periods and PeriodIndex objects can be converted to another frequency with their<br/>asfreq method. As an example, suppose we had an annual period and wanted to<br/></p>
<p>340 | Chapter 11: Time Series</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>convert it into a monthly period either at the start or end of the year. This is fairly<br/>straightforward:<br/></p>
<p>In [160]: p = pd.Period('2007', freq='A-DEC')<br/></p>
<p>In [161]: p<br/>Out[161]: Period('2007', 'A-DEC')<br/></p>
<p>In [162]: p.asfreq('M', how='start')<br/>Out[162]: Period('2007-01', 'M')<br/></p>
<p>In [163]: p.asfreq('M', how='end')<br/>Out[163]: Period('2007-12', 'M')<br/></p>
<p>You can think of Period('2007', 'A-DEC') as being a sort of cursor pointing to a<br/>span of time, subdivided by monthly periods. See Figure 11-1 for an illustration of<br/>this. For a fiscal year ending on a month other than December, the corresponding<br/>monthly subperiods are different:<br/></p>
<p>In [164]: p = pd.Period('2007', freq='A-JUN')<br/></p>
<p>In [165]: p<br/>Out[165]: Period('2007', 'A-JUN')<br/></p>
<p>In [166]: p.asfreq('M', 'start')<br/>Out[166]: Period('2006-07', 'M')<br/></p>
<p>In [167]: p.asfreq('M', 'end')<br/>Out[167]: Period('2007-06', 'M')<br/></p>
<p><i>Figure 11-1. Period frequency conversion illustration<br/></i>When you are converting from high to low frequency, pandas determines the super&#8208;<br/>period depending on where the subperiod &#8220;belongs.&#8221; For example, in A-JUN fre&#8208;<br/>quency, the month Aug-2007 is actually part of the 2008 period:<br/></p>
<p>In [168]: p = pd.Period('Aug-2007', 'M')<br/></p>
<p>In [169]: p.asfreq('A-JUN')<br/>Out[169]: Period('2008', 'A-JUN')<br/></p>
<p>11.5 Periods and Period Arithmetic | 341</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Whole PeriodIndex objects or time series can be similarly converted with the same<br/>semantics:<br/></p>
<p>In [170]: rng = pd.period_range('2006', '2009', freq='A-DEC')<br/></p>
<p>In [171]: ts = pd.Series(np.random.randn(len(rng)), index=rng)<br/></p>
<p>In [172]: ts<br/>Out[172]: <br/>2006    1.607578<br/>2007    0.200381<br/>2008   -0.834068<br/>2009   -0.302988<br/>Freq: A-DEC, dtype: float64<br/></p>
<p>In [173]: ts.asfreq('M', how='start')<br/>Out[173]: <br/>2006-01    1.607578<br/>2007-01    0.200381<br/>2008-01   -0.834068<br/>2009-01   -0.302988<br/>Freq: M, dtype: float64<br/></p>
<p>Here, the annual periods are replaced with monthly periods corresponding to the first<br/>month falling within each annual period. If we instead wanted the last business day of<br/>each year, we can use the 'B' frequency and indicate that we want the end of the<br/>period:<br/></p>
<p>In [174]: ts.asfreq('B', how='end')<br/>Out[174]: <br/>2006-12-29    1.607578<br/>2007-12-31    0.200381<br/>2008-12-31   -0.834068<br/>2009-12-31   -0.302988<br/>Freq: B, dtype: float64<br/></p>
<p>Quarterly Period Frequencies<br/>Quarterly data is standard in accounting, finance, and other fields. Much quarterly<br/>data is reported relative to a fiscal year end, typically the last calendar or business day<br/>of one of the 12 months of the year. Thus, the period 2012Q4 has a different meaning<br/>depending on fiscal year end. pandas supports all 12 possible quarterly frequencies as<br/>Q-JAN through Q-DEC:<br/></p>
<p>In [175]: p = pd.Period('2012Q4', freq='Q-JAN')<br/></p>
<p>In [176]: p<br/>Out[176]: Period('2012Q4', 'Q-JAN')<br/></p>
<p>342 | Chapter 11: Time Series</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In the case of fiscal year ending in January, 2012Q4 runs from November through Jan&#8208;<br/>uary, which you can check by converting to daily frequency. See Figure 11-2 for an<br/>illustration.<br/></p>
<p><i>Figure 11-2. Different quarterly frequency conventions<br/></i>In [177]: p.asfreq('D', 'start')<br/>Out[177]: Period('2011-11-01', 'D')<br/></p>
<p>In [178]: p.asfreq('D', 'end')<br/>Out[178]: Period('2012-01-31', 'D')<br/></p>
<p>Thus, it&#8217;s possible to do easy period arithmetic; for example, to get the timestamp at 4<br/>PM on the second-to-last business day of the quarter, you could do:<br/></p>
<p>In [179]: p4pm = (p.asfreq('B', 'e') - 1).asfreq('T', 's') + 16 * 60<br/></p>
<p>In [180]: p4pm<br/>Out[180]: Period('2012-01-30 16:00', 'T')<br/></p>
<p>In [181]: p4pm.to_timestamp()<br/>Out[181]: Timestamp('2012-01-30 16:00:00')<br/></p>
<p>You can generate quarterly ranges using period_range. Arithmetic is identical, too:<br/>In [182]: rng = pd.period_range('2011Q3', '2012Q4', freq='Q-JAN')<br/></p>
<p>In [183]: ts = pd.Series(np.arange(len(rng)), index=rng)<br/></p>
<p>In [184]: ts<br/>Out[184]: <br/>2011Q3    0<br/>2011Q4    1<br/>2012Q1    2<br/>2012Q2    3<br/>2012Q3    4<br/>2012Q4    5<br/>Freq: Q-JAN, dtype: int64<br/></p>
<p>In [185]: new_rng = (rng.asfreq('B', 'e') - 1).asfreq('T', 's') + 16 * 60<br/></p>
<p>In [186]: ts.index = new_rng.to_timestamp()<br/></p>
<p>11.5 Periods and Period Arithmetic | 343</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In [187]: ts<br/>Out[187]: <br/>2010-10-28 16:00:00    0<br/>2011-01-28 16:00:00    1<br/>2011-04-28 16:00:00    2<br/>2011-07-28 16:00:00    3<br/>2011-10-28 16:00:00    4<br/>2012-01-30 16:00:00    5<br/>dtype: int64<br/></p>
<p>Converting Timestamps to Periods (and Back)<br/>Series and DataFrame objects indexed by timestamps can be converted to periods<br/>with the to_period method:<br/></p>
<p>In [188]: rng = pd.date_range('2000-01-01', periods=3, freq='M')<br/></p>
<p>In [189]: ts = pd.Series(np.random.randn(3), index=rng)<br/></p>
<p>In [190]: ts<br/>Out[190]: <br/>2000-01-31    1.663261<br/>2000-02-29   -0.996206<br/>2000-03-31    1.521760<br/>Freq: M, dtype: float64<br/></p>
<p>In [191]: pts = ts.to_period()<br/></p>
<p>In [192]: pts<br/>Out[192]: <br/>2000-01    1.663261<br/>2000-02   -0.996206<br/>2000-03    1.521760<br/>Freq: M, dtype: float64<br/></p>
<p>Since periods refer to non-overlapping timespans, a timestamp can only belong to a<br/>single period for a given frequency. While the frequency of the new PeriodIndex is<br/>inferred from the timestamps by default, you can specify any frequency you want.<br/>There is also no problem with having duplicate periods in the result:<br/></p>
<p>In [193]: rng = pd.date_range('1/29/2000', periods=6, freq='D')<br/></p>
<p>In [194]: ts2 = pd.Series(np.random.randn(6), index=rng)<br/></p>
<p>In [195]: ts2<br/>Out[195]: <br/>2000-01-29    0.244175<br/>2000-01-30    0.423331<br/>2000-01-31   -0.654040<br/>2000-02-01    2.089154<br/>2000-02-02   -0.060220<br/></p>
<p>344 | Chapter 11: Time Series</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>2000-02-03   -0.167933<br/>Freq: D, dtype: float64<br/></p>
<p>In [196]: ts2.to_period('M')<br/>Out[196]: <br/>2000-01    0.244175<br/>2000-01    0.423331<br/>2000-01   -0.654040<br/>2000-02    2.089154<br/>2000-02   -0.060220<br/>2000-02   -0.167933<br/>Freq: M, dtype: float64<br/></p>
<p>To convert back to timestamps, use to_timestamp:<br/>In [197]: pts = ts2.to_period()<br/></p>
<p>In [198]: pts<br/>Out[198]: <br/>2000-01-29    0.244175<br/>2000-01-30    0.423331<br/>2000-01-31   -0.654040<br/>2000-02-01    2.089154<br/>2000-02-02   -0.060220<br/>2000-02-03   -0.167933<br/>Freq: D, dtype: float64<br/></p>
<p>In [199]: pts.to_timestamp(how='end')<br/>Out[199]: <br/>2000-01-29    0.244175<br/>2000-01-30    0.423331<br/>2000-01-31   -0.654040<br/>2000-02-01    2.089154<br/>2000-02-02   -0.060220<br/>2000-02-03   -0.167933<br/>Freq: D, dtype: float64<br/></p>
<p>Creating a PeriodIndex from Arrays<br/>Fixed frequency datasets are sometimes stored with timespan information spread<br/>across multiple columns. For example, in this macroeconomic dataset, the year and<br/>quarter are in different columns:<br/></p>
<p>In [200]: data = pd.read_csv('examples/macrodata.csv')<br/></p>
<p>In [201]: data.head(5)<br/>Out[201]: <br/>     year  quarter   realgdp  realcons  realinv  realgovt  realdpi    cpi  \<br/>0  1959.0      1.0  2710.349    1707.4  286.898   470.045   1886.9  28.98   <br/>1  1959.0      2.0  2778.801    1733.7  310.859   481.301   1919.7  29.15   <br/>2  1959.0      3.0  2775.488    1751.8  289.226   491.260   1916.4  29.35   <br/>3  1959.0      4.0  2785.204    1753.7  299.356   484.052   1931.3  29.37   <br/></p>
<p>11.5 Periods and Period Arithmetic | 345</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>4  1960.0      1.0  2847.699    1770.5  331.722   462.199   1955.5  29.54   <br/>      m1  tbilrate  unemp      pop  infl  realint  <br/>0  139.7      2.82    5.8  177.146  0.00     0.00  <br/>1  141.7      3.08    5.1  177.830  2.34     0.74  <br/>2  140.5      3.82    5.3  178.657  2.74     1.09  <br/>3  140.0      4.33    5.6  179.386  0.27     4.06  <br/>4  139.6      3.50    5.2  180.007  2.31     1.19  <br/></p>
<p>In [202]: data.year<br/>Out[202]: <br/>0      1959.0<br/>1      1959.0<br/>2      1959.0<br/>3      1959.0<br/>4      1960.0<br/>5      1960.0<br/>6      1960.0<br/>7      1960.0<br/>8      1961.0<br/>9      1961.0<br/>        ...  <br/>193    2007.0<br/>194    2007.0<br/>195    2007.0<br/>196    2008.0<br/>197    2008.0<br/>198    2008.0<br/>199    2008.0<br/>200    2009.0<br/>201    2009.0<br/>202    2009.0<br/>Name: year, Length: 203, dtype: float64<br/></p>
<p>In [203]: data.quarter<br/>Out[203]: <br/>0      1.0<br/>1      2.0<br/>2      3.0<br/>3      4.0<br/>4      1.0<br/>5      2.0<br/>6      3.0<br/>7      4.0<br/>8      1.0<br/>9      2.0<br/>      ... <br/>193    2.0<br/>194    3.0<br/>195    4.0<br/>196    1.0<br/>197    2.0<br/>198    3.0<br/></p>
<p>346 | Chapter 11: Time Series</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>199    4.0<br/>200    1.0<br/>201    2.0<br/>202    3.0<br/>Name: quarter, Length: 203, dtype: float64<br/></p>
<p>By passing these arrays to PeriodIndex with a frequency, you can combine them to<br/>form an index for the DataFrame:<br/></p>
<p>In [204]: index = pd.PeriodIndex(year=data.year, quarter=data.quarter,<br/>   .....:                        freq='Q-DEC')<br/></p>
<p>In [205]: index<br/>Out[205]: <br/>PeriodIndex(['1959Q1', '1959Q2', '1959Q3', '1959Q4', '1960Q1', '1960Q2',<br/>             '1960Q3', '1960Q4', '1961Q1', '1961Q2',<br/>             ...<br/>             '2007Q2', '2007Q3', '2007Q4', '2008Q1', '2008Q2', '2008Q3',<br/>             '2008Q4', '2009Q1', '2009Q2', '2009Q3'],<br/>            dtype='period[Q-DEC]', length=203, freq='Q-DEC')<br/></p>
<p>In [206]: data.index = index<br/></p>
<p>In [207]: data.infl<br/>Out[207]: <br/>1959Q1    0.00<br/>1959Q2    2.34<br/>1959Q3    2.74<br/>1959Q4    0.27<br/>1960Q1    2.31<br/>1960Q2    0.14<br/>1960Q3    2.70<br/>1960Q4    1.21<br/>1961Q1   -0.40<br/>1961Q2    1.47<br/>          ... <br/>2007Q2    2.75<br/>2007Q3    3.45<br/>2007Q4    6.38<br/>2008Q1    2.82<br/>2008Q2    8.53<br/>2008Q3   -3.16<br/>2008Q4   -8.79<br/>2009Q1    0.94<br/>2009Q2    3.37<br/>2009Q3    3.56<br/>Freq: Q-DEC, Name: infl, Length: 203, dtype: float64<br/></p>
<p>11.5 Periods and Period Arithmetic | 347</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>11.6 Resampling and Frequency Conversion<br/><i>Resampling</i> refers to the process of converting a time series from one frequency to<br/>another. Aggregating higher frequency data to lower frequency is called <i>downsam&#8208;<br/>pling</i>, while converting lower frequency to higher frequency is called <i>upsampling</i>. Not<br/>all resampling falls into either of these categories; for example, converting W-WED<br/>(weekly on Wednesday) to W-FRI is neither upsampling nor downsampling.<br/>pandas objects are equipped with a resample method, which is the workhorse func&#8208;<br/>tion for all frequency conversion. resample has a similar API to groupby; you call<br/>resample to group the data, then call an aggregation function:<br/></p>
<p>In [208]: rng = pd.date_range('2000-01-01', periods=100, freq='D')<br/></p>
<p>In [209]: ts = pd.Series(np.random.randn(len(rng)), index=rng)<br/></p>
<p>In [210]: ts<br/>Out[210]: <br/>2000-01-01    0.631634<br/>2000-01-02   -1.594313<br/>2000-01-03   -1.519937<br/>2000-01-04    1.108752<br/>2000-01-05    1.255853<br/>2000-01-06   -0.024330<br/>2000-01-07   -2.047939<br/>2000-01-08   -0.272657<br/>2000-01-09   -1.692615<br/>2000-01-10    1.423830<br/>                ...   <br/>2000-03-31   -0.007852<br/>2000-04-01   -1.638806<br/>2000-04-02    1.401227<br/>2000-04-03    1.758539<br/>2000-04-04    0.628932<br/>2000-04-05   -0.423776<br/>2000-04-06    0.789740<br/>2000-04-07    0.937568<br/>2000-04-08   -2.253294<br/>2000-04-09   -1.772919<br/>Freq: D, Length: 100, dtype: float64<br/></p>
<p>In [211]: ts.resample('M').mean()<br/>Out[211]: <br/>2000-01-31   -0.165893<br/>2000-02-29    0.078606<br/>2000-03-31    0.223811<br/>2000-04-30   -0.063643<br/>Freq: M, dtype: float64<br/></p>
<p>In [212]: ts.resample('M', kind='period').mean()<br/>Out[212]: <br/></p>
<p>348 | Chapter 11: Time Series</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>2000-01   -0.165893<br/>2000-02    0.078606<br/>2000-03    0.223811<br/>2000-04   -0.063643<br/>Freq: M, dtype: float64<br/></p>
<p>resample is a flexible and high-performance method that can be used to process very<br/>large time series. The examples in the following sections illustrate its semantics and<br/>use. Table 11-5 summarizes some of its options.<br/><i>Table 11-5. Resample method arguments<br/></i></p>
<p>Argument Description<br/>freq String or DateOffset indicating desired resampled frequency (e.g., &#8216;M', &#8217;5min', or Second(15))<br/>axis Axis to resample on; default axis=0<br/>fill_method How to interpolate when upsampling, as in 'ffill' or 'bfill'; by default does no interpolation<br/>closed In downsampling, which end of each interval is closed (inclusive), 'right' or 'left'<br/>label In downsampling, how to label the aggregated result, with the 'right' or 'left' bin edge (e.g., the<br/></p>
<p>9:30 to 9:35 five-minute interval could be labeled 9:30 or 9:35)<br/>loffset Time adjustment to the bin labels, such as '-1s' / Second(-1) to shift the aggregate labels one<br/></p>
<p>second earlier<br/>limit When forward or backward filling, the maximum number of periods to fill<br/>kind Aggregate to periods ('period') or timestamps ('timestamp'); defaults to the type of index the<br/></p>
<p>time series has<br/>convention When resampling periods, the convention ('start' or 'end') for converting the low-frequency period<br/></p>
<p>to high frequency; defaults to 'end'<br/></p>
<p>Downsampling<br/>Aggregating data to a regular, lower frequency is a pretty normal time series task. The<br/>data you&#8217;re aggregating doesn&#8217;t need to be fixed frequently; the desired frequency<br/>defines <i>bin edges</i> that are used to slice the time series into pieces to aggregate. For<br/>example, to convert to monthly, 'M' or 'BM', you need to chop up the data into one-<br/>month intervals. Each interval is said to be <i>half-open</i>; a data point can only belong to<br/>one interval, and the union of the intervals must make up the whole time frame.<br/>There are a couple things to think about when using resample to downsample data:<br/></p>
<p>&#8226; Which side of each interval is <i>closed<br/></i>&#8226; How to label each aggregated bin, either with the start of the interval or the end<br/></p>
<p>To illustrate, let&#8217;s look at some one-minute data:<br/>In [213]: rng = pd.date_range('2000-01-01', periods=12, freq='T')<br/></p>
<p>In [214]: ts = pd.Series(np.arange(12), index=rng)<br/></p>
<p>11.6 Resampling and Frequency Conversion | 349</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>1 The choice of the default values for closed and label might seem a bit odd to some users. In practice the<br/>choice is somewhat arbitrary; for some target frequencies, closed='left' is preferable, while for others<br/>closed='right' makes more sense. The important thing is that you keep in mind exactly how you are seg&#8208;<br/>menting the data.<br/></p>
<p>In [215]: ts<br/>Out[215]: <br/>2000-01-01 00:00:00     0<br/>2000-01-01 00:01:00     1<br/>2000-01-01 00:02:00     2<br/>2000-01-01 00:03:00     3<br/>2000-01-01 00:04:00     4<br/>2000-01-01 00:05:00     5<br/>2000-01-01 00:06:00     6<br/>2000-01-01 00:07:00     7<br/>2000-01-01 00:08:00     8<br/>2000-01-01 00:09:00     9<br/>2000-01-01 00:10:00    10<br/>2000-01-01 00:11:00    11<br/>Freq: T, dtype: int64<br/></p>
<p>Suppose you wanted to aggregate this data into five-minute chunks or <i>bars</i> by taking<br/>the sum of each group:<br/></p>
<p>In [216]: ts.resample('5min', closed='right').sum()<br/>Out[216]: <br/>1999-12-31 23:55:00     0<br/>2000-01-01 00:00:00    15<br/>2000-01-01 00:05:00    40<br/>2000-01-01 00:10:00    11<br/>Freq: 5T, dtype: int64<br/></p>
<p>The frequency you pass defines bin edges in five-minute increments. By default,<br/>the left bin edge is inclusive, so the 00:00 value is included in the 00:00 to 00:05<br/>interval.1 Passing closed='right' changes the interval to be closed on the right:<br/></p>
<p>In [217]: ts.resample('5min', closed='right').sum()<br/>Out[217]: <br/>1999-12-31 23:55:00     0<br/>2000-01-01 00:00:00    15<br/>2000-01-01 00:05:00    40<br/>2000-01-01 00:10:00    11<br/>Freq: 5T, dtype: int64<br/></p>
<p>The resulting time series is labeled by the timestamps from the left side of each bin.<br/>By passing label='right' you can label them with the right bin edge:<br/></p>
<p>In [218]: ts.resample('5min', closed='right', label='right').sum()<br/>Out[218]: <br/>2000-01-01 00:00:00     0<br/>2000-01-01 00:05:00    15<br/></p>
<p>350 | Chapter 11: Time Series</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>2000-01-01 00:10:00    40<br/>2000-01-01 00:15:00    11<br/>Freq: 5T, dtype: int64<br/></p>
<p>See Figure 11-3 for an illustration of minute frequency data being resampled to five-<br/>minute frequency.<br/></p>
<p><i>Figure 11-3. Five-minute resampling illustration of closed, label conventions<br/></i>Lastly, you might want to shift the result index by some amount, say subtracting one<br/>second from the right edge to make it more clear which interval the timestamp refers<br/>to. To do this, pass a string or date offset to loffset:<br/></p>
<p>In [219]: ts.resample('5min', closed='right',<br/>   .....:             label='right', loffset='-1s').sum()<br/>Out[219]: <br/>1999-12-31 23:59:59     0<br/>2000-01-01 00:04:59    15<br/>2000-01-01 00:09:59    40<br/>2000-01-01 00:14:59    11<br/>Freq: 5T, dtype: int64<br/></p>
<p>You also could have accomplished the effect of loffset by calling the shift method<br/>on the result without the loffset.<br/></p>
<p>Open-High-Low-Close (OHLC) resampling<br/>In finance, a popular way to aggregate a time series is to compute four values for each<br/>bucket: the first (open), last (close), maximum (high), and minimal (low) values. By<br/>using the ohlc aggregate function you will obtain a DataFrame having columns con&#8208;<br/>taining these four aggregates, which are efficiently computed in a single sweep of the <br/>data:<br/></p>
<p>In [220]: ts.resample('5min').ohlc()<br/>Out[220]: <br/>                     open  high  low  close<br/>2000-01-01 00:00:00     0     4    0      4<br/>2000-01-01 00:05:00     5     9    5      9<br/>2000-01-01 00:10:00    10    11   10     11<br/></p>
<p>11.6 Resampling and Frequency Conversion | 351</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Upsampling and Interpolation<br/>When converting from a low frequency to a higher frequency, no aggregation is<br/>needed. Let&#8217;s consider a DataFrame with some weekly data:<br/></p>
<p>In [221]: frame = pd.DataFrame(np.random.randn(2, 4),<br/>   .....:                      index=pd.date_range('1/1/2000', periods=2,<br/>   .....:                                          freq='W-WED'),<br/>   .....:                      columns=['Colorado', 'Texas', 'New York', 'Ohio'])<br/></p>
<p>In [222]: frame<br/>Out[222]: <br/>            Colorado     Texas  New York      Ohio<br/>2000-01-05 -0.896431  0.677263  0.036503  0.087102<br/>2000-01-12 -0.046662  0.927238  0.482284 -0.867130<br/></p>
<p>When you are using an aggregation function with this data, there is only one value<br/>per group, and missing values result in the gaps. We use the asfreq method to con&#8208;<br/>vert to the higher frequency without any aggregation:<br/></p>
<p>In [223]: df_daily = frame.resample('D').asfreq()<br/></p>
<p>In [224]: df_daily<br/>Out[224]: <br/>            Colorado     Texas  New York      Ohio<br/>2000-01-05 -0.896431  0.677263  0.036503  0.087102<br/>2000-01-06       NaN       NaN       NaN       NaN<br/>2000-01-07       NaN       NaN       NaN       NaN<br/>2000-01-08       NaN       NaN       NaN       NaN<br/>2000-01-09       NaN       NaN       NaN       NaN<br/>2000-01-10       NaN       NaN       NaN       NaN<br/>2000-01-11       NaN       NaN       NaN       NaN<br/>2000-01-12 -0.046662  0.927238  0.482284 -0.867130<br/></p>
<p>Suppose you wanted to fill forward each weekly value on the non-Wednesdays. The<br/>same filling or interpolation methods available in the fillna and reindex methods<br/>are available for resampling:<br/></p>
<p>In [225]: frame.resample('D').ffill()<br/>Out[225]: <br/>            Colorado     Texas  New York      Ohio<br/>2000-01-05 -0.896431  0.677263  0.036503  0.087102<br/>2000-01-06 -0.896431  0.677263  0.036503  0.087102<br/>2000-01-07 -0.896431  0.677263  0.036503  0.087102<br/>2000-01-08 -0.896431  0.677263  0.036503  0.087102<br/>2000-01-09 -0.896431  0.677263  0.036503  0.087102<br/>2000-01-10 -0.896431  0.677263  0.036503  0.087102<br/>2000-01-11 -0.896431  0.677263  0.036503  0.087102<br/>2000-01-12 -0.046662  0.927238  0.482284 -0.867130<br/></p>
<p>You can similarly choose to only fill a certain number of periods forward to limit how<br/>far to continue using an observed value:<br/></p>
<p>352 | Chapter 11: Time Series</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In [226]: frame.resample('D').ffill(limit=2)<br/>Out[226]: <br/>            Colorado     Texas  New York      Ohio<br/>2000-01-05 -0.896431  0.677263  0.036503  0.087102<br/>2000-01-06 -0.896431  0.677263  0.036503  0.087102<br/>2000-01-07 -0.896431  0.677263  0.036503  0.087102<br/>2000-01-08       NaN       NaN       NaN       NaN<br/>2000-01-09       NaN       NaN       NaN       NaN<br/>2000-01-10       NaN       NaN       NaN       NaN<br/>2000-01-11       NaN       NaN       NaN       NaN<br/>2000-01-12 -0.046662  0.927238  0.482284 -0.867130<br/></p>
<p>Notably, the new date index need not overlap with the old one at all:<br/>In [227]: frame.resample('W-THU').ffill()<br/>Out[227]: <br/>            Colorado     Texas  New York      Ohio<br/>2000-01-06 -0.896431  0.677263  0.036503  0.087102<br/>2000-01-13 -0.046662  0.927238  0.482284 -0.867130<br/></p>
<p>Resampling with Periods<br/>Resampling data indexed by periods is similar to timestamps:<br/></p>
<p>In [228]: frame = pd.DataFrame(np.random.randn(24, 4),<br/>   .....:                      index=pd.period_range('1-2000', '12-2001',<br/>   .....:                                            freq='M'),<br/>   .....:                      columns=['Colorado', 'Texas', 'New York', 'Ohio'])<br/></p>
<p>In [229]: frame[:5]<br/>Out[229]: <br/>         Colorado     Texas  New York      Ohio<br/>2000-01  0.493841 -0.155434  1.397286  1.507055<br/>2000-02 -1.179442  0.443171  1.395676 -0.529658<br/>2000-03  0.787358  0.248845  0.743239  1.267746<br/>2000-04  1.302395 -0.272154 -0.051532 -0.467740<br/>2000-05 -1.040816  0.426419  0.312945 -1.115689<br/></p>
<p>In [230]: annual_frame = frame.resample('A-DEC').mean()<br/></p>
<p>In [231]: annual_frame<br/>Out[231]: <br/>      Colorado     Texas  New York      Ohio<br/>2000  0.556703  0.016631  0.111873 -0.027445<br/>2001  0.046303  0.163344  0.251503 -0.157276<br/></p>
<p>Upsampling is more nuanced, as you must make a decision about which end of the<br/>timespan in the new frequency to place the values before resampling, just like the<br/>asfreq method. The convention argument defaults to 'start' but can also be 'end':<br/></p>
<p><i># Q-DEC: Quarterly, year ending in December<br/></i>In [232]: annual_frame.resample('Q-DEC').ffill()<br/>Out[232]: <br/></p>
<p>11.6 Resampling and Frequency Conversion | 353</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>        Colorado     Texas  New York      Ohio<br/>2000Q1  0.556703  0.016631  0.111873 -0.027445<br/>2000Q2  0.556703  0.016631  0.111873 -0.027445<br/>2000Q3  0.556703  0.016631  0.111873 -0.027445<br/>2000Q4  0.556703  0.016631  0.111873 -0.027445<br/>2001Q1  0.046303  0.163344  0.251503 -0.157276<br/>2001Q2  0.046303  0.163344  0.251503 -0.157276<br/>2001Q3  0.046303  0.163344  0.251503 -0.157276<br/>2001Q4  0.046303  0.163344  0.251503 -0.157276<br/></p>
<p>In [233]: annual_frame.resample('Q-DEC', convention='end').ffill()<br/>Out[233]: <br/>        Colorado     Texas  New York      Ohio<br/>2000Q4  0.556703  0.016631  0.111873 -0.027445<br/>2001Q1  0.556703  0.016631  0.111873 -0.027445<br/>2001Q2  0.556703  0.016631  0.111873 -0.027445<br/>2001Q3  0.556703  0.016631  0.111873 -0.027445<br/>2001Q4  0.046303  0.163344  0.251503 -0.157276<br/></p>
<p>Since periods refer to timespans, the rules about upsampling and downsampling are<br/>more rigid:<br/></p>
<p>&#8226; In downsampling, the target frequency must be a <i>subperiod</i> of the source<br/>frequency.<br/></p>
<p>&#8226; In upsampling, the target frequency must be a <i>superperiod</i> of the source<br/>frequency.<br/></p>
<p>If these rules are not satisfied, an exception will be raised. This mainly affects the<br/>quarterly, annual, and weekly frequencies; for example, the timespans defined by Q-<br/>MAR only line up with A-MAR, A-JUN, A-SEP, and A-DEC:<br/></p>
<p>In [234]: annual_frame.resample('Q-MAR').ffill()<br/>Out[234]: <br/>        Colorado     Texas  New York      Ohio<br/>2000Q4  0.556703  0.016631  0.111873 -0.027445<br/>2001Q1  0.556703  0.016631  0.111873 -0.027445<br/>2001Q2  0.556703  0.016631  0.111873 -0.027445<br/>2001Q3  0.556703  0.016631  0.111873 -0.027445<br/>2001Q4  0.046303  0.163344  0.251503 -0.157276<br/>2002Q1  0.046303  0.163344  0.251503 -0.157276<br/>2002Q2  0.046303  0.163344  0.251503 -0.157276<br/>2002Q3  0.046303  0.163344  0.251503 -0.157276<br/></p>
<p>11.7 Moving Window Functions<br/>An important class of array transformations used for time series operations are statis&#8208;<br/>tics and other functions evaluated over a sliding window or with exponentially decay&#8208;<br/>ing weights. This can be useful for smoothing noisy or gappy data. I call these <i>moving<br/>window functions</i>, even though it includes functions without a fixed-length window<br/></p>
<p>354 | Chapter 11: Time Series</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>like exponentially weighted moving average. Like other statistical functions, these<br/>also automatically exclude missing data.<br/>Before digging in, we can load up some time series data and resample it to business<br/>day frequency:<br/></p>
<p>In [235]: close_px_all = pd.read_csv('examples/stock_px_2.csv',<br/>   .....:                            parse_dates=True, index_col=0)<br/></p>
<p>In [236]: close_px = close_px_all[['AAPL', 'MSFT', 'XOM']]<br/></p>
<p>In [237]: close_px = close_px.resample('B').ffill()<br/></p>
<p>I now introduce the rolling operator, which behaves similarly to resample and<br/>groupby. It can be called on a Series or DataFrame along with a window (expressed as<br/>a number of periods; see Figure 11-4 for the plot created):<br/></p>
<p>In [238]: close_px.AAPL.plot()<br/>Out[238]: &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f2f2570cf98&gt;<br/></p>
<p>In [239]: close_px.AAPL.rolling(250).mean().plot()<br/></p>
<p><i>Figure 11-4. Apple Price with 250-day MA<br/></i></p>
<p>The expression rolling(250) is similar in behavior to groupby, but instead of group&#8208;<br/>ing it creates an object that enables grouping over a 250-day sliding window. So here<br/>we have the 250-day moving window average of Apple&#8217;s stock price.<br/></p>
<p>11.7 Moving Window Functions | 355</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>By default rolling functions require all of the values in the window to be non-NA.<br/>This behavior can be changed to account for missing data and, in particular, the fact<br/>that you will have fewer than window periods of data at the beginning of the time<br/>series (see Figure 11-5):<br/></p>
<p>In [241]: appl_std250 = close_px.AAPL.rolling(250, min_periods=10).std()<br/></p>
<p>In [242]: appl_std250[5:12]<br/>Out[242]: <br/>2003-01-09         NaN<br/>2003-01-10         NaN<br/>2003-01-13         NaN<br/>2003-01-14         NaN<br/>2003-01-15    0.077496<br/>2003-01-16    0.074760<br/>2003-01-17    0.112368<br/>Freq: B, Name: AAPL, dtype: float64<br/></p>
<p>In [243]: appl_std250.plot()<br/></p>
<p><i>Figure 11-5. Apple 250-day daily return standard deviation<br/></i></p>
<p>In order to compute an <i>expanding window mean</i>, use the expanding operator instead<br/>of rolling. The expanding mean starts the time window from the beginning of the<br/>time series and increases the size of the window until it encompasses the whole series.<br/>An expanding window mean on the apple_std250 time series looks like this:<br/></p>
<p>In [244]: expanding_mean = appl_std250.expanding().mean()<br/></p>
<p>356 | Chapter 11: Time Series</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Calling a moving window function on a DataFrame applies the transformation to<br/>each column (see Figure 11-6):<br/></p>
<p>In [246]: close_px.rolling(60).mean().plot(logy=True)<br/></p>
<p><i>Figure 11-6. Stocks prices 60-day MA (log Y-axis)<br/></i></p>
<p>The rolling function also accepts a string indicating a fixed-size time offset rather<br/>than a set number of periods. Using this notation can be useful for irregular time ser&#8208;<br/>ies. These are the same strings that you can pass to resample. For example, we could<br/>compute a 20-day rolling mean like so:<br/></p>
<p>In [247]: close_px.rolling('20D').mean()<br/>Out[247]: <br/>                  AAPL       MSFT        XOM<br/>2003-01-02    7.400000  21.110000  29.220000<br/>2003-01-03    7.425000  21.125000  29.230000<br/>2003-01-06    7.433333  21.256667  29.473333<br/>2003-01-07    7.432500  21.425000  29.342500<br/>2003-01-08    7.402000  21.402000  29.240000<br/>2003-01-09    7.391667  21.490000  29.273333<br/>2003-01-10    7.387143  21.558571  29.238571<br/>2003-01-13    7.378750  21.633750  29.197500<br/>2003-01-14    7.370000  21.717778  29.194444<br/>2003-01-15    7.355000  21.757000  29.152000<br/>...                ...        ...        ...<br/>2011-10-03  398.002143  25.890714  72.413571<br/>2011-10-04  396.802143  25.807857  72.427143<br/>2011-10-05  395.751429  25.729286  72.422857<br/></p>
<p>11.7 Moving Window Functions | 357</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>2011-10-06  394.099286  25.673571  72.375714<br/>2011-10-07  392.479333  25.712000  72.454667<br/>2011-10-10  389.351429  25.602143  72.527857<br/>2011-10-11  388.505000  25.674286  72.835000<br/>2011-10-12  388.531429  25.810000  73.400714<br/>2011-10-13  388.826429  25.961429  73.905000<br/>2011-10-14  391.038000  26.048667  74.185333<br/>[2292 rows x 3 columns]<br/></p>
<p>Exponentially Weighted Functions<br/>An alternative to using a static window size with equally weighted observations is to<br/>specify a constant <i>decay factor</i> to give more weight to more recent observations.<br/>There are a couple of ways to specify the decay factor. A popular one is using a <i>span</i>,<br/>which makes the result comparable to a simple moving window function with win&#8208;<br/>dow size equal to the span.<br/>Since an exponentially weighted statistic places more weight on more recent observa&#8208;<br/>tions, it &#8220;adapts&#8221; faster to changes compared with the equal-weighted version.<br/>pandas has the ewm operator to go along with rolling and expanding. Here&#8217;s an<br/>example comparing a 60-day moving average of Apple&#8217;s stock price with an EW mov&#8208;<br/>ing average with span=60 (see Figure 11-7):<br/></p>
<p>In [249]: aapl_px = close_px.AAPL['2006':'2007']<br/></p>
<p>In [250]: ma60 = aapl_px.rolling(30, min_periods=20).mean()<br/></p>
<p>In [251]: ewma60 = aapl_px.ewm(span=30).mean()<br/></p>
<p>In [252]: ma60.plot(style='k--', label='Simple MA')<br/>Out[252]: &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f2f252161d0&gt;<br/></p>
<p>In [253]: ewma60.plot(style='k-', label='EW MA')<br/>Out[253]: &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f2f252161d0&gt;<br/></p>
<p>In [254]: plt.legend()<br/></p>
<p>358 | Chapter 11: Time Series</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><i>Figure 11-7. Simple moving average versus exponentially weighted<br/></i></p>
<p>Binary Moving Window Functions<br/>Some statistical operators, like correlation and covariance, need to operate on two<br/>time series. As an example, financial analysts are often interested in a stock&#8217;s correla&#8208;<br/>tion to a benchmark index like the S&amp;P 500. To have a look at this, we first compute<br/>the percent change for all of our time series of interest:<br/></p>
<p>In [256]: spx_px = close_px_all['SPX']<br/></p>
<p>In [257]: spx_rets = spx_px.pct_change()<br/></p>
<p>In [258]: returns = close_px.pct_change()<br/></p>
<p>The corr aggregation function after we call rolling can then compute the rolling<br/>correlation with spx_rets (see Figure 11-8 for the resulting plot):<br/></p>
<p>In [259]: corr = returns.AAPL.rolling(125, min_periods=100).corr(spx_rets)<br/></p>
<p>In [260]: corr.plot()<br/></p>
<p>11.7 Moving Window Functions | 359</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><i>Figure 11-8. Six-month AAPL return correlation to S&amp;P 500<br/></i>Suppose you wanted to compute the correlation of the S&amp;P 500 index with many<br/>stocks at once. Writing a loop and creating a new DataFrame would be easy but might<br/>get repetitive, so if you pass a Series and a DataFrame, a function like rolling_corr<br/>will compute the correlation of the Series (spx_rets, in this case) with each column<br/>in the DataFrame (see Figure 11-9 for the plot of the result):<br/></p>
<p>In [262]: corr = returns.rolling(125, min_periods=100).corr(spx_rets)<br/></p>
<p>In [263]: corr.plot()<br/></p>
<p>360 | Chapter 11: Time Series</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><i>Figure 11-9. Six-month return correlations to S&amp;P 500<br/></i></p>
<p>User-Defined Moving Window Functions<br/>The apply method on rolling and related methods provides a means to apply an<br/>array function of your own devising over a moving window. The only requirement is<br/>that the function produce a single value (a reduction) from each piece of the array.<br/>For example, while we can compute sample quantiles using rolling(...).quan<br/>tile(q), we might be interested in the percentile rank of a particular value over the<br/>sample. The scipy.stats.percentileofscore function does just this (see<br/>Figure 11-10 for the resulting plot):<br/></p>
<p>In [265]: <b>from</b> <b>scipy.stats</b> <b>import</b> percentileofscore<br/></p>
<p>In [266]: score_at_2percent = <b>lambda</b> x: percentileofscore(x, 0.02)<br/></p>
<p>In [267]: result = returns.AAPL.rolling(250).apply(score_at_2percent)<br/></p>
<p>In [268]: result.plot()<br/></p>
<p>11.7 Moving Window Functions | 361</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><i>Figure 11-10. Percentile rank of 2% AAPL return over one-year window<br/></i>If you don&#8217;t have SciPy installed already, you can install it with conda or pip.<br/></p>
<p>11.8 Conclusion<br/>Time series data calls for different types of analysis and data transformation tools<br/>than the other types of data we have explored in previous chapters.<br/>In the following chapters, we will move on to some advanced pandas methods and<br/>show how to start using modeling libraries like statsmodels and scikit-learn.<br/></p>
<p>362 | Chapter 11: Time Series</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>CHAPTER 12<br/>Advanced pandas<br/></p>
<p>The preceding chapters have focused on introducing different types of data wrangling<br/>workflows and features of NumPy, pandas, and other libraries. Over time, pandas has<br/>developed a depth of features for power users. This chapter digs into a few more<br/>advanced feature areas to help you deepen your expertise as a pandas user.<br/></p>
<p>12.1 Categorical Data<br/>This section introduces the pandas Categorical type. I will show how you can ach&#8208;<br/>ieve better performance and memory use in some pandas operations by using it. I<br/>also introduce some tools for using categorical data in statistics and machine learning<br/>applications.<br/></p>
<p>Background and Motivation<br/>Frequently, a column in a table may contain repeated instances of a smaller set of dis&#8208;<br/>tinct values. We have already seen functions like unique and value_counts, which<br/>enable us to extract the distinct values from an array and compute their frequencies,<br/>respectively:<br/></p>
<p>In [10]: <b>import</b> <b>numpy</b> <b>as</b> <b>np</b>; <b>import</b> <b>pandas</b> <b>as</b> <b>pd<br/></b></p>
<p>In [11]: values = pd.Series(['apple', 'orange', 'apple',<br/>   ....:                     'apple'] * 2)<br/></p>
<p>In [12]: values<br/>Out[12]: <br/>0     apple<br/>1    orange<br/>2     apple<br/>3     apple<br/></p>
<p>363</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>4     apple<br/>5    orange<br/>6     apple<br/>7     apple<br/>dtype: object<br/></p>
<p>In [13]: pd.unique(values)<br/>Out[13]: array(['apple', 'orange'], dtype=object)<br/></p>
<p>In [14]: pd.value_counts(values)<br/>Out[14]: <br/>apple     6<br/>orange    2<br/>dtype: int64<br/></p>
<p>Many data systems (for data warehousing, statistical computing, or other uses) have<br/>developed specialized approaches for representing data with repeated values for more<br/>efficient storage and computation. In data warehousing, a best practice is to use so-<br/>called <i>dimension tables</i> containing the distinct values and storing the primary obser&#8208;<br/>vations as integer keys referencing the dimension table:<br/></p>
<p>In [15]: values = pd.Series([0, 1, 0, 0] * 2)<br/></p>
<p>In [16]: dim = pd.Series(['apple', 'orange'])<br/></p>
<p>In [17]: values<br/>Out[17]: <br/>0    0<br/>1    1<br/>2    0<br/>3    0<br/>4    0<br/>5    1<br/>6    0<br/>7    0<br/>dtype: int64<br/></p>
<p>In [18]: dim<br/>Out[18]: <br/>0     apple<br/>1    orange<br/>dtype: object<br/></p>
<p>We can use the take method to restore the original Series of strings:<br/>In [19]: dim.take(values)<br/>Out[19]: <br/>0     apple<br/>1    orange<br/>0     apple<br/>0     apple<br/>0     apple<br/>1    orange<br/></p>
<p>364 | Chapter 12: Advanced pandas</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>0     apple<br/>0     apple<br/>dtype: object<br/></p>
<p>This representation as integers is called the <i>categorical</i> or <i>dictionary-encoded</i> repre&#8208;<br/>sentation. The array of distinct values can be called the <i>categories</i>, <i>dictionary</i>, or <i>levels<br/></i>of the data. In this book we will use the terms <i>categorical</i> and <i>categories</i>. The integer<br/>values that reference the categories are called the <i>category codes</i> or simply <i>codes</i>.<br/>The categorical representation can yield significant performance improvements when<br/>you are doing analytics. You can also perform transformations on the categories while<br/>leaving the codes unmodified. Some example transformations that can be made at<br/>relatively low cost are:<br/></p>
<p>&#8226; Renaming categories<br/>&#8226; Appending a new category without changing the order or position of the existing<br/></p>
<p>categories<br/></p>
<p>Categorical Type in pandas<br/>pandas has a special Categorical type for holding data that uses the integer-based<br/>categorical representation or <i>encoding</i>. Let&#8217;s consider the example Series from before:<br/></p>
<p>In [20]: fruits = ['apple', 'orange', 'apple', 'apple'] * 2<br/></p>
<p>In [21]: N = len(fruits)<br/></p>
<p>In [22]: df = pd.DataFrame({'fruit': fruits,<br/>   ....:                    'basket_id': np.arange(N),<br/>   ....:                    'count': np.random.randint(3, 15, size=N),<br/>   ....:                    'weight': np.random.uniform(0, 4, size=N)},<br/>   ....:                   columns=['basket_id', 'fruit', 'count', 'weight'])<br/></p>
<p>In [23]: df<br/>Out[23]: <br/>   basket_id   fruit  count    weight<br/>0          0   apple      5  3.858058<br/>1          1  orange      8  2.612708<br/>2          2   apple      4  2.995627<br/>3          3   apple      7  2.614279<br/>4          4   apple     12  2.990859<br/>5          5  orange      8  3.845227<br/>6          6   apple      5  0.033553<br/>7          7   apple      4  0.425778<br/></p>
<p>Here, df['fruit'] is an array of Python string objects. We can convert it to categori&#8208;<br/>cal by calling:<br/></p>
<p>12.1 Categorical Data | 365</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In [24]: fruit_cat = df['fruit'].astype('category')<br/></p>
<p>In [25]: fruit_cat<br/>Out[25]: <br/>0     apple<br/>1    orange<br/>2     apple<br/>3     apple<br/>4     apple<br/>5    orange<br/>6     apple<br/>7     apple<br/>Name: fruit, dtype: category<br/>Categories (2, object): [apple, orange]<br/></p>
<p>The values for fruit_cat are not a NumPy array, but an instance of pandas.Catego<br/>rical:<br/></p>
<p>In [26]: c = fruit_cat.values<br/></p>
<p>In [27]: type(c)<br/>Out[27]: pandas.core.categorical.Categorical<br/></p>
<p>The Categorical object has categories and codes attributes:<br/>In [28]: c.categories<br/>Out[28]: Index(['apple', 'orange'], dtype='object')<br/></p>
<p>In [29]: c.codes<br/>Out[29]: array([0, 1, 0, 0, 0, 1, 0, 0], dtype=int8)<br/></p>
<p>You can convert a DataFrame column to categorical by assigning the converted result:<br/>In [30]: df['fruit'] = df['fruit'].astype('category')<br/></p>
<p>In [31]: df.fruit<br/>Out[31]: <br/>0     apple<br/>1    orange<br/>2     apple<br/>3     apple<br/>4     apple<br/>5    orange<br/>6     apple<br/>7     apple<br/>Name: fruit, dtype: category<br/>Categories (2, object): [apple, orange]<br/></p>
<p>You can also create pandas.Categorical directly from other types of Python<br/>sequences:<br/></p>
<p>In [32]: my_categories = pd.Categorical(['foo', 'bar', 'baz', 'foo', 'bar'])<br/></p>
<p>In [33]: my_categories<br/></p>
<p>366 | Chapter 12: Advanced pandas</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Out[33]: <br/>[foo, bar, baz, foo, bar]<br/>Categories (3, object): [bar, baz, foo]<br/></p>
<p>If you have obtained categorical encoded data from another source, you can use the <br/>alternative from_codes constructor:<br/></p>
<p>In [34]: categories = ['foo', 'bar', 'baz']<br/></p>
<p>In [35]: codes = [0, 1, 2, 0, 0, 1]<br/></p>
<p>In [36]: my_cats_2 = pd.Categorical.from_codes(codes, categories)<br/></p>
<p>In [37]: my_cats_2<br/>Out[37]: <br/>[foo, bar, baz, foo, foo, bar]<br/>Categories (3, object): [foo, bar, baz]<br/></p>
<p>Unless explicitly specified, categorical conversions assume no specific ordering of the<br/>categories. So the categories array may be in a different order depending on the<br/>ordering of the input data. When using from_codes or any of the other constructors,<br/>you can indicate that the categories have a meaningful ordering:<br/></p>
<p>In [38]: ordered_cat = pd.Categorical.from_codes(codes, categories,<br/>   ....:                                         ordered=True)<br/></p>
<p>In [39]: ordered_cat<br/>Out[39]: <br/>[foo, bar, baz, foo, foo, bar]<br/>Categories (3, object): [foo &lt; bar &lt; baz]<br/></p>
<p>The output [foo &lt; bar &lt; baz] indicates that 'foo' precedes 'bar' in the ordering,<br/>and so on. An unordered categorical instance can be made ordered with as_ordered:<br/></p>
<p>In [40]: my_cats_2.as_ordered()<br/>Out[40]: <br/>[foo, bar, baz, foo, foo, bar]<br/>Categories (3, object): [foo &lt; bar &lt; baz]<br/></p>
<p>As a last note, categorical data need not be strings, even though I have only showed<br/>string examples. A categorical array can consist of any immutable value types.<br/></p>
<p>Computations with Categoricals<br/>Using Categorical in pandas compared with the non-encoded version (like an array<br/>of strings) generally behaves the same way. Some parts of pandas, like the groupby<br/>function, perform better when working with categoricals. There are also some func&#8208;<br/>tions that can utilize the ordered flag.<br/></p>
<p>12.1 Categorical Data | 367</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Let&#8217;s consider some random numeric data, and use the pandas.qcut binning func&#8208;<br/>tion. This return pandas.Categorical; we used pandas.cut earlier in the book but<br/>glossed over the details of how categoricals work:<br/></p>
<p>In [41]: np.random.seed(12345)<br/></p>
<p>In [42]: draws = np.random.randn(1000)<br/></p>
<p>In [43]: draws[:5]<br/>Out[43]: array([-0.2047,  0.4789, -0.5194, -0.5557,  1.9658])<br/></p>
<p>Let&#8217;s compute a quartile binning of this data and extract some statistics:<br/>In [44]: bins = pd.qcut(draws, 4)<br/></p>
<p>In [45]: bins<br/>Out[45]: <br/>[(-0.684, -0.0101], (-0.0101, 0.63], (-0.684, -0.0101], (-0.684, -0.0101], (0.63,<br/> 3.928], ..., (-0.0101, 0.63], (-0.684, -0.0101], (-2.95, -0.684], (-0.0101, 0.63<br/>], (0.63, 3.928]]<br/>Length: 1000<br/>Categories (4, interval[float64]): [(-2.95, -0.684] &lt; (-0.684, -0.0101] &lt; (-0.010<br/>1, 0.63] &lt;<br/>                                    (0.63, 3.928]]<br/></p>
<p>While useful, the exact sample quartiles may be less useful for producing a report<br/>than quartile names. We can achieve this with the labels argument to qcut:<br/></p>
<p>In [46]: bins = pd.qcut(draws, 4, labels=['Q1', 'Q2', 'Q3', 'Q4'])<br/></p>
<p>In [47]: bins<br/>Out[47]: <br/>[Q2, Q3, Q2, Q2, Q4, ..., Q3, Q2, Q1, Q3, Q4]<br/>Length: 1000<br/>Categories (4, object): [Q1 &lt; Q2 &lt; Q3 &lt; Q4]<br/></p>
<p>In [48]: bins.codes[:10]<br/>Out[48]: array([1, 2, 1, 1, 3, 3, 2, 2, 3, 3], dtype=int8)<br/></p>
<p>The labeled bins categorical does not contain information about the bin edges in the<br/>data, so we can use groupby to extract some summary statistics:<br/></p>
<p>In [49]: bins = pd.Series(bins, name='quartile')<br/></p>
<p>In [50]: results = (pd.Series(draws)<br/>   ....:            .groupby(bins)<br/>   ....:            .agg(['count', 'min', 'max'])<br/>   ....:            .reset_index())<br/></p>
<p>In [51]: results<br/>Out[51]: <br/>  quartile  count       min       max<br/>0       Q1    250 -2.949343 -0.685484<br/></p>
<p>368 | Chapter 12: Advanced pandas</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>1       Q2    250 -0.683066 -0.010115<br/>2       Q3    250 -0.010032  0.628894<br/>3       Q4    250  0.634238  3.927528<br/></p>
<p>The 'quartile' column in the result retains the original categorical information,<br/>including ordering, from bins:<br/></p>
<p>In [52]: results['quartile']<br/>Out[52]: <br/>0    Q1<br/>1    Q2<br/>2    Q3<br/>3    Q4<br/>Name: quartile, dtype: category<br/>Categories (4, object): [Q1 &lt; Q2 &lt; Q3 &lt; Q4]<br/></p>
<p>Better performance with categoricals<br/>If you do a lot of analytics on a particular dataset, converting to categorical can yield<br/>substantial overall performance gains. A categorical version of a DataFrame column<br/>will often use significantly less memory, too. Let&#8217;s consider some Series with 10 mil&#8208;<br/>lion elements and a small number of distinct categories:<br/></p>
<p>In [53]: N = 10000000<br/></p>
<p>In [54]: draws = pd.Series(np.random.randn(N))<br/></p>
<p>In [55]: labels = pd.Series(['foo', 'bar', 'baz', 'qux'] * (N // 4))<br/></p>
<p>Now we convert labels to categorical:<br/>In [56]: categories = labels.astype('category')<br/></p>
<p>Now we note that labels uses significantly more memory than categories:<br/>In [57]: labels.memory_usage()<br/>Out[57]: 80000080<br/></p>
<p>In [58]: categories.memory_usage()<br/>Out[58]: 10000272<br/></p>
<p>The conversion to category is not free, of course, but it is a one-time cost:<br/>In [59]: %time _ = labels.astype('category')<br/>CPU times: user 490 ms, sys: 240 ms, total: 730 ms<br/>Wall time: 726 ms<br/></p>
<p>GroupBy operations can be significantly faster with categoricals because the underly&#8208;<br/>ing algorithms use the integer-based codes array instead of an array of strings.<br/></p>
<p>12.1 Categorical Data | 369</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Categorical Methods<br/>Series containing categorical data have several special methods similar to the Ser<br/>ies.str specialized string methods. This also provides convenient access to the cate&#8208;<br/>gories and codes. Consider the Series:<br/></p>
<p>In [60]: s = pd.Series(['a', 'b', 'c', 'd'] * 2)<br/></p>
<p>In [61]: cat_s = s.astype('category')<br/></p>
<p>In [62]: cat_s<br/>Out[62]: <br/>0    a<br/>1    b<br/>2    c<br/>3    d<br/>4    a<br/>5    b<br/>6    c<br/>7    d<br/>dtype: category<br/>Categories (4, object): [a, b, c, d]<br/></p>
<p>The special attribute cat provides access to categorical methods:<br/>In [63]: cat_s.cat.codes<br/>Out[63]: <br/>0    0<br/>1    1<br/>2    2<br/>3    3<br/>4    0<br/>5    1<br/>6    2<br/>7    3<br/>dtype: int8<br/></p>
<p>In [64]: cat_s.cat.categories<br/>Out[64]: Index(['a', 'b', 'c', 'd'], dtype='object')<br/></p>
<p>Suppose that we know the actual set of categories for this data extends beyond the<br/>four values observed in the data. We can use the set_categories method to change<br/>them:<br/></p>
<p>In [65]: actual_categories = ['a', 'b', 'c', 'd', 'e']<br/></p>
<p>In [66]: cat_s2 = cat_s.cat.set_categories(actual_categories)<br/></p>
<p>In [67]: cat_s2<br/>Out[67]: <br/>0    a<br/>1    b<br/></p>
<p>370 | Chapter 12: Advanced pandas</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>2    c<br/>3    d<br/>4    a<br/>5    b<br/>6    c<br/>7    d<br/>dtype: category<br/>Categories (5, object): [a, b, c, d, e]<br/></p>
<p>While it appears that the data is unchanged, the new categories will be reflected in<br/>operations that use them. For example, value_counts respects the categories, if<br/>present:<br/></p>
<p>In [68]: cat_s.value_counts()<br/>Out[68]: <br/>d    2<br/>c    2<br/>b    2<br/>a    2<br/>dtype: int64<br/></p>
<p>In [69]: cat_s2.value_counts()<br/>Out[69]: <br/>d    2<br/>c    2<br/>b    2<br/>a    2<br/>e    0<br/>dtype: int64<br/></p>
<p>In large datasets, categoricals are often used as a convenient tool for memory savings<br/>and better performance. After you filter a large DataFrame or Series, many of the<br/>categories may not appear in the data. To help with this, we can use the<br/>remove_unused_categories method to trim unobserved categories:<br/></p>
<p>In [70]: cat_s3 = cat_s[cat_s.isin(['a', 'b'])]<br/></p>
<p>In [71]: cat_s3<br/>Out[71]: <br/>0    a<br/>1    b<br/>4    a<br/>5    b<br/>dtype: category<br/>Categories (4, object): [a, b, c, d]<br/></p>
<p>In [72]: cat_s3.cat.remove_unused_categories()<br/>Out[72]: <br/>0    a<br/>1    b<br/>4    a<br/>5    b<br/></p>
<p>12.1 Categorical Data | 371</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>dtype: category<br/>Categories (2, object): [a, b]<br/></p>
<p>See Table 12-1 for a listing of available categorical methods.<br/><i>Table 12-1. Categorical methods for Series in pandas<br/></i></p>
<p>Method Description<br/>add_categories Append new (unused) categories at end of existing categories<br/>as_ordered Make categories ordered<br/>as_unordered Make categories unordered<br/>remove_categories Remove categories, setting any removed values to null<br/>remove_unused_categories Remove any category values which do not appear in the data<br/>rename_categories Replace categories with indicated set of new category names; cannot change the<br/></p>
<p>number of categories<br/>reorder_categories Behaves like rename_categories, but can also change the result to have ordered<br/></p>
<p>categories<br/>set_categories Replace the categories with the indicated set of new categories; can add or remove <br/></p>
<p>categories<br/></p>
<p>Creating dummy variables for modeling<br/>When you&#8217;re using statistics or machine learning tools, you&#8217;ll often transform catego&#8208;<br/>rical data into <i>dummy variables</i>, also known as <i>one-hot</i> encoding. This involves creat&#8208;<br/>ing a DataFrame with a column for each distinct category; these columns contain 1s<br/>for occurrences of a given category and 0 otherwise.<br/>Consider the previous example:<br/></p>
<p>In [73]: cat_s = pd.Series(['a', 'b', 'c', 'd'] * 2, dtype='category')<br/></p>
<p>As mentioned previously in Chapter 7, the pandas.get_dummies function converts<br/>this one-dimensional categorical data into a DataFrame containing the dummy <br/>variable:<br/></p>
<p>In [74]: pd.get_dummies(cat_s)<br/>Out[74]: <br/>   a  b  c  d<br/>0  1  0  0  0<br/>1  0  1  0  0<br/>2  0  0  1  0<br/>3  0  0  0  1<br/>4  1  0  0  0<br/>5  0  1  0  0<br/>6  0  0  1  0<br/>7  0  0  0  1<br/></p>
<p>372 | Chapter 12: Advanced pandas</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>12.2 Advanced GroupBy Use<br/>While we&#8217;ve already discussed using the groupby method for Series and DataFrame in<br/>depth in Chapter 10, there are some additional techniques that you may find of use.<br/></p>
<p>Group Transforms and &#8220;Unwrapped&#8221; GroupBys<br/>In Chapter 10 we looked at the apply method in grouped operations for performing<br/>transformations. There is another built-in method called transform, which is similar<br/>to apply but imposes more constraints on the kind of function you can use:<br/></p>
<p>&#8226; It can produce a scalar value to be broadcast to the shape of the group<br/>&#8226; It can produce an object of the same shape as the input group<br/>&#8226; It must not mutate its input<br/></p>
<p>Let&#8217;s consider a simple example for illustration:<br/>In [75]: df = pd.DataFrame({'key': ['a', 'b', 'c'] * 4,<br/>   ....:                    'value': np.arange(12.)})<br/></p>
<p>In [76]: df<br/>Out[76]: <br/>   key  value<br/>0    a    0.0<br/>1    b    1.0<br/>2    c    2.0<br/>3    a    3.0<br/>4    b    4.0<br/>5    c    5.0<br/>6    a    6.0<br/>7    b    7.0<br/>8    c    8.0<br/>9    a    9.0<br/>10   b   10.0<br/>11   c   11.0<br/></p>
<p>Here are the group means by key:<br/>In [77]: g = df.groupby('key').value<br/></p>
<p>In [78]: g.mean()<br/>Out[78]: <br/>key<br/>a    4.5<br/>b    5.5<br/>c    6.5<br/>Name: value, dtype: float64<br/></p>
<p>12.2 Advanced GroupBy Use | 373</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Suppose instead we wanted to produce a Series of the same shape as df['value'] but<br/>with values replaced by the average grouped by 'key'. We can pass the function<br/>lambda x: x.mean() to transform:<br/></p>
<p>In [79]: g.transform(<b>lambda</b> x: x.mean())<br/>Out[79]: <br/>0     4.5<br/>1     5.5<br/>2     6.5<br/>3     4.5<br/>4     5.5<br/>5     6.5<br/>6     4.5<br/>7     5.5<br/>8     6.5<br/>9     4.5<br/>10    5.5<br/>11    6.5<br/>Name: value, dtype: float64<br/></p>
<p>For built-in aggregation functions, we can pass a string alias as with the GroupBy agg<br/>method:<br/></p>
<p>In [80]: g.transform('mean')<br/>Out[80]: <br/>0     4.5<br/>1     5.5<br/>2     6.5<br/>3     4.5<br/>4     5.5<br/>5     6.5<br/>6     4.5<br/>7     5.5<br/>8     6.5<br/>9     4.5<br/>10    5.5<br/>11    6.5<br/>Name: value, dtype: float64<br/></p>
<p>Like apply, transform works with functions that return Series, but the result must be<br/>the same size as the input. For example, we can multiply each group by 2 using a<br/>lambda function:<br/></p>
<p>In [81]: g.transform(<b>lambda</b> x: x * 2)<br/>Out[81]: <br/>0      0.0<br/>1      2.0<br/>2      4.0<br/>3      6.0<br/>4      8.0<br/>5     10.0<br/>6     12.0<br/></p>
<p>374 | Chapter 12: Advanced pandas</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>7     14.0<br/>8     16.0<br/>9     18.0<br/>10    20.0<br/>11    22.0<br/>Name: value, dtype: float64<br/></p>
<p>As a more complicated example, we can compute the ranks in descending order for<br/>each group:<br/></p>
<p>In [82]: g.transform(<b>lambda</b> x: x.rank(ascending=False))<br/>Out[82]: <br/>0     4.0<br/>1     4.0<br/>2     4.0<br/>3     3.0<br/>4     3.0<br/>5     3.0<br/>6     2.0<br/>7     2.0<br/>8     2.0<br/>9     1.0<br/>10    1.0<br/>11    1.0<br/>Name: value, dtype: float64<br/></p>
<p>Consider a group transformation function composed from simple aggregations:<br/><b>def</b> normalize(x):<br/>    <b>return</b> (x - x.mean()) / x.std()<br/></p>
<p>We can obtain equivalent results in this case either using transform or apply:<br/>In [84]: g.transform(normalize)<br/>Out[84]: <br/>0    -1.161895<br/>1    -1.161895<br/>2    -1.161895<br/>3    -0.387298<br/>4    -0.387298<br/>5    -0.387298<br/>6     0.387298<br/>7     0.387298<br/>8     0.387298<br/>9     1.161895<br/>10    1.161895<br/>11    1.161895<br/>Name: value, dtype: float64<br/></p>
<p>In [85]: g.apply(normalize)<br/>Out[85]: <br/>0    -1.161895<br/>1    -1.161895<br/>2    -1.161895<br/></p>
<p>12.2 Advanced GroupBy Use | 375</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>3    -0.387298<br/>4    -0.387298<br/>5    -0.387298<br/>6     0.387298<br/>7     0.387298<br/>8     0.387298<br/>9     1.161895<br/>10    1.161895<br/>11    1.161895<br/>Name: value, dtype: float64<br/></p>
<p>Built-in aggregate functions like 'mean' or 'sum' are often much faster than a general<br/>apply function. These also have a &#8220;fast past&#8221; when used with transform. This allows<br/>us to perform a so-called <i>unwrapped</i> group operation:<br/></p>
<p>In [86]: g.transform('mean')<br/>Out[86]: <br/>0     4.5<br/>1     5.5<br/>2     6.5<br/>3     4.5<br/>4     5.5<br/>5     6.5<br/>6     4.5<br/>7     5.5<br/>8     6.5<br/>9     4.5<br/>10    5.5<br/>11    6.5<br/>Name: value, dtype: float64<br/></p>
<p>In [87]: normalized = (df['value'] - g.transform('mean')) / g.transform('std')<br/></p>
<p>In [88]: normalized<br/>Out[88]: <br/>0    -1.161895<br/>1    -1.161895<br/>2    -1.161895<br/>3    -0.387298<br/>4    -0.387298<br/>5    -0.387298<br/>6     0.387298<br/>7     0.387298<br/>8     0.387298<br/>9     1.161895<br/>10    1.161895<br/>11    1.161895<br/>Name: value, dtype: float64<br/></p>
<p>While an unwrapped group operation may involve multiple group aggregations, the<br/>overall benefit of vectorized operations often outweighs this.<br/></p>
<p>376 | Chapter 12: Advanced pandas</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Grouped Time Resampling<br/>For time series data, the resample method is semantically a group operation based on<br/>a time intervalization. Here&#8217;s a small example table:<br/></p>
<p>In [89]: N = 15<br/></p>
<p>In [90]: times = pd.date_range('2017-05-20 00:00', freq='1min', periods=N)<br/></p>
<p>In [91]: df = pd.DataFrame({'time': times,<br/>   ....:                    'value': np.arange(N)})<br/></p>
<p>In [92]: df<br/>Out[92]: <br/>                  time  value<br/>0  2017-05-20 00:00:00      0<br/>1  2017-05-20 00:01:00      1<br/>2  2017-05-20 00:02:00      2<br/>3  2017-05-20 00:03:00      3<br/>4  2017-05-20 00:04:00      4<br/>5  2017-05-20 00:05:00      5<br/>6  2017-05-20 00:06:00      6<br/>7  2017-05-20 00:07:00      7<br/>8  2017-05-20 00:08:00      8<br/>9  2017-05-20 00:09:00      9<br/>10 2017-05-20 00:10:00     10<br/>11 2017-05-20 00:11:00     11<br/>12 2017-05-20 00:12:00     12<br/>13 2017-05-20 00:13:00     13<br/>14 2017-05-20 00:14:00     14<br/></p>
<p>Here, we can index by 'time' and then resample:<br/>In [93]: df.set_index('time').resample('5min').count()<br/>Out[93]: <br/>                     value<br/>time                      <br/>2017-05-20 00:00:00      5<br/>2017-05-20 00:05:00      5<br/>2017-05-20 00:10:00      5<br/></p>
<p>Suppose that a DataFrame contains multiple time series, marked by an additional<br/>group key column:<br/></p>
<p>In [94]: df2 = pd.DataFrame({'time': times.repeat(3),<br/>   ....:                     'key': np.tile(['a', 'b', 'c'], N),<br/>   ....:                     'value': np.arange(N * 3.)})<br/></p>
<p>In [95]: df2[:7]<br/>Out[95]: <br/>  key                time  value<br/>0   a 2017-05-20 00:00:00    0.0<br/>1   b 2017-05-20 00:00:00    1.0<br/></p>
<p>12.2 Advanced GroupBy Use | 377</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>2   c 2017-05-20 00:00:00    2.0<br/>3   a 2017-05-20 00:01:00    3.0<br/>4   b 2017-05-20 00:01:00    4.0<br/>5   c 2017-05-20 00:01:00    5.0<br/>6   a 2017-05-20 00:02:00    6.0<br/></p>
<p>To do the same resampling for each value of 'key', we introduce the pandas.Time<br/>Grouper object:<br/></p>
<p>In [96]: time_key = pd.TimeGrouper('5min')<br/></p>
<p>We can then set the time index, group by 'key' and time_key, and aggregate:<br/>In [97]: resampled = (df2.set_index('time')<br/>   ....:              .groupby(['key', time_key])<br/>   ....:              .sum())<br/></p>
<p>In [98]: resampled<br/>Out[98]: <br/>                         value<br/>key time                      <br/>a   2017-05-20 00:00:00   30.0<br/>    2017-05-20 00:05:00  105.0<br/>    2017-05-20 00:10:00  180.0<br/>b   2017-05-20 00:00:00   35.0<br/>    2017-05-20 00:05:00  110.0<br/>    2017-05-20 00:10:00  185.0<br/>c   2017-05-20 00:00:00   40.0<br/>    2017-05-20 00:05:00  115.0<br/>    2017-05-20 00:10:00  190.0<br/></p>
<p>In [99]: resampled.reset_index()<br/>Out[99]: <br/>  key                time  value<br/>0   a 2017-05-20 00:00:00   30.0<br/>1   a 2017-05-20 00:05:00  105.0<br/>2   a 2017-05-20 00:10:00  180.0<br/>3   b 2017-05-20 00:00:00   35.0<br/>4   b 2017-05-20 00:05:00  110.0<br/>5   b 2017-05-20 00:10:00  185.0<br/>6   c 2017-05-20 00:00:00   40.0<br/>7   c 2017-05-20 00:05:00  115.0<br/>8   c 2017-05-20 00:10:00  190.0<br/></p>
<p>One constraint with using TimeGrouper is that the time must be the index of the Ser&#8208;<br/>ies or DataFrame.<br/></p>
<p>12.3 Techniques for Method Chaining<br/>When applying a sequence of transformations to a dataset, you may find yourself cre&#8208;<br/>ating numerous temporary variables that are never used in your analysis. Consider<br/>this example, for instance:<br/></p>
<p>378 | Chapter 12: Advanced pandas</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>df = load_data()<br/>df2 = df[df['col2'] &lt; 0]<br/>df2['col1_demeaned'] = df2['col1'] - df2['col1'].mean()<br/>result = df2.groupby('key').col1_demeaned.std()<br/></p>
<p>While we&#8217;re not using any real data here, this example highlights some new methods.<br/>First, the DataFrame.assign method is a <i>functional</i> alternative to column assign&#8208;<br/>ments of the form df[k] = v. Rather than modifying the object in-place, it returns a<br/>new DataFrame with the indicated modifications. So these statements are equivalent:<br/></p>
<p><i># Usual non-functional way<br/></i>df2 = df.copy()<br/>df2['k'] = v<br/></p>
<p><i># Functional assign way<br/></i>df2 = df.assign(k=v)<br/></p>
<p>Assigning in-place may execute faster than using assign, but assign enables easier<br/>method chaining:<br/></p>
<p>result = (df2.assign(col1_demeaned=df2.col1 - df2.col2.mean())<br/>          .groupby('key')<br/>          .col1_demeaned.std())<br/></p>
<p>I used the outer parentheses to make it more convenient to add line breaks.<br/>One thing to keep in mind when doing method chaining is that you may need to<br/>refer to temporary objects. In the preceding example, we cannot refer to the result of<br/>load_data until it has been assigned to the temporary variable df. To help with this,<br/>assign and many other pandas functions accept function-like arguments, also known<br/>as <i>callables</i>.<br/>To show callables in action, consider a fragment of the example from before:<br/></p>
<p>df = load_data()<br/>df2 = df[df['col2'] &lt; 0]<br/></p>
<p>This can be rewritten as:<br/>df = (load_data()<br/>      [<b>lambda</b> x: x['col2'] &lt; 0])<br/></p>
<p>Here, the result of load_data is not assigned to a variable, so the function passed into<br/>[] is then <i>bound</i> to the object at that stage of the method chain.<br/>We can continue, then, and write the entire sequence as a single chained expression:<br/></p>
<p>result = (load_data()<br/>          [<b>lambda</b> x: x.col2 &lt; 0]<br/>          .assign(col1_demeaned=<b>lambda</b> x: x.col1 - x.col1.mean())<br/>          .groupby('key')<br/>          .col1_demeaned.std())<br/></p>
<p>12.3 Techniques for Method Chaining | 379</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Whether you prefer to write code in this style is a matter of taste, and splitting up the<br/>expression into multiple steps may make your code more readable.<br/></p>
<p>The pipe Method<br/>You can accomplish a lot with built-in pandas functions and the approaches to<br/>method chaining with callables that we just looked at. However, sometimes you need<br/>to use your own functions or functions from third-party libraries. This is where the<br/>pipe method comes in.<br/>Consider a sequence of function calls:<br/></p>
<p>a = f(df, arg1=v1)<br/>b = g(a, v2, arg3=v3)<br/>c = h(b, arg4=v4)<br/></p>
<p>When using functions that accept and return Series or DataFrame objects, you can<br/>rewrite this using calls to pipe:<br/></p>
<p>result = (df.pipe(f, arg1=v1)<br/>          .pipe(g, v2, arg3=v3)<br/>          .pipe(h, arg4=v4))<br/></p>
<p>The statement f(df) and df.pipe(f) are equivalent, but pipe makes chained invoca&#8208;<br/>tion easier.<br/>A potentially useful pattern for pipe is to generalize sequences of operations into<br/>reusable functions. As an example, let&#8217;s consider substracting group means from a<br/>column:<br/></p>
<p>g = df.groupby(['key1', 'key2'])<br/>df['col1'] = df['col1'] - g.transform('mean')<br/></p>
<p>Suppose that you wanted to be able to demean more than one column and easily<br/>change the group keys. Additionally, you might want to perform this transformation<br/>in a method chain. Here is an example implementation:<br/></p>
<p><b>def</b> group_demean(df, by, cols):<br/>    result = df.copy()<br/>    g = df.groupby(by)<br/>    <b>for</b> c <b>in</b> cols:<br/>        result[c] = df[c] - g[c].transform('mean')<br/>    <b>return</b> result<br/></p>
<p>Then it is possible to write:<br/>result = (df[df.col1 &lt; 0]<br/>          .pipe(group_demean, ['key1', 'key2'], ['col1']))<br/></p>
<p>380 | Chapter 12: Advanced pandas</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>12.4 Conclusion<br/>pandas, like many open source software projects, is still changing and acquiring new<br/>and improved functionality. As elsewhere in this book, the focus here has been on the<br/>most stable functionality that is less likely to change over the next several years.<br/>To deepen your expertise as a pandas user, I encourage you to explore the documen&#8208;<br/>tation and read the release notes as the development team makes new open source<br/>releases. We also invite you to join in on pandas development: fixing bugs, building<br/>new features, and improving the documentation.<br/></p>
<p>12.4 Conclusion | 381</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div>
</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>CHAPTER 13<br/>Introduction to Modeling Libraries<br/></p>
<p>in Python<br/></p>
<p>In this book, I have focused on providing a programming foundation for doing data<br/>analysis in Python. Since data analysts and scientists often report spending a dispro&#8208;<br/>portionate amount of time with data wrangling and preparation, the book&#8217;s structure<br/>reflects the importance of mastering these techniques.<br/>Which library you use for developing models will depend on the application. Many<br/>statistical problems can be solved by simpler techniques like ordinary least squares<br/>regression, while other problems may call for more advanced machine learning<br/>methods. Fortunately, Python has become one of the languages of choice for imple&#8208;<br/>menting analytical methods, so there are many tools you can explore after completing<br/>this book.<br/>In this chapter, I will review some features of pandas that may be helpful when you&#8217;re<br/>crossing back and forth between data wrangling with pandas and model fitting and<br/>scoring. I will then give short introductions to two popular modeling toolkits, stats&#8208;<br/>models and scikit-learn. Since each of these projects is large enough to warrant its<br/>own dedicated book, I make no effort to be comprehensive and instead direct you to<br/>both projects&#8217; online documentation along with some other Python-based books on<br/>data science, statistics, and machine learning.<br/></p>
<p>13.1 Interfacing Between pandas and Model Code<br/>A common workflow for model development is to use pandas for data loading and<br/>cleaning before switching over to a modeling library to build the model itself. An<br/>important part of the model development process is called <i>feature engineering</i> in<br/>machine learning. This can describe any data transformation or analytics that extract<br/></p>
<p>383</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>information from a raw dataset that may be useful in a modeling context. The data<br/>aggregation and GroupBy tools we have explored in this book are used often in a fea&#8208;<br/>ture engineering context.<br/>While details of &#8220;good&#8221; feature engineering are out of scope for this book, I will show<br/>some methods to make switching between data manipulation with pandas and mod&#8208;<br/>eling as painless as possible.<br/>The point of contact between pandas and other analysis libraries is usually NumPy<br/>arrays. To turn a DataFrame into a NumPy array, use the .values property:<br/></p>
<p>In [10]: <b>import</b> <b>pandas</b> <b>as</b> <b>pd<br/></b></p>
<p>In [11]: <b>import</b> <b>numpy</b> <b>as</b> <b>np<br/></b></p>
<p>In [12]: data = pd.DataFrame({<br/>   ....:     'x0': [1, 2, 3, 4, 5],<br/>   ....:     'x1': [0.01, -0.01, 0.25, -4.1, 0.],<br/>   ....:     'y': [-1.5, 0., 3.6, 1.3, -2.]})<br/></p>
<p>In [13]: data<br/>Out[13]: <br/>   x0    x1    y<br/>0   1  0.01 -1.5<br/>1   2 -0.01  0.0<br/>2   3  0.25  3.6<br/>3   4 -4.10  1.3<br/>4   5  0.00 -2.0<br/></p>
<p>In [14]: data.columns<br/>Out[14]: Index(['x0', 'x1', 'y'], dtype='object')<br/></p>
<p>In [15]: data.values<br/>Out[15]: <br/>array([[ 1.  ,  0.01, -1.5 ],<br/>       [ 2.  , -0.01,  0.  ],<br/>       [ 3.  ,  0.25,  3.6 ],<br/>       [ 4.  , -4.1 ,  1.3 ],<br/>       [ 5.  ,  0.  , -2.  ]])<br/></p>
<p>To convert back to a DataFrame, as you may recall from earlier chapters, you can pass<br/>a two-dimensional ndarray with optional column names:<br/></p>
<p>In [16]: df2 = pd.DataFrame(data.values, columns=['one', 'two', 'three'])<br/></p>
<p>In [17]: df2<br/>Out[17]: <br/>   one   two  three<br/>0  1.0  0.01   -1.5<br/>1  2.0 -0.01    0.0<br/>2  3.0  0.25    3.6<br/></p>
<p>384 | Chapter 13: Introduction to Modeling Libraries in Python</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>3  4.0 -4.10    1.3<br/>4  5.0  0.00   -2.0<br/></p>
<p>The .values attribute is intended to be used when your data is<br/>homogeneous&#8212;for example, all numeric types. If you have hetero&#8208;<br/>geneous data, the result will be an ndarray of Python objects:<br/></p>
<p>In [18]: df3 = data.copy()<br/></p>
<p>In [19]: df3['strings'] = ['a', 'b', 'c', 'd', 'e']<br/></p>
<p>In [20]: df3<br/>Out[20]: <br/>   x0    x1    y strings<br/>0   1  0.01 -1.5       a<br/>1   2 -0.01  0.0       b<br/>2   3  0.25  3.6       c<br/>3   4 -4.10  1.3       d<br/>4   5  0.00 -2.0       e<br/></p>
<p>In [21]: df3.values<br/>Out[21]: <br/>array([[1, 0.01, -1.5, 'a'],<br/>       [2, -0.01, 0.0, 'b'],<br/>       [3, 0.25, 3.6, 'c'],<br/>       [4, -4.1, 1.3, 'd'],<br/>       [5, 0.0, -2.0, 'e']], dtype=object)<br/></p>
<p>For some models, you may only wish to use a subset of the columns. I recommend <br/>using loc indexing with values:<br/></p>
<p>In [22]: model_cols = ['x0', 'x1']<br/></p>
<p>In [23]: data.loc[:, model_cols].values<br/>Out[23]: <br/>array([[ 1.  ,  0.01],<br/>       [ 2.  , -0.01],<br/>       [ 3.  ,  0.25],<br/>       [ 4.  , -4.1 ],<br/>       [ 5.  ,  0.  ]])<br/></p>
<p>Some libraries have native support for pandas and do some of this work for you auto&#8208;<br/>matically: converting to NumPy from DataFrame and attaching model parameter<br/>names to the columns of output tables or Series. In other cases, you will have to per&#8208;<br/>form this &#8220;metadata management&#8221; manually.<br/>In Chapter 12 we looked at pandas&#8217;s Categorical type and the pandas.get_dummies<br/>function. Suppose we had a non-numeric column in our example dataset:<br/></p>
<p>13.1 Interfacing Between pandas and Model Code | 385</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In [24]: data['category'] = pd.Categorical(['a', 'b', 'a', 'a', 'b'],<br/>   ....:                                   categories=['a', 'b'])<br/></p>
<p>In [25]: data<br/>Out[25]: <br/>   x0    x1    y category<br/>0   1  0.01 -1.5        a<br/>1   2 -0.01  0.0        b<br/>2   3  0.25  3.6        a<br/>3   4 -4.10  1.3        a<br/>4   5  0.00 -2.0        b<br/></p>
<p>If we wanted to replace the 'category' column with dummy variables, we create <br/>dummy variables, drop the 'category' column, and then join the result:<br/></p>
<p>In [26]: dummies = pd.get_dummies(data.category, prefix='category')<br/></p>
<p>In [27]: data_with_dummies = data.drop('category', axis=1).join(dummies)<br/></p>
<p>In [28]: data_with_dummies<br/>Out[28]: <br/>   x0    x1    y  category_a  category_b<br/>0   1  0.01 -1.5           1           0<br/>1   2 -0.01  0.0           0           1<br/>2   3  0.25  3.6           1           0<br/>3   4 -4.10  1.3           1           0<br/>4   5  0.00 -2.0           0           1<br/></p>
<p>There are some nuances to fitting certain statistical models with dummy variables. It<br/>may be simpler and less error-prone to use Patsy (the subject of the next section)<br/>when you have more than simple numeric columns.<br/></p>
<p>13.2 Creating Model Descriptions with Patsy<br/>Patsy is a Python library for describing statistical models (especially linear models)<br/>with a small string-based &#8220;formula syntax,&#8221; which is inspired by (but not exactly the<br/>same as) the formula syntax used by the R and S statistical programming languages.<br/>Patsy is well supported for specifying linear models in statsmodels, so I will focus on<br/>some of the main features to help you get up and running. Patsy&#8217;s <i>formulas</i> are a spe&#8208;<br/>cial string syntax that looks like:<br/></p>
<p>y ~ x0 + x1<br/></p>
<p>The syntax a + b does not mean to add a to b, but rather that these are <i>terms</i> in the<br/><i>design matrix</i> created for the model. The patsy.dmatrices function takes a formula<br/>string along with a dataset (which can be a DataFrame or a dict of arrays) and pro&#8208;<br/>duces design matrices for a linear model:<br/></p>
<p>In [29]: data = pd.DataFrame({<br/>   ....:     'x0': [1, 2, 3, 4, 5],<br/></p>
<p>386 | Chapter 13: Introduction to Modeling Libraries in Python</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>   ....:     'x1': [0.01, -0.01, 0.25, -4.1, 0.],<br/>   ....:     'y': [-1.5, 0., 3.6, 1.3, -2.]})<br/></p>
<p>In [30]: data<br/>Out[30]: <br/>   x0    x1    y<br/>0   1  0.01 -1.5<br/>1   2 -0.01  0.0<br/>2   3  0.25  3.6<br/>3   4 -4.10  1.3<br/>4   5  0.00 -2.0<br/></p>
<p>In [31]: <b>import</b> <b>patsy<br/></b></p>
<p>In [32]: y, X = patsy.dmatrices('y ~ x0 + x1', data)<br/></p>
<p>Now we have:<br/>In [33]: y<br/>Out[33]: <br/>DesignMatrix <b>with</b> shape (5, 1)<br/>     y<br/>  -1.5<br/>   0.0<br/>   3.6<br/>   1.3<br/>  -2.0<br/>  Terms:<br/>    'y' (column 0)<br/></p>
<p>In [34]: X<br/>Out[34]: <br/>DesignMatrix <b>with</b> shape (5, 3)<br/>  Intercept  x0     x1<br/>          1   1   0.01<br/>          1   2  -0.01<br/>          1   3   0.25<br/>          1   4  -4.10<br/>          1   5   0.00<br/>  Terms:<br/>    'Intercept' (column 0)<br/>    'x0' (column 1)<br/>    'x1' (column 2)<br/></p>
<p>These Patsy DesignMatrix instances are NumPy ndarrays with additional metadata:<br/>In [35]: np.asarray(y)<br/>Out[35]: <br/>array([[-1.5],<br/>       [ 0. ],<br/>       [ 3.6],<br/>       [ 1.3],<br/>       [-2. ]])<br/></p>
<p>13.2 Creating Model Descriptions with Patsy | 387</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In [36]: np.asarray(X)<br/>Out[36]: <br/>array([[ 1.  ,  1.  ,  0.01],<br/>       [ 1.  ,  2.  , -0.01],<br/>       [ 1.  ,  3.  ,  0.25],<br/>       [ 1.  ,  4.  , -4.1 ],<br/>       [ 1.  ,  5.  ,  0.  ]])<br/></p>
<p>You might wonder where the Intercept term came from. This is a convention for<br/>linear models like ordinary least squares (OLS) regression. You can suppress the<br/>intercept by adding the term + 0 to the model:<br/></p>
<p>In [37]: patsy.dmatrices('y ~ x0 + x1 + 0', data)[1]<br/>Out[37]: <br/>DesignMatrix <b>with</b> shape (5, 2)<br/>  x0     x1<br/>   1   0.01<br/>   2  -0.01<br/>   3   0.25<br/>   4  -4.10<br/>   5   0.00<br/>  Terms:<br/>    'x0' (column 0)<br/>    'x1' (column 1)<br/></p>
<p>The Patsy objects can be passed directly into algorithms like numpy.linalg.lstsq,<br/>which performs an ordinary least squares regression:<br/></p>
<p>In [38]: coef, resid, _, _ = np.linalg.lstsq(X, y)<br/></p>
<p>The model metadata is retained in the design_info attribute, so you can reattach the<br/>model column names to the fitted coefficients to obtain a Series, for example:<br/></p>
<p>In [39]: coef<br/>Out[39]: <br/>array([[ 0.3129],<br/>       [-0.0791],<br/>       [-0.2655]])<br/></p>
<p>In [40]: coef = pd.Series(coef.squeeze(), index=X.design_info.column_names)<br/></p>
<p>In [41]: coef<br/>Out[41]: <br/>Intercept    0.312910<br/>x0          -0.079106<br/>x1          -0.265464<br/>dtype: float64<br/></p>
<p>388 | Chapter 13: Introduction to Modeling Libraries in Python</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Data Transformations in Patsy Formulas<br/>You can mix Python code into your Patsy formulas; when evaluating the formula the<br/>library will try to find the functions you use in the enclosing scope:<br/></p>
<p>In [42]: y, X = patsy.dmatrices('y ~ x0 + np.log(np.abs(x1) + 1)', data)<br/></p>
<p>In [43]: X<br/>Out[43]: <br/>DesignMatrix <b>with</b> shape (5, 3)<br/>  Intercept  x0  np.log(np.abs(x1) + 1)<br/>          1   1                 0.00995<br/>          1   2                 0.00995<br/>          1   3                 0.22314<br/>          1   4                 1.62924<br/>          1   5                 0.00000<br/>  Terms:<br/>    'Intercept' (column 0)<br/>    'x0' (column 1)<br/>    'np.log(np.abs(x1) + 1)' (column 2)<br/></p>
<p>Some commonly used variable transformations include standardizing (to mean 0 and<br/>variance 1) and centering (subtracting the mean). Patsy has built-in functions for this<br/>purpose:<br/></p>
<p>In [44]: y, X = patsy.dmatrices('y ~ standardize(x0) + center(x1)', data)<br/></p>
<p>In [45]: X<br/>Out[45]: <br/>DesignMatrix <b>with</b> shape (5, 3)<br/>  Intercept  standardize(x0)  center(x1)<br/>          1         -1.41421        0.78<br/>          1         -0.70711        0.76<br/>          1          0.00000        1.02<br/>          1          0.70711       -3.33<br/>          1          1.41421        0.77<br/>  Terms:<br/>    'Intercept' (column 0)<br/>    'standardize(x0)' (column 1)<br/>    'center(x1)' (column 2)<br/></p>
<p>As part of a modeling process, you may fit a model on one dataset, then evaluate the<br/>model based on another. This might be a <i>hold-out</i> portion or new data that is<br/>observed later. When applying transformations like center and standardize, you<br/>should be careful when using the model to form predications based on new data.<br/>These are called <i>stateful</i> transformations, because you must use statistics like the<br/>mean or standard deviation of the original dataset when transforming a new dataset.<br/>The patsy.build_design_matrices function can apply transformations to new <i>out-<br/>of-sample</i> data using the saved information from the original <i>in-sample</i> dataset:<br/></p>
<p>13.2 Creating Model Descriptions with Patsy | 389</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In [46]: new_data = pd.DataFrame({<br/>   ....:     'x0': [6, 7, 8, 9],<br/>   ....:     'x1': [3.1, -0.5, 0, 2.3],<br/>   ....:     'y': [1, 2, 3, 4]})<br/></p>
<p>In [47]: new_X = patsy.build_design_matrices([X.design_info], new_data)<br/></p>
<p>In [48]: new_X<br/>Out[48]: <br/>[DesignMatrix <b>with</b> shape (4, 3)<br/>   Intercept  standardize(x0)  center(x1)<br/>           1          2.12132        3.87<br/>           1          2.82843        0.27<br/>           1          3.53553        0.77<br/>           1          4.24264        3.07<br/>   Terms:<br/>     'Intercept' (column 0)<br/>     'standardize(x0)' (column 1)<br/>     'center(x1)' (column 2)]<br/></p>
<p>Because the plus symbol (+) in the context of Patsy formulas does not mean addition,<br/>when you want to add columns from a dataset by name, you must wrap them in the<br/>special <i>I</i> function:<br/></p>
<p>In [49]: y, X = patsy.dmatrices('y ~ I(x0 + x1)', data)<br/></p>
<p>In [50]: X<br/>Out[50]: <br/>DesignMatrix <b>with</b> shape (5, 2)<br/>  Intercept  I(x0 + x1)<br/>          1        1.01<br/>          1        1.99<br/>          1        3.25<br/>          1       -0.10<br/>          1        5.00<br/>  Terms:<br/>    'Intercept' (column 0)<br/>    'I(x0 + x1)' (column 1)<br/></p>
<p>Patsy has several other built-in transforms in the patsy.builtins module. See the<br/>online documentation for more.<br/>Categorical data has a special class of transformations, which I explain next.<br/></p>
<p>Categorical Data and Patsy<br/>Non-numeric data can be transformed for a model design matrix in many different<br/>ways. A complete treatment of this topic is outside the scope of this book and would<br/>be best studied along with a course in statistics.<br/></p>
<p>390 | Chapter 13: Introduction to Modeling Libraries in Python</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>When you use non-numeric terms in a Patsy formula, they are converted to dummy<br/>variables by default. If there is an intercept, one of the levels will be left out to avoid<br/>collinearity:<br/></p>
<p>In [51]: data = pd.DataFrame({<br/>   ....:     'key1': ['a', 'a', 'b', 'b', 'a', 'b', 'a', 'b'],<br/>   ....:     'key2': [0, 1, 0, 1, 0, 1, 0, 0],<br/>   ....:     'v1': [1, 2, 3, 4, 5, 6, 7, 8],<br/>   ....:     'v2': [-1, 0, 2.5, -0.5, 4.0, -1.2, 0.2, -1.7]<br/>   ....: })<br/></p>
<p>In [52]: y, X = patsy.dmatrices('v2 ~ key1', data)<br/></p>
<p>In [53]: X<br/>Out[53]: <br/>DesignMatrix <b>with</b> shape (8, 2)<br/>  Intercept  key1[T.b]<br/>          1          0<br/>          1          0<br/>          1          1<br/>          1          1<br/>          1          0<br/>          1          1<br/>          1          0<br/>          1          1<br/>  Terms:<br/>    'Intercept' (column 0)<br/>    'key1' (column 1)<br/></p>
<p>If you omit the intercept from the model, then columns for each category value will<br/>be included in the model design matrix:<br/></p>
<p>In [54]: y, X = patsy.dmatrices('v2 ~ key1 + 0', data)<br/></p>
<p>In [55]: X<br/>Out[55]: <br/>DesignMatrix <b>with</b> shape (8, 2)<br/>  key1[a]  key1[b]<br/>        1        0<br/>        1        0<br/>        0        1<br/>        0        1<br/>        1        0<br/>        0        1<br/>        1        0<br/>        0        1<br/>  Terms:<br/>    'key1' (columns 0:2)<br/></p>
<p>Numeric columns can be interpreted as categorical with the C function:<br/>In [56]: y, X = patsy.dmatrices('v2 ~ C(key2)', data)<br/></p>
<p>13.2 Creating Model Descriptions with Patsy | 391</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In [57]: X<br/>Out[57]: <br/>DesignMatrix <b>with</b> shape (8, 2)<br/>  Intercept  C(key2)[T.1]<br/>          1             0<br/>          1             1<br/>          1             0<br/>          1             1<br/>          1             0<br/>          1             1<br/>          1             0<br/>          1             0<br/>  Terms:<br/>    'Intercept' (column 0)<br/>    'C(key2)' (column 1)<br/></p>
<p>When you&#8217;re using multiple categorical terms in a model, things can be more compli&#8208;<br/>cated, as you can include interaction terms of the form key1:key2, which can be<br/>used, for example, in analysis of variance (ANOVA) models:<br/></p>
<p>In [58]: data['key2'] = data['key2'].map({0: 'zero', 1: 'one'})<br/></p>
<p>In [59]: data<br/>Out[59]: <br/>  key1  key2  v1   v2<br/>0    a  zero   1 -1.0<br/>1    a   one   2  0.0<br/>2    b  zero   3  2.5<br/>3    b   one   4 -0.5<br/>4    a  zero   5  4.0<br/>5    b   one   6 -1.2<br/>6    a  zero   7  0.2<br/>7    b  zero   8 -1.7<br/></p>
<p>In [60]: y, X = patsy.dmatrices('v2 ~ key1 + key2', data)<br/></p>
<p>In [61]: X<br/>Out[61]: <br/>DesignMatrix <b>with</b> shape (8, 3)<br/>  Intercept  key1[T.b]  key2[T.zero]<br/>          1          0             1<br/>          1          0             0<br/>          1          1             1<br/>          1          1             0<br/>          1          0             1<br/>          1          1             0<br/>          1          0             1<br/>          1          1             1<br/>  Terms:<br/>    'Intercept' (column 0)<br/>    'key1' (column 1)<br/>    'key2' (column 2)<br/></p>
<p>392 | Chapter 13: Introduction to Modeling Libraries in Python</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In [62]: y, X = patsy.dmatrices('v2 ~ key1 + key2 + key1:key2', data)<br/></p>
<p>In [63]: X<br/>Out[63]: <br/>DesignMatrix <b>with</b> shape (8, 4)<br/>  Intercept  key1[T.b]  key2[T.zero]  key1[T.b]:key2[T.zero]<br/>          1          0             1                       0<br/>          1          0             0                       0<br/>          1          1             1                       1<br/>          1          1             0                       0<br/>          1          0             1                       0<br/>          1          1             0                       0<br/>          1          0             1                       0<br/>          1          1             1                       1<br/>  Terms:<br/>    'Intercept' (column 0)<br/>    'key1' (column 1)<br/>    'key2' (column 2)<br/>    'key1:key2' (column 3)<br/></p>
<p>Patsy provides for other ways to transform categorical data, including transforma&#8208;<br/>tions for terms with a particular ordering. See the online documentation for more.<br/></p>
<p>13.3 Introduction to statsmodels<br/>statsmodels is a Python library for fitting many kinds of statistical models, perform&#8208;<br/>ing statistical tests, and data exploration and visualization. Statsmodels contains more<br/>&#8220;classical&#8221; frequentist statistical methods, while Bayesian methods and machine learn&#8208;<br/>ing models are found in other libraries.<br/>Some kinds of models found in statsmodels include:<br/></p>
<p>&#8226; Linear models, generalized linear models, and robust linear models<br/>&#8226; Linear mixed effects models<br/>&#8226; Analysis of variance (ANOVA) methods<br/>&#8226; Time series processes and state space models<br/>&#8226; Generalized method of moments<br/></p>
<p>In the next few pages, we will use a few basic tools in statsmodels and explore how to<br/>use the modeling interfaces with Patsy formulas and pandas DataFrame objects.<br/></p>
<p>Estimating Linear Models<br/>There are several kinds of linear regression models in statsmodels, from the more<br/>basic (e.g., ordinary least squares) to more complex (e.g., iteratively reweighted least<br/>squares).<br/></p>
<p>13.3 Introduction to statsmodels | 393</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Linear models in statsmodels have two different main interfaces: array-based and<br/>formula-based. These are accessed through these API module imports:<br/></p>
<p><b>import</b> <b>statsmodels.api</b> <b>as</b> <b>sm<br/>import</b> <b>statsmodels.formula.api</b> <b>as</b> <b>smf<br/></b></p>
<p>To show how to use these, we generate a linear model from some random data:<br/><b>def</b> dnorm(mean, variance, size=1):<br/>    <b>if</b> isinstance(size, int):<br/>        size = size,<br/>    <b>return</b> mean + np.sqrt(variance) * np.random.randn(*size)<br/></p>
<p><i># For reproducibility<br/></i>np.random.seed(12345)<br/></p>
<p>N = 100<br/>X = np.c_[dnorm(0, 0.4, size=N),<br/>          dnorm(0, 0.6, size=N),<br/>          dnorm(0, 0.2, size=N)]<br/>eps = dnorm(0, 0.1, size=N)<br/>beta = [0.1, 0.3, 0.5]<br/></p>
<p>y = np.dot(X, beta) + eps<br/></p>
<p>Here, I wrote down the &#8220;true&#8221; model with known parameters beta. In this case, dnorm<br/>is a helper function for generating normally distributed data with a particular mean<br/>and variance. So now we have:<br/></p>
<p>In [66]: X[:5]<br/>Out[66]: <br/>array([[-0.1295, -1.2128,  0.5042],<br/>       [ 0.3029, -0.4357, -0.2542],<br/>       [-0.3285, -0.0253,  0.1384],<br/>       [-0.3515, -0.7196, -0.2582],<br/>       [ 1.2433, -0.3738, -0.5226]])<br/></p>
<p>In [67]: y[:5]<br/>Out[67]: array([ 0.4279, -0.6735, -0.0909, -0.4895, -0.1289])<br/></p>
<p>A linear model is generally fitted with an intercept term as we saw before with Patsy.<br/>The sm.add_constant function can add an intercept column to an existing matrix:<br/></p>
<p>In [68]: X_model = sm.add_constant(X)<br/></p>
<p>In [69]: X_model[:5]<br/>Out[69]: <br/>array([[ 1.    , -0.1295, -1.2128,  0.5042],<br/>       [ 1.    ,  0.3029, -0.4357, -0.2542],<br/>       [ 1.    , -0.3285, -0.0253,  0.1384],<br/>       [ 1.    , -0.3515, -0.7196, -0.2582],<br/>       [ 1.    ,  1.2433, -0.3738, -0.5226]])<br/></p>
<p>394 | Chapter 13: Introduction to Modeling Libraries in Python</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>The sm.OLS class can fit an ordinary least squares linear regression:<br/>In [70]: model = sm.OLS(y, X)<br/></p>
<p>The model&#8217;s fit method returns a regression results object containing estimated<br/>model parameters and other diagnostics:<br/></p>
<p>In [71]: results = model.fit()<br/></p>
<p>In [72]: results.params<br/>Out[72]: array([ 0.1783,  0.223 ,  0.501 ])<br/></p>
<p>The summary method on results can print a model detailing diagnostic output of the<br/>model:<br/></p>
<p>In [73]: <b>print</b>(results.summary())<br/>OLS Regression Results                            <br/>==============================================================================<br/>Dep. Variable:                      y   R-squared:                       0.430<br/>Model:                            OLS   Adj. R-squared:                  0.413<br/>Method:                 Least Squares   F-statistic:                     24.42<br/>Date:                Mon, 25 Sep 2017   Prob (F-statistic):           7.44e-12<br/>Time:                        14:06:15   Log-Likelihood:                -34.305<br/>No. Observations:                 100   AIC:                             74.61<br/>Df Residuals:                      97   BIC:                             82.42<br/>Df Model:                           3                                         <br/>Covariance Type:            nonrobust                                         <br/>==============================================================================<br/>                 coef    std err          t      P&gt;|t|      [0.025      0.975]<br/>------------------------------------------------------------------------------<br/>x1             0.1783      0.053      3.364      0.001       0.073       0.283<br/>x2             0.2230      0.046      4.818      0.000       0.131       0.315<br/>x3             0.5010      0.080      6.237      0.000       0.342       0.660<br/>==============================================================================<br/>Omnibus:                        4.662   Durbin-Watson:                   2.201<br/>Prob(Omnibus):                  0.097   Jarque-Bera (JB):                4.098<br/>Skew:                           0.481   Prob(JB):                        0.129<br/>Kurtosis:                       3.243   Cond. No.                         1.74<br/>==============================================================================<br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors <b>is</b> correctly <br/>specified.<br/></p>
<p>The parameter names here have been given the generic names x1, x2, and so on.<br/>Suppose instead that all of the model parameters are in a DataFrame:<br/></p>
<p>In [74]: data = pd.DataFrame(X, columns=['col0', 'col1', 'col2'])<br/></p>
<p>In [75]: data['y'] = y<br/></p>
<p>In [76]: data[:5]<br/>Out[76]: <br/>       col0      col1      col2         y<br/></p>
<p>13.3 Introduction to statsmodels | 395</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>0 -0.129468 -1.212753  0.504225  0.427863<br/>1  0.302910 -0.435742 -0.254180 -0.673480<br/>2 -0.328522 -0.025302  0.138351 -0.090878<br/>3 -0.351475 -0.719605 -0.258215 -0.489494<br/>4  1.243269 -0.373799 -0.522629 -0.128941<br/></p>
<p>Now we can use the statsmodels formula API and Patsy formula strings:<br/>In [77]: results = smf.ols('y ~ col0 + col1 + col2', data=data).fit()<br/></p>
<p>In [78]: results.params<br/>Out[78]: <br/>Intercept    0.033559<br/>col0         0.176149<br/>col1         0.224826<br/>col2         0.514808<br/>dtype: float64<br/></p>
<p>In [79]: results.tvalues<br/>Out[79]: <br/>Intercept    0.952188<br/>col0         3.319754<br/>col1         4.850730<br/>col2         6.303971<br/>dtype: float64<br/></p>
<p>Observe how statsmodels has returned results as Series with the DataFrame column<br/>names attached. We also do not need to use add_constant when using formulas and<br/>pandas objects.<br/>Given new out-of-sample data, you can compute predicted values given the estimated<br/>model parameters:<br/></p>
<p>In [80]: results.predict(data[:5])<br/>Out[80]: <br/>0   -0.002327<br/>1   -0.141904<br/>2    0.041226<br/>3   -0.323070<br/>4   -0.100535<br/>dtype: float64<br/></p>
<p>There are many additional tools for analysis, diagnostics, and visualization of linear<br/>model results in statsmodels that you can explore. There are also other kinds of linear<br/>models beyond ordinary least squares.<br/></p>
<p>Estimating Time Series Processes<br/>Another class of models in statsmodels are for time series analysis. Among these are<br/>autoregressive processes, Kalman filtering and other state space models, and multi&#8208;<br/>variate autoregressive models.<br/></p>
<p>396 | Chapter 13: Introduction to Modeling Libraries in Python</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Let&#8217;s simulate some time series data with an autoregressive structure and noise:<br/>init_x = 4<br/></p>
<p><b>import</b> <b>random<br/></b>values = [init_x, init_x]<br/>N = 1000<br/></p>
<p>b0 = 0.8<br/>b1 = -0.4<br/>noise = dnorm(0, 0.1, N)<br/><b>for</b> i <b>in</b> range(N):<br/>    new_x = values[-1] * b0 + values[-2] * b1 + noise[i]<br/>    values.append(new_x)<br/></p>
<p>This data has an AR(2) structure (two <i>lags</i>) with parameters 0.8 and &#8211;0.4. When you<br/>fit an AR model, you may not know the number of lagged terms to include, so you<br/>can fit the model with some larger number of lags:<br/></p>
<p>In [82]: MAXLAGS = 5<br/></p>
<p>In [83]: model = sm.tsa.AR(values)<br/></p>
<p>In [84]: results = model.fit(MAXLAGS)<br/></p>
<p>The estimated parameters in the results have the intercept first and the estimates for<br/>the first two lags next:<br/></p>
<p>In [85]: results.params<br/>Out[85]: array([-0.0062,  0.7845, -0.4085, -0.0136,  0.015 ,  0.0143])<br/></p>
<p>Deeper details of these models and how to interpret their results is beyond what I can<br/>cover in this book, but there&#8217;s plenty more to discover in the statsmodels documenta&#8208;<br/>tion.<br/></p>
<p>13.4 Introduction to scikit-learn<br/>scikit-learn is one of the most widely used and trusted general-purpose Python<br/>machine learning toolkits. It contains a broad selection of standard supervised and<br/>unsupervised machine learning methods with tools for model selection and evalua&#8208;<br/>tion, data transformation, data loading, and model persistence. These models can be<br/>used for classification, clustering, prediction, and other common tasks.<br/>There are excellent online and printed resources for learning about machine learning<br/>and how to apply libraries like scikit-learn and TensorFlow to solve real-world prob&#8208;<br/>lems. In this section, I will give a brief flavor of the scikit-learn API style.<br/>At the time of this writing, scikit-learn does not have deep pandas integration, though<br/>there are some add-on third-party packages that are still in development. pandas can<br/>be very useful for massaging datasets prior to model fitting, though.<br/></p>
<p>13.4 Introduction to scikit-learn | 397</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>As an example, I use a now-classic dataset from a Kaggle competition about passen&#8208;<br/>ger survival rates on the <i>Titanic</i>, which sank in 1912. We load the test and training<br/>dataset using pandas:<br/></p>
<p>In [86]: train = pd.read_csv('datasets/titanic/train.csv')<br/></p>
<p>In [87]: test = pd.read_csv('datasets/titanic/test.csv')<br/></p>
<p>In [88]: train[:4]<br/>Out[88]: <br/>   PassengerId  Survived  Pclass  \<br/>0            1         0       3   <br/>1            2         1       1   <br/>2            3         1       3   <br/>3            4         1       1   <br/>                                                Name     Sex   Age  SibSp  \<br/>0                            Braund, Mr. Owen Harris    male  22.0      1   <br/>1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   <br/>2                             Heikkinen, Miss. Laina  female  26.0      0   <br/>3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   <br/>   Parch            Ticket     Fare Cabin Embarked  <br/>0      0         A/5 21171   7.2500   NaN        S  <br/>1      0          PC 17599  71.2833   C85        C  <br/>2      0  STON/O2. 3101282   7.9250   NaN        S  <br/>3      0            113803  53.1000  C123        S  <br/></p>
<p>Libraries like statsmodels and scikit-learn generally cannot be fed missing data, so we<br/>look at the columns to see if there are any that contain missing data:<br/></p>
<p>In [89]: train.isnull().sum()<br/>Out[89]: <br/>PassengerId      0<br/>Survived         0<br/>Pclass           0<br/>Name             0<br/>Sex              0<br/>Age            177<br/>SibSp            0<br/>Parch            0<br/>Ticket           0<br/>Fare             0<br/>Cabin          687<br/>Embarked         2<br/>dtype: int64<br/></p>
<p>In [90]: test.isnull().sum()<br/>Out[90]: <br/>PassengerId      0<br/>Pclass           0<br/>Name             0<br/>Sex              0<br/>Age             86<br/></p>
<p>398 | Chapter 13: Introduction to Modeling Libraries in Python</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>SibSp            0<br/>Parch            0<br/>Ticket           0<br/>Fare             1<br/>Cabin          327<br/>Embarked         0<br/>dtype: int64<br/></p>
<p>In statistics and machine learning examples like this one, a typical task is to predict<br/>whether a passenger would survive based on features in the data. A model is fitted on<br/>a <i>training</i> dataset and then evaluated on an out-of-sample <i>testing</i> dataset.<br/>I would like to use Age as a predictor, but it has missing data. There are a number of<br/>ways to do missing data imputation, but I will do a simple one and use the median of<br/>the training dataset to fill the nulls in both tables:<br/></p>
<p>In [91]: impute_value = train['Age'].median()<br/></p>
<p>In [92]: train['Age'] = train['Age'].fillna(impute_value)<br/></p>
<p>In [93]: test['Age'] = test['Age'].fillna(impute_value)<br/></p>
<p>Now we need to specify our models. I add a column IsFemale as an encoded version<br/>of the 'Sex' column:<br/></p>
<p>In [94]: train['IsFemale'] = (train['Sex'] == 'female').astype(int)<br/></p>
<p>In [95]: test['IsFemale'] = (test['Sex'] == 'female').astype(int)<br/></p>
<p>Then we decide on some model variables and create NumPy arrays:<br/>In [96]: predictors = ['Pclass', 'IsFemale', 'Age']<br/></p>
<p>In [97]: X_train = train[predictors].values<br/></p>
<p>In [98]: X_test = test[predictors].values<br/></p>
<p>In [99]: y_train = train['Survived'].values<br/></p>
<p>In [100]: X_train[:5]<br/>Out[100]: <br/>array([[  3.,   0.,  22.],<br/>       [  1.,   1.,  38.],<br/>       [  3.,   1.,  26.],<br/>       [  1.,   1.,  35.],<br/>       [  3.,   0.,  35.]])<br/></p>
<p>In [101]: y_train[:5]<br/>Out[101]: array([0, 1, 1, 1, 0])<br/></p>
<p>I make no claims that this is a good model nor that these features are engineered<br/>properly. We use the LogisticRegression model from scikit-learn and create a<br/>model instance:<br/></p>
<p>13.4 Introduction to scikit-learn | 399</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In [102]: <b>from</b> <b>sklearn.linear_model</b> <b>import</b> LogisticRegression<br/></p>
<p>In [103]: model = LogisticRegression()<br/></p>
<p>Similar to statsmodels, we can fit this model to the training data using the model&#8217;s fit<br/>method:<br/></p>
<p>In [104]: model.fit(X_train, y_train)<br/>Out[104]: <br/>LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,<br/>          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,<br/>          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,<br/>          verbose=0, warm_start=False)<br/></p>
<p>Now, we can form predictions for the test dataset using model.predict:<br/>In [105]: y_predict = model.predict(X_test)<br/></p>
<p>In [106]: y_predict[:10]<br/>Out[106]: array([0, 0, 0, 0, 1, 0, 1, 0, 1, 0])<br/></p>
<p>If you had the true values for the test dataset, you could compute an accuracy per&#8208;<br/>centage or some other error metric:<br/></p>
<p>(y_true == y_predict).mean()<br/></p>
<p>In practice, there are often many additional layers of complexity in model training.<br/>Many models have parameters that can be tuned, and there are techniques such as<br/><i>cross-validation</i> that can be used for parameter tuning to avoid overfitting to the<br/>training data. This can often yield better predictive performance or robustness on<br/>new data.<br/>Cross-validation works by splitting the training data to simulate out-of-sample pre&#8208;<br/>diction. Based on a model accuracy score like mean squared error, one can perform a<br/>grid search on model parameters. Some models, like logistic regression, have estima&#8208;<br/>tor classes with built-in cross-validation. For example, the LogisticRegressionCV<br/>class can be used with a parameter indicating how fine-grained of a grid search to do<br/>on the model regularization parameter C:<br/></p>
<p>In [107]: <b>from</b> <b>sklearn.linear_model</b> <b>import</b> LogisticRegressionCV<br/></p>
<p>In [108]: model_cv = LogisticRegressionCV(10)<br/></p>
<p>In [109]: model_cv.fit(X_train, y_train)<br/>Out[109]: <br/>LogisticRegressionCV(Cs=10, class_weight=None, cv=None, dual=False,<br/>           fit_intercept=True, intercept_scaling=1.0, max_iter=100,<br/>           multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,<br/>           refit=True, scoring=None, solver='lbfgs', tol=0.0001, verbose=0)<br/></p>
<p>400 | Chapter 13: Introduction to Modeling Libraries in Python</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>To do cross-validation by hand, you can use the cross_val_score helper function,<br/>which handles the data splitting process. For example, to cross-validate our model<br/>with four non-overlapping splits of the training data, we can do:<br/></p>
<p>In [110]: <b>from</b> <b>sklearn.model_selection</b> <b>import</b> cross_val_score<br/></p>
<p>In [111]: model = LogisticRegression(C=10)<br/></p>
<p>In [112]: scores = cross_val_score(model, X_train, y_train, cv=4)<br/></p>
<p>In [113]: scores<br/>Out[113]: array([ 0.7723,  0.8027,  0.7703,  0.7883])<br/></p>
<p>The default scoring metric is model-dependent, but it is possible to choose an explicit<br/>scoring function. Cross-validated models take longer to train, but can often yield bet&#8208;<br/>ter model performance.<br/></p>
<p>13.5 Continuing Your Education<br/>While I have only skimmed the surface of some Python modeling libraries, there are<br/>more and more frameworks for various kinds of statistics and machine learning<br/>either implemented in Python or with a Python user interface.<br/>This book is focused especially on data wrangling, but there are many others dedica&#8208;<br/>ted to modeling and data science tools. Some excellent ones are:<br/></p>
<p>&#8226; <i>Introduction to Machine Learning with Python</i> by Andreas Mueller and Sarah<br/>Guido (O&#8217;Reilly)<br/></p>
<p>&#8226; <i>Python Data Science Handbook</i> by Jake VanderPlas (O&#8217;Reilly)<br/>&#8226; <i>Data Science from Scratch: First Principles with Python</i> by Joel Grus (O&#8217;Reilly)<br/>&#8226; <i>Python Machine Learning</i> by Sebastian Raschka (Packt Publishing)<br/>&#8226; <i>Hands-On Machine Learning with Scikit-Learn and TensorFlow</i> by Aur&#233;lien<br/></p>
<p>G&#233;ron (O&#8217;Reilly)<br/>While books can be valuable resources for learning, they can sometimes grow out of<br/>date when the underlying open source software changes. It&#8217;s a good idea to be familiar<br/>with the documentation for the various statistics or machine learning frameworks to<br/>stay up to date on the latest features and API.<br/></p>
<p>13.5 Continuing Your Education | 401</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div>
</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>CHAPTER 14<br/>Data Analysis Examples<br/></p>
<p>Now that we&#8217;ve reached the end of this book&#8217;s main chapters, we&#8217;re going to take a<br/>look at a number of real-world datasets. For each dataset, we&#8217;ll use the techniques<br/>presented in this book to extract meaning from the raw data. The demonstrated tech&#8208;<br/>niques can be applied to all manner of other datasets, including your own. This chap&#8208;<br/>ter contains a collection of miscellaneous example datasets that you can use for<br/>practice with the tools in this book.<br/>The example datasets are found in the book&#8217;s accompanying GitHub repository.<br/></p>
<p>14.1 1.USA.gov Data from Bitly<br/>In 2011, URL shortening service Bitly partnered with the US government website<br/>USA.gov to provide a feed of anonymous data gathered from users who shorten links<br/>ending with <i>.gov</i> or <i>.mil</i>. In 2011, a live feed as well as hourly snapshots were available<br/>as downloadable text files. This service is shut down at the time of this writing (2017),<br/>but we preserved one of the data files for the book&#8217;s examples.<br/>In the case of the hourly snapshots, each line in each file contains a common form of<br/>web data known as JSON, which stands for JavaScript Object Notation. For example,<br/>if we read just the first line of a file we may see something like this:<br/></p>
<p>In [5]: path = 'datasets/bitly_usagov/example.txt'<br/></p>
<p>In [6]: open(path).readline()<br/>Out[6]: '{ &quot;a&quot;: &quot;Mozilla<b>\\</b>/5.0 (Windows NT 6.1; WOW64) AppleWebKit<b>\\</b>/535.11<br/>(KHTML, like Gecko) Chrome\\/17.0.963.78 Safari\\/535.11&quot;, &quot;c&quot;: &quot;US&quot;, &quot;nk&quot;: 1,<br/>&quot;tz&quot;: &quot;America<b>\\</b>/New_York&quot;, &quot;gr&quot;: &quot;MA&quot;, &quot;g&quot;: &quot;A6qOVH&quot;, &quot;h&quot;: &quot;wfLQtf&quot;, &quot;l&quot;:<br/>&quot;orofrog&quot;, &quot;al&quot;: &quot;en-US,en;q=0.8&quot;, &quot;hh&quot;: &quot;1.usa.gov&quot;, &quot;r&quot;:<br/>&quot;http:<b>\\</b>/<b>\\</b>/www.facebook.com<b>\\</b>/l<b>\\</b>/7AQEFzjSi<b>\\</b>/1.usa.gov<b>\\</b>/wfLQtf&quot;, &quot;u&quot;:<br/>&quot;http:<b>\\</b>/<b>\\</b>/www.ncbi.nlm.nih.gov<b>\\</b>/pubmed<b>\\</b>/22415991&quot;, &quot;t&quot;: 1331923247, &quot;hc&quot;:<br/>1331822918, &quot;cy&quot;: &quot;Danvers&quot;, &quot;ll&quot;: [ 42.576698, -70.954903 ] }\n'<br/></p>
<p>403</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Python has both built-in and third-party libraries for converting a JSON string into a<br/>Python dictionary object. Here we&#8217;ll use the json module and its loads function<br/>invoked on each line in the sample file we downloaded:<br/></p>
<p><b>import</b> <b>json<br/></b>path = 'datasets/bitly_usagov/example.txt'<br/>records = [json.loads(line) <b>for</b> line <b>in</b> open(path)]<br/></p>
<p>The resulting object records is now a list of Python dicts:<br/>In [18]: records[0]<br/>Out[18]:<br/>{'a': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.11 (KHTML, like Gecko)<br/>Chrome/17.0.963.78 Safari/535.11',<br/> 'al': 'en-US,en;q=0.8',<br/> 'c': 'US',<br/> 'cy': 'Danvers',<br/> 'g': 'A6qOVH',<br/> 'gr': 'MA',<br/> 'h': 'wfLQtf',<br/> 'hc': 1331822918,<br/> 'hh': '1.usa.gov',<br/> 'l': 'orofrog',<br/> 'll': [42.576698, -70.954903],<br/> 'nk': 1,<br/> 'r': 'http://www.facebook.com/l/7AQEFzjSi/1.usa.gov/wfLQtf',<br/> 't': 1331923247,<br/> 'tz': 'America/New_York',<br/> 'u': 'http://www.ncbi.nlm.nih.gov/pubmed/22415991'}<br/></p>
<p>Counting Time Zones in Pure Python<br/>Suppose we were interested in finding the most often-occurring time zones in the<br/>dataset (the tz field). There are many ways we could do this. First, let&#8217;s extract a list of<br/>time zones again using a list comprehension:<br/></p>
<p>In [12]: time_zones = [rec['tz'] <b>for</b> rec <b>in</b> records]<br/>---------------------------------------------------------------------------<br/><b>KeyError</b>                                  Traceback (most recent call last)<br/>&lt;ipython-input-12-db4fbd348da9&gt; <b>in</b> &lt;module&gt;()<br/>----&gt; 1 time_zones = [rec['tz'] <b>for</b> rec <b>in</b> records]<br/>&lt;ipython-input-12-db4fbd348da9&gt; <b>in</b> &lt;listcomp&gt;(.0)<br/>----&gt; 1 time_zones = [rec['tz'] <b>for</b> rec <b>in</b> records]<br/><b>KeyError</b>: 'tz'<br/></p>
<p>Oops! Turns out that not all of the records have a time zone field. This is easy to han&#8208;<br/>dle, as we can add the check if 'tz' in rec at the end of the list comprehension:<br/></p>
<p>In [13]: time_zones = [rec['tz'] <b>for</b> rec <b>in</b> records <b>if</b> 'tz' <b>in</b> rec]<br/></p>
<p>In [14]: time_zones[:10]<br/>Out[14]: <br/></p>
<p>404 | Chapter 14: Data Analysis Examples</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>['America/New_York',<br/> 'America/Denver',<br/> 'America/New_York',<br/> 'America/Sao_Paulo',<br/> 'America/New_York',<br/> 'America/New_York',<br/> 'Europe/Warsaw',<br/> '',<br/> '',<br/> '']<br/></p>
<p>Just looking at the first 10 time zones, we see that some of them are unknown (empty<br/>string). You can filter these out also, but I&#8217;ll leave them in for now. Now, to produce<br/>counts by time zone I&#8217;ll show two approaches: the harder way (using just the Python<br/>standard library) and the easier way (using pandas). One way to do the counting is to<br/>use a dict to store counts while we iterate through the time zones:<br/></p>
<p><b>def</b> get_counts(sequence):<br/>    counts = {}<br/>    <b>for</b> x <b>in</b> sequence:<br/>        <b>if</b> x <b>in</b> counts:<br/>            counts[x] += 1<br/>        <b>else</b>:<br/>            counts[x] = 1<br/>    <b>return</b> counts<br/></p>
<p>Using more advanced tools in the Python standard library, you can write the same<br/>thing more briefly:<br/></p>
<p><b>from</b> <b>collections</b> <b>import</b> defaultdict<br/></p>
<p><b>def</b> get_counts2(sequence):<br/>    counts = defaultdict(int) <i># values will initialize to 0<br/></i>    <b>for</b> x <b>in</b> sequence:<br/>        counts[x] += 1<br/>    <b>return</b> counts<br/></p>
<p>I put this logic in a function just to make it more reusable. To use it on the time<br/>zones, just pass the time_zones list:<br/></p>
<p>In [17]: counts = get_counts(time_zones)<br/></p>
<p>In [18]: counts['America/New_York']<br/>Out[18]: 1251<br/></p>
<p>In [19]: len(time_zones)<br/>Out[19]: 3440<br/></p>
<p>If we wanted the top 10 time zones and their counts, we can do a bit of dictionary<br/>acrobatics:<br/></p>
<p><b>def</b> top_counts(count_dict, n=10):<br/>    value_key_pairs = [(count, tz) <b>for</b> tz, count <b>in</b> count_dict.items()]<br/></p>
<p>14.1 1.USA.gov Data from Bitly | 405</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>    value_key_pairs.sort()<br/>    <b>return</b> value_key_pairs[-n:]<br/></p>
<p>We have then:<br/>In [21]: top_counts(counts)<br/>Out[21]: <br/>[(33, 'America/Sao_Paulo'),<br/> (35, 'Europe/Madrid'),<br/> (36, 'Pacific/Honolulu'),<br/> (37, 'Asia/Tokyo'),<br/> (74, 'Europe/London'),<br/> (191, 'America/Denver'),<br/> (382, 'America/Los_Angeles'),<br/> (400, 'America/Chicago'),<br/> (521, ''),<br/> (1251, 'America/New_York')]<br/></p>
<p>If you search the Python standard library, you may find the collections.Counter<br/>class, which makes this task a lot easier:<br/></p>
<p>In [22]: <b>from</b> <b>collections</b> <b>import</b> Counter<br/></p>
<p>In [23]: counts = Counter(time_zones)<br/></p>
<p>In [24]: counts.most_common(10)<br/>Out[24]: <br/>[('America/New_York', 1251),<br/> ('', 521),<br/> ('America/Chicago', 400),<br/> ('America/Los_Angeles', 382),<br/> ('America/Denver', 191),<br/> ('Europe/London', 74),<br/> ('Asia/Tokyo', 37),<br/> ('Pacific/Honolulu', 36),<br/> ('Europe/Madrid', 35),<br/> ('America/Sao_Paulo', 33)]<br/></p>
<p>Counting Time Zones with pandas<br/>Creating a DataFrame from the original set of records is as easy as passing the list of<br/>records to pandas.DataFrame:<br/></p>
<p>In [25]: <b>import</b> <b>pandas</b> <b>as</b> <b>pd<br/></b></p>
<p>In [26]: frame = pd.DataFrame(records)<br/></p>
<p>In [27]: frame.info()<br/>&lt;<b>class</b> '<b>pandas</b>.core.frame.DataFrame'&gt;<br/>RangeIndex: 3560 entries, 0 to 3559<br/>Data columns (total 18 columns):<br/>_heartbeat_    120 non-null float64<br/>a              3440 non-null object<br/></p>
<p>406 | Chapter 14: Data Analysis Examples</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>al             3094 non-null object<br/>c              2919 non-null object<br/>cy             2919 non-null object<br/>g              3440 non-null object<br/>gr             2919 non-null object<br/>h              3440 non-null object<br/>hc             3440 non-null float64<br/>hh             3440 non-null object<br/>kw             93 non-null object<br/>l              3440 non-null object<br/>ll             2919 non-null object<br/>nk             3440 non-null float64<br/>r              3440 non-null object<br/>t              3440 non-null float64<br/>tz             3440 non-null object<br/>u              3440 non-null object<br/>dtypes: float64(4), object(14)<br/>memory usage: 500.7+ KB<br/></p>
<p>In [28]: frame['tz'][:10]<br/>Out[28]: <br/>0     America/New_York<br/>1       America/Denver<br/>2     America/New_York<br/>3    America/Sao_Paulo<br/>4     America/New_York<br/>5     America/New_York<br/>6        Europe/Warsaw<br/>7                     <br/>8                     <br/>9                     <br/>Name: tz, dtype: object<br/></p>
<p>The output shown for the frame is the <i>summary view</i>, shown for large DataFrame<br/>objects. We can then use the value_counts method for Series:<br/></p>
<p>In [29]: tz_counts = frame['tz'].value_counts()<br/></p>
<p>In [30]: tz_counts[:10]<br/>Out[30]: <br/>America/New_York       1251<br/>                        521<br/>America/Chicago         400<br/>America/Los_Angeles     382<br/>America/Denver          191<br/>Europe/London            74<br/>Asia/Tokyo               37<br/>Pacific/Honolulu         36<br/>Europe/Madrid            35<br/>America/Sao_Paulo        33<br/>Name: tz, dtype: int64<br/></p>
<p>14.1 1.USA.gov Data from Bitly | 407</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>We can visualize this data using matplotlib. You can do a bit of munging to fill in a<br/>substitute value for unknown and missing time zone data in the records. We replace<br/>the missing values with the fillna method and use boolean array indexing for the<br/>empty strings:<br/></p>
<p>In [31]: clean_tz = frame['tz'].fillna('Missing')<br/></p>
<p>In [32]: clean_tz[clean_tz == ''] = 'Unknown'<br/></p>
<p>In [33]: tz_counts = clean_tz.value_counts()<br/></p>
<p>In [34]: tz_counts[:10]<br/>Out[34]: <br/>America/New_York       1251<br/>Unknown                 521<br/>America/Chicago         400<br/>America/Los_Angeles     382<br/>America/Denver          191<br/>Missing                 120<br/>Europe/London            74<br/>Asia/Tokyo               37<br/>Pacific/Honolulu         36<br/>Europe/Madrid            35<br/>Name: tz, dtype: int64<br/></p>
<p>At this point, we can use the seaborn package to make a horizontal bar plot (see<br/>Figure 14-1 for the resulting visualization):<br/></p>
<p>In [36]: <b>import</b> <b>seaborn</b> <b>as</b> <b>sns<br/></b></p>
<p>In [37]: subset = tz_counts[:10]<br/></p>
<p>In [38]: sns.barplot(y=subset.index, x=subset.values)<br/></p>
<p><i>Figure 14-1. Top time zones in the 1.usa.gov sample data<br/></i></p>
<p>408 | Chapter 14: Data Analysis Examples</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>The a field contains information about the browser, device, or application used to<br/>perform the URL shortening:<br/></p>
<p>In [39]: frame['a'][1]<br/>Out[39]: 'GoogleMaps/RochesterNY'<br/></p>
<p>In [40]: frame['a'][50]<br/>Out[40]: 'Mozilla/5.0 (Windows NT 5.1; rv:10.0.2) Gecko/20100101 Firefox/10.0.2'<br/></p>
<p>In [41]: frame['a'][51][:50]  <i># long line<br/></i>Out[41]: 'Mozilla/5.0 (Linux; U; Android 2.2.2; en-us; LG-P9'<br/></p>
<p>Parsing all of the interesting information in these &#8220;agent&#8221; strings may seem like a<br/>daunting task. One possible strategy is to split off the first token in the string (corre&#8208;<br/>sponding roughly to the browser capability) and make another summary of the user<br/>behavior:<br/></p>
<p>In [42]: results = pd.Series([x.split()[0] <b>for</b> x <b>in</b> frame.a.dropna()])<br/></p>
<p>In [43]: results[:5]<br/>Out[43]: <br/>0               Mozilla/5.0<br/>1    GoogleMaps/RochesterNY<br/>2               Mozilla/4.0<br/>3               Mozilla/5.0<br/>4               Mozilla/5.0<br/>dtype: object<br/></p>
<p>In [44]: results.value_counts()[:8]<br/>Out[44]: <br/>Mozilla/5.0                 2594<br/>Mozilla/4.0                  601<br/>GoogleMaps/RochesterNY       121<br/>Opera/9.80                    34<br/>TEST_INTERNET_AGENT           24<br/>GoogleProducer                21<br/>Mozilla/6.0                    5<br/>BlackBerry8520/5.0.0.681       4<br/>dtype: int64<br/></p>
<p>Now, suppose you wanted to decompose the top time zones into Windows and non-<br/>Windows users. As a simplification, let&#8217;s say that a user is on Windows if the string<br/>'Windows' is in the agent string. Since some of the agents are missing, we&#8217;ll exclude<br/>these from the data:<br/></p>
<p>In [45]: cframe = frame[frame.a.notnull()]<br/></p>
<p>We want to then compute a value for whether each row is Windows or not:<br/>In [47]: cframe['os'] = np.where(cframe['a'].str.contains('Windows'),<br/>   ....:                         'Windows', 'Not Windows')<br/></p>
<p>In [48]: cframe['os'][:5]<br/></p>
<p>14.1 1.USA.gov Data from Bitly | 409</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Out[48]: <br/>0        Windows<br/>1    Not Windows<br/>2        Windows<br/>3    Not Windows<br/>4        Windows<br/>Name: os, dtype: object<br/></p>
<p>Then, you can group the data by its time zone column and this new list of operating<br/>systems:<br/></p>
<p>In [49]: by_tz_os = cframe.groupby(['tz', 'os'])<br/></p>
<p>The group counts, analogous to the value_counts function, can be computed with<br/>size. This result is then reshaped into a table with unstack:<br/></p>
<p>In [50]: agg_counts = by_tz_os.size().unstack().fillna(0)<br/></p>
<p>In [51]: agg_counts[:10]<br/>Out[51]: <br/>os                              Not Windows  Windows<br/>tz                                                  <br/>                                      245.0    276.0<br/>Africa/Cairo                            0.0      3.0<br/>Africa/Casablanca                       0.0      1.0<br/>Africa/Ceuta                            0.0      2.0<br/>Africa/Johannesburg                     0.0      1.0<br/>Africa/Lusaka                           0.0      1.0<br/>America/Anchorage                       4.0      1.0<br/>America/Argentina/Buenos_Aires          1.0      0.0<br/>America/Argentina/Cordoba               0.0      1.0<br/>America/Argentina/Mendoza               0.0      1.0<br/></p>
<p>Finally, let&#8217;s select the top overall time zones. To do so, I construct an indirect index<br/>array from the row counts in agg_counts:<br/></p>
<p><i># Use to sort in ascending order<br/></i>In [52]: indexer = agg_counts.sum(1).argsort()<br/></p>
<p>In [53]: indexer[:10]<br/>Out[53]: <br/>tz<br/>                                  24<br/>Africa/Cairo                      20<br/>Africa/Casablanca                 21<br/>Africa/Ceuta                      92<br/>Africa/Johannesburg               87<br/>Africa/Lusaka                     53<br/>America/Anchorage                 54<br/>America/Argentina/Buenos_Aires    57<br/>America/Argentina/Cordoba         26<br/>America/Argentina/Mendoza         55<br/>dtype: int64<br/></p>
<p>410 | Chapter 14: Data Analysis Examples</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>I use take to select the rows in that order, then slice off the last 10 rows (largest<br/>values):<br/></p>
<p>In [54]: count_subset = agg_counts.take(indexer[-10:])<br/></p>
<p>In [55]: count_subset<br/>Out[55]: <br/>os                   Not Windows  Windows<br/>tz                                       <br/>America/Sao_Paulo           13.0     20.0<br/>Europe/Madrid               16.0     19.0<br/>Pacific/Honolulu             0.0     36.0<br/>Asia/Tokyo                   2.0     35.0<br/>Europe/London               43.0     31.0<br/>America/Denver             132.0     59.0<br/>America/Los_Angeles        130.0    252.0<br/>America/Chicago            115.0    285.0<br/>                           245.0    276.0<br/>America/New_York           339.0    912.0<br/></p>
<p>pandas has a convenience method called nlargest that does the same thing:<br/>In [56]: agg_counts.sum(1).nlargest(10)<br/>Out[56]: <br/>tz<br/>America/New_York       1251.0<br/>                        521.0<br/>America/Chicago         400.0<br/>America/Los_Angeles     382.0<br/>America/Denver          191.0<br/>Europe/London            74.0<br/>Asia/Tokyo               37.0<br/>Pacific/Honolulu         36.0<br/>Europe/Madrid            35.0<br/>America/Sao_Paulo        33.0<br/>dtype: float64<br/></p>
<p>Then, as shown in the preceding code block, this can be plotted in a bar plot; I&#8217;ll<br/>make it a stacked bar plot by passing an additional argument to seaborn&#8217;s barplot<br/>function (see Figure 14-2):<br/></p>
<p><i># Rearrange the data for plotting<br/></i>In [58]: count_subset = count_subset.stack()<br/></p>
<p>In [59]: count_subset.name = 'total'<br/></p>
<p>In [60]: count_subset = count_subset.reset_index()<br/></p>
<p>In [61]: count_subset[:10]<br/>Out[61]: <br/>                  tz           os  total<br/>0  America/Sao_Paulo  Not Windows   13.0<br/>1  America/Sao_Paulo      Windows   20.0<br/></p>
<p>14.1 1.USA.gov Data from Bitly | 411</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>2      Europe/Madrid  Not Windows   16.0<br/>3      Europe/Madrid      Windows   19.0<br/>4   Pacific/Honolulu  Not Windows    0.0<br/>5   Pacific/Honolulu      Windows   36.0<br/>6         Asia/Tokyo  Not Windows    2.0<br/>7         Asia/Tokyo      Windows   35.0<br/>8      Europe/London  Not Windows   43.0<br/>9      Europe/London      Windows   31.0<br/></p>
<p>In [62]: sns.barplot(x='total', y='tz', hue='os',  data=count_subset)<br/></p>
<p><i>Figure 14-2. Top time zones by Windows and non-Windows users<br/></i>The plot doesn&#8217;t make it easy to see the relative percentage of Windows users in the<br/>smaller groups, so let&#8217;s normalize the group percentages to sum to 1:<br/></p>
<p><b>def</b> norm_total(group):<br/>    group['normed_total'] = group.total / group.total.sum()<br/>    <b>return</b> group<br/></p>
<p>results = count_subset.groupby('tz').apply(norm_total)<br/></p>
<p>Then plot this in Figure 14-3:<br/>In [65]: sns.barplot(x='normed_total', y='tz', hue='os',  data=results)<br/></p>
<p>412 | Chapter 14: Data Analysis Examples</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><i>Figure 14-3. Percentage Windows and non-Windows users in top-occurring time zones<br/></i></p>
<p>We could have computed the normalized sum more efficiently by using the trans<br/>form method with groupby:<br/></p>
<p>In [66]: g = count_subset.groupby('tz')<br/></p>
<p>In [67]: results2 = count_subset.total / g.total.transform('sum')<br/></p>
<p>14.2 MovieLens 1M Dataset<br/>GroupLens Research provides a number of collections of movie ratings data collected<br/>from users of MovieLens in the late 1990s and early 2000s. The data provide movie<br/>ratings, movie metadata (genres and year), and demographic data about the users<br/>(age, zip code, gender identification, and occupation). Such data is often of interest in<br/>the development of recommendation systems based on machine learning algorithms.<br/>While we do not explore machine learning techniques in detail in this book, I will<br/>show you how to slice and dice datasets like these into the exact form you need.<br/>The MovieLens 1M dataset contains 1 million ratings collected from 6,000 users on<br/>4,000 movies. It&#8217;s spread across three tables: ratings, user information, and movie<br/>information. After extracting the data from the ZIP file, we can load each table into a<br/>pandas DataFrame object using pandas.read_table:<br/></p>
<p>14.2 MovieLens 1M Dataset | 413</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><b>import</b> <b>pandas</b> <b>as</b> <b>pd<br/></b></p>
<p><i># Make display smaller<br/></i>pd.options.display.max_rows = 10<br/></p>
<p>unames = ['user_id', 'gender', 'age', 'occupation', 'zip']<br/>users = pd.read_table('datasets/movielens/users.dat', sep='::',<br/>                      header=None, names=unames)<br/></p>
<p>rnames = ['user_id', 'movie_id', 'rating', 'timestamp']<br/>ratings = pd.read_table('datasets/movielens/ratings.dat', sep='::',<br/>                        header=None, names=rnames)<br/></p>
<p>mnames = ['movie_id', 'title', 'genres']<br/>movies = pd.read_table('datasets/movielens/movies.dat', sep='::',<br/>                       header=None, names=mnames)<br/></p>
<p>You can verify that everything succeeded by looking at the first few rows of each<br/>DataFrame with Python&#8217;s slice syntax:<br/></p>
<p>In [69]: users[:5]<br/>Out[69]: <br/>   user_id gender  age  occupation    zip<br/>0        1      F    1          10  48067<br/>1        2      M   56          16  70072<br/>2        3      M   25          15  55117<br/>3        4      M   45           7  02460<br/>4        5      M   25          20  55455<br/></p>
<p>In [70]: ratings[:5]<br/>Out[70]: <br/>   user_id  movie_id  rating  timestamp<br/>0        1      1193       5  978300760<br/>1        1       661       3  978302109<br/>2        1       914       3  978301968<br/>3        1      3408       4  978300275<br/>4        1      2355       5  978824291<br/></p>
<p>In [71]: movies[:5]<br/>Out[71]: <br/>   movie_id                               title                        genres<br/>0         1                    Toy Story (1995)   Animation|Children's|Comedy<br/>1         2                      Jumanji (1995)  Adventure|Children's|Fantasy<br/>2         3             Grumpier Old Men (1995)                Comedy|Romance<br/>3         4            Waiting to Exhale (1995)                  Comedy|Drama<br/>4         5  Father of the Bride Part II (1995)                        Comedy<br/></p>
<p>In [72]: ratings<br/>Out[72]: <br/>         user_id  movie_id  rating  timestamp<br/>0              1      1193       5  978300760<br/>1              1       661       3  978302109<br/>2              1       914       3  978301968<br/></p>
<p>414 | Chapter 14: Data Analysis Examples</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>3              1      3408       4  978300275<br/>4              1      2355       5  978824291<br/>...          ...       ...     ...        ...<br/>1000204     6040      1091       1  956716541<br/>1000205     6040      1094       5  956704887<br/>1000206     6040       562       5  956704746<br/>1000207     6040      1096       4  956715648<br/>1000208     6040      1097       4  956715569<br/>[1000209 rows x 4 columns]<br/></p>
<p>Note that ages and occupations are coded as integers indicating groups described in<br/>the dataset&#8217;s <i>README</i> file. Analyzing the data spread across three tables is not a sim&#8208;<br/>ple task; for example, suppose you wanted to compute mean ratings for a particular<br/>movie by sex and age. As you will see, this is much easier to do with all of the data<br/>merged together into a single table. Using pandas&#8217;s merge function, we first merge<br/>ratings with users and then merge that result with the movies data. pandas infers<br/>which columns to use as the merge (or <i>join</i>) keys based on overlapping names:<br/></p>
<p>In [73]: data = pd.merge(pd.merge(ratings, users), movies)<br/></p>
<p>In [74]: data<br/>Out[74]: <br/>         user_id  movie_id  rating  timestamp gender  age  occupation    zip  \<br/>0              1      1193       5  978300760      F    1          10  48067   <br/>1              2      1193       5  978298413      M   56          16  70072   <br/>2             12      1193       4  978220179      M   25          12  32793   <br/>3             15      1193       4  978199279      M   25           7  22903   <br/>4             17      1193       5  978158471      M   50           1  95350   <br/>...          ...       ...     ...        ...    ...  ...         ...    ...   <br/>1000204     5949      2198       5  958846401      M   18          17  47901   <br/>1000205     5675      2703       3  976029116      M   35          14  30030   <br/>1000206     5780      2845       1  958153068      M   18          17  92886   <br/>1000207     5851      3607       5  957756608      F   18          20  55410   <br/>1000208     5938      2909       4  957273353      M   25           1  35401   <br/>                                               title                genres  <br/>0             One Flew Over the Cuckoo's Nest (1975)                 Drama  <br/>1             One Flew Over the Cuckoo's Nest (1975)                 Drama  <br/>2             One Flew Over the Cuckoo's Nest (1975)                 Drama  <br/>3             One Flew Over the Cuckoo's Nest (1975)                 Drama  <br/>4             One Flew Over the Cuckoo's Nest (1975)                 Drama  <br/>...                                              ...                   ...  <br/>1000204                           Modulations (1998)           Documentary  <br/>1000205                        Broken Vessels (1998)                 Drama  <br/>1000206                            White Boys (1999)                 Drama  <br/>1000207                     One Little Indian (1973)  Comedy|Drama|Western  <br/>1000208  Five Wives, Three Secretaries <b>and</b> Me (1998)           Documentary  <br/>[1000209 rows x 10 columns]<br/></p>
<p>In [75]: data.iloc[0]<br/>Out[75]: <br/>user_id                                            1<br/></p>
<p>14.2 MovieLens 1M Dataset | 415</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>movie_id                                        1193<br/>rating                                             5<br/>timestamp                                  978300760<br/>gender                                             F<br/>age                                                1<br/>occupation                                        10<br/>zip                                            48067<br/>title         One Flew Over the Cuckoo's Nest (1975)<br/>genres                                         Drama<br/>Name: 0, dtype: object<br/></p>
<p>To get mean movie ratings for each film grouped by gender, we can use the<br/>pivot_table method:<br/></p>
<p>In [76]: mean_ratings = data.pivot_table('rating', index='title',<br/>   ....:                                 columns='gender', aggfunc='mean')<br/></p>
<p>In [77]: mean_ratings[:5]<br/>Out[77]: <br/>gender                                F         M<br/>title                                            <br/>$1,000,000 Duck (1971)         3.375000  2.761905<br/>'Night Mother (1986)           3.388889  3.352941<br/>'Til There Was You (1997)      2.675676  2.733333<br/>'burbs, The (1989)             2.793478  2.962085<br/>...And Justice <b>for</b> All (1979)  3.828571  3.689024<br/></p>
<p>This produced another DataFrame containing mean ratings with movie titles as row<br/>labels (the &#8220;index&#8221;) and gender as column labels. I first filter down to movies that<br/>received at least 250 ratings (a completely arbitrary number); to do this, I then group<br/>the data by title and use size() to get a Series of group sizes for each title:<br/></p>
<p>In [78]: ratings_by_title = data.groupby('title').size()<br/></p>
<p>In [79]: ratings_by_title[:10]<br/>Out[79]: <br/>title<br/>$1,000,000 Duck (1971)                37<br/>'Night Mother (1986)                  70<br/>'Til There Was You (1997)             52<br/>'burbs, The (1989)                   303<br/>...And Justice <b>for</b> All (1979)        199<br/>1-900 (1994)                           2<br/>10 Things I Hate About You (1999)    700<br/>101 Dalmatians (1961)                565<br/>101 Dalmatians (1996)                364<br/>12 Angry Men (1957)                  616<br/>dtype: int64<br/></p>
<p>In [80]: active_titles = ratings_by_title.index[ratings_by_title &gt;= 250]<br/></p>
<p>In [81]: active_titles<br/></p>
<p>416 | Chapter 14: Data Analysis Examples</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Out[81]: <br/>Index([''burbs, The (1989)', '10 Things I Hate About You (1999)',<br/>       '101 Dalmatians (1961)', '101 Dalmatians (1996)', '12 Angry Men (1957)',<br/>       '13th Warrior, The (1999)', '2 Days in the Valley (1996)',<br/>       '20,000 Leagues Under the Sea (1954)', '2001: A Space Odyssey (1968)',<br/>       '2010 (1984)',<br/>       ...<br/>       'X-Men (2000)', 'Year of Living Dangerously (1982)',<br/>       'Yellow Submarine (1968)', 'You've Got Mail (1998)',<br/>       'Young Frankenstein (1974)', 'Young Guns (1988)',<br/>       'Young Guns II (1990)', 'Young Sherlock Holmes (1985)',<br/>       'Zero Effect (1998)', 'eXistenZ (1999)'],<br/>      dtype='object', name='title', length=1216)<br/></p>
<p>The index of titles receiving at least 250 ratings can then be used to select rows from<br/>mean_ratings:<br/></p>
<p><i># Select rows on the index<br/></i>In [82]: mean_ratings = mean_ratings.loc[active_titles]<br/></p>
<p>In [83]: mean_ratings<br/>Out[83]: <br/>gender                                    F         M<br/>title                                                <br/>'burbs, The (1989)                 2.793478  2.962085<br/>10 Things I Hate About You (1999)  3.646552  3.311966<br/>101 Dalmatians (1961)              3.791444  3.500000<br/>101 Dalmatians (1996)              3.240000  2.911215<br/>12 Angry Men (1957)                4.184397  4.328421<br/>...                                     ...       ...<br/>Young Guns (1988)                  3.371795  3.425620<br/>Young Guns II (1990)               2.934783  2.904025<br/>Young Sherlock Holmes (1985)       3.514706  3.363344<br/>Zero Effect (1998)                 3.864407  3.723140<br/>eXistenZ (1999)                    3.098592  3.289086<br/>[1216 rows x 2 columns]<br/></p>
<p>To see the top films among female viewers, we can sort by the F column in descend&#8208;<br/>ing order:<br/></p>
<p>In [85]: top_female_ratings = mean_ratings.sort_values(by='F', ascending=False)<br/></p>
<p>In [86]: top_female_ratings[:10]<br/>Out[86]: <br/>gender                                                     F         M<br/>title                                                                 <br/>Close Shave, A (1995)                               4.644444  4.473795<br/>Wrong Trousers, The (1993)                          4.588235  4.478261<br/>Sunset Blvd. (a.k.a. Sunset Boulevard) (1950)       4.572650  4.464589<br/>Wallace &amp; Gromit: The Best of Aardman Animation...  4.563107  4.385075<br/>Schindler's List (1993)                             4.562602  4.491415<br/>Shawshank Redemption, The (1994)                    4.539075  4.560625<br/>Grand Day Out, A (1992)                             4.537879  4.293255<br/></p>
<p>14.2 MovieLens 1M Dataset | 417</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>To Kill a Mockingbird (1962)                        4.536667  4.372611<br/>Creature Comforts (1990)                            4.513889  4.272277<br/>Usual Suspects, The (1995)                          4.513317  4.518248<br/></p>
<p>Measuring Rating Disagreement<br/>Suppose you wanted to find the movies that are most divisive between male and<br/>female viewers. One way is to add a column to mean_ratings containing the differ&#8208;<br/>ence in means, then sort by that:<br/></p>
<p>In [87]: mean_ratings['diff'] = mean_ratings['M'] - mean_ratings['F']<br/></p>
<p>Sorting by 'diff' yields the movies with the greatest rating difference so that we can<br/>see which ones were preferred by women:<br/></p>
<p>In [88]: sorted_by_diff = mean_ratings.sort_values(by='diff')<br/></p>
<p>In [89]: sorted_by_diff[:10]<br/>Out[89]: <br/>gender                                        F         M      diff<br/>title                                                              <br/>Dirty Dancing (1987)                   3.790378  2.959596 -0.830782<br/>Jumpin' Jack Flash (1986)              3.254717  2.578358 -0.676359<br/>Grease (1978)                          3.975265  3.367041 -0.608224<br/>Little Women (1994)                    3.870588  3.321739 -0.548849<br/>Steel Magnolias (1989)                 3.901734  3.365957 -0.535777<br/>Anastasia (1997)                       3.800000  3.281609 -0.518391<br/>Rocky Horror Picture Show, The (1975)  3.673016  3.160131 -0.512885<br/>Color Purple, The (1985)               4.158192  3.659341 -0.498851<br/>Age of Innocence, The (1993)           3.827068  3.339506 -0.487561<br/>Free Willy (1993)                      2.921348  2.438776 -0.482573<br/></p>
<p>Reversing the order of the rows and again slicing off the top 10 rows, we get the mov&#8208;<br/>ies preferred by men that women didn&#8217;t rate as highly:<br/></p>
<p><i># Reverse order of rows, take first 10 rows<br/></i>In [90]: sorted_by_diff[::-1][:10]<br/>Out[90]: <br/>gender                                         F         M      diff<br/>title                                                               <br/>Good, The Bad <b>and</b> The Ugly, The (1966)  3.494949  4.221300  0.726351<br/>Kentucky Fried Movie, The (1977)        2.878788  3.555147  0.676359<br/>Dumb &amp; Dumber (1994)                    2.697987  3.336595  0.638608<br/>Longest Day, The (1962)                 3.411765  4.031447  0.619682<br/>Cable Guy, The (1996)                   2.250000  2.863787  0.613787<br/>Evil Dead II (Dead By Dawn) (1987)      3.297297  3.909283  0.611985<br/>Hidden, The (1987)                      3.137931  3.745098  0.607167<br/>Rocky III (1982)                        2.361702  2.943503  0.581801<br/>Caddyshack (1980)                       3.396135  3.969737  0.573602<br/>For a Few Dollars More (1965)           3.409091  3.953795  0.544704<br/></p>
<p>418 | Chapter 14: Data Analysis Examples</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Suppose instead you wanted the movies that elicited the most disagreement among<br/>viewers, independent of gender identification. Disagreement can be measured by the<br/>variance or standard deviation of the ratings:<br/></p>
<p><i># Standard deviation of rating grouped by title<br/></i>In [91]: rating_std_by_title = data.groupby('title')['rating'].std()<br/></p>
<p><i># Filter down to active_titles<br/></i>In [92]: rating_std_by_title = rating_std_by_title.loc[active_titles]<br/></p>
<p><i># Order Series by value in descending order<br/></i>In [93]: rating_std_by_title.sort_values(ascending=False)[:10]<br/>Out[93]: <br/>title<br/>Dumb &amp; Dumber (1994)                     1.321333<br/>Blair Witch Project, The (1999)          1.316368<br/>Natural Born Killers (1994)              1.307198<br/>Tank Girl (1995)                         1.277695<br/>Rocky Horror Picture Show, The (1975)    1.260177<br/>Eyes Wide Shut (1999)                    1.259624<br/>Evita (1996)                             1.253631<br/>Billy Madison (1995)                     1.249970<br/>Fear <b>and</b> Loathing <b>in</b> Las Vegas (1998)    1.246408<br/>Bicentennial Man (1999)                  1.245533<br/>Name: rating, dtype: float64<br/></p>
<p>You may have noticed that movie genres are given as a pipe-separated (|) string. If<br/>you wanted to do some analysis by genre, more work would be required to transform<br/>the genre information into a more usable form.<br/></p>
<p>14.3 US Baby Names 1880&#8211;2010<br/>The United States Social Security Administration (SSA) has made available data on<br/>the frequency of baby names from 1880 through the present. Hadley Wickham, an<br/>author of several popular R packages, has often made use of this dataset in illustrating<br/>data manipulation in R.<br/>We need to do some data wrangling to load this dataset, but once we do that we will<br/>have a DataFrame that looks like this:<br/></p>
<p>In [4]: names.head(10)<br/>Out[4]:<br/>        name sex  births  year<br/>0       Mary   F    7065  1880<br/>1       Anna   F    2604  1880<br/>2       Emma   F    2003  1880<br/>3  Elizabeth   F    1939  1880<br/>4     Minnie   F    1746  1880<br/>5   Margaret   F    1578  1880<br/>6        Ida   F    1472  1880<br/></p>
<p>14.3 US Baby Names 1880&#8211;2010 | 419</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>7      Alice   F    1414  1880<br/>8     Bertha   F    1320  1880<br/>9      Sarah   F    1288  1880<br/></p>
<p>There are many things you might want to do with the dataset:<br/>&#8226; Visualize the proportion of babies given a particular name (your own, or another<br/></p>
<p>name) over time<br/>&#8226; Determine the relative rank of a name<br/>&#8226; Determine the most popular names in each year or the names whose popularity<br/></p>
<p>has advanced or declined the most<br/>&#8226; Analyze trends in names: vowels, consonants, length, overall diversity, changes in<br/></p>
<p>spelling, first and last letters<br/>&#8226; Analyze external sources of trends: biblical names, celebrities, demographic<br/></p>
<p>changes<br/>With the tools in this book, many of these kinds of analyses are within reach, so I will<br/>walk you through some of them.<br/>As of this writing, the US Social Security Administration makes available data files,<br/>one per year, containing the total number of births for each sex/name combination.<br/>The raw archive of these files can be obtained from <i>http://www.ssa.gov/oact/baby<br/>names/limits.html</i>.<br/>In the event that this page has been moved by the time you&#8217;re reading this, it can most<br/>likely be located again by an internet search. After downloading the &#8220;National data&#8221;<br/>file <i>names.zip</i> and unzipping it, you will have a directory containing a series of files<br/>like <i>yob1880.txt</i>. I use the Unix head command to look at the first 10 lines of one of<br/>the files (on Windows, you can use the more command or open it in a text editor):<br/></p>
<p>In [94]: !head -n 10 datasets/babynames/yob1880.txt<br/>Mary,F,7065<br/>Anna,F,2604<br/>Emma,F,2003<br/>Elizabeth,F,1939<br/>Minnie,F,1746<br/>Margaret,F,1578<br/>Ida,F,1472<br/>Alice,F,1414<br/>Bertha,F,1320<br/>Sarah,F,1288<br/></p>
<p>As this is already in a nicely comma-separated form, it can be loaded into a Data&#8208;<br/>Frame with pandas.read_csv:<br/></p>
<p>420 | Chapter 14: Data Analysis Examples</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In [95]: <b>import</b> <b>pandas</b> <b>as</b> <b>pd<br/></b></p>
<p>In [96]: names1880 = pd.read_csv('datasets/babynames/yob1880.txt',<br/>   ....:                         names=['name', 'sex', 'births'])<br/></p>
<p>In [97]: names1880<br/>Out[97]: <br/>           name sex  births<br/>0          Mary   F    7065<br/>1          Anna   F    2604<br/>2          Emma   F    2003<br/>3     Elizabeth   F    1939<br/>4        Minnie   F    1746<br/>...         ...  ..     ...<br/>1995     Woodie   M       5<br/>1996     Worthy   M       5<br/>1997     Wright   M       5<br/>1998       York   M       5<br/>1999  Zachariah   M       5<br/>[2000 rows x 3 columns]<br/></p>
<p>These files only contain names with at least five occurrences in each year, so for sim&#8208;<br/>plicity&#8217;s sake we can use the sum of the births column by sex as the total number of<br/>births in that year:<br/></p>
<p>In [98]: names1880.groupby('sex').births.sum()<br/>Out[98]: <br/>sex<br/>F     90993<br/>M    110493<br/>Name: births, dtype: int64<br/></p>
<p>Since the dataset is split into files by year, one of the first things to do is to assemble<br/>all of the data into a single DataFrame and further to add a year field. You can do this<br/>using pandas.concat:<br/></p>
<p>years = range(1880, 2011)<br/></p>
<p>pieces = []<br/>columns = ['name', 'sex', 'births']<br/></p>
<p><b>for</b> year <b>in</b> years:<br/>    path = 'datasets/babynames/yob%d.txt' % year<br/>    frame = pd.read_csv(path, names=columns)<br/></p>
<p>    frame['year'] = year<br/>    pieces.append(frame)<br/></p>
<p><i># Concatenate everything into a single DataFrame<br/></i>names = pd.concat(pieces, ignore_index=True)<br/></p>
<p>14.3 US Baby Names 1880&#8211;2010 | 421</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>There are a couple things to note here. First, remember that concat glues the Data&#8208;<br/>Frame objects together row-wise by default. Secondly, you have to pass<br/>ignore_index=True because we&#8217;re not interested in preserving the original row num&#8208;<br/>bers returned from read_csv. So we now have a very large DataFrame containing all<br/>of the names data:<br/></p>
<p>In [100]: names<br/>Out[100]: <br/>              name sex  births  year<br/>0             Mary   F    7065  1880<br/>1             Anna   F    2604  1880<br/>2             Emma   F    2003  1880<br/>3        Elizabeth   F    1939  1880<br/>4           Minnie   F    1746  1880<br/>...            ...  ..     ...   ...<br/>1690779    Zymaire   M       5  2010<br/>1690780     Zyonne   M       5  2010<br/>1690781  Zyquarius   M       5  2010<br/>1690782      Zyran   M       5  2010<br/>1690783      Zzyzx   M       5  2010<br/>[1690784 rows x 4 columns]<br/></p>
<p>With this data in hand, we can already start aggregating the data at the year and sex<br/>level using groupby or pivot_table (see Figure 14-4):<br/></p>
<p>In [101]: total_births = names.pivot_table('births', index='year',<br/>   .....:                                  columns='sex', aggfunc=sum)<br/></p>
<p>In [102]: total_births.tail()<br/>Out[102]: <br/>sex         F        M<br/>year                  <br/>2006  1896468  2050234<br/>2007  1916888  2069242<br/>2008  1883645  2032310<br/>2009  1827643  1973359<br/>2010  1759010  1898382<br/></p>
<p>In [103]: total_births.plot(title='Total births by sex and year')<br/></p>
<p>422 | Chapter 14: Data Analysis Examples</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><i>Figure 14-4. Total births by sex and year<br/></i></p>
<p>Next, let&#8217;s insert a column prop with the fraction of babies given each name relative to<br/>the total number of births. A prop value of 0.02 would indicate that 2 out of every<br/>100 babies were given a particular name. Thus, we group the data by year and sex,<br/>then add the new column to each group:<br/></p>
<p><b>def</b> add_prop(group):<br/>    group['prop'] = group.births / group.births.sum()<br/>    <b>return</b> group<br/>names = names.groupby(['year', 'sex']).apply(add_prop)<br/></p>
<p>The resulting complete dataset now has the following columns:<br/>In [105]: names<br/>Out[105]: <br/>              name sex  births  year      prop<br/>0             Mary   F    7065  1880  0.077643<br/>1             Anna   F    2604  1880  0.028618<br/>2             Emma   F    2003  1880  0.022013<br/>3        Elizabeth   F    1939  1880  0.021309<br/>4           Minnie   F    1746  1880  0.019188<br/>...            ...  ..     ...   ...       ...<br/>1690779    Zymaire   M       5  2010  0.000003<br/>1690780     Zyonne   M       5  2010  0.000003<br/>1690781  Zyquarius   M       5  2010  0.000003<br/></p>
<p>14.3 US Baby Names 1880&#8211;2010 | 423</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>1690782      Zyran   M       5  2010  0.000003<br/>1690783      Zzyzx   M       5  2010  0.000003<br/>[1690784 rows x 5 columns]<br/></p>
<p>When performing a group operation like this, it&#8217;s often valuable to do a sanity check,<br/>like verifying that the prop column sums to 1 within all the groups:<br/></p>
<p>In [106]: names.groupby(['year', 'sex']).prop.sum()<br/>Out[106]: <br/>year  sex<br/>1880  F      1.0<br/>      M      1.0<br/>1881  F      1.0<br/>      M      1.0<br/>1882  F      1.0<br/>            ... <br/>2008  M      1.0<br/>2009  F      1.0<br/>      M      1.0<br/>2010  F      1.0<br/>      M      1.0<br/>Name: prop, Length: 262, dtype: float64<br/></p>
<p>Now that this is done, I&#8217;m going to extract a subset of the data to facilitate further<br/>analysis: the top 1,000 names for each sex/year combination. This is yet another<br/>group operation:<br/></p>
<p><b>def</b> get_top1000(group):<br/>    <b>return</b> group.sort_values(by='births', ascending=False)[:1000]<br/>grouped = names.groupby(['year', 'sex'])<br/>top1000 = grouped.apply(get_top1000)<br/><i># Drop the group index, not needed<br/></i>top1000.reset_index(inplace=True, drop=True)<br/></p>
<p>If you prefer a do-it-yourself approach, try this instead:<br/>pieces = []<br/><b>for</b> year, group <b>in</b> names.groupby(['year', 'sex']):<br/>    pieces.append(group.sort_values(by='births', ascending=False)[:1000])<br/>top1000 = pd.concat(pieces, ignore_index=True)<br/></p>
<p>The resulting dataset is now quite a bit smaller:<br/>In [108]: top1000<br/>Out[108]: <br/>             name sex  births  year      prop<br/>0            Mary   F    7065  1880  0.077643<br/>1            Anna   F    2604  1880  0.028618<br/>2            Emma   F    2003  1880  0.022013<br/>3       Elizabeth   F    1939  1880  0.021309<br/>4          Minnie   F    1746  1880  0.019188<br/>...           ...  ..     ...   ...       ...<br/>261872     Camilo   M     194  2010  0.000102<br/>261873     Destin   M     194  2010  0.000102<br/></p>
<p>424 | Chapter 14: Data Analysis Examples</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>261874     Jaquan   M     194  2010  0.000102<br/>261875     Jaydan   M     194  2010  0.000102<br/>261876     Maxton   M     193  2010  0.000102<br/>[261877 rows x 5 columns]<br/></p>
<p>We&#8217;ll use this Top 1,000 dataset in the following investigations into the data.<br/></p>
<p>Analyzing Naming Trends<br/>With the full dataset and Top 1,000 dataset in hand, we can start analyzing various<br/>naming trends of interest. Splitting the Top 1,000 names into the boy and girl por&#8208;<br/>tions is easy to do first:<br/></p>
<p>In [109]: boys = top1000[top1000.sex == 'M']<br/></p>
<p>In [110]: girls = top1000[top1000.sex == 'F']<br/></p>
<p>Simple time series, like the number of Johns or Marys for each year, can be plotted<br/>but require a bit of munging to be more useful. Let&#8217;s form a pivot table of the total<br/>number of births by year and name:<br/></p>
<p>In [111]: total_births = top1000.pivot_table('births', index='year',<br/>   .....:                                    columns='name',<br/>   .....:                                    aggfunc=sum)<br/></p>
<p>Now, this can be plotted for a handful of names with DataFrame&#8217;s plot method<br/>(Figure 14-5 shows the result):<br/></p>
<p>In [112]: total_births.info()<br/>&lt;<b>class</b> '<b>pandas</b>.core.frame.DataFrame'&gt;<br/>Int64Index: 131 entries, 1880 to 2010<br/>Columns: 6868 entries, Aaden to Zuri<br/>dtypes: float64(6868)<br/>memory usage: 6.9 MB<br/></p>
<p>In [113]: subset = total_births[['John', 'Harry', 'Mary', 'Marilyn']]<br/></p>
<p>In [114]: subset.plot(subplots=True, figsize=(12, 10), grid=False,<br/>   .....:             title=&quot;Number of births per year&quot;)<br/></p>
<p>14.3 US Baby Names 1880&#8211;2010 | 425</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><i>Figure 14-5. A few boy and girl names over time<br/></i>On looking at this, you might conclude that these names have grown out of favor<br/>with the American population. But the story is actually more complicated than that,<br/>as will be explored in the next section.<br/></p>
<p>Measuring the increase in naming diversity<br/>One explanation for the decrease in plots is that fewer parents are choosing common<br/>names for their children. This hypothesis can be explored and confirmed in the data.<br/>One measure is the proportion of births represented by the top 1,000 most popular<br/>names, which I aggregate and plot by year and sex (Figure 14-6 shows the resulting<br/>plot):<br/></p>
<p>426 | Chapter 14: Data Analysis Examples</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In [116]: table = top1000.pivot_table('prop', index='year',<br/>   .....:                             columns='sex', aggfunc=sum)<br/></p>
<p>In [117]: table.plot(title='Sum of table1000.prop by year and sex',<br/>   .....:            yticks=np.linspace(0, 1.2, 13), xticks=range(1880, 2020, 10)<br/>)<br/></p>
<p><i>Figure 14-6. Proportion of births represented in top 1000 names by sex<br/></i>You can see that, indeed, there appears to be increasing name diversity (decreasing<br/>total proportion in the top 1,000). Another interesting metric is the number of dis&#8208;<br/>tinct names, taken in order of popularity from highest to lowest, in the top 50% of<br/>births. This number is a bit more tricky to compute. Let&#8217;s consider just the boy names<br/>from 2010:<br/></p>
<p>In [118]: df = boys[boys.year == 2010]<br/></p>
<p>In [119]: df<br/>Out[119]: <br/>           name sex  births  year      prop<br/>260877    Jacob   M   21875  2010  0.011523<br/>260878    Ethan   M   17866  2010  0.009411<br/>260879  Michael   M   17133  2010  0.009025<br/>260880   Jayden   M   17030  2010  0.008971<br/>260881  William   M   16870  2010  0.008887<br/></p>
<p>14.3 US Baby Names 1880&#8211;2010 | 427</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>...         ...  ..     ...   ...       ...<br/>261872   Camilo   M     194  2010  0.000102<br/>261873   Destin   M     194  2010  0.000102<br/>261874   Jaquan   M     194  2010  0.000102<br/>261875   Jaydan   M     194  2010  0.000102<br/>261876   Maxton   M     193  2010  0.000102<br/>[1000 rows x 5 columns]<br/></p>
<p>After sorting prop in descending order, we want to know how many of the most pop&#8208;<br/>ular names it takes to reach 50%. You could write a for loop to do this, but a vector&#8208;<br/>ized NumPy way is a bit more clever. Taking the cumulative sum, cumsum, of prop and<br/>then calling the method searchsorted returns the position in the cumulative sum at<br/>which 0.5 would need to be inserted to keep it in sorted order:<br/></p>
<p>In [120]: prop_cumsum = df.sort_values(by='prop', ascending=False).prop.cumsum()<br/></p>
<p>In [121]: prop_cumsum[:10]<br/>Out[121]: <br/>260877    0.011523<br/>260878    0.020934<br/>260879    0.029959<br/>260880    0.038930<br/>260881    0.047817<br/>260882    0.056579<br/>260883    0.065155<br/>260884    0.073414<br/>260885    0.081528<br/>260886    0.089621<br/>Name: prop, dtype: float64<br/></p>
<p>In [122]: prop_cumsum.values.searchsorted(0.5)<br/>Out[122]: 116<br/></p>
<p>Since arrays are zero-indexed, adding 1 to this result gives you a result of 117. By con&#8208;<br/>trast, in 1900 this number was much smaller:<br/></p>
<p>In [123]: df = boys[boys.year == 1900]<br/></p>
<p>In [124]: in1900 = df.sort_values(by='prop', ascending=False).prop.cumsum()<br/></p>
<p>In [125]: in1900.values.searchsorted(0.5) + 1<br/>Out[125]: 25<br/></p>
<p>You can now apply this operation to each year/sex combination, groupby those fields,<br/>and apply a function returning the count for each group:<br/></p>
<p><b>def</b> get_quantile_count(group, q=0.5):<br/>    group = group.sort_values(by='prop', ascending=False)<br/>    <b>return</b> group.prop.cumsum().values.searchsorted(q) + 1<br/></p>
<p>diversity = top1000.groupby(['year', 'sex']).apply(get_quantile_count)<br/>diversity = diversity.unstack('sex')<br/></p>
<p>428 | Chapter 14: Data Analysis Examples</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>This resulting DataFrame diversity now has two time series, one for each sex,<br/>indexed by year. This can be inspected in IPython and plotted as before (see<br/>Figure 14-7):<br/></p>
<p>In [128]: diversity.head()<br/>Out[128]: <br/>sex    F   M<br/>year        <br/>1880  38  14<br/>1881  38  14<br/>1882  38  15<br/>1883  39  15<br/>1884  39  16<br/></p>
<p>In [129]: diversity.plot(title=&quot;Number of popular names in top 50%&quot;)<br/></p>
<p><i>Figure 14-7. Plot of diversity metric by year<br/></i>As you can see, girl names have always been more diverse than boy names, and they<br/>have only become more so over time. Further analysis of what exactly is driving the<br/>diversity, like the increase of alternative spellings, is left to the reader.<br/></p>
<p>14.3 US Baby Names 1880&#8211;2010 | 429</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>The &#8220;last letter&#8221; revolution<br/>In 2007, baby name researcher Laura Wattenberg pointed out on her website that the<br/>distribution of boy names by final letter has changed significantly over the last 100<br/>years. To see this, we first aggregate all of the births in the full dataset by year, sex, and<br/>final letter:<br/></p>
<p><i># extract last letter from name column<br/></i>get_last_letter = <b>lambda</b> x: x[-1]<br/>last_letters = names.name.map(get_last_letter)<br/>last_letters.name = 'last_letter'<br/></p>
<p>table = names.pivot_table('births', index=last_letters,<br/>                          columns=['sex', 'year'], aggfunc=sum)<br/></p>
<p>Then we select out three representative years spanning the history and print the first<br/>few rows:<br/></p>
<p>In [131]: subtable = table.reindex(columns=[1910, 1960, 2010], level='year')<br/></p>
<p>In [132]: subtable.head()<br/>Out[132]: <br/>sex                 F                            M                    <br/>year             1910      1960      2010     1910      1960      2010<br/>last_letter                                                           <br/>a            108376.0  691247.0  670605.0    977.0    5204.0   28438.0<br/>b                 NaN     694.0     450.0    411.0    3912.0   38859.0<br/>c                 5.0      49.0     946.0    482.0   15476.0   23125.0<br/>d              6750.0    3729.0    2607.0  22111.0  262112.0   44398.0<br/>e            133569.0  435013.0  313833.0  28655.0  178823.0  129012.0<br/></p>
<p>Next, normalize the table by total births to compute a new table containing propor&#8208;<br/>tion of total births for each sex ending in each letter:<br/></p>
<p>In [133]: subtable.sum()<br/>Out[133]: <br/>sex  year<br/>F    1910     396416.0<br/>     1960    2022062.0<br/>     2010    1759010.0<br/>M    1910     194198.0<br/>     1960    2132588.0<br/>     2010    1898382.0<br/>dtype: float64<br/></p>
<p>In [134]: letter_prop = subtable / subtable.sum()<br/></p>
<p>In [135]: letter_prop<br/>Out[135]: <br/>sex                 F                             M                    <br/>year             1910      1960      2010      1910      1960      2010<br/>last_letter                                                            <br/>a            0.273390  0.341853  0.381240  0.005031  0.002440  0.014980<br/></p>
<p>430 | Chapter 14: Data Analysis Examples</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>b                 NaN  0.000343  0.000256  0.002116  0.001834  0.020470<br/>c            0.000013  0.000024  0.000538  0.002482  0.007257  0.012181<br/>d            0.017028  0.001844  0.001482  0.113858  0.122908  0.023387<br/>e            0.336941  0.215133  0.178415  0.147556  0.083853  0.067959<br/>...               ...       ...       ...       ...       ...       ...<br/>v                 NaN  0.000060  0.000117  0.000113  0.000037  0.001434<br/>w            0.000020  0.000031  0.001182  0.006329  0.007711  0.016148<br/>x            0.000015  0.000037  0.000727  0.003965  0.001851  0.008614<br/>y            0.110972  0.152569  0.116828  0.077349  0.160987  0.058168<br/>z            0.002439  0.000659  0.000704  0.000170  0.000184  0.001831<br/>[26 rows x 6 columns]<br/></p>
<p>With the letter proportions now in hand, we can make bar plots for each sex broken<br/>down by year (see Figure 14-8):<br/></p>
<p><b>import</b> <b>matplotlib.pyplot</b> <b>as</b> <b>plt<br/></b></p>
<p>fig, axes = plt.subplots(2, 1, figsize=(10, 8))<br/>letter_prop['M'].plot(kind='bar', rot=0, ax=axes[0], title='Male')<br/>letter_prop['F'].plot(kind='bar', rot=0, ax=axes[1], title='Female',<br/>                      legend=False)<br/></p>
<p><i>Figure 14-8. Proportion of boy and girl names ending in each letter<br/></i></p>
<p>14.3 US Baby Names 1880&#8211;2010 | 431</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>As you can see, boy names ending in <i>n</i> have experienced significant growth since the<br/>1960s. Going back to the full table created before, I again normalize by year and sex<br/>and select a subset of letters for the boy names, finally transposing to make each col&#8208;<br/>umn a time series:<br/></p>
<p>In [138]: letter_prop = table / table.sum()<br/></p>
<p>In [139]: dny_ts = letter_prop.loc[['d', 'n', 'y'], 'M'].T<br/></p>
<p>In [140]: dny_ts.head()<br/>Out[140]: <br/>last_letter         d         n         y<br/>year                                     <br/>1880         0.083055  0.153213  0.075760<br/>1881         0.083247  0.153214  0.077451<br/>1882         0.085340  0.149560  0.077537<br/>1883         0.084066  0.151646  0.079144<br/>1884         0.086120  0.149915  0.080405<br/></p>
<p>With this DataFrame of time series in hand, I can make a plot of the trends over time<br/>again with its plot method (see Figure 14-9):<br/></p>
<p>In [143]: dny_ts.plot()<br/></p>
<p><i>Figure 14-9. Proportion of boys born with names ending in d/n/y over time<br/></i></p>
<p>432 | Chapter 14: Data Analysis Examples</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Boy names that became girl names (and vice versa)<br/>Another fun trend is looking at boy names that were more popular with one sex ear&#8208;<br/>lier in the sample but have &#8220;changed sexes&#8221; in the present. One example is the name<br/>Lesley or Leslie. Going back to the top1000 DataFrame, I compute a list of names<br/>occurring in the dataset starting with &#8220;lesl&#8221;:<br/></p>
<p>In [144]: all_names = pd.Series(top1000.name.unique())<br/></p>
<p>In [145]: lesley_like = all_names[all_names.str.lower().str.contains('lesl')]<br/></p>
<p>In [146]: lesley_like<br/>Out[146]: <br/>632     Leslie<br/>2294    Lesley<br/>4262    Leslee<br/>4728     Lesli<br/>6103     Lesly<br/>dtype: object<br/></p>
<p>From there, we can filter down to just those names and sum births grouped by name<br/>to see the relative frequencies:<br/></p>
<p>In [147]: filtered = top1000[top1000.name.isin(lesley_like)]<br/></p>
<p>In [148]: filtered.groupby('name').births.sum()<br/>Out[148]: <br/>name<br/>Leslee      1082<br/>Lesley     35022<br/>Lesli        929<br/>Leslie    370429<br/>Lesly      10067<br/>Name: births, dtype: int64<br/></p>
<p>Next, let&#8217;s aggregate by sex and year and normalize within year:<br/>In [149]: table = filtered.pivot_table('births', index='year',<br/>   .....:                              columns='sex', aggfunc='sum')<br/></p>
<p>In [150]: table = table.div(table.sum(1), axis=0)<br/></p>
<p>In [151]: table.tail()<br/>Out[151]: <br/>sex     F   M<br/>year         <br/>2006  1.0 NaN<br/>2007  1.0 NaN<br/>2008  1.0 NaN<br/>2009  1.0 NaN<br/>2010  1.0 NaN<br/></p>
<p>14.3 US Baby Names 1880&#8211;2010 | 433</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Lastly, it&#8217;s now possible to make a plot of the breakdown by sex over time<br/>(Figure 14-10):<br/></p>
<p>In [153]: table.plot(style={'M': 'k-', 'F': 'k--'})<br/></p>
<p><i>Figure 14-10. Proportion of male/female Lesley-like names over time<br/></i></p>
<p>14.4 USDA Food Database<br/>The US Department of Agriculture makes available a database of food nutrient infor&#8208;<br/>mation. Programmer Ashley Williams made available a version of this database in<br/>JSON format. The records look like this:<br/></p>
<p>{<br/>  <b>&quot;id&quot;</b>: 21441,<br/>  <b>&quot;description&quot;</b>: &quot;KENTUCKY FRIED CHICKEN, Fried Chicken, EXTRA CRISPY,<br/>Wing, meat and skin with breading&quot;,<br/>  <b>&quot;tags&quot;</b>: [&quot;KFC&quot;],<br/>  <b>&quot;manufacturer&quot;</b>: &quot;Kentucky Fried Chicken&quot;,<br/>  <b>&quot;group&quot;</b>: &quot;Fast Foods&quot;,<br/>  <b>&quot;portions&quot;</b>: [<br/>    {<br/>      <b>&quot;amount&quot;</b>: 1,<br/>      <b>&quot;unit&quot;</b>: &quot;wing, with skin&quot;,<br/>      <b>&quot;grams&quot;</b>: 68.0<br/></p>
<p>434 | Chapter 14: Data Analysis Examples</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>    },<br/></p>
<p>    ...<br/>  ],<br/>  <b>&quot;nutrients&quot;</b>: [<br/>    {<br/>      <b>&quot;value&quot;</b>: 20.8,<br/>      <b>&quot;units&quot;</b>: &quot;g&quot;,<br/>      <b>&quot;description&quot;</b>: &quot;Protein&quot;,<br/>      <b>&quot;group&quot;</b>: &quot;Composition&quot;<br/>    },<br/></p>
<p>    ...<br/>  ]<br/>}<br/></p>
<p>Each food has a number of identifying attributes along with two lists of nutrients and<br/>portion sizes. Data in this form is not particularly amenable to analysis, so we need to<br/>do some work to wrangle the data into a better form.<br/>After downloading and extracting the data from the link, you can load it into Python<br/>with any JSON library of your choosing. I&#8217;ll use the built-in Python json module:<br/></p>
<p>In [154]: <b>import</b> <b>json<br/></b></p>
<p>In [155]: db = json.load(open('datasets/usda_food/database.json'))<br/></p>
<p>In [156]: len(db)<br/>Out[156]: 6636<br/></p>
<p>Each entry in db is a dict containing all the data for a single food. The 'nutrients'<br/>field is a list of dicts, one for each nutrient:<br/></p>
<p>In [157]: db[0].keys()<br/>Out[157]: dict_keys(['id', 'description', 'tags', 'manufacturer', 'group', 'porti<br/>ons', 'nutrients'])<br/></p>
<p>In [158]: db[0]['nutrients'][0]<br/>Out[158]: <br/>{'description': 'Protein',<br/> 'group': 'Composition',<br/> 'units': 'g',<br/> 'value': 25.18}<br/></p>
<p>In [159]: nutrients = pd.DataFrame(db[0]['nutrients'])<br/></p>
<p>In [160]: nutrients[:7]<br/>Out[160]: <br/>                   description        group units    value<br/>0                      Protein  Composition     g    25.18<br/>1            Total lipid (fat)  Composition     g    29.20<br/>2  Carbohydrate, by difference  Composition     g     3.06<br/>3                          Ash        Other     g     3.28<br/></p>
<p>14.4 USDA Food Database | 435</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>4                       Energy       Energy  kcal   376.00<br/>5                        Water  Composition     g    39.28<br/>6                       Energy       Energy    kJ  1573.00<br/></p>
<p>When converting a list of dicts to a DataFrame, we can specify a list of fields to<br/>extract. We&#8217;ll take the food names, group, ID, and manufacturer:<br/></p>
<p>In [161]: info_keys = ['description', 'group', 'id', 'manufacturer']<br/></p>
<p>In [162]: info = pd.DataFrame(db, columns=info_keys)<br/></p>
<p>In [163]: info[:5]<br/>Out[163]: <br/>                          description                   group    id  \<br/>0                     Cheese, caraway  Dairy <b>and</b> Egg Products  1008   <br/>1                     Cheese, cheddar  Dairy <b>and</b> Egg Products  1009   <br/>2                        Cheese, edam  Dairy <b>and</b> Egg Products  1018   <br/>3                        Cheese, feta  Dairy <b>and</b> Egg Products  1019   <br/>4  Cheese, mozzarella, part skim milk  Dairy <b>and</b> Egg Products  1028   <br/>  manufacturer  <br/>0               <br/>1               <br/>2               <br/>3               <br/>4               <br/></p>
<p>In [164]: info.info()<br/>&lt;<b>class</b> '<b>pandas</b>.core.frame.DataFrame'&gt;<br/>RangeIndex: 6636 entries, 0 to 6635<br/>Data columns (total 4 columns):<br/>description     6636 non-null object<br/>group           6636 non-null object<br/>id              6636 non-null int64<br/>manufacturer    5195 non-null object<br/>dtypes: int64(1), object(3)<br/>memory usage: 207.5+ KB<br/></p>
<p>You can see the distribution of food groups with value_counts:<br/>In [165]: pd.value_counts(info.group)[:10]<br/>Out[165]: <br/>Vegetables <b>and</b> Vegetable Products    812<br/>Beef Products                        618<br/>Baked Products                       496<br/>Breakfast Cereals                    403<br/>Fast Foods                           365<br/>Legumes <b>and</b> Legume Products          365<br/>Lamb, Veal, <b>and</b> Game Products        345<br/>Sweets                               341<br/>Pork Products                        328<br/>Fruits <b>and</b> Fruit Juices              328<br/>Name: group, dtype: int64<br/></p>
<p>436 | Chapter 14: Data Analysis Examples</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Now, to do some analysis on all of the nutrient data, it&#8217;s easiest to assemble the<br/>nutrients for each food into a single large table. To do so, we need to take several<br/>steps. First, I&#8217;ll convert each list of food nutrients to a DataFrame, add a column for<br/>the food id, and append the DataFrame to a list. Then, these can be concatenated<br/>together with concat:<br/>If all goes well, nutrients should look like this:<br/></p>
<p>In [167]: nutrients<br/>Out[167]: <br/>                               description        group units    value     id<br/>0                                  Protein  Composition     g   25.180   1008<br/>1                        Total lipid (fat)  Composition     g   29.200   1008<br/>2              Carbohydrate, by difference  Composition     g    3.060   1008<br/>3                                      Ash        Other     g    3.280   1008<br/>4                                   Energy       Energy  kcal  376.000   1008<br/>...                                    ...          ...   ...      ...    ...<br/>389350                 Vitamin B-12, added     Vitamins   mcg    0.000  43546<br/>389351                         Cholesterol        Other    mg    0.000  43546<br/>389352        Fatty acids, total saturated        Other     g    0.072  43546<br/>389353  Fatty acids, total monounsaturated        Other     g    0.028  43546<br/>389354  Fatty acids, total polyunsaturated        Other     g    0.041  43546<br/>[389355 rows x 5 columns]<br/></p>
<p>I noticed that there are duplicates in this DataFrame, so it makes things easier to drop<br/>them:<br/></p>
<p>In [168]: nutrients.duplicated().sum()  <i># number of duplicates<br/></i>Out[168]: 14179<br/></p>
<p>In [169]: nutrients = nutrients.drop_duplicates()<br/></p>
<p>Since 'group' and 'description' are in both DataFrame objects, we can rename for<br/>clarity:<br/></p>
<p>In [170]: col_mapping = {'description' : 'food',<br/>   .....:                'group'       : 'fgroup'}<br/></p>
<p>In [171]: info = info.rename(columns=col_mapping, copy=False)<br/></p>
<p>In [172]: info.info()<br/>&lt;<b>class</b> '<b>pandas</b>.core.frame.DataFrame'&gt;<br/>RangeIndex: 6636 entries, 0 to 6635<br/>Data columns (total 4 columns):<br/>food            6636 non-null object<br/>fgroup          6636 non-null object<br/>id              6636 non-null int64<br/>manufacturer    5195 non-null object<br/>dtypes: int64(1), object(3)<br/>memory usage: 207.5+ KB<br/></p>
<p>In [173]: col_mapping = {'description' : 'nutrient',<br/></p>
<p>14.4 USDA Food Database | 437</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>   .....:                'group' : 'nutgroup'}<br/></p>
<p>In [174]: nutrients = nutrients.rename(columns=col_mapping, copy=False)<br/></p>
<p>In [175]: nutrients<br/>Out[175]: <br/>                                  nutrient     nutgroup units    value     id<br/>0                                  Protein  Composition     g   25.180   1008<br/>1                        Total lipid (fat)  Composition     g   29.200   1008<br/>2              Carbohydrate, by difference  Composition     g    3.060   1008<br/>3                                      Ash        Other     g    3.280   1008<br/>4                                   Energy       Energy  kcal  376.000   1008<br/>...                                    ...          ...   ...      ...    ...<br/>389350                 Vitamin B-12, added     Vitamins   mcg    0.000  43546<br/>389351                         Cholesterol        Other    mg    0.000  43546<br/>389352        Fatty acids, total saturated        Other     g    0.072  43546<br/>389353  Fatty acids, total monounsaturated        Other     g    0.028  43546<br/>389354  Fatty acids, total polyunsaturated        Other     g    0.041  43546<br/>[375176 rows x 5 columns]<br/></p>
<p>With all of this done, we&#8217;re ready to merge info with nutrients:<br/>In [176]: ndata = pd.merge(nutrients, info, on='id', how='outer')<br/></p>
<p>In [177]: ndata.info()<br/>&lt;<b>class</b> '<b>pandas</b>.core.frame.DataFrame'&gt;<br/>Int64Index: 375176 entries, 0 to 375175<br/>Data columns (total 8 columns):<br/>nutrient        375176 non-null object<br/>nutgroup        375176 non-null object<br/>units           375176 non-null object<br/>value           375176 non-null float64<br/>id              375176 non-null int64<br/>food            375176 non-null object<br/>fgroup          375176 non-null object<br/>manufacturer    293054 non-null object<br/>dtypes: float64(1), int64(1), object(6)<br/>memory usage: 25.8+ MB<br/></p>
<p>In [178]: ndata.iloc[30000]<br/>Out[178]: <br/>nutrient                                       Glycine<br/>nutgroup                                   Amino Acids<br/>units                                                g<br/>value                                             0.04<br/>id                                                6158<br/>food            Soup, tomato bisque, canned, condensed<br/>fgroup                      Soups, Sauces, <b>and</b> Gravies<br/>manufacturer                                          <br/>Name: 30000, dtype: object<br/></p>
<p>We could now make a plot of median values by food group and nutrient type (see<br/>Figure 14-11):<br/></p>
<p>438 | Chapter 14: Data Analysis Examples</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In [180]: result = ndata.groupby(['nutrient', 'fgroup'])['value'].quantile(0.5)<br/></p>
<p>In [181]: result['Zinc, Zn'].sort_values().plot(kind='barh')<br/></p>
<p><i>Figure 14-11. Median zinc values by nutrient group<br/></i>With a little cleverness, you can find which food is most dense in each nutrient:<br/></p>
<p>by_nutrient = ndata.groupby(['nutgroup', 'nutrient'])<br/></p>
<p>get_maximum = <b>lambda</b> x: x.loc[x.value.idxmax()]<br/>get_minimum = <b>lambda</b> x: x.loc[x.value.idxmin()]<br/></p>
<p>max_foods = by_nutrient.apply(get_maximum)[['value', 'food']]<br/></p>
<p><i># make the food a little smaller<br/></i>max_foods.food = max_foods.food.str[:50]<br/></p>
<p>The resulting DataFrame is a bit too large to display in the book; here is only the<br/>'Amino Acids' nutrient group:<br/></p>
<p>In [183]: max_foods.loc['Amino Acids']['food']<br/>Out[183]: <br/>nutrient<br/>Alanine                          Gelatins, dry powder, unsweetened<br/>Arginine                              Seeds, sesame flour, low-fat<br/>Aspartic acid                                  Soy protein isolate<br/>Cystine               Seeds, cottonseed flour, low fat (glandless)<br/>Glutamic acid                                  Soy protein isolate<br/>                                       ...                        <br/>Serine           Soy protein isolate, PROTEIN TECHNOLOGIES INTE...<br/>Threonine        Soy protein isolate, PROTEIN TECHNOLOGIES INTE...<br/>Tryptophan        Sea lion, Steller, meat <b>with</b> fat (Alaska Native)<br/>Tyrosine         Soy protein isolate, PROTEIN TECHNOLOGIES INTE...<br/></p>
<p>14.4 USDA Food Database | 439</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Valine           Soy protein isolate, PROTEIN TECHNOLOGIES INTE...<br/>Name: food, Length: 19, dtype: object<br/></p>
<p>14.5 2012 Federal Election Commission Database<br/>The US Federal Election Commission publishes data on contributions to political<br/>campaigns. This includes contributor names, occupation and employer, address, and<br/>contribution amount. An interesting dataset is from the 2012 US presidential elec&#8208;<br/>tion. A version of the dataset I downloaded in June 2012 is a 150 megabyte CSV file<br/><i>P00000001-ALL.csv</i> (see the book&#8217;s data repository), which can be loaded with pan<br/>das.read_csv:<br/></p>
<p>In [184]: fec = pd.read_csv('datasets/fec/P00000001-ALL.csv')<br/></p>
<p>In [185]: fec.info()<br/>&lt;<b>class</b> '<b>pandas</b>.core.frame.DataFrame'&gt;<br/>RangeIndex: 1001731 entries, 0 to 1001730<br/>Data columns (total 16 columns):<br/>cmte_id              1001731 non-null object<br/>cand_id              1001731 non-null object<br/>cand_nm              1001731 non-null object<br/>contbr_nm            1001731 non-null object<br/>contbr_city          1001712 non-null object<br/>contbr_st            1001727 non-null object<br/>contbr_zip           1001620 non-null object<br/>contbr_employer      988002 non-null object<br/>contbr_occupation    993301 non-null object<br/>contb_receipt_amt    1001731 non-null float64<br/>contb_receipt_dt     1001731 non-null object<br/>receipt_desc         14166 non-null object<br/>memo_cd              92482 non-null object<br/>memo_text            97770 non-null object<br/>form_tp              1001731 non-null object<br/>file_num             1001731 non-null int64<br/>dtypes: float64(1), int64(1), object(14)<br/>memory usage: 122.3+ MB<br/></p>
<p>A sample record in the DataFrame looks like this:<br/>In [186]: fec.iloc[123456]<br/>Out[186]: <br/>cmte_id             C00431445<br/>cand_id             P80003338<br/>cand_nm         Obama, Barack<br/>contbr_nm         ELLMAN, IRA<br/>contbr_city             TEMPE<br/>                    ...      <br/>receipt_desc              NaN<br/>memo_cd                   NaN<br/>memo_text                 NaN<br/>form_tp                 SA17A<br/></p>
<p>440 | Chapter 14: Data Analysis Examples</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>1 This makes the simplifying assumption that Gary Johnson is a Republican even though he later became the<br/>Libertarian party candidate.<br/></p>
<p>file_num               772372<br/>Name: 123456, Length: 16, dtype: object<br/></p>
<p>You may think of some ways to start slicing and dicing this data to extract informa&#8208;<br/>tive statistics about donors and patterns in the campaign contributions. I&#8217;ll show you<br/>a number of different analyses that apply techniques in this book.<br/>You can see that there are no political party affiliations in the data, so this would be<br/>useful to add. You can get a list of all the unique political candidates using unique:<br/></p>
<p>In [187]: unique_cands = fec.cand_nm.unique()<br/></p>
<p>In [188]: unique_cands<br/>Out[188]: <br/>array(['Bachmann, Michelle', 'Romney, Mitt', 'Obama, Barack',<br/>       &quot;Roemer, Charles E. 'Buddy' III&quot;, 'Pawlenty, Timothy',<br/>       'Johnson, Gary Earl', 'Paul, Ron', 'Santorum, Rick', 'Cain, Herman',<br/>       'Gingrich, Newt', 'McCotter, Thaddeus G', 'Huntsman, Jon',<br/>       'Perry, Rick'], dtype=object)<br/></p>
<p>In [189]: unique_cands[2]<br/>Out[189]: 'Obama, Barack'<br/></p>
<p>One way to indicate party affiliation is using a dict:1<br/></p>
<p>parties = {'Bachmann, Michelle': 'Republican',<br/>           'Cain, Herman': 'Republican',<br/>           'Gingrich, Newt': 'Republican',<br/>           'Huntsman, Jon': 'Republican',<br/>           'Johnson, Gary Earl': 'Republican',<br/>           'McCotter, Thaddeus G': 'Republican',<br/>           'Obama, Barack': 'Democrat',<br/>           'Paul, Ron': 'Republican',<br/>           'Pawlenty, Timothy': 'Republican',<br/>           'Perry, Rick': 'Republican',<br/>           &quot;Roemer, Charles E. 'Buddy' III&quot;: 'Republican',<br/>           'Romney, Mitt': 'Republican',<br/>           'Santorum, Rick': 'Republican'}<br/></p>
<p>Now, using this mapping and the map method on Series objects, you can compute an<br/>array of political parties from the candidate names:<br/></p>
<p>In [191]: fec.cand_nm[123456:123461]<br/>Out[191]: <br/>123456    Obama, Barack<br/>123457    Obama, Barack<br/>123458    Obama, Barack<br/>123459    Obama, Barack<br/>123460    Obama, Barack<br/></p>
<p>14.5 2012 Federal Election Commission Database | 441</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Name: cand_nm, dtype: object<br/></p>
<p>In [192]: fec.cand_nm[123456:123461].map(parties)<br/>Out[192]: <br/>123456    Democrat<br/>123457    Democrat<br/>123458    Democrat<br/>123459    Democrat<br/>123460    Democrat<br/>Name: cand_nm, dtype: object<br/></p>
<p><i># Add it as a column<br/></i>In [193]: fec['party'] = fec.cand_nm.map(parties)<br/></p>
<p>In [194]: fec['party'].value_counts()<br/>Out[194]: <br/>Democrat      593746<br/>Republican    407985<br/>Name: party, dtype: int64<br/></p>
<p>A couple of data preparation points. First, this data includes both contributions and<br/>refunds (negative contribution amount):<br/></p>
<p>In [195]: (fec.contb_receipt_amt &gt; 0).value_counts()<br/>Out[195]: <br/>True     991475<br/>False     10256<br/>Name: contb_receipt_amt, dtype: int64<br/></p>
<p>To simplify the analysis, I&#8217;ll restrict the dataset to positive contributions:<br/>In [196]: fec = fec[fec.contb_receipt_amt &gt; 0]<br/></p>
<p>Since Barack Obama and Mitt Romney were the main two candidates, I&#8217;ll also pre&#8208;<br/>pare a subset that just has contributions to their campaigns:<br/></p>
<p>In [197]: fec_mrbo = fec[fec.cand_nm.isin(['Obama, Barack', 'Romney, Mitt'])]<br/></p>
<p>Donation Statistics by Occupation and Employer<br/>Donations by occupation is another oft-studied statistic. For example, lawyers (attor&#8208;<br/>neys) tend to donate more money to Democrats, while business executives tend to<br/>donate more to Republicans. You have no reason to believe me; you can see for your&#8208;<br/>self in the data. First, the total number of donations by occupation is easy:<br/></p>
<p>In [198]: fec.contbr_occupation.value_counts()[:10]<br/>Out[198]: <br/>RETIRED                                   233990<br/>INFORMATION REQUESTED                      35107<br/>ATTORNEY                                   34286<br/>HOMEMAKER                                  29931<br/>PHYSICIAN                                  23432<br/>INFORMATION REQUESTED PER BEST EFFORTS     21138<br/></p>
<p>442 | Chapter 14: Data Analysis Examples</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>ENGINEER                                   14334<br/>TEACHER                                    13990<br/>CONSULTANT                                 13273<br/>PROFESSOR                                  12555<br/>Name: contbr_occupation, dtype: int64<br/></p>
<p>You will notice by looking at the occupations that many refer to the same basic job<br/>type, or there are several variants of the same thing. The following code snippet illus&#8208;<br/>trates a technique for cleaning up a few of them by mapping from one occupation to<br/>another; note the &#8220;trick&#8221; of using dict.get to allow occupations with no mapping to<br/>&#8220;pass through&#8221;:<br/></p>
<p>occ_mapping = {<br/>   'INFORMATION REQUESTED PER BEST EFFORTS' : 'NOT PROVIDED',<br/>   'INFORMATION REQUESTED' : 'NOT PROVIDED',<br/>   'INFORMATION REQUESTED (BEST EFFORTS)' : 'NOT PROVIDED',<br/>   'C.E.O.': 'CEO'<br/>}<br/></p>
<p><i># If no mapping provided, return x<br/></i>f = <b>lambda</b> x: occ_mapping.get(x, x)<br/>fec.contbr_occupation = fec.contbr_occupation.map(f)<br/></p>
<p>I&#8217;ll also do the same thing for employers:<br/>emp_mapping = {<br/>   'INFORMATION REQUESTED PER BEST EFFORTS' : 'NOT PROVIDED',<br/>   'INFORMATION REQUESTED' : 'NOT PROVIDED',<br/>   'SELF' : 'SELF-EMPLOYED',<br/>   'SELF EMPLOYED' : 'SELF-EMPLOYED',<br/>}<br/></p>
<p><i># If no mapping provided, return x<br/></i>f = <b>lambda</b> x: emp_mapping.get(x, x)<br/>fec.contbr_employer = fec.contbr_employer.map(f)<br/></p>
<p>Now, you can use pivot_table to aggregate the data by party and occupation, then<br/>filter down to the subset that donated at least $2 million overall:<br/></p>
<p>In [201]: by_occupation = fec.pivot_table('contb_receipt_amt',<br/>   .....:                                 index='contbr_occupation',<br/>   .....:                                 columns='party', aggfunc='sum')<br/></p>
<p>In [202]: over_2mm = by_occupation[by_occupation.sum(1) &gt; 2000000]<br/></p>
<p>In [203]: over_2mm<br/>Out[203]: <br/>party                 Democrat    Republican<br/>contbr_occupation                           <br/>ATTORNEY           11141982.97  7.477194e+06<br/>CEO                 2074974.79  4.211041e+06<br/>CONSULTANT          2459912.71  2.544725e+06<br/>ENGINEER             951525.55  1.818374e+06<br/></p>
<p>14.5 2012 Federal Election Commission Database | 443</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>EXECUTIVE           1355161.05  4.138850e+06<br/>...                        ...           ...<br/>PRESIDENT           1878509.95  4.720924e+06<br/>PROFESSOR           2165071.08  2.967027e+05<br/>REAL ESTATE          528902.09  1.625902e+06<br/>RETIRED            25305116.38  2.356124e+07<br/>SELF-EMPLOYED        672393.40  1.640253e+06<br/>[17 rows x 2 columns]<br/></p>
<p>It can be easier to look at this data graphically as a bar plot ('barh' means horizontal<br/>bar plot; see Figure 14-12):<br/></p>
<p>In [205]: over_2mm.plot(kind='barh')<br/></p>
<p><i>Figure 14-12. Total donations by party for top occupations<br/></i>You might be interested in the top donor occupations or top companies that donated<br/>to Obama and Romney. To do this, you can group by candidate name and use a var&#8208;<br/>iant of the top method from earlier in the chapter:<br/></p>
<p><b>def</b> get_top_amounts(group, key, n=5):<br/>    totals = group.groupby(key)['contb_receipt_amt'].sum()<br/>    <b>return</b> totals.nlargest(n)<br/></p>
<p>Then aggregate by occupation and employer:<br/>In [207]: grouped = fec_mrbo.groupby('cand_nm')<br/></p>
<p>In [208]: grouped.apply(get_top_amounts, 'contbr_occupation', n=7)<br/>Out[208]: <br/></p>
<p>444 | Chapter 14: Data Analysis Examples</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>cand_nm        contbr_occupation    <br/>Obama, Barack  RETIRED                  25305116.38<br/>               ATTORNEY                 11141982.97<br/>               INFORMATION REQUESTED     4866973.96<br/>               HOMEMAKER                 4248875.80<br/>               PHYSICIAN                 3735124.94<br/>                                           ...     <br/>Romney, Mitt   HOMEMAKER                 8147446.22<br/>               ATTORNEY                  5364718.82<br/>               PRESIDENT                 2491244.89<br/>               EXECUTIVE                 2300947.03<br/>               C.E.O.                    1968386.11<br/>Name: contb_receipt_amt, Length: 14, dtype: float64<br/></p>
<p>In [209]: grouped.apply(get_top_amounts, 'contbr_employer', n=10)<br/>Out[209]: <br/>cand_nm        contbr_employer      <br/>Obama, Barack  RETIRED                  22694358.85<br/>               SELF-EMPLOYED            17080985.96<br/>               NOT EMPLOYED              8586308.70<br/>               INFORMATION REQUESTED     5053480.37<br/>               HOMEMAKER                 2605408.54<br/>                                           ...     <br/>Romney, Mitt   CREDIT SUISSE              281150.00<br/>               MORGAN STANLEY             267266.00<br/>               GOLDMAN SACH &amp; CO.         238250.00<br/>               BARCLAYS CAPITAL           162750.00<br/>               H.I.G. CAPITAL             139500.00<br/>Name: contb_receipt_amt, Length: 20, dtype: float64<br/></p>
<p>Bucketing Donation Amounts<br/>A useful way to analyze this data is to use the cut function to discretize the contribu&#8208;<br/>tor amounts into buckets by contribution size:<br/></p>
<p>In [210]: bins = np.array([0, 1, 10, 100, 1000, 10000,<br/>   .....:                  100000, 1000000, 10000000])<br/></p>
<p>In [211]: labels = pd.cut(fec_mrbo.contb_receipt_amt, bins)<br/></p>
<p>In [212]: labels<br/>Out[212]: <br/>411         (10, 100]<br/>412       (100, 1000]<br/>413       (100, 1000]<br/>414         (10, 100]<br/>415         (10, 100]<br/>             ...     <br/>701381      (10, 100]<br/>701382    (100, 1000]<br/>701383        (1, 10]<br/>701384      (10, 100]<br/></p>
<p>14.5 2012 Federal Election Commission Database | 445</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>701385    (100, 1000]<br/>Name: contb_receipt_amt, Length: 694282, dtype: category<br/>Categories (8, interval[int64]): [(0, 1] &lt; (1, 10] &lt; (10, 100] &lt; (100, 1000] &lt; (1<br/>000, 10000] &lt;<br/>                                  (10000, 100000] &lt; (100000, 1000000] &lt; (1000000,<br/> 10000000]]<br/></p>
<p>We can then group the data for Obama and Romney by name and bin label to get a<br/>histogram by donation size:<br/></p>
<p>In [213]: grouped = fec_mrbo.groupby(['cand_nm', labels])<br/></p>
<p>In [214]: grouped.size().unstack(0)<br/>Out[214]: <br/>cand_nm              Obama, Barack  Romney, Mitt<br/>contb_receipt_amt                               <br/>(0, 1]                       493.0          77.0<br/>(1, 10]                    40070.0        3681.0<br/>(10, 100]                 372280.0       31853.0<br/>(100, 1000]               153991.0       43357.0<br/>(1000, 10000]              22284.0       26186.0<br/>(10000, 100000]                2.0           1.0<br/>(100000, 1000000]              3.0           NaN<br/>(1000000, 10000000]            4.0           NaN<br/></p>
<p>This data shows that Obama received a significantly larger number of small donations<br/>than Romney. You can also sum the contribution amounts and normalize within<br/>buckets to visualize percentage of total donations of each size by candidate<br/>(Figure 14-13 shows the resulting plot):<br/></p>
<p>In [216]: bucket_sums = grouped.contb_receipt_amt.sum().unstack(0)<br/></p>
<p>In [217]: normed_sums = bucket_sums.div(bucket_sums.sum(axis=1), axis=0)<br/></p>
<p>In [218]: normed_sums<br/>Out[218]: <br/>cand_nm              Obama, Barack  Romney, Mitt<br/>contb_receipt_amt                               <br/>(0, 1]                    0.805182      0.194818<br/>(1, 10]                   0.918767      0.081233<br/>(10, 100]                 0.910769      0.089231<br/>(100, 1000]               0.710176      0.289824<br/>(1000, 10000]             0.447326      0.552674<br/>(10000, 100000]           0.823120      0.176880<br/>(100000, 1000000]         1.000000           NaN<br/>(1000000, 10000000]       1.000000           NaN<br/></p>
<p>In [219]: normed_sums[:-2].plot(kind='barh')<br/></p>
<p>446 | Chapter 14: Data Analysis Examples</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><i>Figure 14-13. Percentage of total donations received by candidates for each donation size<br/></i>I excluded the two largest bins as these are not donations by individuals.<br/>This analysis can be refined and improved in many ways. For example, you could<br/>aggregate donations by donor name and zip code to adjust for donors who gave many<br/>small amounts versus one or more large donations. I encourage you to download and<br/>explore the dataset yourself.<br/></p>
<p>Donation Statistics by State<br/>Aggregating the data by candidate and state is a routine affair:<br/></p>
<p>In [220]: grouped = fec_mrbo.groupby(['cand_nm', 'contbr_st'])<br/></p>
<p>In [221]: totals = grouped.contb_receipt_amt.sum().unstack(0).fillna(0)<br/></p>
<p>In [222]: totals = totals[totals.sum(1) &gt; 100000]<br/></p>
<p>In [223]: totals[:10]<br/>Out[223]: <br/>cand_nm    Obama, Barack  Romney, Mitt<br/>contbr_st                             <br/>AK             281840.15      86204.24<br/>AL             543123.48     527303.51<br/>AR             359247.28     105556.00<br/>AZ            1506476.98    1888436.23<br/>CA           23824984.24   11237636.60<br/>CO            2132429.49    1506714.12<br/>CT            2068291.26    3499475.45<br/></p>
<p>14.5 2012 Federal Election Commission Database | 447</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>DC            4373538.80    1025137.50<br/>DE             336669.14      82712.00<br/>FL            7318178.58    8338458.81<br/></p>
<p>If you divide each row by the total contribution amount, you get the relative percent&#8208;<br/>age of total donations by state for each candidate:<br/></p>
<p>In [224]: percent = totals.div(totals.sum(1), axis=0)<br/></p>
<p>In [225]: percent[:10]<br/>Out[225]: <br/>cand_nm    Obama, Barack  Romney, Mitt<br/>contbr_st                             <br/>AK              0.765778      0.234222<br/>AL              0.507390      0.492610<br/>AR              0.772902      0.227098<br/>AZ              0.443745      0.556255<br/>CA              0.679498      0.320502<br/>CO              0.585970      0.414030<br/>CT              0.371476      0.628524<br/>DC              0.810113      0.189887<br/>DE              0.802776      0.197224<br/>FL              0.467417      0.532583<br/></p>
<p>14.6 Conclusion<br/>We&#8217;ve reached the end of the book&#8217;s main chapters. I have included some additional<br/>content you may find useful in the appendixes.<br/>In the five years since the first edition of this book was published, Python has become<br/>a popular and widespread language for data analysis. The programming skills you<br/>have developed here will stay relevant for a long time into the future. I hope the pro&#8208;<br/>gramming tools and libraries we&#8217;ve explored serve you well in your work.<br/></p>
<p>448 | Chapter 14: Data Analysis Examples</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>APPENDIX A<br/>Advanced NumPy<br/></p>
<p>In this appendix, I will go deeper into the NumPy library for array computing. This<br/>will include more internal detail about the ndarray type and more advanced array<br/>manipulations and algorithms.<br/>This appendix contains miscellaneous topics and does not necessarily need to be read<br/>linearly.<br/></p>
<p>A.1 ndarray Object Internals<br/>The NumPy ndarray provides a means to interpret a block of homogeneous data<br/>(either contiguous or strided) as a multidimensional array object. The data type, or<br/><i>dtype</i>, determines how the data is interpreted as being floating point, integer, boolean,<br/>or any of the other types we&#8217;ve been looking at.<br/>Part of what makes ndarray flexible is that every array object is a <i>strided</i> view on a<br/>block of data. You might wonder, for example, how the array view arr[::2, ::-1]<br/>does not copy any data. The reason is that the ndarray is more than just a chunk of<br/>memory and a dtype; it also has &#8220;striding&#8221; information that enables the array to move<br/>through memory with varying step sizes. More precisely, the ndarray internally con&#8208;<br/>sists of the following:<br/></p>
<p>&#8226; A <i>pointer to data</i>&#8212;that is, a block of data in RAM or in a memory-mapped file<br/>&#8226; The <i>data type</i> or dtype, describing fixed-size value cells in the array<br/>&#8226; A tuple indicating the array&#8217;s <i>shape<br/></i>&#8226; A tuple of <i>strides</i>, integers indicating the number of bytes to &#8220;step&#8221; in order to<br/></p>
<p>advance one element along a dimension<br/>See Figure A-1 for a simple mockup of the ndarray innards.<br/></p>
<p>449</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>For example, a 10 &#215; 5 array would have shape (10, 5):<br/>In [10]: np.ones((10, 5)).shape<br/>Out[10]: (10, 5)<br/></p>
<p>A typical (C order) 3 &#215; 4 &#215; 5 array of float64 (8-byte) values has strides (160, 40,<br/>8) (knowing about the strides can be useful because, in general, the larger the strides<br/>on a particular axis, the more costly it is to perform computation along that axis):<br/></p>
<p>In [11]: np.ones((3, 4, 5), dtype=np.float64).strides<br/>Out[11]: (160, 40, 8)<br/></p>
<p>While it is rare that a typical NumPy user would be interested in the array strides,<br/>they are the critical ingredient in constructing &#8220;zero-copy&#8221; array views. Strides can<br/>even be negative, which enables an array to move &#8220;backward&#8221; through memory (this<br/>would be the case, for example, in a slice like obj[::-1] or obj[:, ::-1]).<br/></p>
<p><i>Figure A-1. The NumPy ndarray object<br/></i></p>
<p>NumPy dtype Hierarchy<br/>You may occasionally have code that needs to check whether an array contains inte&#8208;<br/>gers, floating-point numbers, strings, or Python objects. Because there are multiple<br/>types of floating-point numbers (float16 through float128), checking that the dtype<br/>is among a list of types would be very verbose. Fortunately, the dtypes have super&#8208;<br/>classes such as np.integer and np.floating, which can be used in conjunction with <br/>the np.issubdtype function:<br/></p>
<p>In [12]: ints = np.ones(10, dtype=np.uint16)<br/></p>
<p>In [13]: floats = np.ones(10, dtype=np.float32)<br/></p>
<p>In [14]: np.issubdtype(ints.dtype, np.integer)<br/>Out[14]: True<br/></p>
<p>In [15]: np.issubdtype(floats.dtype, np.floating)<br/>Out[15]: True<br/></p>
<p>You can see all of the parent classes of a specific dtype by calling the type&#8217;s mro<br/>method:<br/></p>
<p>450 | Appendix A: Advanced NumPy</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>1 Some of the dtypes have trailing underscores in their names. These are there to avoid variable name conflicts<br/>between the NumPy-specific types and the Python built-in ones.<br/></p>
<p>In [16]: np.float64.mro()<br/>Out[16]: <br/>[numpy.float64,<br/> numpy.floating,<br/> numpy.inexact,<br/> numpy.number,<br/> numpy.generic,<br/> float,<br/> object]<br/></p>
<p>Therefore, we also have:<br/>In [17]: np.issubdtype(ints.dtype, np.number)<br/>Out[17]: True<br/></p>
<p>Most NumPy users will never have to know about this, but it occasionally comes in<br/>handy. See Figure A-2 for a graph of the dtype hierarchy and parent&#8211;subclass<br/>relationships.1<br/></p>
<p><i>Figure A-2. The NumPy dtype class hierarchy<br/></i></p>
<p>A.2 Advanced Array Manipulation<br/>There are many ways to work with arrays beyond fancy indexing, slicing, and boolean<br/>subsetting. While much of the heavy lifting for data analysis applications is handled<br/>by higher-level functions in pandas, you may at some point need to write a data algo&#8208;<br/>rithm that is not found in one of the existing libraries.<br/></p>
<p>Advanced NumPy | 451</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Reshaping Arrays<br/>In many cases, you can convert an array from one shape to another without copying<br/>any data. To do this, pass a tuple indicating the new shape to the reshape array<br/>instance method. For example, suppose we had a one-dimensional array of values<br/>that we wished to rearrange into a matrix (the result is shown in Figure A-3):<br/></p>
<p>In [18]: arr = np.arange(8)<br/></p>
<p>In [19]: arr<br/>Out[19]: array([0, 1, 2, 3, 4, 5, 6, 7])<br/></p>
<p>In [20]: arr.reshape((4, 2))<br/>Out[20]: <br/>array([[0, 1],<br/>       [2, 3],<br/>       [4, 5],<br/>       [6, 7]])<br/></p>
<p><i>Figure A-3. Reshaping in C (row major) or Fortran (column major) order<br/></i>A multidimensional array can also be reshaped:<br/></p>
<p>In [21]: arr.reshape((4, 2)).reshape((2, 4))<br/>Out[21]: <br/>array([[0, 1, 2, 3],<br/>       [4, 5, 6, 7]])<br/></p>
<p>One of the passed shape dimensions can be &#8211;1, in which case the value used for that<br/>dimension will be inferred from the data:<br/></p>
<p>452 | Appendix A: Advanced NumPy</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In [22]: arr = np.arange(15)<br/></p>
<p>In [23]: arr.reshape((5, -1))<br/>Out[23]: <br/>array([[ 0,  1,  2],<br/>       [ 3,  4,  5],<br/>       [ 6,  7,  8],<br/>       [ 9, 10, 11],<br/>       [12, 13, 14]])<br/></p>
<p>Since an array&#8217;s shape attribute is a tuple, it can be passed to reshape, too:<br/>In [24]: other_arr = np.ones((3, 5))<br/></p>
<p>In [25]: other_arr.shape<br/>Out[25]: (3, 5)<br/></p>
<p>In [26]: arr.reshape(other_arr.shape)<br/>Out[26]: <br/>array([[ 0,  1,  2,  3,  4],<br/>       [ 5,  6,  7,  8,  9],<br/>       [10, 11, 12, 13, 14]])<br/></p>
<p>The opposite operation of reshape from one-dimensional to a higher dimension is<br/>typically known as flattening or raveling:<br/></p>
<p>In [27]: arr = np.arange(15).reshape((5, 3))<br/></p>
<p>In [28]: arr<br/>Out[28]: <br/>array([[ 0,  1,  2],<br/>       [ 3,  4,  5],<br/>       [ 6,  7,  8],<br/>       [ 9, 10, 11],<br/>       [12, 13, 14]])<br/></p>
<p>In [29]: arr.ravel()<br/>Out[29]: array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14])<br/></p>
<p>ravel does not produce a copy of the underlying values if the values in the result<br/>were contiguous in the original array. The flatten method behaves like ravel except<br/>it always returns a copy of the data:<br/></p>
<p>In [30]: arr.flatten()<br/>Out[30]: array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14])<br/></p>
<p>The data can be reshaped or raveled in different orders. This is a slightly nuanced<br/>topic for new NumPy users and is therefore the next subtopic.<br/></p>
<p>Advanced NumPy | 453</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>C Versus Fortran Order<br/>NumPy gives you control and flexibility over the layout of your data in memory. By<br/>default, NumPy arrays are created in <i>row major</i> order. Spatially this means that if you<br/>have a two-dimensional array of data, the items in each row of the array are stored in<br/>adjacent memory locations. The alternative to row major ordering is <i>column major<br/></i>order, which means that values within each column of data are stored in adjacent<br/>memory locations.<br/>For historical reasons, row and column major order are also know as C and Fortran<br/>order, respectively. In the FORTRAN 77 language, matrices are all column major.<br/>Functions like reshape and ravel accept an order argument indicating the order to<br/>use the data in the array. This is usually set to 'C' or 'F' in most cases (there are also<br/>less commonly used options 'A' and 'K'; see the NumPy documentation, and refer<br/>back to Figure A-3 for an illustration of these options):<br/></p>
<p>In [31]: arr = np.arange(12).reshape((3, 4))<br/></p>
<p>In [32]: arr<br/>Out[32]: <br/>array([[ 0,  1,  2,  3],<br/>       [ 4,  5,  6,  7],<br/>       [ 8,  9, 10, 11]])<br/></p>
<p>In [33]: arr.ravel()<br/>Out[33]: array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])<br/></p>
<p>In [34]: arr.ravel('F')<br/>Out[34]: array([ 0,  4,  8,  1,  5,  9,  2,  6, 10,  3,  7, 11])<br/></p>
<p>Reshaping arrays with more than two dimensions can be a bit mind-bending (see<br/>Figure A-3). The key difference between C and Fortran order is the way in which the<br/>dimensions are walked:<br/><i>C/row major order<br/></i></p>
<p>Traverse higher dimensions first (e.g., axis 1 before advancing on axis 0).<br/><i>Fortran/column major order<br/></i></p>
<p>Traverse higher dimensions <i>last</i> (e.g., axis 0 before advancing on axis 1).<br/></p>
<p>Concatenating and Splitting Arrays<br/>numpy.concatenate takes a sequence (tuple, list, etc.) of arrays and joins them<br/>together in order along the input axis:<br/></p>
<p>In [35]: arr1 = np.array([[1, 2, 3], [4, 5, 6]])<br/></p>
<p>In [36]: arr2 = np.array([[7, 8, 9], [10, 11, 12]])<br/></p>
<p>454 | Appendix A: Advanced NumPy</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In [37]: np.concatenate([arr1, arr2], axis=0)<br/>Out[37]: <br/>array([[ 1,  2,  3],<br/>       [ 4,  5,  6],<br/>       [ 7,  8,  9],<br/>       [10, 11, 12]])<br/></p>
<p>In [38]: np.concatenate([arr1, arr2], axis=1)<br/>Out[38]: <br/>array([[ 1,  2,  3,  7,  8,  9],<br/>       [ 4,  5,  6, 10, 11, 12]])<br/></p>
<p>There are some convenience functions, like vstack and hstack, for common kinds of<br/>concatenation. The preceding operations could have been expressed as:<br/></p>
<p>In [39]: np.vstack((arr1, arr2))<br/>Out[39]: <br/>array([[ 1,  2,  3],<br/>       [ 4,  5,  6],<br/>       [ 7,  8,  9],<br/>       [10, 11, 12]])<br/></p>
<p>In [40]: np.hstack((arr1, arr2))<br/>Out[40]: <br/>array([[ 1,  2,  3,  7,  8,  9],<br/>       [ 4,  5,  6, 10, 11, 12]])<br/></p>
<p>split, on the other hand, slices apart an array into multiple arrays along an axis:<br/>In [41]: arr = np.random.randn(5, 2)<br/></p>
<p>In [42]: arr<br/>Out[42]: <br/>array([[-0.2047,  0.4789],<br/>       [-0.5194, -0.5557],<br/>       [ 1.9658,  1.3934],<br/>       [ 0.0929,  0.2817],<br/>       [ 0.769 ,  1.2464]])<br/></p>
<p>In [43]: first, second, third = np.split(arr, [1, 3])<br/></p>
<p>In [44]: first<br/>Out[44]: array([[-0.2047,  0.4789]])<br/></p>
<p>In [45]: second<br/>Out[45]: <br/>array([[-0.5194, -0.5557],<br/>       [ 1.9658,  1.3934]])<br/></p>
<p>In [46]: third<br/>Out[46]: <br/>array([[ 0.0929,  0.2817],<br/>       [ 0.769 ,  1.2464]])<br/></p>
<p>Advanced NumPy | 455</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>The value [1, 3] passed to np.split indicate the indices at which to split the array<br/>into pieces.<br/>See Table A-1 for a list of all relevant concatenation and splitting functions, some of<br/>which are provided only as a convenience of the very general-purpose concatenate.<br/><i>Table A-1. Array concatenation functions<br/></i></p>
<p>Function Description<br/>concatenate Most general function, concatenates collection of arrays along one axis<br/>vstack, row_stack Stack arrays row-wise (along axis 0)<br/>hstack Stack arrays column-wise (along axis 1)<br/>column_stack Like hstack, but converts 1D arrays to 2D column vectors first<br/>dstack Stack arrays &#8220;depth&#8221;-wise (along axis 2)<br/>split Split array at passed locations along a particular axis<br/>hsplit/vsplit Convenience functions for splitting on axis 0 and 1, respectively<br/></p>
<p>Stacking helpers: r_ and c_<br/>There are two special objects in the NumPy namespace, r_ and c_, that make stacking<br/>arrays more concise:<br/></p>
<p>In [47]: arr = np.arange(6)<br/></p>
<p>In [48]: arr1 = arr.reshape((3, 2))<br/></p>
<p>In [49]: arr2 = np.random.randn(3, 2)<br/></p>
<p>In [50]: np.r_[arr1, arr2]<br/>Out[50]: <br/>array([[ 0.    ,  1.    ],<br/>       [ 2.    ,  3.    ],<br/>       [ 4.    ,  5.    ],<br/>       [ 1.0072, -1.2962],<br/>       [ 0.275 ,  0.2289],<br/>       [ 1.3529,  0.8864]])<br/></p>
<p>In [51]: np.c_[np.r_[arr1, arr2], arr]<br/>Out[51]: <br/>array([[ 0.    ,  1.    ,  0.    ],<br/>       [ 2.    ,  3.    ,  1.    ],<br/>       [ 4.    ,  5.    ,  2.    ],<br/>       [ 1.0072, -1.2962,  3.    ],<br/>       [ 0.275 ,  0.2289,  4.    ],<br/>       [ 1.3529,  0.8864,  5.    ]])<br/></p>
<p>These additionally can translate slices to arrays:<br/>In [52]: np.c_[1:6, -10:-5]<br/>Out[52]: <br/></p>
<p>456 | Appendix A: Advanced NumPy</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>array([[  1, -10],<br/>       [  2,  -9],<br/>       [  3,  -8],<br/>       [  4,  -7],<br/>       [  5,  -6]])<br/></p>
<p>See the docstring for more on what you can do with c_ and r_.<br/></p>
<p>Repeating Elements: tile and repeat<br/>Two useful tools for repeating or replicating arrays to produce larger arrays are the<br/>repeat and tile functions. repeat replicates each element in an array some number<br/>of times, producing a larger array:<br/></p>
<p>In [53]: arr = np.arange(3)<br/></p>
<p>In [54]: arr<br/>Out[54]: array([0, 1, 2])<br/></p>
<p>In [55]: arr.repeat(3)<br/>Out[55]: array([0, 0, 0, 1, 1, 1, 2, 2, 2])<br/></p>
<p>The need to replicate or repeat arrays can be less common with<br/>NumPy than it is with other array programming frameworks like<br/>MATLAB. One reason for this is that <i>broadcasting</i> often fills this<br/>need better, which is the subject of the next section.<br/></p>
<p>By default, if you pass an integer, each element will be repeated that number of times.<br/>If you pass an array of integers, each element can be repeated a different number of<br/>times:<br/></p>
<p>In [56]: arr.repeat([2, 3, 4])<br/>Out[56]: array([0, 0, 1, 1, 1, 2, 2, 2, 2])<br/></p>
<p>Multidimensional arrays can have their elements repeated along a particular axis.<br/>In [57]: arr = np.random.randn(2, 2)<br/></p>
<p>In [58]: arr<br/>Out[58]: <br/>array([[-2.0016, -0.3718],<br/>       [ 1.669 , -0.4386]])<br/></p>
<p>In [59]: arr.repeat(2, axis=0)<br/>Out[59]: <br/>array([[-2.0016, -0.3718],<br/>       [-2.0016, -0.3718],<br/>       [ 1.669 , -0.4386],<br/>       [ 1.669 , -0.4386]])<br/></p>
<p>Advanced NumPy | 457</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Note that if no axis is passed, the array will be flattened first, which is likely not what<br/>you want. Similarly, you can pass an array of integers when repeating a multidimen&#8208;<br/>sional array to repeat a given slice a different number of times:<br/></p>
<p>In [60]: arr.repeat([2, 3], axis=0)<br/>Out[60]: <br/>array([[-2.0016, -0.3718],<br/>       [-2.0016, -0.3718],<br/>       [ 1.669 , -0.4386],<br/>       [ 1.669 , -0.4386],<br/>       [ 1.669 , -0.4386]])<br/></p>
<p>In [61]: arr.repeat([2, 3], axis=1)<br/>Out[61]: <br/>array([[-2.0016, -2.0016, -0.3718, -0.3718, -0.3718],<br/>       [ 1.669 ,  1.669 , -0.4386, -0.4386, -0.4386]])<br/></p>
<p>tile, on the other hand, is a shortcut for stacking copies of an array along an axis.<br/>Visually you can think of it as being akin to &#8220;laying down tiles&#8221;:<br/></p>
<p>In [62]: arr<br/>Out[62]: <br/>array([[-2.0016, -0.3718],<br/>       [ 1.669 , -0.4386]])<br/></p>
<p>In [63]: np.tile(arr, 2)<br/>Out[63]: <br/>array([[-2.0016, -0.3718, -2.0016, -0.3718],<br/>       [ 1.669 , -0.4386,  1.669 , -0.4386]])<br/></p>
<p>The second argument is the number of tiles; with a scalar, the tiling is made row by<br/>row, rather than column by column. The second argument to tile can be a tuple<br/>indicating the layout of the &#8220;tiling&#8221;:<br/></p>
<p>In [64]: arr<br/>Out[64]: <br/>array([[-2.0016, -0.3718],<br/>       [ 1.669 , -0.4386]])<br/></p>
<p>In [65]: np.tile(arr, (2, 1))<br/>Out[65]: <br/>array([[-2.0016, -0.3718],<br/>       [ 1.669 , -0.4386],<br/>       [-2.0016, -0.3718],<br/>       [ 1.669 , -0.4386]])<br/></p>
<p>In [66]: np.tile(arr, (3, 2))<br/>Out[66]: <br/>array([[-2.0016, -0.3718, -2.0016, -0.3718],<br/>       [ 1.669 , -0.4386,  1.669 , -0.4386],<br/>       [-2.0016, -0.3718, -2.0016, -0.3718],<br/>       [ 1.669 , -0.4386,  1.669 , -0.4386],<br/></p>
<p>458 | Appendix A: Advanced NumPy</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>       [-2.0016, -0.3718, -2.0016, -0.3718],<br/>       [ 1.669 , -0.4386,  1.669 , -0.4386]])<br/></p>
<p>Fancy Indexing Equivalents: take and put<br/>As you may recall from Chapter 4, one way to get and set subsets of arrays is by <i>fancy<br/></i>indexing using integer arrays:<br/></p>
<p>In [67]: arr = np.arange(10) * 100<br/></p>
<p>In [68]: inds = [7, 1, 2, 6]<br/></p>
<p>In [69]: arr[inds]<br/>Out[69]: array([700, 100, 200, 600])<br/></p>
<p>There are alternative ndarray methods that are useful in the special case of only mak&#8208;<br/>ing a selection on a single axis:<br/></p>
<p>In [70]: arr.take(inds)<br/>Out[70]: array([700, 100, 200, 600])<br/></p>
<p>In [71]: arr.put(inds, 42)<br/></p>
<p>In [72]: arr<br/>Out[72]: array([  0,  42,  42, 300, 400, 500,  42,  42, 800, 900])<br/></p>
<p>In [73]: arr.put(inds, [40, 41, 42, 43])<br/></p>
<p>In [74]: arr<br/>Out[74]: array([  0,  41,  42, 300, 400, 500,  43,  40, 800, 900])<br/></p>
<p>To use take along other axes, you can pass the axis keyword:<br/>In [75]: inds = [2, 0, 2, 1]<br/></p>
<p>In [76]: arr = np.random.randn(2, 4)<br/></p>
<p>In [77]: arr<br/>Out[77]: <br/>array([[-0.5397,  0.477 ,  3.2489, -1.0212],<br/>       [-0.5771,  0.1241,  0.3026,  0.5238]])<br/></p>
<p>In [78]: arr.take(inds, axis=1)<br/>Out[78]: <br/>array([[ 3.2489, -0.5397,  3.2489,  0.477 ],<br/>       [ 0.3026, -0.5771,  0.3026,  0.1241]])<br/></p>
<p>put does not accept an axis argument but rather indexes into the flattened (one-<br/>dimensional, C order) version of the array. Thus, when you need to set elements<br/>using an index array on other axes, it is often easiest to use fancy indexing.<br/></p>
<p>Advanced NumPy | 459</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>A.3 Broadcasting<br/><i>Broadcasting</i> describes how arithmetic works between arrays of different shapes. It<br/>can be a powerful feature, but one that can cause confusion, even for experienced<br/>users. The simplest example of broadcasting occurs when combining a scalar value<br/>with an array:<br/></p>
<p>In [79]: arr = np.arange(5)<br/></p>
<p>In [80]: arr<br/>Out[80]: array([0, 1, 2, 3, 4])<br/></p>
<p>In [81]: arr * 4<br/>Out[81]: array([ 0,  4,  8, 12, 16])<br/></p>
<p>Here we say that the scalar value 4 has been <i>broadcast</i> to all of the other elements in<br/>the multiplication operation.<br/>For example, we can demean each column of an array by subtracting the column<br/>means. In this case, it is very simple:<br/></p>
<p>In [82]: arr = np.random.randn(4, 3)<br/></p>
<p>In [83]: arr.mean(0)<br/>Out[83]: array([-0.3928, -0.3824, -0.8768])<br/></p>
<p>In [84]: demeaned = arr - arr.mean(0)<br/></p>
<p>In [85]: demeaned<br/>Out[85]: <br/>array([[ 0.3937,  1.7263,  0.1633],<br/>       [-0.4384, -1.9878, -0.9839],<br/>       [-0.468 ,  0.9426, -0.3891],<br/>       [ 0.5126, -0.6811,  1.2097]])<br/></p>
<p>In [86]: demeaned.mean(0)<br/>Out[86]: array([-0.,  0., -0.])<br/></p>
<p>See Figure A-4 for an illustration of this operation. Demeaning the rows as a broad&#8208;<br/>cast operation requires a bit more care. Fortunately, broadcasting potentially lower<br/>dimensional values across any dimension of an array (like subtracting the row means<br/>from each column of a two-dimensional array) is possible as long as you follow the<br/>rules.<br/>This brings us to:<br/></p>
<p>460 | Appendix A: Advanced NumPy</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>The Broadcasting Rule<br/>Two arrays are compatible for broadcasting if for each <i>trailing dimension</i> (i.e., starting<br/>from the end) the axis lengths match or if either of the lengths is 1. Broadcasting is<br/>then performed over the missing or length 1 dimensions.<br/></p>
<p><i>Figure A-4. Broadcasting over axis 0 with a 1D array<br/></i>Even as an experienced NumPy user, I often find myself having to pause and draw a<br/>diagram as I think about the broadcasting rule. Consider the last example and sup&#8208;<br/>pose we wished instead to subtract the mean value from each row. Since arr.mean(0)<br/>has length 3, it is compatible for broadcasting across axis 0 because the trailing<br/>dimension in arr is 3 and therefore matches. According to the rules, to subtract over<br/>axis 1 (i.e., subtract the row mean from each row), the smaller array must have shape<br/>(4, 1):<br/></p>
<p>In [87]: arr<br/>Out[87]: <br/>array([[ 0.0009,  1.3438, -0.7135],<br/>       [-0.8312, -2.3702, -1.8608],<br/>       [-0.8608,  0.5601, -1.2659],<br/>       [ 0.1198, -1.0635,  0.3329]])<br/></p>
<p>In [88]: row_means = arr.mean(1)<br/></p>
<p>In [89]: row_means.shape<br/>Out[89]: (4,)<br/></p>
<p>In [90]: row_means.reshape((4, 1))<br/>Out[90]: <br/>array([[ 0.2104],<br/>       [-1.6874],<br/>       [-0.5222],<br/>       [-0.2036]])<br/></p>
<p>Advanced NumPy | 461</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In [91]: demeaned = arr - row_means.reshape((4, 1))<br/></p>
<p>In [92]: demeaned.mean(1)<br/>Out[92]: array([ 0., -0.,  0.,  0.])<br/></p>
<p>See Figure A-5 for an illustration of this operation.<br/></p>
<p><i>Figure A-5. Broadcasting over axis 1 of a 2D array<br/></i>See Figure A-6 for another illustration, this time adding a two-dimensional array to a<br/>three-dimensional one across axis 0.<br/></p>
<p><i>Figure A-6. Broadcasting over axis 0 of a 3D array<br/></i></p>
<p>Broadcasting Over Other Axes<br/>Broadcasting with higher dimensional arrays can seem even more mind-bending, but<br/>it is really a matter of following the rules. If you don&#8217;t, you&#8217;ll get an error like this:<br/></p>
<p>462 | Appendix A: Advanced NumPy</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In [93]: arr - arr.mean(1)<br/>---------------------------------------------------------------------------<br/><b>ValueError</b>                                Traceback (most recent call last)<br/>&lt;ipython-input-93-7b87b85a20b2&gt; <b>in</b> &lt;module&gt;()<br/>----&gt; 1 arr - arr.mean(1)<br/><b>ValueError</b>: operands could <b>not</b> be broadcast together <b>with</b> shapes (4,3) (4,) <br/></p>
<p>It&#8217;s quite common to want to perform an arithmetic operation with a lower dimen&#8208;<br/>sional array across axes other than axis 0. According to the broadcasting rule, the<br/>&#8220;broadcast dimensions&#8221; must be 1 in the smaller array. In the example of row<br/>demeaning shown here, this meant reshaping the row means to be shape (4, 1)<br/>instead of (4,):<br/></p>
<p>In [94]: arr - arr.mean(1).reshape((4, 1))<br/>Out[94]: <br/>array([[-0.2095,  1.1334, -0.9239],<br/>       [ 0.8562, -0.6828, -0.1734],<br/>       [-0.3386,  1.0823, -0.7438],<br/>       [ 0.3234, -0.8599,  0.5365]])<br/></p>
<p>In the three-dimensional case, broadcasting over any of the three dimensions is only<br/>a matter of reshaping the data to be shape-compatible. Figure A-7 nicely visualizes the<br/>shapes required to broadcast over each axis of a three-dimensional array.<br/>A common problem, therefore, is needing to add a new axis with length 1 specifically<br/>for broadcasting purposes. Using reshape is one option, but inserting an axis<br/>requires constructing a tuple indicating the new shape. This can often be a tedious<br/>exercise. Thus, NumPy arrays offer a special syntax for inserting new axes by index&#8208;<br/>ing. We use the special np.newaxis attribute along with &#8220;full&#8221; slices to insert the new<br/>axis:<br/></p>
<p>In [95]: arr = np.zeros((4, 4))<br/></p>
<p>In [96]: arr_3d = arr[:, np.newaxis, :]<br/></p>
<p>In [97]: arr_3d.shape<br/>Out[97]: (4, 1, 4)<br/></p>
<p>In [98]: arr_1d = np.random.normal(size=3)<br/></p>
<p>In [99]: arr_1d[:, np.newaxis]<br/>Out[99]: <br/>array([[-2.3594],<br/>       [-0.1995],<br/>       [-1.542 ]])<br/></p>
<p>In [100]: arr_1d[np.newaxis, :]<br/>Out[100]: array([[-2.3594, -0.1995, -1.542 ]])<br/></p>
<p>Advanced NumPy | 463</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><i>Figure A-7. Compatible 2D array shapes for broadcasting over a 3D array<br/></i>Thus, if we had a three-dimensional array and wanted to demean axis 2, say, we<br/>would need to write:<br/></p>
<p>In [101]: arr = np.random.randn(3, 4, 5)<br/></p>
<p>In [102]: depth_means = arr.mean(2)<br/></p>
<p>In [103]: depth_means<br/>Out[103]: <br/>array([[-0.4735,  0.3971, -0.0228,  0.2001],<br/>       [-0.3521, -0.281 , -0.071 , -0.1586],<br/>       [ 0.6245,  0.6047,  0.4396, -0.2846]])<br/></p>
<p>In [104]: depth_means.shape<br/>Out[104]: (3, 4)<br/></p>
<p>In [105]: demeaned = arr - depth_means[:, :, np.newaxis]<br/></p>
<p>In [106]: demeaned.mean(2)<br/>Out[106]: <br/>array([[ 0.,  0., -0., -0.],<br/>       [ 0.,  0., -0.,  0.],<br/>       [ 0.,  0., -0., -0.]])<br/></p>
<p>You might be wondering if there&#8217;s a way to generalize demeaning over an axis without<br/>sacrificing performance. There is, but it requires some indexing gymnastics:<br/></p>
<p>464 | Appendix A: Advanced NumPy</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><b>def</b> demean_axis(arr, axis=0):<br/>    means = arr.mean(axis)<br/></p>
<p>    <i># This generalizes things like [:, :, np.newaxis] to N dimensions<br/></i>    indexer = [slice(None)] * arr.ndim<br/>    indexer[axis] = np.newaxis<br/>    <b>return</b> arr - means[indexer]<br/></p>
<p>Setting Array Values by Broadcasting<br/>The same broadcasting rule governing arithmetic operations also applies to setting<br/>values via array indexing. In a simple case, we can do things like:<br/></p>
<p>In [107]: arr = np.zeros((4, 3))<br/></p>
<p>In [108]: arr[:] = 5<br/></p>
<p>In [109]: arr<br/>Out[109]: <br/>array([[ 5.,  5.,  5.],<br/>       [ 5.,  5.,  5.],<br/>       [ 5.,  5.,  5.],<br/>       [ 5.,  5.,  5.]])<br/></p>
<p>However, if we had a one-dimensional array of values we wanted to set into the col&#8208;<br/>umns of the array, we can do that as long as the shape is compatible:<br/></p>
<p>In [110]: col = np.array([1.28, -0.42, 0.44, 1.6])<br/></p>
<p>In [111]: arr[:] = col[:, np.newaxis]<br/></p>
<p>In [112]: arr<br/>Out[112]: <br/>array([[ 1.28,  1.28,  1.28],<br/>       [-0.42, -0.42, -0.42],<br/>       [ 0.44,  0.44,  0.44],<br/>       [ 1.6 ,  1.6 ,  1.6 ]])<br/></p>
<p>In [113]: arr[:2] = [[-1.37], [0.509]]<br/></p>
<p>In [114]: arr<br/>Out[114]: <br/>array([[-1.37 , -1.37 , -1.37 ],<br/>       [ 0.509,  0.509,  0.509],<br/>       [ 0.44 ,  0.44 ,  0.44 ],<br/>       [ 1.6  ,  1.6  ,  1.6  ]])<br/></p>
<p>Advanced NumPy | 465</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>A.4 Advanced ufunc Usage<br/>While many NumPy users will only make use of the fast element-wise operations pro&#8208;<br/>vided by the universal functions, there are a number of additional features that occa&#8208;<br/>sionally can help you write more concise code without loops.<br/></p>
<p>ufunc Instance Methods<br/>Each of NumPy&#8217;s binary ufuncs has special methods for performing certain kinds of<br/>special vectorized operations. These are summarized in Table A-2, but I&#8217;ll give a few<br/>concrete examples to illustrate how they work.<br/>reduce takes a single array and aggregates its values, optionally along an axis, by per&#8208;<br/>forming a sequence of binary operations. For example, an alternative way to sum ele&#8208;<br/>ments in an array is to use np.add.reduce:<br/></p>
<p>In [115]: arr = np.arange(10)<br/></p>
<p>In [116]: np.add.reduce(arr)<br/>Out[116]: 45<br/></p>
<p>In [117]: arr.sum()<br/>Out[117]: 45<br/></p>
<p>The starting value (0 for add) depends on the ufunc. If an axis is passed, the reduction<br/>is performed along that axis. This allows you to answer certain kinds of questions in a<br/>concise way. As a less trivial example, we can use np.logical_and to check whether<br/>the values in each row of an array are sorted:<br/></p>
<p>In [118]: np.random.seed(12346)  <i># for reproducibility<br/></i></p>
<p>In [119]: arr = np.random.randn(5, 5)<br/></p>
<p>In [120]: arr[::2].sort(1) <i># sort a few rows<br/></i></p>
<p>In [121]: arr[:, :-1] &lt; arr[:, 1:]<br/>Out[121]: <br/>array([[ True,  True,  True,  True],<br/>       [False,  True, False, False],<br/>       [ True,  True,  True,  True],<br/>       [ True, False,  True,  True],<br/>       [ True,  True,  True,  True]], dtype=bool)<br/></p>
<p>In [122]: np.logical_and.reduce(arr[:, :-1] &lt; arr[:, 1:], axis=1)<br/>Out[122]: array([ True, False,  True, False,  True], dtype=bool)<br/></p>
<p>Note that logical_and.reduce is equivalent to the all method.<br/>accumulate is related to reduce like cumsum is related to sum. It produces an array of<br/>the same size with the intermediate &#8220;accumulated&#8221; values:<br/></p>
<p>466 | Appendix A: Advanced NumPy</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In [123]: arr = np.arange(15).reshape((3, 5))<br/></p>
<p>In [124]: np.add.accumulate(arr, axis=1)<br/>Out[124]: <br/>array([[ 0,  1,  3,  6, 10],<br/>       [ 5, 11, 18, 26, 35],<br/>       [10, 21, 33, 46, 60]])<br/></p>
<p>outer performs a pairwise cross-product between two arrays:<br/>In [125]: arr = np.arange(3).repeat([1, 2, 2])<br/></p>
<p>In [126]: arr<br/>Out[126]: array([0, 1, 1, 2, 2])<br/></p>
<p>In [127]: np.multiply.outer(arr, np.arange(5))<br/>Out[127]: <br/>array([[0, 0, 0, 0, 0],<br/>       [0, 1, 2, 3, 4],<br/>       [0, 1, 2, 3, 4],<br/>       [0, 2, 4, 6, 8],<br/>       [0, 2, 4, 6, 8]])<br/></p>
<p>The output of outer will have a dimension that is the sum of the dimensions of the<br/>inputs:<br/></p>
<p>In [128]: x, y = np.random.randn(3, 4), np.random.randn(5)<br/></p>
<p>In [129]: result = np.subtract.outer(x, y)<br/></p>
<p>In [130]: result.shape<br/>Out[130]: (3, 4, 5)<br/></p>
<p>The last method, reduceat, performs a &#8220;local reduce,&#8221; in essence an array groupby<br/>operation in which slices of the array are aggregated together. It accepts a sequence of<br/>&#8220;bin edges&#8221; that indicate how to split and aggregate the values:<br/></p>
<p>In [131]: arr = np.arange(10)<br/></p>
<p>In [132]: np.add.reduceat(arr, [0, 5, 8])<br/>Out[132]: array([10, 18, 17])<br/></p>
<p>The results are the reductions (here, sums) performed over arr[0:5], arr[5:8], and<br/>arr[8:]. As with the other methods, you can pass an axis argument:<br/></p>
<p>In [133]: arr = np.multiply.outer(np.arange(4), np.arange(5))<br/></p>
<p>In [134]: arr<br/>Out[134]: <br/>array([[ 0,  0,  0,  0,  0],<br/>       [ 0,  1,  2,  3,  4],<br/>       [ 0,  2,  4,  6,  8],<br/>       [ 0,  3,  6,  9, 12]])<br/></p>
<p>Advanced NumPy | 467</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In [135]: np.add.reduceat(arr, [0, 2, 4], axis=1)<br/>Out[135]: <br/>array([[ 0,  0,  0],<br/>       [ 1,  5,  4],<br/>       [ 2, 10,  8],<br/>       [ 3, 15, 12]])<br/></p>
<p>See Table A-2 for a partial listing of ufunc methods.<br/><i>Table A-2. ufunc methods<br/></i></p>
<p>Method Description<br/>reduce(x) Aggregate values by successive applications of the operation<br/>accumulate(x) Aggregate values, preserving all partial aggregates<br/>reduceat(x, bins) &#8220;Local&#8221; reduce or &#8220;group by&#8221;; reduce contiguous slices of data to produce aggregated array<br/>outer(x, y) Apply operation to all pairs of elements in x and y; the resulting array has shape x.shape + <br/></p>
<p>y.shape<br/></p>
<p>Writing New ufuncs in Python<br/>There are a number of facilities for creating your own NumPy ufuncs. The most gen&#8208;<br/>eral is to use the NumPy C API, but that is beyond the scope of this book. In this<br/>section, we will look at pure Python ufuncs.<br/>numpy.frompyfunc accepts a Python function along with a specification for the num&#8208;<br/>ber of inputs and outputs. For example, a simple function that adds element-wise<br/>would be specified as:<br/></p>
<p>In [136]: <b>def</b> add_elements(x, y):<br/>   .....:     <b>return</b> x + y<br/></p>
<p>In [137]: add_them = np.frompyfunc(add_elements, 2, 1)<br/></p>
<p>In [138]: add_them(np.arange(8), np.arange(8))<br/>Out[138]: array([0, 2, 4, 6, 8, 10, 12, 14], dtype=object)<br/></p>
<p>Functions created using frompyfunc always return arrays of Python objects, which<br/>can be inconvenient. Fortunately, there is an alternative (but slightly less featureful)<br/>function, numpy.vectorize, that allows you to specify the output type:<br/></p>
<p>In [139]: add_them = np.vectorize(add_elements, otypes=[np.float64])<br/></p>
<p>In [140]: add_them(np.arange(8), np.arange(8))<br/>Out[140]: array([  0.,   2.,   4.,   6.,   8.,  10.,  12.,  14.])<br/></p>
<p>These functions provide a way to create ufunc-like functions, but they are very slow<br/>because they require a Python function call to compute each element, which is a lot<br/>slower than NumPy&#8217;s C-based ufunc loops:<br/></p>
<p>468 | Appendix A: Advanced NumPy</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In [141]: arr = np.random.randn(10000)<br/></p>
<p>In [142]: %timeit add_them(arr, arr)<br/>4.12 ms +- 182 us per loop (mean +- std. dev. of 7 runs, 100 loops each)<br/></p>
<p>In [143]: %timeit np.add(arr, arr)<br/>6.89 us +- 504 ns per loop (mean +- std. dev. of 7 runs, 100000 loops each)<br/></p>
<p>Later in this chapter we&#8217;ll show how to create fast ufuncs in Python using the Numba<br/>project.<br/></p>
<p>A.5 Structured and Record Arrays<br/>You may have noticed up until now that ndarray is a <i>homogeneous</i> data container;<br/>that is, it represents a block of memory in which each element takes up the same<br/>number of bytes, determined by the dtype. On the surface, this would appear to not<br/>allow you to represent heterogeneous or tabular-like data. A <i>structured</i> array is an<br/>ndarray in which each element can be thought of as representing a <i>struct</i> in C (hence<br/>the &#8220;structured&#8221; name) or a row in a SQL table with multiple named fields:<br/></p>
<p>In [144]: dtype = [('x', np.float64), ('y', np.int32)]<br/></p>
<p>In [145]: sarr = np.array([(1.5, 6), (np.pi, -2)], dtype=dtype)<br/></p>
<p>In [146]: sarr<br/>Out[146]: <br/>array([( 1.5   ,  6), ( 3.1416, -2)],<br/>      dtype=[('x', '&lt;f8'), ('y', '&lt;i4')])<br/></p>
<p>There are several ways to specify a structured dtype (see the online NumPy documen&#8208;<br/>tation). One typical way is as a list of tuples with (field_name, field_data_type).<br/>Now, the elements of the array are tuple-like objects whose elements can be accessed<br/>like a dictionary:<br/></p>
<p>In [147]: sarr[0]<br/>Out[147]: ( 1.5, 6)<br/></p>
<p>In [148]: sarr[0]['y']<br/>Out[148]: 6<br/></p>
<p>The field names are stored in the dtype.names attribute. When you access a field on<br/>the structured array, a strided view on the data is returned, thus copying nothing:<br/></p>
<p>In [149]: sarr['x']<br/>Out[149]: array([ 1.5   ,  3.1416])<br/></p>
<p>Nested dtypes and Multidimensional Fields<br/>When specifying a structured dtype, you can additionally pass a shape (as an int or<br/>tuple):<br/></p>
<p>Advanced NumPy | 469</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In [150]: dtype = [('x', np.int64, 3), ('y', np.int32)]<br/></p>
<p>In [151]: arr = np.zeros(4, dtype=dtype)<br/></p>
<p>In [152]: arr<br/>Out[152]: <br/>array([([0, 0, 0], 0), ([0, 0, 0], 0), ([0, 0, 0], 0), ([0, 0, 0], 0)],<br/>      dtype=[('x', '&lt;i8', (3,)), ('y', '&lt;i4')])<br/></p>
<p>In this case, the x field now refers to an array of length 3 for each record:<br/>In [153]: arr[0]['x']<br/>Out[153]: array([0, 0, 0])<br/></p>
<p>Conveniently, accessing arr['x'] then returns a two-dimensional array instead of a<br/>one-dimensional array as in prior examples:<br/></p>
<p>In [154]: arr['x']<br/>Out[154]: <br/>array([[0, 0, 0],<br/>       [0, 0, 0],<br/>       [0, 0, 0],<br/>       [0, 0, 0]])<br/></p>
<p>This enables you to express more complicated, nested structures as a single block of<br/>memory in an array. You can also nest dtypes to make more complex structures. Here<br/>is an example:<br/></p>
<p>In [155]: dtype = [('x', [('a', 'f8'), ('b', 'f4')]), ('y', np.int32)]<br/></p>
<p>In [156]: data = np.array([((1, 2), 5), ((3, 4), 6)], dtype=dtype)<br/></p>
<p>In [157]: data['x']<br/>Out[157]: <br/>array([( 1.,  2.), ( 3.,  4.)],<br/>      dtype=[('a', '&lt;f8'), ('b', '&lt;f4')])<br/></p>
<p>In [158]: data['y']<br/>Out[158]: array([5, 6], dtype=int32)<br/></p>
<p>In [159]: data['x']['a']<br/>Out[159]: array([ 1.,  3.])<br/></p>
<p>pandas DataFrame does not support this feature directly, though it is similar to hier&#8208;<br/>archical indexing.<br/></p>
<p>Why Use Structured Arrays?<br/>Compared with, say, a pandas DataFrame, NumPy structured arrays are a compara&#8208;<br/>tively low-level tool. They provide a means to interpreting a block of memory as a<br/>tabular structure with arbitrarily complex nested columns. Since each element in the<br/>array is represented in memory as a fixed number of bytes, structured arrays provide<br/></p>
<p>470 | Appendix A: Advanced NumPy</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>a very fast and efficient way of writing data to and from disk (including memory<br/>maps), transporting it over the network, and other such uses.<br/>As another common use for structured arrays, writing data files as fixed-length<br/>record byte streams is a common way to serialize data in C and C++ code, which is<br/>commonly found in legacy systems in industry. As long as the format of the file is<br/>known (the size of each record and the order, byte size, and data type of each ele&#8208;<br/>ment), the data can be read into memory with np.fromfile. Specialized uses like this<br/>are beyond the scope of this book, but it&#8217;s worth knowing that such things are <br/>possible.<br/></p>
<p>A.6 More About Sorting<br/>Like Python&#8217;s built-in list, the ndarray sort instance method is an <i>in-place</i> sort,<br/>meaning that the array contents are rearranged without producing a new array:<br/></p>
<p>In [160]: arr = np.random.randn(6)<br/></p>
<p>In [161]: arr.sort()<br/></p>
<p>In [162]: arr<br/>Out[162]: array([-1.082 ,  0.3759,  0.8014,  1.1397,  1.2888,  1.8413])<br/></p>
<p>When sorting arrays in-place, remember that if the array is a view on a different<br/>ndarray, the original array will be modified:<br/></p>
<p>In [163]: arr = np.random.randn(3, 5)<br/></p>
<p>In [164]: arr<br/>Out[164]: <br/>array([[-0.3318, -1.4711,  0.8705, -0.0847, -1.1329],<br/>       [-1.0111, -0.3436,  2.1714,  0.1234, -0.0189],<br/>       [ 0.1773,  0.7424,  0.8548,  1.038 , -0.329 ]])<br/></p>
<p>In [165]: arr[:, 0].sort()  <i># Sort first column values in-place<br/></i></p>
<p>In [166]: arr<br/>Out[166]: <br/>array([[-1.0111, -1.4711,  0.8705, -0.0847, -1.1329],<br/>       [-0.3318, -0.3436,  2.1714,  0.1234, -0.0189],<br/>       [ 0.1773,  0.7424,  0.8548,  1.038 , -0.329 ]])<br/></p>
<p>On the other hand, numpy.sort creates a new, sorted copy of an array. Otherwise, it<br/>accepts the same arguments (such as kind) as ndarray.sort:<br/></p>
<p>In [167]: arr = np.random.randn(5)<br/></p>
<p>In [168]: arr<br/>Out[168]: array([-1.1181, -0.2415, -2.0051,  0.7379, -1.0614])<br/></p>
<p>Advanced NumPy | 471</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In [169]: np.sort(arr)<br/>Out[169]: array([-2.0051, -1.1181, -1.0614, -0.2415,  0.7379])<br/></p>
<p>In [170]: arr<br/>Out[170]: array([-1.1181, -0.2415, -2.0051,  0.7379, -1.0614])<br/></p>
<p>All of these sort methods take an axis argument for sorting the sections of data along<br/>the passed axis independently:<br/></p>
<p>In [171]: arr = np.random.randn(3, 5)<br/></p>
<p>In [172]: arr<br/>Out[172]: <br/>array([[ 0.5955, -0.2682,  1.3389, -0.1872,  0.9111],<br/>       [-0.3215,  1.0054, -0.5168,  1.1925, -0.1989],<br/>       [ 0.3969, -1.7638,  0.6071, -0.2222, -0.2171]])<br/></p>
<p>In [173]: arr.sort(axis=1)<br/></p>
<p>In [174]: arr<br/>Out[174]: <br/>array([[-0.2682, -0.1872,  0.5955,  0.9111,  1.3389],<br/>       [-0.5168, -0.3215, -0.1989,  1.0054,  1.1925],<br/>       [-1.7638, -0.2222, -0.2171,  0.3969,  0.6071]])<br/></p>
<p>You may notice that none of the sort methods have an option to sort in descending<br/>order. This is a problem in practice because array slicing produces views, thus not<br/>producing a copy or requiring any computational work. Many Python users are<br/>familiar with the &#8220;trick&#8221; that for a list values, values[::-1] returns a list in reverse<br/>order. The same is true for ndarrays:<br/></p>
<p>In [175]: arr[:, ::-1]<br/>Out[175]: <br/>array([[ 1.3389,  0.9111,  0.5955, -0.1872, -0.2682],<br/>       [ 1.1925,  1.0054, -0.1989, -0.3215, -0.5168],<br/>       [ 0.6071,  0.3969, -0.2171, -0.2222, -1.7638]])<br/></p>
<p>Indirect Sorts: argsort and lexsort<br/>In data analysis you may need to reorder datasets by one or more keys. For example, a<br/>table of data about some students might need to be sorted by last name, then by first<br/>name. This is an example of an <i>indirect</i> sort, and if you&#8217;ve read the pandas-related<br/>chapters you have already seen many higher-level examples. Given a key or keys (an<br/>array of values or multiple arrays of values), you wish to obtain an array of integer<br/><i>indices</i> (I refer to them colloquially as <i>indexers</i>) that tells you how to reorder the data<br/>to be in sorted order. Two methods for this are argsort and numpy.lexsort. As an<br/>example:<br/></p>
<p>In [176]: values = np.array([5, 0, 1, 3, 2])<br/></p>
<p>In [177]: indexer = values.argsort()<br/></p>
<p>472 | Appendix A: Advanced NumPy</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In [178]: indexer<br/>Out[178]: array([1, 2, 4, 3, 0])<br/></p>
<p>In [179]: values[indexer]<br/>Out[179]: array([0, 1, 2, 3, 5])<br/></p>
<p>As a more complicated example, this code reorders a two-dimensional array by its<br/>first row:<br/></p>
<p>In [180]: arr = np.random.randn(3, 5)<br/></p>
<p>In [181]: arr[0] = values<br/></p>
<p>In [182]: arr<br/>Out[182]: <br/>array([[ 5.    ,  0.    ,  1.    ,  3.    ,  2.    ],<br/>       [-0.3636, -0.1378,  2.1777, -0.4728,  0.8356],<br/>       [-0.2089,  0.2316,  0.728 , -1.3918,  1.9956]])<br/></p>
<p>In [183]: arr[:, arr[0].argsort()]<br/>Out[183]: <br/>array([[ 0.    ,  1.    ,  2.    ,  3.    ,  5.    ],<br/>       [-0.1378,  2.1777,  0.8356, -0.4728, -0.3636],<br/>       [ 0.2316,  0.728 ,  1.9956, -1.3918, -0.2089]])<br/></p>
<p>lexsort is similar to argsort, but it performs an indirect <i>lexicographical</i> sort on multi&#8208;<br/>ple key arrays. Suppose we wanted to sort some data identified by first and last<br/>names:<br/></p>
<p>In [184]: first_name = np.array(['Bob', 'Jane', 'Steve', 'Bill', 'Barbara'])<br/></p>
<p>In [185]: last_name = np.array(['Jones', 'Arnold', 'Arnold', 'Jones', 'Walters'])<br/></p>
<p>In [186]: sorter = np.lexsort((first_name, last_name))<br/></p>
<p>In [187]: sorter<br/>Out[187]: array([1, 2, 3, 0, 4])<br/></p>
<p>In [188]: zip(last_name[sorter], first_name[sorter])<br/>Out[188]: &lt;zip at 0x7fa203eda1c8&gt;<br/></p>
<p>lexsort can be a bit confusing the first time you use it because the order in which the<br/>keys are used to order the data starts with the <i>last</i> array passed. Here, last_name was<br/>used before first_name.<br/></p>
<p>pandas methods like Series&#8217;s and DataFrame&#8217;s sort_values method<br/>are implemented with variants of these functions (which also must<br/>take into account missing values).<br/></p>
<p>Advanced NumPy | 473</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Alternative Sort Algorithms<br/>A <i>stable</i> sorting algorithm preserves the relative position of equal elements. This can<br/>be especially important in indirect sorts where the relative ordering is meaningful:<br/></p>
<p>In [189]: values = np.array(['2:first', '2:second', '1:first', '1:second',<br/>   .....:                    '1:third'])<br/></p>
<p>In [190]: key = np.array([2, 2, 1, 1, 1])<br/></p>
<p>In [191]: indexer = key.argsort(kind='mergesort')<br/></p>
<p>In [192]: indexer<br/>Out[192]: array([2, 3, 4, 0, 1])<br/></p>
<p>In [193]: values.take(indexer)<br/>Out[193]: <br/>array(['1:first', '1:second', '1:third', '2:first', '2:second'],<br/>      dtype='&lt;U8')<br/></p>
<p>The only stable sort available is <i>mergesort</i>, which has guaranteed O(n log n) perfor&#8208;<br/>mance (for complexity buffs), but its performance is on average worse than the<br/>default quicksort method. See Table A-3 for a summary of available methods and<br/>their relative performance (and performance guarantees). This is not something that<br/>most users will ever have to think about, but it&#8217;s useful to know that it&#8217;s there.<br/><i>Table A-3. Array sorting methods<br/></i></p>
<p>Kind Speed Stable Work space Worst case<br/>'quicksort' 1 No 0 O(n^2)<br/>'mergesort' 2 Yes n / 2 O(n log n)<br/>'heapsort' 3 No 0 O(n log n)<br/></p>
<p>Partially Sorting Arrays<br/>One of the goals of sorting can be to determine the largest or smallest elements in an<br/>array. NumPy has optimized methods, numpy.partition and np.argpartition, for <br/>partitioning an array around the k-th smallest element:<br/></p>
<p>In [194]: np.random.seed(12345)<br/></p>
<p>In [195]: arr = np.random.randn(20)<br/></p>
<p>In [196]: arr<br/>Out[196]: <br/>array([-0.2047,  0.4789, -0.5194, -0.5557,  1.9658,  1.3934,  0.0929,<br/>        0.2817,  0.769 ,  1.2464,  1.0072, -1.2962,  0.275 ,  0.2289,<br/>        1.3529,  0.8864, -2.0016, -0.3718,  1.669 , -0.4386])<br/></p>
<p>In [197]: np.partition(arr, 3)<br/></p>
<p>474 | Appendix A: Advanced NumPy</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Out[197]: <br/>array([-2.0016, -1.2962, -0.5557, -0.5194, -0.3718, -0.4386, -0.2047,<br/>        0.2817,  0.769 ,  0.4789,  1.0072,  0.0929,  0.275 ,  0.2289,<br/>        1.3529,  0.8864,  1.3934,  1.9658,  1.669 ,  1.2464])<br/></p>
<p>After you call partition(arr, 3), the first three elements in the result are the small&#8208;<br/>est three values in no particular order. numpy.argpartition, similar to numpy.arg<br/>sort, returns the indices that rearrange the data into the equivalent order:<br/></p>
<p>In [198]: indices = np.argpartition(arr, 3)<br/></p>
<p>In [199]: indices<br/>Out[199]: <br/>array([16, 11,  3,  2, 17, 19,  0,  7,  8,  1, 10,  6, 12, 13, 14, 15,  5,<br/>        4, 18,  9])<br/></p>
<p>In [200]: arr.take(indices)<br/>Out[200]: <br/>array([-2.0016, -1.2962, -0.5557, -0.5194, -0.3718, -0.4386, -0.2047,<br/>        0.2817,  0.769 ,  0.4789,  1.0072,  0.0929,  0.275 ,  0.2289,<br/>        1.3529,  0.8864,  1.3934,  1.9658,  1.669 ,  1.2464])<br/></p>
<p>numpy.searchsorted: Finding Elements in a Sorted Array<br/>searchsorted is an array method that performs a binary search on a sorted array,<br/>returning the location in the array where the value would need to be inserted to<br/>maintain sortedness:<br/></p>
<p>In [201]: arr = np.array([0, 1, 7, 12, 15])<br/></p>
<p>In [202]: arr.searchsorted(9)<br/>Out[202]: 3<br/></p>
<p>You can also pass an array of values to get an array of indices back:<br/>In [203]: arr.searchsorted([0, 8, 11, 16])<br/>Out[203]: array([0, 3, 3, 5])<br/></p>
<p>You might have noticed that searchsorted returned 0 for the 0 element. This is<br/>because the default behavior is to return the index at the left side of a group of equal<br/>values:<br/></p>
<p>In [204]: arr = np.array([0, 0, 0, 1, 1, 1, 1])<br/></p>
<p>In [205]: arr.searchsorted([0, 1])<br/>Out[205]: array([0, 3])<br/></p>
<p>In [206]: arr.searchsorted([0, 1], side='right')<br/>Out[206]: array([3, 7])<br/></p>
<p>Advanced NumPy | 475</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>As another application of searchsorted, suppose we had an array of values between<br/>0 and 10,000, and a separate array of &#8220;bucket edges&#8221; that we wanted to use to bin the<br/>data:<br/></p>
<p>In [207]: data = np.floor(np.random.uniform(0, 10000, size=50))<br/></p>
<p>In [208]: bins = np.array([0, 100, 1000, 5000, 10000])<br/></p>
<p>In [209]: data<br/>Out[209]: <br/>array([ 9940.,  6768.,  7908.,  1709.,   268.,  8003.,  9037.,   246.,<br/>        4917.,  5262.,  5963.,   519.,  8950.,  7282.,  8183.,  5002.,<br/>        8101.,   959.,  2189.,  2587.,  4681.,  4593.,  7095.,  1780.,<br/>        5314.,  1677.,  7688.,  9281.,  6094.,  1501.,  4896.,  3773.,<br/>        8486.,  9110.,  3838.,  3154.,  5683.,  1878.,  1258.,  6875.,<br/>        7996.,  5735.,  9732.,  6340.,  8884.,  4954.,  3516.,  7142.,<br/>        5039.,  2256.])<br/></p>
<p>To then get a labeling of which interval each data point belongs to (where 1 would<br/>mean the bucket [0, 100)), we can simply use searchsorted:<br/></p>
<p>In [210]: labels = bins.searchsorted(data)<br/></p>
<p>In [211]: labels<br/>Out[211]: <br/>array([4, 4, 4, 3, 2, 4, 4, 2, 3, 4, 4, 2, 4, 4, 4, 4, 4, 2, 3, 3, 3, 3, 4,<br/>       3, 4, 3, 4, 4, 4, 3, 3, 3, 4, 4, 3, 3, 4, 3, 3, 4, 4, 4, 4, 4, 4, 3,<br/>       3, 4, 4, 3])<br/></p>
<p>This, combined with pandas&#8217;s groupby, can be used to bin data:<br/>In [212]: pd.Series(data).groupby(labels).mean()<br/>Out[212]: <br/>2     498.000000<br/>3    3064.277778<br/>4    7389.035714<br/>dtype: float64<br/></p>
<p>A.7 Writing Fast NumPy Functions with Numba<br/>Numba is an open source project that creates fast functions for NumPy-like data<br/>using CPUs, GPUs, or other hardware. It uses the LLVM Project to translate Python<br/>code into compiled machine code.<br/>To introduce Numba, let&#8217;s consider a pure Python function that computes the expres&#8208;<br/>sion (x - y).mean() using a for loop:<br/></p>
<p><b>import</b> <b>numpy</b> <b>as</b> <b>np<br/></b></p>
<p><b>def</b> mean_distance(x, y):<br/>    nx = len(x)<br/>    result = 0.0<br/></p>
<p>476 | Appendix A: Advanced NumPy</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>    count = 0<br/>    <b>for</b> i <b>in</b> range(nx):<br/>        result += x[i] - y[i]<br/>        count += 1<br/>    <b>return</b> result / count<br/></p>
<p>This function is very slow:<br/>In [209]: x = np.random.randn(10000000)<br/></p>
<p>In [210]: y = np.random.randn(10000000)<br/></p>
<p>In [211]: %timeit mean_distance(x, y)<br/>1 loop, best of 3: 2 s per loop<br/></p>
<p>In [212]: %timeit (x - y).mean()<br/>100 loops, best of 3: 14.7 ms per loop<br/></p>
<p>The NumPy version is over 100 times faster. We can turn this function into a com&#8208;<br/>piled Numba function using the numba.jit function:<br/></p>
<p>In [213]: <b>import</b> <b>numba</b> <b>as</b> <b>nb<br/></b></p>
<p>In [214]: numba_mean_distance = nb.jit(mean_distance)<br/></p>
<p>We could also have written this as a decorator:<br/>@nb.jit<br/><b>def</b> mean_distance(x, y):<br/>    nx = len(x)<br/>    result = 0.0<br/>    count = 0<br/>    <b>for</b> i <b>in</b> range(nx):<br/>        result += x[i] - y[i]<br/>        count += 1<br/>    <b>return</b> result / count<br/></p>
<p>The resulting function is actually faster than the vectorized NumPy version:<br/>In [215]: %timeit numba_mean_distance(x, y)<br/>100 loops, best of 3: 10.3 ms per loop<br/></p>
<p>Numba cannot compile arbitrary Python code, but it supports a significant subset of<br/>pure Python that is most useful for writing numerical algorithms.<br/>Numba is a deep library, supporting different kinds of hardware, modes of compila&#8208;<br/>tion, and user extensions. It is also able to compile a substantial subset of the NumPy<br/>Python API without explicit for loops. Numba is able to recognize constructs that<br/>can be compiled to machine code, while substituting calls to the CPython API for<br/>functions that it does not know how to compile. Numba&#8217;s jit function has an option,<br/>nopython=True, which restricts allowed code to Python code that can be compiled to<br/>LLVM without any Python C API calls. jit(nopython=True) has a shorter alias<br/>numba.njit.<br/></p>
<p>Advanced NumPy | 477</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In the previous example, we could have written:<br/><b>from</b> <b>numba</b> <b>import</b> float64, njit<br/></p>
<p>@njit(float64(float64[:], float64[:]))<br/><b>def</b> mean_distance(x, y):<br/>    <b>return</b> (x - y).mean()<br/></p>
<p>I encourage you to learn more by reading the online documentation for Numba. The<br/>next section shows an example of creating custom NumPy ufunc objects.<br/></p>
<p>Creating Custom numpy.ufunc Objects with Numba<br/>The numba.vectorize function creates compiled NumPy ufuncs, which behave like<br/>built-in ufuncs. Let&#8217;s consider a Python implementation of numpy.add:<br/></p>
<p><b>from</b> <b>numba</b> <b>import</b> vectorize<br/></p>
<p>@vectorize<br/><b>def</b> nb_add(x, y):<br/>    <b>return</b> x + y<br/></p>
<p>Now we have:<br/>In [13]: x = np.arange(10)<br/></p>
<p>In [14]: nb_add(x, x)<br/>Out[14]: array([  0.,   2.,   4.,   6.,   8.,  10.,  12.,  14.,  16.,  18.])<br/></p>
<p>In [15]: nb_add.accumulate(x, 0)<br/>Out[15]: array([  0.,   1.,   3.,   6.,  10.,  15.,  21.,  28.,  36.,  45.])<br/></p>
<p>A.8 Advanced Array Input and Output<br/>In Chapter 4, we became acquainted with np.save and np.load for storing arrays in<br/>binary format on disk. There are a number of additional options to consider for more<br/>sophisticated use. In particular, memory maps have the additional benefit of enabling<br/>you to work with datasets that do not fit into RAM.<br/></p>
<p>Memory-Mapped Files<br/>A <i>memory-mapped</i> file is a method for interacting with binary data on disk as though<br/>it is stored in an in-memory array. NumPy implements a memmap object that is<br/>ndarray-like, enabling small segments of a large file to be read and written without<br/>reading the whole array into memory. Additionally, a memmap has the same methods<br/>as an in-memory array and thus can be substituted into many algorithms where an<br/>ndarray would be expected.<br/></p>
<p>478 | Appendix A: Advanced NumPy</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>To create a new memory map, use the function np.memmap and pass a file path, dtype,<br/>shape, and file mode:<br/></p>
<p>In [214]: mmap = np.memmap('mymmap', dtype='float64', mode='w+',<br/>   .....:                  shape=(10000, 10000))<br/></p>
<p>In [215]: mmap<br/>Out[215]: <br/>memmap([[ 0.,  0.,  0., ...,  0.,  0.,  0.],<br/>        [ 0.,  0.,  0., ...,  0.,  0.,  0.],<br/>        [ 0.,  0.,  0., ...,  0.,  0.,  0.],<br/>        ..., <br/>        [ 0.,  0.,  0., ...,  0.,  0.,  0.],<br/>        [ 0.,  0.,  0., ...,  0.,  0.,  0.],<br/>        [ 0.,  0.,  0., ...,  0.,  0.,  0.]])<br/></p>
<p>Slicing a memmap returns views on the data on disk:<br/>In [216]: section = mmap[:5]<br/></p>
<p>If you assign data to these, it will be buffered in memory (like a Python file object),<br/>but you can write it to disk by calling flush:<br/></p>
<p>In [217]: section[:] = np.random.randn(5, 10000)<br/></p>
<p>In [218]: mmap.flush()<br/></p>
<p>In [219]: mmap<br/>Out[219]: <br/>memmap([[ 0.7584, -0.6605,  0.8626, ...,  0.6046, -0.6212,  2.0542],<br/>        [-1.2113, -1.0375,  0.7093, ..., -1.4117, -0.1719, -0.8957],<br/>        [-0.1419, -0.3375,  0.4329, ...,  1.2914, -0.752 , -0.44  ],<br/>        ..., <br/>        [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ],<br/>        [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ],<br/>        [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ]])<br/></p>
<p>In [220]: <b>del</b> mmap<br/></p>
<p>Whenever a memory map falls out of scope and is garbage-collected, any changes will<br/>be flushed to disk also. When <i>opening an existing memory map</i>, you still have to spec&#8208;<br/>ify the dtype and shape, as the file is only a block of binary data with no metadata on<br/>disk:<br/></p>
<p>In [221]: mmap = np.memmap('mymmap', dtype='float64', shape=(10000, 10000))<br/></p>
<p>In [222]: mmap<br/>Out[222]: <br/>memmap([[ 0.7584, -0.6605,  0.8626, ...,  0.6046, -0.6212,  2.0542],<br/>        [-1.2113, -1.0375,  0.7093, ..., -1.4117, -0.1719, -0.8957],<br/>        [-0.1419, -0.3375,  0.4329, ...,  1.2914, -0.752 , -0.44  ],<br/>        ..., <br/>        [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ],<br/></p>
<p>Advanced NumPy | 479</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>        [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ],<br/>        [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ]])<br/></p>
<p>Memory maps also work with structured or nested dtypes as described in a previous<br/>section.<br/></p>
<p>HDF5 and Other Array Storage Options<br/>PyTables and h5py are two Python projects providing NumPy-friendly interfaces for<br/>storing array data in the efficient and compressible HDF5 format (HDF stands for<br/><i>hierarchical data format</i>). You can safely store hundreds of gigabytes or even terabytes<br/>of data in HDF5 format. To learn more about using HDF5 with Python, I recommend <br/>reading the pandas online documentation.<br/></p>
<p>A.9 Performance Tips<br/>Getting good performance out of code utilizing NumPy is often straightforward, as<br/>array operations typically replace otherwise comparatively extremely slow pure<br/>Python loops. The following list briefly summarizes some things to keep in mind:<br/></p>
<p>&#8226; Convert Python loops and conditional logic to array operations and boolean<br/>array operations<br/></p>
<p>&#8226; Use broadcasting whenever possible<br/>&#8226; Use arrays views (slicing) to avoid copying data<br/>&#8226; Utilize ufuncs and ufunc methods<br/></p>
<p>If you can&#8217;t get the performance you require after exhausting the capabilities provided<br/>by NumPy alone, consider writing code in C, Fortran, or Cython. I use Cython fre&#8208;<br/>quently in my own work as an easy way to get C-like performance with minimal<br/>development.<br/></p>
<p>The Importance of Contiguous Memory<br/>While the full extent of this topic is a bit outside the scope of this book, in some<br/>applications the memory layout of an array can significantly affect the speed of com&#8208;<br/>putations. This is based partly on performance differences having to do with the<br/>cache hierarchy of the CPU; operations accessing contiguous blocks of memory (e.g.,<br/>summing the rows of a C order array) will generally be the fastest because the mem&#8208;<br/>ory subsystem will buffer the appropriate blocks of memory into the ultrafast L1 or<br/>L2 CPU cache. Also, certain code paths inside NumPy&#8217;s C codebase have been opti&#8208;<br/>mized for the contiguous case in which generic strided memory access can be<br/>avoided.<br/></p>
<p>480 | Appendix A: Advanced NumPy</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>To say that an array&#8217;s memory layout is <i>contiguous</i> means that the elements are stored<br/>in memory in the order that they appear in the array with respect to Fortran (column<br/>major) or C (row major) ordering. By default, NumPy arrays are created as <i>C-<br/>contiguous</i> or just simply contiguous. A column major array, such as the transpose of<br/>a C-contiguous array, is thus said to be Fortran-contiguous. These properties can be<br/>explicitly checked via the flags attribute on the ndarray:<br/></p>
<p>In [225]: arr_c = np.ones((1000, 1000), order='C')<br/></p>
<p>In [226]: arr_f = np.ones((1000, 1000), order='F')<br/></p>
<p>In [227]: arr_c.flags<br/>Out[227]: <br/>  C_CONTIGUOUS : True<br/>  F_CONTIGUOUS : False<br/>  OWNDATA : True<br/>  WRITEABLE : True<br/>  ALIGNED : True<br/>  UPDATEIFCOPY : False<br/></p>
<p>In [228]: arr_f.flags<br/>Out[228]: <br/>  C_CONTIGUOUS : False<br/>  F_CONTIGUOUS : True<br/>  OWNDATA : True<br/>  WRITEABLE : True<br/>  ALIGNED : True<br/>  UPDATEIFCOPY : False<br/></p>
<p>In [229]: arr_f.flags.f_contiguous<br/>Out[229]: True<br/></p>
<p>In this example, summing the rows of these arrays should, in theory, be faster for<br/>arr_c than arr_f since the rows are contiguous in memory. Here I check for sure <br/>using %timeit in IPython:<br/></p>
<p>In [230]: %timeit arr_c.sum(1)<br/>784 us +- 10.4 us per loop (mean +- std. dev. of 7 runs, 1000 loops each)<br/></p>
<p>In [231]: %timeit arr_f.sum(1)<br/>934 us +- 29 us per loop (mean +- std. dev. of 7 runs, 1000 loops each)<br/></p>
<p>When you&#8217;re looking to squeeze more performance out of NumPy, this is often a<br/>place to invest some effort. If you have an array that does not have the desired mem&#8208;<br/>ory order, you can use copy and pass either 'C' or 'F':<br/></p>
<p>In [232]: arr_f.copy('C').flags<br/>Out[232]: <br/>  C_CONTIGUOUS : True<br/>  F_CONTIGUOUS : False<br/>  OWNDATA : True<br/></p>
<p>Advanced NumPy | 481</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>  WRITEABLE : True<br/>  ALIGNED : True<br/>  UPDATEIFCOPY : False<br/></p>
<p>When constructing a view on an array, keep in mind that the result is not guaranteed<br/>to be contiguous:<br/></p>
<p>In [233]: arr_c[:50].flags.contiguous<br/>Out[233]: True<br/></p>
<p>In [234]: arr_c[:, :50].flags<br/>Out[234]: <br/>  C_CONTIGUOUS : False<br/>  F_CONTIGUOUS : False<br/>  OWNDATA : False<br/>  WRITEABLE : True<br/>  ALIGNED : True<br/>  UPDATEIFCOPY : False<br/></p>
<p>482 | Appendix A: Advanced NumPy</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>APPENDIX B<br/>More on the IPython System<br/></p>
<p>In Chapter 2 we looked at the basics of using the IPython shell and Jupyter notebook.<br/>In this chapter, we explore some deeper functionality in the IPython system that can<br/>either be used from the console or within Jupyter.<br/></p>
<p>B.1 Using the Command History<br/>IPython maintains a small on-disk database containing the text of each command<br/>that you execute. This serves various purposes:<br/></p>
<p>&#8226; Searching, completing, and executing previously executed commands with mini&#8208;<br/>mal typing<br/></p>
<p>&#8226; Persisting the command history between sessions<br/>&#8226; Logging the input/output history to a file<br/></p>
<p>These features are more useful in the shell than in the notebook, since the notebook<br/>by design keeps a log of the input and output in each code cell.<br/></p>
<p>Searching and Reusing the Command History<br/>The IPython shell lets you search and execute previous code or other commands.<br/>This is useful, as you may often find yourself repeating the same commands, such as a<br/>%run command or some other code snippet. Suppose you had run:<br/></p>
<p>In[7]: %run first/second/third/data_script.py<br/></p>
<p>and then explored the results of the script (assuming it ran successfully) only to find<br/>that you made an incorrect calculation. After figuring out the problem and modifying<br/><i>data_script.py</i>, you can start typing a few letters of the %run command and then press<br/>either the Ctrl-P key combination or the up arrow key. This will search the command<br/></p>
<p>483</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>history for the first prior command matching the letters you typed. Pressing either<br/>Ctrl-P or the up arrow key multiple times will continue to search through the history.<br/>If you pass over the command you wish to execute, fear not. You can move <i>forward<br/></i>through the command history by pressing either Ctrl-N or the down arrow key. After<br/>doing this a few times, you may start pressing these keys without thinking!<br/>Using Ctrl-R gives you the same partial incremental searching capability provided by<br/>the readline used in Unix-style shells, such as the bash shell. On Windows, readline<br/>functionality is emulated by IPython. To use this, press Ctrl-R and then type a few<br/>characters contained in the input line you want to search for:<br/></p>
<p>In [1]: a_command = foo(x, y, z)<br/></p>
<p>(reverse-i-search)`com': a_command = foo(x, y, z)<br/></p>
<p>Pressing Ctrl-R will cycle through the history for each line matching the characters<br/>you&#8217;ve typed.<br/></p>
<p>Input and Output Variables<br/>Forgetting to assign the result of a function call to a variable can be very annoying.<br/>An IPython session stores references to <i>both</i> the input commands and output Python<br/>objects in special variables. The previous two outputs are stored in the _ (one under&#8208;<br/>score) and __ (two underscores) variables, respectively:<br/></p>
<p>In [24]: 2 ** 27<br/>Out[24]: 134217728<br/></p>
<p>In [25]: _<br/>Out[25]: 134217728<br/></p>
<p>Input variables are stored in variables named like _iX, where X is the input line num&#8208;<br/>ber. For each input variable there is a corresponding output variable _X. So after input<br/>line 27, say, there will be two new variables _27 (for the output) and _i27 for the<br/>input:<br/></p>
<p>In [26]: foo = 'bar'<br/></p>
<p>In [27]: foo<br/>Out[27]: 'bar'<br/></p>
<p>In [28]: _i27<br/>Out[28]: u'foo'<br/></p>
<p>In [29]: _27<br/>Out[29]: 'bar'<br/></p>
<p>484 | Appendix B: More on the IPython System</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Since the input variables are strings they can be executed again with the Python exec<br/>keyword:<br/></p>
<p>In [30]: <b>exec</b>(_i27)<br/></p>
<p>Here _i27 refers to the code input in In [27].<br/>Several magic functions allow you to work with the input and output history. %hist is<br/>capable of printing all or part of the input history, with or without line numbers.<br/>%reset is for clearing the interactive namespace and optionally the input and output<br/>caches. The %xdel magic function is intended for removing all references to a <i>particu&#8208;<br/>lar</i> object from the IPython machinery. See the documentation for both of these mag&#8208;<br/>ics for more details.<br/></p>
<p>When working with very large datasets, keep in mind that IPy&#8208;<br/>thon&#8217;s input and output history causes any object referenced there<br/>to not be garbage-collected (freeing up the memory), even if you<br/>delete the variables from the interactive namespace using the del<br/>keyword. In such cases, careful usage of %xdel and %reset can help<br/>you avoid running into memory problems.<br/></p>
<p>B.2 Interacting with the Operating System<br/>Another feature of IPython is that it allows you to seamlessly access the filesystem<br/>and operating system shell. This means, among other things, that you can perform<br/>most standard command-line actions as you would in the Windows or Unix (Linux,<br/>macOS) shell without having to exit IPython. This includes shell commands, chang&#8208;<br/>ing directories, and storing the results of a command in a Python object (list or<br/>string). There are also simple command aliasing and directory bookmarking features.<br/>See Table B-1 for a summary of magic functions and syntax for calling shell com&#8208;<br/>mands. I&#8217;ll briefly visit these features in the next few sections.<br/><i>Table B-1. IPython system-related commands<br/></i></p>
<p>Command Description<br/>!cmd Execute cmd in the system shell<br/>output = !cmd args Run cmd and store the stdout in output<br/>%alias <i>alias_name cmd </i>Define an alias for a system (shell) command<br/>%bookmark Utilize IPython&#8217;s directory bookmarking system<br/>%cd <i>directory </i>Change system working directory to passed directory<br/>%pwd Return the current system working directory<br/>%pushd <i>directory </i>Place current directory on stack and change to target directory<br/>%popd Change to directory popped off the top of the stack<br/>%dirs Return a list containing the current directory stack<br/></p>
<p>More on the IPython System | 485</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Command Description<br/>%dhist Print the history of visited directories<br/>%env Return the system environment variables as a dict<br/>%matplotlib Configure matplotlib integration options<br/></p>
<p>Shell Commands and Aliases<br/>Starting a line in IPython with an exclamation point !, or bang, tells IPython to exe&#8208;<br/>cute everything after the bang in the system shell. This means that you can delete files<br/>(using rm or del, depending on your OS), change directories, or execute any other<br/>process.<br/>You can store the console output of a shell command in a variable by assigning the<br/>expression escaped with ! to a variable. For example, on my Linux-based machine<br/>connected to the internet via ethernet, I can get my IP address as a Python variable:<br/></p>
<p>In [1]: ip_info = !ifconfig wlan0 | grep &quot;inet &quot;<br/></p>
<p>In [2]: ip_info[0].strip()<br/>Out[2]: 'inet addr:10.0.0.11  Bcast:10.0.0.255  Mask:255.255.255.0'<br/></p>
<p>The returned Python object ip_info is actually a custom list type containing various<br/>versions of the console output.<br/>IPython can also substitute in Python values defined in the current environment<br/>when using !. To do this, preface the variable name by the dollar sign $:<br/></p>
<p>In [3]: foo = 'test*'<br/></p>
<p>In [4]: !ls $foo<br/>test4.py  test.py  test.xml<br/></p>
<p>The %alias magic function can define custom shortcuts for shell commands. As a<br/>simple example:<br/></p>
<p>In [1]: %alias ll ls -l<br/></p>
<p>In [2]: ll /usr<br/>total 332<br/>drwxr-xr-x   2 root root  69632 2012-01-29 20:36 bin/<br/>drwxr-xr-x   2 root root   4096 2010-08-23 12:05 games/<br/>drwxr-xr-x 123 root root  20480 2011-12-26 18:08 include/<br/>drwxr-xr-x 265 root root 126976 2012-01-29 20:36 lib/<br/>drwxr-xr-x  44 root root  69632 2011-12-26 18:08 lib32/<br/>lrwxrwxrwx   1 root root      3 2010-08-23 16:02 lib64 -&gt; lib/<br/>drwxr-xr-x  15 root root   4096 2011-10-13 19:03 local/<br/>drwxr-xr-x   2 root root  12288 2012-01-12 09:32 sbin/<br/>drwxr-xr-x 387 root root  12288 2011-11-04 22:53 share/<br/>drwxrwsr-x  24 root src    4096 2011-07-17 18:38 src/<br/></p>
<p>486 | Appendix B: More on the IPython System</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>You can execute multiple commands just as on the command line by separating them<br/>with semicolons:<br/></p>
<p>In [558]: %alias test_alias (cd examples; ls; cd ..)<br/></p>
<p>In [559]: test_alias<br/>macrodata.csv  spx.csv tips.csv<br/></p>
<p>You&#8217;ll notice that IPython &#8220;forgets&#8221; any aliases you define interactively as soon as the<br/>session is closed. To create permanent aliases, you will need to use the configuration<br/>system.<br/></p>
<p>Directory Bookmark System<br/>IPython has a simple directory bookmarking system to enable you to save aliases for<br/>common directories so that you can jump around very easily. For example, suppose<br/>you wanted to create a bookmark that points to the supplementary materials for this<br/>book:<br/></p>
<p>In [6]: %bookmark py4da /home/wesm/code/pydata-book<br/></p>
<p>Once you&#8217;ve done this, when we use the %cd magic, we can use any bookmarks we&#8217;ve<br/>defined:<br/></p>
<p>In [7]: cd py4da<br/>(bookmark:py4da) -&gt; /home/wesm/code/pydata-book<br/>/home/wesm/code/pydata-book<br/></p>
<p>If a bookmark name conflicts with a directory name in your current working direc&#8208;<br/>tory, you can use the -b flag to override and use the bookmark location. Using the -l<br/>option with %bookmark lists all of your bookmarks:<br/></p>
<p>In [8]: %bookmark -l<br/>Current bookmarks:<br/>py4da -&gt; /home/wesm/code/pydata-book-source<br/></p>
<p>Bookmarks, unlike aliases, are automatically persisted between IPython sessions.<br/></p>
<p>B.3 Software Development Tools<br/>In addition to being a comfortable environment for interactive computing and data<br/>exploration, IPython can also be a useful companion for general Python software<br/>development. In data analysis applications, it&#8217;s important first to have <i>correct</i> code.<br/>Fortunately, IPython has closely integrated and enhanced the built-in Python pdb<br/>debugger. Secondly you want your code to be <i>fast</i>. For this IPython has easy-to-use<br/>code timing and profiling tools. I will give an overview of these tools in detail here.<br/></p>
<p>More on the IPython System | 487</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Interactive Debugger<br/>IPython&#8217;s debugger enhances pdb with tab completion, syntax highlighting, and con&#8208;<br/>text for each line in exception tracebacks. One of the best times to debug code is right<br/>after an error has occurred. The %debug command, when entered immediately after<br/>an exception, invokes the &#8220;post-mortem&#8221; debugger and drops you into the stack<br/>frame where the exception was raised:<br/></p>
<p>In [2]: run examples/ipython_bug.py<br/>---------------------------------------------------------------------------<br/><b>AssertionError</b>                            Traceback (most recent call last)<br/>/home/wesm/code/pydata-book/examples/ipython_bug.py <b>in</b> &lt;module&gt;()<br/>     13     throws_an_exception()<br/>     14<br/>---&gt; 15 calling_things()<br/></p>
<p>/home/wesm/code/pydata-book/examples/ipython_bug.py <b>in</b> calling_things()<br/>     11 <b>def</b> calling_things():<br/>     12     works_fine()<br/>---&gt; 13     throws_an_exception()<br/>     14<br/>     15 calling_things()<br/></p>
<p>/home/wesm/code/pydata-book/examples/ipython_bug.py <b>in</b> throws_an_exception()<br/>      7     a = 5<br/>      8     b = 6<br/>----&gt; 9     <b>assert</b>(a + b == 10)<br/>     10<br/>     11 <b>def</b> calling_things():<br/></p>
<p><b>AssertionError</b>:<br/></p>
<p>In [3]: %debug<br/>&gt; /home/wesm/code/pydata-book/examples/ipython_bug.py(9)throws_an_exception()<br/>      8     b = 6<br/>----&gt; 9     <b>assert</b>(a + b == 10)<br/>     10<br/></p>
<p>ipdb&gt;<br/></p>
<p>Once inside the debugger, you can execute arbitrary Python code and explore all of<br/>the objects and data (which have been &#8220;kept alive&#8221; by the interpreter) inside each<br/>stack frame. By default you start in the lowest level, where the error occurred. By<br/>pressing u (up) and d (down), you can switch between the levels of the stack trace:<br/></p>
<p>ipdb&gt; u<br/>&gt; /home/wesm/code/pydata-book/examples/ipython_bug.py(13)calling_things()<br/>     12     works_fine()<br/>---&gt; 13     throws_an_exception()<br/>     14<br/></p>
<p>488 | Appendix B: More on the IPython System</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Executing the %pdb command makes it so that IPython automatically invokes the<br/>debugger after any exception, a mode that many users will find especially useful.<br/>It&#8217;s also easy to use the debugger to help develop code, especially when you wish to set<br/>breakpoints or step through the execution of a function or script to examine the state<br/>at each stage. There are several ways to accomplish this. The first is by using %run<br/>with the -d flag, which invokes the debugger before executing any code in the passed<br/>script. You must immediately press s (step) to enter the script:<br/></p>
<p>In [5]: run -d examples/ipython_bug.py<br/>Breakpoint 1 at /home/wesm/code/pydata-book/examples/ipython_bug.py:1<br/>NOTE: Enter 'c' at the ipdb&gt;  prompt to start your script.<br/>&gt; &lt;string&gt;(1)&lt;module&gt;()<br/></p>
<p>ipdb&gt; s<br/>--Call--<br/>&gt; /home/wesm/code/pydata-book/examples/ipython_bug.py(1)&lt;module&gt;()<br/>1---&gt; 1 <b>def</b> works_fine():<br/>      2     a = 5<br/>      3     b = 6<br/></p>
<p>After this point, it&#8217;s up to you how you want to work your way through the file. For<br/>example, in the preceding exception, we could set a breakpoint right before calling<br/>the works_fine method and run the script until we reach the breakpoint by pressing<br/>c (continue):<br/></p>
<p>ipdb&gt; b 12<br/>ipdb&gt; c<br/>&gt; /home/wesm/code/pydata-book/examples/ipython_bug.py(12)calling_things()<br/>     11 <b>def</b> calling_things():<br/>2--&gt; 12     works_fine()<br/>     13     throws_an_exception()<br/></p>
<p>At this point, you can step into works_fine() or execute works_fine() by pressing n<br/>(next) to advance to the next line:<br/></p>
<p>ipdb&gt; n<br/>&gt; /home/wesm/code/pydata-book/examples/ipython_bug.py(13)calling_things()<br/>2    12     works_fine()<br/>---&gt; 13     throws_an_exception()<br/>     14<br/></p>
<p>Then, we could step into throws_an_exception and advance to the line where the<br/>error occurs and look at the variables in the scope. Note that debugger commands<br/>take precedence over variable names; in such cases, preface the variables with ! to<br/>examine their contents:<br/></p>
<p>ipdb&gt; s<br/>--Call--<br/>&gt; /home/wesm/code/pydata-book/examples/ipython_bug.py(6)throws_an_exception()<br/>      5<br/></p>
<p>More on the IPython System | 489</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>----&gt; 6 <b>def</b> throws_an_exception():<br/>      7     a = 5<br/></p>
<p>ipdb&gt; n<br/>&gt; /home/wesm/code/pydata-book/examples/ipython_bug.py(7)throws_an_exception()<br/>      6 <b>def</b> throws_an_exception():<br/>----&gt; 7     a = 5<br/>      8     b = 6<br/></p>
<p>ipdb&gt; n<br/>&gt; /home/wesm/code/pydata-book/examples/ipython_bug.py(8)throws_an_exception()<br/>      7     a = 5<br/>----&gt; 8     b = 6<br/>      9     <b>assert</b>(a + b == 10)<br/></p>
<p>ipdb&gt; n<br/>&gt; /home/wesm/code/pydata-book/examples/ipython_bug.py(9)throws_an_exception()<br/>      8     b = 6<br/>----&gt; 9     <b>assert</b>(a + b == 10)<br/>     10<br/></p>
<p>ipdb&gt; !a<br/>5<br/>ipdb&gt; !b<br/>6<br/></p>
<p>Developing proficiency with the interactive debugger is largely a matter of practice<br/>and experience. See Table B-2 for a full catalog of the debugger commands. If you are<br/>accustomed to using an IDE, you might find the terminal-driven debugger to be a bit<br/>unforgiving at first, but that will improve in time. Some of the Python IDEs have<br/>excellent GUI debuggers, so most users can find something that works for them.<br/><i>Table B-2. (I)Python debugger commands<br/></i></p>
<p>Command Action<br/>h(elp) Display command list<br/>help <i>command </i>Show documentation for <i>command<br/></i>c(ontinue) Resume program execution<br/>q(uit) Exit debugger without executing any more code<br/>b(reak) <i>number </i>Set breakpoint at number in current file<br/>b <i>path/to/file.py:number </i>Set breakpoint at line number in specified file<br/>s(tep) Step <i>into</i> function call<br/>n(ext) Execute current line and advance to next line at current level<br/>u(p)/d(own) Move up/down in function call stack<br/>a(rgs) Show arguments for current function<br/>debug <i>statement </i>Invoke statement <i>statement</i> in new (recursive) debugger<br/>l(ist) <i>statement </i>Show current position and context at current level of stack<br/>w(here) Print full stack trace with context at current position<br/></p>
<p>490 | Appendix B: More on the IPython System</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Other ways to make use of the debugger<br/>There are a couple of other useful ways to invoke the debugger. The first is by using a<br/>special set_trace function (named after pdb.set_trace), which is basically a &#8220;poor<br/>man&#8217;s breakpoint.&#8221; Here are two small recipes you might want to put somewhere for<br/>your general use (potentially adding them to your IPython profile as I do):<br/></p>
<p><b>from</b> <b>IPython.core.debugger</b> <b>import</b> Pdb<br/></p>
<p><b>def</b> set_trace():<br/>    Pdb(color_scheme='Linux').set_trace(sys._getframe().f_back)<br/></p>
<p><b>def</b> debug(f, *args, **kwargs):<br/>    pdb = Pdb(color_scheme='Linux')<br/>    <b>return</b> pdb.runcall(f, *args, **kwargs)<br/></p>
<p>The first function, set_trace, is very simple. You can use a set_trace in any part of<br/>your code that you want to temporarily stop in order to more closely examine it (e.g.,<br/>right before an exception occurs):<br/></p>
<p>In [7]: run examples/ipython_bug.py<br/>&gt; /home/wesm/code/pydata-book/examples/ipython_bug.py(16)calling_things()<br/>     15     set_trace()<br/>---&gt; 16     throws_an_exception()<br/>     17<br/></p>
<p>Pressing c (continue) will cause the code to resume normally with no harm done.<br/>The debug function we just looked at enables you to invoke the interactive debugger<br/>easily on an arbitrary function call. Suppose we had written a function like the fol&#8208;<br/>lowing and we wished to step through its logic:<br/></p>
<p><b>def</b> f(x, y, z=1):<br/>    tmp = x + y<br/>    <b>return</b> tmp / z<br/></p>
<p>Ordinarily using f would look like f(1, 2, z=3). To instead step into f, pass f as the<br/>first argument to debug followed by the positional and keyword arguments to be<br/>passed to f:<br/></p>
<p>In [6]: debug(f, 1, 2, z=3)<br/>&gt; &lt;ipython-input&gt;(2)f()<br/>      1 <b>def</b> f(x, y, z):<br/>----&gt; 2     tmp = x + y<br/>      3     <b>return</b> tmp / z<br/></p>
<p>ipdb&gt;<br/></p>
<p>I find that these two simple recipes save me a lot of time on a day-to-day basis.<br/></p>
<p>More on the IPython System | 491</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Lastly, the debugger can be used in conjunction with %run. By running a script with<br/>%run -d, you will be dropped directly into the debugger, ready to set any breakpoints<br/>and start the script:<br/></p>
<p>In [1]: %run -d examples/ipython_bug.py<br/>Breakpoint 1 at /home/wesm/code/pydata-book/examples/ipython_bug.py:1<br/>NOTE: Enter 'c' at the ipdb&gt;  prompt to start your script.<br/>&gt; &lt;string&gt;(1)&lt;module&gt;()<br/></p>
<p>ipdb&gt;<br/></p>
<p>Adding -b with a line number starts the debugger with a breakpoint set already:<br/>In [2]: %run -d -b2 examples/ipython_bug.py<br/>Breakpoint 1 at /home/wesm/code/pydata-book/examples/ipython_bug.py:2<br/>NOTE: Enter 'c' at the ipdb&gt;  prompt to start your script.<br/>&gt; &lt;string&gt;(1)&lt;module&gt;()<br/></p>
<p>ipdb&gt; c<br/>&gt; /home/wesm/code/pydata-book/examples/ipython_bug.py(2)works_fine()<br/>      1 <b>def</b> works_fine():<br/>1---&gt; 2     a = 5<br/>      3     b = 6<br/></p>
<p>ipdb&gt;<br/></p>
<p>Timing Code: %time and %timeit<br/>For larger-scale or longer-running data analysis applications, you may wish to meas&#8208;<br/>ure the execution time of various components or of individual statements or function<br/>calls. You may want a report of which functions are taking up the most time in a com&#8208;<br/>plex process. Fortunately, IPython enables you to get this information very easily<br/>while you are developing and testing your code.<br/>Timing code by hand using the built-in time module and its functions time.clock<br/>and time.time is often tedious and repetitive, as you must write the same uninterest&#8208;<br/>ing boilerplate code:<br/></p>
<p><b>import</b> <b>time<br/></b>start = time.time()<br/><b>for</b> i <b>in</b> range(iterations):<br/>    <i># some code to run here<br/></i>elapsed_per = (time.time() - start) / iterations<br/></p>
<p>Since this is such a common operation, IPython has two magic functions, %time and<br/>%timeit, to automate this process for you.<br/>%time runs a statement once, reporting the total execution time. Suppose we had a<br/>large list of strings and we wanted to compare different methods of selecting all<br/></p>
<p>492 | Appendix B: More on the IPython System</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>strings starting with a particular prefix. Here is a simple list of 600,000 strings and<br/>two identical methods of selecting only the ones that start with 'foo':<br/></p>
<p><i># a very large list of strings<br/></i>strings = ['foo', 'foobar', 'baz', 'qux',<br/>           'python', 'Guido Van Rossum'] * 100000<br/></p>
<p>method1 = [x <b>for</b> x <b>in</b> strings <b>if</b> x.startswith('foo')]<br/></p>
<p>method2 = [x <b>for</b> x <b>in</b> strings <b>if</b> x[:3] == 'foo']<br/></p>
<p>It looks like they should be about the same performance-wise, right? We can check<br/>for sure using %time:<br/></p>
<p>In [561]: %time method1 = [x <b>for</b> x <b>in</b> strings <b>if</b> x.startswith('foo')]<br/>CPU times: user 0.19 s, sys: 0.00 s, total: 0.19 s<br/>Wall time: 0.19 s<br/></p>
<p>In [562]: %time method2 = [x <b>for</b> x <b>in</b> strings <b>if</b> x[:3] == 'foo']<br/>CPU times: user 0.09 s, sys: 0.00 s, total: 0.09 s<br/>Wall time: 0.09 s<br/></p>
<p>The Wall time (short for &#8220;wall-clock time&#8221;) is the main number of interest. So, it<br/>looks like the first method takes more than twice as long, but it&#8217;s not a very precise<br/>measurement. If you try %time-ing those statements multiple times yourself, you&#8217;ll<br/>find that the results are somewhat variable. To get a more precise measurement, use<br/>the %timeit magic function. Given an arbitrary statement, it has a heuristic to run a<br/>statement multiple times to produce a more accurate average runtime:<br/></p>
<p>In [563]: %timeit [x <b>for</b> x <b>in</b> strings <b>if</b> x.startswith('foo')]<br/>10 loops, best of 3: 159 ms per loop<br/></p>
<p>In [564]: %timeit [x <b>for</b> x <b>in</b> strings <b>if</b> x[:3] == 'foo']<br/>10 loops, best of 3: 59.3 ms per loop<br/></p>
<p>This seemingly innocuous example illustrates that it is worth understanding the per&#8208;<br/>formance characteristics of the Python standard library, NumPy, pandas, and other<br/>libraries used in this book. In larger-scale data analysis applications, those milli&#8208;<br/>seconds will start to add up!<br/>%timeit is especially useful for analyzing statements and functions with very short<br/>execution times, even at the level of microseconds (millionths of a second) or nano&#8208;<br/>seconds (billionths of a second). These may seem like insignificant amounts of time,<br/>but of course a 20 microsecond function invoked 1 million times takes 15 seconds<br/>longer than a 5 microsecond function. In the preceding example, we could very<br/>directly compare the two string operations to understand their performance <br/>characteristics:<br/></p>
<p>In [565]: x = 'foobar'<br/></p>
<p>More on the IPython System | 493</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In [566]: y = 'foo'<br/></p>
<p>In [567]: %timeit x.startswith(y)<br/>1000000 loops, best of 3: 267 ns per loop<br/></p>
<p>In [568]: %timeit x[:3] == y<br/>10000000 loops, best of 3: 147 ns per loop<br/></p>
<p>Basic Profiling: %prun and %run -p<br/>Profiling code is closely related to timing code, except it is concerned with determin&#8208;<br/>ing <i>where</i> time is spent. The main Python profiling tool is the cProfile module,<br/>which is not specific to IPython at all. cProfile executes a program or any arbitrary<br/>block of code while keeping track of how much time is spent in each function.<br/>A common way to use cProfile is on the command line, running an entire program<br/>and outputting the aggregated time per function. Suppose we had a simple script that<br/>does some linear algebra in a loop (computing the maximum absolute eigenvalues of<br/>a series of 100 &#215; 100 matrices):<br/></p>
<p><b>import</b> <b>numpy</b> <b>as</b> <b>np<br/>from</b> <b>numpy.linalg</b> <b>import</b> eigvals<br/></p>
<p><b>def</b> run_experiment(niter=100):<br/>    K = 100<br/>    results = []<br/>    <b>for</b> _ <b>in</b> xrange(niter):<br/>        mat = np.random.randn(K, K)<br/>        max_eigenvalue = np.abs(eigvals(mat)).max()<br/>        results.append(max_eigenvalue)<br/>    <b>return</b> results<br/>some_results = run_experiment()<br/><b>print</b> 'Largest one we saw: %s' % np.max(some_results)<br/></p>
<p>You can run this script through cProfile using the following in the command line:<br/>python -m cProfile cprof_example.py<br/></p>
<p>If you try that, you&#8217;ll find that the output is sorted by function name. This makes it a<br/>bit hard to get an idea of where the most time is spent, so it&#8217;s very common to specify<br/>a <i>sort order</i> using the -s flag:<br/></p>
<p>$ python -m cProfile -s cumulative cprof_example.py<br/>Largest one we saw: 11.923204422<br/>    15116 function calls (14927 primitive calls) <b>in</b> 0.720 seconds<br/></p>
<p>Ordered by: cumulative time<br/></p>
<p>ncalls  tottime  percall  cumtime  percall filename:lineno(function)<br/>     1    0.001    0.001    0.721    0.721 cprof_example.py:1(&lt;module&gt;)<br/>   100    0.003    0.000    0.586    0.006 linalg.py:702(eigvals)<br/></p>
<p>494 | Appendix B: More on the IPython System</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>   200    0.572    0.003    0.572    0.003 {numpy.linalg.lapack_lite.dgeev}<br/>     1    0.002    0.002    0.075    0.075 <b>__init__</b>.py:106(&lt;module&gt;)<br/>   100    0.059    0.001    0.059    0.001 {method 'randn')<br/>     1    0.000    0.000    0.044    0.044 add_newdocs.py:9(&lt;module&gt;)<br/>     2    0.001    0.001    0.037    0.019 <b>__init__</b>.py:1(&lt;module&gt;)<br/>     2    0.003    0.002    0.030    0.015 <b>__init__</b>.py:2(&lt;module&gt;)<br/>     1    0.000    0.000    0.030    0.030 type_check.py:3(&lt;module&gt;)<br/>     1    0.001    0.001    0.021    0.021 <b>__init__</b>.py:15(&lt;module&gt;)<br/>     1    0.013    0.013    0.013    0.013 numeric.py:1(&lt;module&gt;)<br/>     1    0.000    0.000    0.009    0.009 <b>__init__</b>.py:6(&lt;module&gt;)<br/>     1    0.001    0.001    0.008    0.008 <b>__init__</b>.py:45(&lt;module&gt;)<br/>   262    0.005    0.000    0.007    0.000 function_base.py:3178(add_newdoc)<br/>   100    0.003    0.000    0.005    0.000 linalg.py:162(_assertFinite)<br/>   ...<br/></p>
<p>Only the first 15 rows of the output are shown. It&#8217;s easiest to read by scanning down<br/>the cumtime column to see how much total time was spent <i>inside</i> each function. Note<br/>that if a function calls some other function, <i>the clock does not stop running</i>. cProfile<br/>records the start and end time of each function call and uses that to produce the<br/>timing.<br/>In addition to the command-line usage, cProfile can also be used programmatically<br/>to profile arbitrary blocks of code without having to run a new process. IPython has a<br/>convenient interface to this capability using the %prun command and the -p option to<br/>%run. %prun takes the same &#8220;command-line options&#8221; as cProfile but will profile an<br/>arbitrary Python statement instead of a whole <i>.py</i> file:<br/></p>
<p>In [4]: %prun -l 7 -s cumulative run_experiment()<br/>         4203 function calls <b>in</b> 0.643 seconds<br/></p>
<p>Ordered by: cumulative time<br/>List reduced <b>from</b> 32 to 7 due to restriction &lt;7&gt;<br/></p>
<p>ncalls  tottime  percall  cumtime  percall filename:lineno(function)<br/>     1    0.000    0.000    0.643    0.643 &lt;string&gt;:1(&lt;module&gt;)<br/>     1    0.001    0.001    0.643    0.643 cprof_example.py:4(run_experiment)<br/>   100    0.003    0.000    0.583    0.006 linalg.py:702(eigvals)<br/>   200    0.569    0.003    0.569    0.003 {numpy.linalg.lapack_lite.dgeev}<br/>   100    0.058    0.001    0.058    0.001 {method 'randn'}<br/>   100    0.003    0.000    0.005    0.000 linalg.py:162(_assertFinite)<br/>   200    0.002    0.000    0.002    0.000 {method 'all' of 'numpy.ndarray'}<br/></p>
<p>Similarly, calling %run -p -s cumulative cprof_example.py has the same effect as<br/>the command-line approach, except you never have to leave IPython.<br/>In the Jupyter notebook, you can use the %%prun magic (two % signs) to profile an<br/>entire code block. This pops up a separate window with the profile output. This can<br/>be useful in getting possibly quick answers to questions like, &#8220;Why did that code<br/>block take so long to run?&#8221;<br/></p>
<p>More on the IPython System | 495</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>There are other tools available that help make profiles easier to understand when you<br/>are using IPython or Jupyter. One of these is SnakeViz, which produces an interactive<br/>visualization of the profile results using d3.js.<br/></p>
<p>Profiling a Function Line by Line<br/>In some cases the information you obtain from %prun (or another cProfile-based<br/>profile method) may not tell the whole story about a function&#8217;s execution time, or it<br/>may be so complex that the results, aggregated by function name, are hard to inter&#8208;<br/>pret. For this case, there is a small library called line_profiler (obtainable via PyPI<br/>or one of the package management tools). It contains an IPython extension enabling<br/>a new magic function %lprun that computes a line-by-line-profiling of one or more<br/>functions. You can enable this extension by modifying your IPython configuration<br/>(see the IPython documentation or the section on configuration later in this chapter)<br/>to include the following line:<br/></p>
<p><i># A list of dotted module names of IPython extensions to load.<br/></i>c.TerminalIPythonApp.extensions = ['line_profiler']<br/></p>
<p>You can also run the command:<br/>%load_ext line_profiler<br/></p>
<p>line_profiler can be used programmatically (see the full documentation), but it is<br/>perhaps most powerful when used interactively in IPython. Suppose you had a mod&#8208;<br/>ule prof_mod with the following code doing some NumPy array operations:<br/></p>
<p><b>from</b> <b>numpy.random</b> <b>import</b> randn<br/></p>
<p><b>def</b> add_and_sum(x, y):<br/>    added = x + y<br/>    summed = added.sum(axis=1)<br/>    <b>return</b> summed<br/></p>
<p><b>def</b> call_function():<br/>    x = randn(1000, 1000)<br/>    y = randn(1000, 1000)<br/>    <b>return</b> add_and_sum(x, y)<br/></p>
<p>If we wanted to understand the performance of the add_and_sum function, %prun<br/>gives us the following:<br/></p>
<p>In [569]: %run prof_mod<br/></p>
<p>In [570]: x = randn(3000, 3000)<br/></p>
<p>In [571]: y = randn(3000, 3000)<br/></p>
<p>In [572]: %prun add_and_sum(x, y)<br/>         4 function calls <b>in</b> 0.049 seconds<br/></p>
<p>496 | Appendix B: More on the IPython System</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>   Ordered by: internal time<br/>   ncalls  tottime  percall  cumtime  percall filename:lineno(function)<br/>        1    0.036    0.036    0.046    0.046 prof_mod.py:3(add_and_sum)<br/>        1    0.009    0.009    0.009    0.009 {method 'sum' of 'numpy.ndarray'}<br/>        1    0.003    0.003    0.049    0.049 &lt;string&gt;:1(&lt;module&gt;)<br/></p>
<p>This is not especially enlightening. With the line_profiler IPython extension acti&#8208;<br/>vated, a new command %lprun is available. The only difference in usage is that we<br/>must instruct %lprun which function or functions we wish to profile. The general<br/>syntax is:<br/></p>
<p>%lprun -f func1 -f func2 <b>statement_to_profile<br/></b></p>
<p>In this case, we want to profile add_and_sum, so we run:<br/>In [573]: %lprun -f add_and_sum add_and_sum(x, y)<br/>Timer unit: 1e-06 s<br/>File: prof_mod.py<br/>Function: add_and_sum at line 3<br/>Total time: 0.045936 s<br/>Line <i>#      Hits         Time  Per Hit   % Time  Line Contents<br/></i>==============================================================<br/>     3                                           <b>def</b> add_and_sum(x, y):<br/>     4         1        36510  36510.0     79.5      added = x + y<br/>     5         1         9425   9425.0     20.5      summed = added.sum(axis=1)<br/>     6         1            1      1.0      0.0      <b>return</b> summed<br/></p>
<p>This can be much easier to interpret. In this case we profiled the same function we<br/>used in the statement. Looking at the preceding module code, we could call<br/>call_function and profile that as well as add_and_sum, thus getting a full picture of<br/>the performance of the code:<br/></p>
<p>In [574]: %lprun -f add_and_sum -f call_function call_function()<br/>Timer unit: 1e-06 s<br/>File: prof_mod.py<br/>Function: add_and_sum at line 3<br/>Total time: 0.005526 s<br/>Line <i>#      Hits         Time  Per Hit   % Time  Line Contents<br/></i>==============================================================<br/>     3                                           <b>def</b> add_and_sum(x, y):<br/>     4         1         4375   4375.0     79.2      added = x + y<br/>     5         1         1149   1149.0     20.8      summed = added.sum(axis=1)<br/>     6         1            2      2.0      0.0      <b>return</b> summed<br/>File: prof_mod.py<br/>Function: call_function at line 8<br/>Total time: 0.121016 s<br/>Line <i>#      Hits         Time  Per Hit   % Time  Line Contents<br/></i>==============================================================<br/>     8                                           <b>def</b> call_function():<br/>     9         1        57169  57169.0     47.2      x = randn(1000, 1000)<br/>    10         1        58304  58304.0     48.2      y = randn(1000, 1000)<br/>    11         1         5543   5543.0      4.6      <b>return</b> add_and_sum(x, y)<br/></p>
<p>More on the IPython System | 497</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>As a general rule of thumb, I tend to prefer %prun (cProfile) for &#8220;macro&#8221; profiling<br/>and %lprun (line_profiler) for &#8220;micro&#8221; profiling. It&#8217;s worthwhile to have a good<br/>understanding of both tools.<br/></p>
<p>The reason that you must explicitly specify the names of the func&#8208;<br/>tions you want to profile with %lprun is that the overhead of &#8220;trac&#8208;<br/>ing&#8221; the execution time of each line is substantial. Tracing<br/>functions that are not of interest has the potential to significantly<br/>alter the profile results.<br/></p>
<p>B.4 Tips for Productive Code Development Using IPython<br/>Writing code in a way that makes it easy to develop, debug, and ultimately <i>use</i> inter&#8208;<br/>actively may be a paradigm shift for many users. There are procedural details like<br/>code reloading that may require some adjustment as well as coding style concerns.<br/>Therefore, implementing most of the strategies described in this section is more of an<br/>art than a science and will require some experimentation on your part to determine a<br/>way to write your Python code that is effective for you. Ultimately you want to struc&#8208;<br/>ture your code in a way that makes it easy to use iteratively and to be able to explore<br/>the results of running a program or function as effortlessly as possible. I have found<br/>software designed with IPython in mind to be easier to work with than code intended<br/>only to be run as as standalone command-line application. This becomes especially<br/>important when something goes wrong and you have to diagnose an error in code<br/>that you or someone else might have written months or years beforehand.<br/></p>
<p>Reloading Module Dependencies<br/>In Python, when you type import some_lib, the code in some_lib is executed and all<br/>the variables, functions, and imports defined within are stored in the newly created<br/>some_lib module namespace. The next time you type import some_lib, you will get<br/>a reference to the existing module namespace. The potential difficulty in interactive<br/>IPython code development comes when you, say, %run a script that depends on some<br/>other module where you may have made changes. Suppose I had the following code<br/>in <i>test_script.py</i>:<br/></p>
<p><b>import</b> <b>some_lib<br/></b></p>
<p>x = 5<br/>y = [1, 2, 3, 4]<br/>result = some_lib.get_answer(x, y)<br/></p>
<p>If you were to execute %run test_script.py then modify <i>some_lib.py</i>, the next time<br/>you execute %run test_script.py you will still get the <i>old version</i> of <i>some_lib.py<br/></i>because of Python&#8217;s &#8220;load-once&#8221; module system. This behavior differs from some<br/></p>
<p>498 | Appendix B: More on the IPython System</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>1 Since a module or package may be imported in many different places in a particular program, Python caches a<br/>module&#8217;s code the first time it is imported rather than executing the code in the module every time. Other&#8208;<br/>wise, modularity and good code organization could potentially cause inefficiency in an application.<br/></p>
<p>other data analysis environments, like MATLAB, which automatically propagate code<br/>changes.1 To cope with this, you have a couple of options. The first way is to use the<br/>reload function in the importlib module in the standard library:<br/></p>
<p><b>import</b> <b>some_lib<br/>import</b> <b>importlib<br/></b></p>
<p>importlib.reload(some_lib)<br/></p>
<p>This guarantees that you will get a fresh copy of <i>some_lib.py</i> every time you run<br/><i>test_script.py</i>. Obviously, if the dependencies go deeper, it might be a bit tricky to be<br/>inserting usages of reload all over the place. For this problem, IPython has a special<br/>dreload function (<i>not</i> a magic function) for &#8220;deep&#8221; (recursive) reloading of modules.<br/>If I were to run <i>some_lib.py</i> then type dreload(some_lib), it will attempt to reload<br/>some_lib as well as all of its dependencies. This will not work in all cases, unfortu&#8208;<br/>nately, but when it does it beats having to restart IPython.<br/></p>
<p>Code Design Tips<br/>There&#8217;s no simple recipe for this, but here are some high-level principles I have found<br/>effective in my own work.<br/></p>
<p>Keep relevant objects and data alive<br/>It&#8217;s not unusual to see a program written for the command line with a structure some&#8208;<br/>what like the following trivial example:<br/></p>
<p><b>from</b> <b>my_functions</b> <b>import</b> g<br/></p>
<p><b>def</b> f(x, y):<br/>    <b>return</b> g(x + y)<br/></p>
<p><b>def</b> main():<br/>    x = 6<br/>    y = 7.5<br/>    result = x + y<br/></p>
<p><b>if</b> <b>__name__</b> == '__main__':<br/>    main()<br/></p>
<p>Do you see what might go wrong if we were to run this program in IPython? After it&#8217;s<br/>done, none of the results or objects defined in the main function will be accessible in<br/>the IPython shell. A better way is to have whatever code is in main execute directly in<br/>the module&#8217;s global namespace (or in the if __name__ == '__main__': block, if you<br/></p>
<p>More on the IPython System | 499</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>want the module to also be importable). That way, when you %run the code, you&#8217;ll be<br/>able to look at all of the variables defined in main. This is equivalent to defining top-<br/>level variables in cells in the Jupyter notebook.<br/></p>
<p>Flat is better than nested<br/>Deeply nested code makes me think about the many layers of an onion. When testing<br/>or debugging a function, how many layers of the onion must you peel back in order<br/>to reach the code of interest? The idea that &#8220;flat is better than nested&#8221; is a part of the<br/>Zen of Python, and it applies generally to developing code for interactive use as well.<br/>Making functions and classes as decoupled and modular as possible makes them eas&#8208;<br/>ier to test (if you are writing unit tests), debug, and use interactively.<br/></p>
<p>Overcome a fear of longer files<br/>If you come from a Java (or another such language) background, you may have been<br/>told to keep files short. In many languages, this is sound advice; long length is usually<br/>a bad &#8220;code smell,&#8221; indicating refactoring or reorganization may be necessary. How&#8208;<br/>ever, while developing code using IPython, working with 10 small but interconnected<br/>files (under, say, 100 lines each) is likely to cause you more headaches in general than<br/>two or three longer files. Fewer files means fewer modules to reload and less jumping<br/>between files while editing, too. I have found maintaining larger modules, each with<br/>high <i>internal</i> cohesion, to be much more useful and Pythonic. After iterating toward<br/>a solution, it sometimes will make sense to refactor larger files into smaller ones.<br/>Obviously, I don&#8217;t support taking this argument to the extreme, which would to be to<br/>put all of your code in a single monstrous file. Finding a sensible and intuitive mod&#8208;<br/>ule and package structure for a large codebase often takes a bit of work, but it is espe&#8208;<br/>cially important to get right in teams. Each module should be internally cohesive, and<br/>it should be as obvious as possible where to find functions and classes responsible for<br/>each area of functionality.<br/></p>
<p>B.5 Advanced IPython Features<br/>Making full use of the IPython system may lead you to write your code in a slightly<br/>different way, or to dig into the configuration.<br/></p>
<p>Making Your Own Classes IPython-Friendly<br/>IPython makes every effort to display a console-friendly string representation of any<br/>object that you inspect. For many objects, like dicts, lists, and tuples, the built-in<br/>pprint module is used to do the nice formatting. In user-defined classes, however,<br/>you have to generate the desired string output yourself. Suppose we had the following<br/>simple class:<br/></p>
<p>500 | Appendix B: More on the IPython System</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><b>class</b> <b>Message</b>:<br/>    <b>def</b> <b>__init__</b>(self, msg):<br/>        self.msg = msg<br/></p>
<p>If you wrote this, you would be disappointed to discover that the default output for<br/>your class isn&#8217;t very nice:<br/></p>
<p>In [576]: x = Message('I have a secret')<br/></p>
<p>In [577]: x<br/>Out[577]: &lt;__main__.Message instance at 0x60ebbd8&gt;<br/></p>
<p>IPython takes the string returned by the __repr__ magic method (by doing output =<br/>repr(obj)) and prints that to the console. Thus, we can add a simple __repr__<br/>method to the preceding class to get a more helpful output:<br/></p>
<p><b>class</b> <b>Message</b>:<br/>    <b>def</b> <b>__init__</b>(self, msg):<br/>        self.msg = msg<br/></p>
<p>    <b>def</b> <b>__repr__</b>(self):<br/>        <b>return</b> 'Message: %s' % self.msg<br/></p>
<p>In [579]: x = Message('I have a secret')<br/></p>
<p>In [580]: x<br/>Out[580]: Message: I have a secret<br/></p>
<p>Profiles and Configuration<br/>Most aspects of the appearance (colors, prompt, spacing between lines, etc.) and<br/>behavior of the IPython and Jupyter environments are configurable through an<br/>extensive configuration system. Here are some things you can do via configuration:<br/></p>
<p>&#8226; Change the color scheme<br/>&#8226; Change how the input and output prompts look, or remove the blank line after<br/>Out and before the next In prompt<br/></p>
<p>&#8226; Execute an arbitrary list of Python statements (e.g., imports that you use all the<br/>time or anything else you want to happen each time you launch IPython)<br/></p>
<p>&#8226; Enable always-on IPython extensions, like the %lprun magic in line_profiler<br/>&#8226; Enabling Jupyter extensions<br/>&#8226; Define your own magics or system aliases<br/></p>
<p>Configurations for the IPython shell are specified in special ipython_config.py files,<br/>which are usually found in the <i>.ipython/</i> directory in your user home directory. Con&#8208;<br/>figuration is performed based on a particular profile. When you start IPython nor&#8208;<br/>mally, you load up, by default, the default profile, stored in the profile_default<br/></p>
<p>More on the IPython System | 501</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>directory. Thus, on my Linux OS the full path to my default IPython configuration<br/>file is:<br/></p>
<p>/home/wesm/.ipython/profile_default/ipython_config.py<br/></p>
<p>To initialize this file on your system, run in the terminal:<br/>ipython profile create<br/></p>
<p>I&#8217;ll spare you the gory details of what&#8217;s in this file. Fortunately it has comments<br/>describing what each configuration option is for, so I will leave it to the reader to<br/>tinker and customize. One additional useful feature is that it&#8217;s possible to have <i>multi&#8208;<br/>ple profiles. Suppose you wanted to have an alternative IPython configuration tailored<br/></i>for a particular application or project. Creating a new profile is as simple as typing<br/>something like the following:<br/></p>
<p>ipython profile create secret_project<br/></p>
<p>Once you&#8217;ve done this, edit the config files in the newly created profile_secret_project<br/>directory and then launch IPython like so:<br/></p>
<p>$ ipython --profile=secret_project<br/>Python 3.5.1 | packaged by conda-forge | (default, May 20 2016, 05:22:56)<br/>Type &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.<br/></p>
<p>IPython 5.1.0 -- An enhanced Interactive Python.<br/>?         -&gt; Introduction and overview of IPython's features.<br/>%quickref -&gt; Quick reference.<br/>help      -&gt; Python's own help system.<br/>object?   -&gt; Details about 'object', use 'object??' for extra details.<br/></p>
<p>IPython profile: secret_project<br/></p>
<p>As always, the online IPython documentation is an excellent resource for more on<br/>profiles and configuration.<br/>Configuration for Jupyter works a little differently because you can use its notebooks<br/>with languages other than Python. To create an analogous Jupyter config file, run:<br/></p>
<p>jupyter notebook --generate-config<br/></p>
<p>This writes a default config file to the .jupyter/jupyter_notebook_config.py directory in<br/>your home directory. After editing this to suit your needs, you may rename it to a<br/>different file, like:<br/></p>
<p>$ mv ~/.jupyter/jupyter_notebook_config.py ~/.jupyter/my_custom_config.py<br/></p>
<p>When launching Jupyter, you can then add the --config argument:<br/>jupyter notebook --config=~/.jupyter/my_custom_config.py<br/></p>
<p>502 | Appendix B: More on the IPython System</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>B.6 Conclusion<br/>As you work through the code examples in this book and grow your skills as a Python<br/>programmer, I encourage you to keep learning about the IPython and Jupyter ecosys&#8208;<br/>tems. Since these projects have been designed to assist user productivity, you may dis&#8208;<br/>cover tools that enable you to do your work more easily than using the Python<br/>language and its computational libraries by themselves.<br/>You can also find a wealth of interesting Jupyter notebooks on the nbviewer website.<br/></p>
<p>More on the IPython System | 503</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div>
</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Index<br/></p>
<p>Symbols<br/>! (exclamation point), 486<br/>!= operator, 38, 100, 108<br/># (hash mark), 31<br/>% (percent sign), 28, 495<br/>%matplotlib magic function, 254<br/>&amp; operator, 37, 65, 66, 101<br/>&amp;= operator, 66<br/>() (parentheses), 32, 51<br/>* (asterisk), 24<br/>* operator, 37<br/>** operator, 37<br/>+ operator, 37, 52, 56<br/>- operator, 37, 66<br/>-= operator, 66<br/>. (period), 21<br/>/ operator, 37<br/>// operator, 37, 39<br/>: (colon), 31<br/>; (semicolon), 31<br/>&lt; operator, 38, 108<br/>&lt;= operator, 38, 108<br/>== operator, 38, 108<br/>&gt; operator, 38, 108<br/>&gt;= operator, 38, 108<br/>&gt;&gt;&gt; prompt, 16<br/>? (question mark), 23-24<br/>@ symbol, 116<br/>[] (square brackets), 52, 54<br/>\ (backslash), 41, 216<br/>^ operator, 37, 66<br/>^= operator, 66<br/>_ (underscore), 22, 54, 451, 484<br/>{} (curly braces), 61, 65<br/></p>
<p>| operator, 37, 65-66, 101<br/>|= operator, 66<br/>~ operator, 101<br/></p>
<p>A<br/>%a datetime format, 321<br/>%A datetime format, 321<br/>a(rgs) debugger command, 490<br/>abs function, 107, 121<br/>accumulate method, 466<br/>accumulations, 159<br/>add binary function, 107<br/>add method, 66, 149<br/>add_categories method, 372<br/>add_constant function, 394<br/>add_patch method, 266<br/>add_subplot method, 255<br/>aggfunc method, 315<br/>aggregate (agg) method, 297, 374<br/>aggregations (reductions), 111<br/>%alias magic function, 485-486<br/>all method, 113, 466<br/>and keyword, 21, 43, 101<br/>annotate function, 265<br/>annotating in matplotlib, 265-267<br/>anonymous (lambda) functions, 73<br/>any built-in function, 21<br/>any method, 113, 122, 206<br/>Apache Parquet format, 186<br/>APIs, pandas interacting with, 187<br/>append method, 55, 136<br/>append mode for files, 82<br/>apply method, 152, 164, 302-312, 373-376<br/>applymap method, 153<br/></p>
<p>505</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>arange function, 14, 90<br/>arccos function, 107<br/>arccosh function, 107<br/>arcsin function, 107<br/>arcsinh function, 107<br/>arctan function, 107<br/>arctanh function, 107<br/>argmax method, 112, 121, 160<br/>argmin method, 112, 160<br/>argpartition method, 474<br/>argsort method, 472, 475<br/>arithmetic operations<br/></p>
<p>between DataFrame and Series, 149<br/>between objects with different indexes, 146<br/>on date and time periods, 339-347<br/>with fill values, 148<br/>with NumPy arrays, 93<br/></p>
<p>array function, 88, 90<br/>arrays (see ndarray object)<br/>arrow function, 265<br/>as keyword, 36<br/>asarray function, 90<br/>asfreq method, 340, 352<br/>assign method, 379<br/>associative arrays (see dicts)<br/>asterisk (*), 24<br/>astype method, 92<br/>as_ordered methdo, 372<br/>as_ordered method, 367<br/>as_unordered method, 372<br/>attributes<br/></p>
<p>for data types, 469<br/>for ndarrays, 89, 453, 463, 481<br/>hidden, 22<br/>in DataFrame data structure, 130<br/>in Python, 35, 161<br/>in Series data structure, 127<br/></p>
<p>automagic feature, 29<br/>%automagic magic function, 29<br/>average method, 156<br/>axes<br/></p>
<p>broadcasting over, 462<br/>concatenating along, 227, 236-241<br/>renaming indexes for, 201<br/>selecting indexes with duplicate labels, 157<br/>swapping in arrays, 103<br/></p>
<p>AxesSubplot object, 256, 262<br/>axis method, 159<br/></p>
<p>B<br/>%b datetime format, 321<br/>%B datetime format, 321<br/>b(reak) debugger command, 490<br/>backslash (\), 41, 216<br/>bang (!), 486<br/>bar method, 272<br/>bar plots, 272-277<br/>barh method, 272<br/>barplot function, 277<br/>base frequency, 330<br/>bcolz binary format, 184<br/>beta function, 119<br/>binary data formats<br/></p>
<p>about, 183<br/>binary mode for files, 82-83<br/>HDF5 format, 184-186<br/>Microsoft Excel files, 186-187<br/></p>
<p>binary moving window functions, 359<br/>binary operators and comparisons in Python,<br/></p>
<p>36, 65<br/>binary searches of lists, 57<br/>binary universal functions, 106, 107<br/>binding, defined, 33, 236<br/>binning continuous data, 203<br/>binomial function, 119<br/>bisect module, 57<br/>Bitly dataset example, 403-413<br/>Blosc compression library, 184<br/>Bokeh tool, 285<br/>%bookmark magic function, 485, 487<br/>bookmarking directories in IPython, 487<br/>bool data type, 39, 43, 91<br/>bool function, 43<br/>boolean arrays, 113<br/>boolean indexing, 99-102<br/>braces {}, 61, 65<br/>break keyword, 47<br/>broadcasting, ndarrays and, 94, 457, 460-465<br/>bucket analysis, 305<br/>build_design_matrices function, 389<br/>builtins module, 390<br/>bytes data type, 39, 43<br/></p>
<p>C<br/>%C datetime format, 321<br/>C order (row major order), 454, 481<br/>c(ontinue) debugger command, 490<br/>calendar module, 318<br/></p>
<p>506 | Index</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Cartesian product, 77, 230<br/>casefold method, 213<br/>cat method, 218<br/>categorical data<br/></p>
<p>basic overview, 363-372<br/>facet grids and, 283<br/>Patsy library and, 390-393<br/></p>
<p>Categorical object, 203, 305, 363-372<br/>%cd magic function, 485, 487<br/>ceil function, 107<br/>center method, 219<br/>chaining methods, 378-380<br/>chisquare function, 119<br/>clear method, 66<br/>clipboard, executing code from, 26<br/>close method, 80, 83<br/>closed attribute, 83<br/>!cmd command, 485<br/>collections module, 64<br/>colon (:), 31<br/>color selection in matplotlib, 259<br/>column major order (Fortran order), 454, 481<br/>columns method, 315<br/>column_stack function, 456<br/>combinations function, 77<br/>combine_first method, 227, 242<br/>combining data (see merging data)<br/>command history<br/></p>
<p>input and output variables, 484<br/>reusing, 483<br/>searching, 483<br/>using in IPython, 483-485<br/></p>
<p>commands<br/>debugger, 490<br/>magic functions, 28-29<br/>updating packages, 10<br/></p>
<p>comments in Python, 31<br/>compile method, 214<br/>complex128 data type, 91<br/>complex256 data type, 91<br/>complex64 data type, 91<br/>concat function, 227, 235, 237-241, 300<br/>concatenate function, 236, 454<br/>concatenating<br/></p>
<p>along an axis, 227, 236-241<br/>lists, 56<br/>strings, 41<br/></p>
<p>conda update command, 10<br/>conditional logic as array operations, 109<br/></p>
<p>configuration for IPython, 501-502<br/>configuring matplotlib, 268<br/>contains method, 218<br/>contiguous memory, 480-482<br/>continue keyword, 47<br/>continuing education, 401<br/>control flow in Python, 46-50<br/>coordinated universal time (UTC), 335<br/>copy method, 95, 132<br/>copysign function, 107<br/>corr aggregation function, 359<br/>corr method, 161<br/>correlation, 160-162, 310<br/>corrwith method, 162<br/>cos function, 107<br/>cosh function, 107<br/>count method, 40, 54, 160, 212-213, 218, 296<br/>cov method, 161<br/>covariance, 160-162<br/>%cpaste magic function, 26, 29<br/>cProfile module, 494-496<br/>cross-tabulation, 315<br/>crosstab function, 316<br/>cross_val_score function, 401<br/>CSV files, 168, 175-178<br/>csv module, 176<br/>Ctrl-A keyboard shortcut, 27<br/>Ctrl-B keyboard shortcut, 27<br/>Ctrl-C keyboard shortcut, 26, 27<br/>Ctrl-D keyboard shortcut, 16<br/>Ctrl-E keyboard shortcut, 27<br/>Ctrl-F keyboard shortcut, 27<br/>Ctrl-K keyboard shortcut, 27<br/>Ctrl-L keyboard shortcut, 27<br/>Ctrl-N keyboard shortcut, 27, 484<br/>Ctrl-P keyboard shortcut, 27, 483<br/>Ctrl-R keyboard shortcut, 27, 484<br/>Ctrl-Shift-V keyboard shortcut, 27<br/>Ctrl-U keyboard shortcut, 27<br/>cummax method, 160<br/>cummin method, 160<br/>cumprod method, 112, 160<br/>cumsum method, 112, 160, 466<br/>curly braces {}, 61, 65<br/>currying, 74<br/>cut function, 203, 305<br/>c_ object, 456<br/></p>
<p>Index | 507</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>D<br/>%d datetime format, 46, 319<br/>%D datetime format, 46, 320<br/>d(own) debugger command, 490<br/>data aggregation<br/></p>
<p>about, 296<br/>column-wise, 298-301<br/>multiple function application, 298-301<br/>returning data without row indexes, 301<br/></p>
<p>data alignment, pandas library and, 146-151<br/>data analysis with Python<br/></p>
<p>about, 2, 15-16<br/>glue code, 2<br/>MovieLens 1M dataset example, 413-419<br/>restrictions to consider, 3<br/>US baby names dataset example, 419-434<br/>US Federal Election Commission database<br/></p>
<p>example, 440-448<br/>USA.gov data from Bitly example, 403-413<br/>USDA food database example, 434-439<br/>&#8220;two-language&#8221; problem, 3<br/></p>
<p>data cleaning and preparation (see data wran&#8208;<br/>gling)<br/></p>
<p>data loading (see reading data)<br/>data manipulation (see data wrangling)<br/>data munging (see data wrangling)<br/>data selection<br/></p>
<p>for axis indexes with duplicate labels, 157<br/>in pandas library, 140-145<br/>time series data, 323<br/></p>
<p>data structures<br/>about, 51<br/>dict comprehensions, 67<br/>dicts, 61-65<br/>for pandas library, 124-136<br/>list comprehensions, 67-69<br/>lists, 54-59<br/>set comprehensions, 68<br/>sets, 65-67<br/>tuples, 51-54<br/></p>
<p>data transformation (see transforming data)<br/>data types<br/></p>
<p>attributes for, 469<br/>defined, 90, 449<br/>for date and time data, 318<br/>for ndarrays, 90-93<br/>in Python, 38-46<br/>nested, 469<br/>NumPy hierarchy, 450<br/></p>
<p>parent classes of, 450<br/>data wrangling<br/></p>
<p>combining and merging datasets, 227-242<br/>defined, 14<br/>handling missing data, 191-197<br/>hierarchical indexing, 221-226, 243<br/>pivoting data, 246-250<br/>reshaping data, 243<br/>string manipulation, 211-219<br/>transforming data, 197-211<br/>working with delimited formats, 176-178<br/></p>
<p>databases<br/>DataFrame joins, 227-232<br/>pandas interacting with, 188<br/>storing data in, 247<br/></p>
<p>DataFrame data structure<br/>about, 4, 128-134, 470<br/>database-stye joins, 227-232<br/>indexing with columns, 225<br/>JSON data and, 180<br/>operations between Series and, 149<br/>optional function arguments, 168<br/>plot method arguments, 271<br/>possible data inputs to, 134<br/>ranking data in, 155<br/>sorting considerations, 153, 473<br/>summary statistics methods for, 161<br/></p>
<p>DataOffset object, 338<br/>datasets<br/></p>
<p>combining and merging, 227-242<br/>MovieLens 1M example, 413-419<br/>US baby names example, 419-434<br/>US Federal Election Commission database<br/></p>
<p>example, 440-448<br/>USA.gov data from Bitly example, 403-413<br/>USDA food database example, 434-439<br/></p>
<p>date data type, 44, 319<br/>date offsets, 330, 333-334<br/>date ranges, generating, 328-330<br/>dates and times<br/></p>
<p>about, 44<br/>converting between strings and datetime,<br/></p>
<p>319-321<br/>data types and tools, 318<br/>formatting specifications, 319, 321<br/>generating date ranges, 328-330<br/>period arithmetic and, 339-347<br/></p>
<p>datetime data type<br/>about, 44, 318-319<br/></p>
<p>508 | Index</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>converting between strings and, 319-321<br/>format specification for, 319<br/></p>
<p>datetime module, 44, 318<br/>datetime64 data type, 322<br/>DatetimeIndex class, 322, 328, 337<br/>dateutil package, 320<br/>date_range function, 328-330<br/>daylight saving time (DST), 335<br/>debug function, 491<br/>%debug magic function, 80, 488<br/>debugger, IPython, 488-492<br/>decode method, 42<br/>def keyword, 69, 74<br/>default values for dicts, 63<br/>defaultdict class, 64<br/>del keyword, 62, 132<br/>del method, 132<br/>delete method, 136<br/>delimited formats, working with, 176-178<br/>dense method, 156<br/>density plots, 277-279<br/>deque (double-ended queue), 55<br/>describe method, 160, 297<br/>design matrix, 386<br/>det function, 117<br/>development tools for IPython (see software<br/></p>
<p>development tools for IPython)<br/>%dhist magic function, 486<br/>diag function, 117<br/>Dialect class, 177<br/>dict comprehensions, 67<br/>dict function, 63<br/>dictionary-encoded representation, 365<br/>dicts (data structures)<br/></p>
<p>about, 61<br/>creating from sequences, 63<br/>DataFrame data structure as, 129<br/>default values, 63<br/>grouping with, 294<br/>Series data structure as, 125<br/>valid key types, 64<br/></p>
<p>diff method, 160<br/>difference method, 66, 136<br/>difference_update method, 66<br/>dimension tables, 364<br/>directories, bookmarking in IPython, 487<br/>%dirs magic function, 485<br/>discretization, 203<br/>distplot method, 279<br/></p>
<p>div method, 149<br/>divide function, 107<br/>divmod function, 106<br/>dmatrices function, 386<br/>dnorm function, 394<br/>dot function, 104, 116-117<br/>downsampling, 348, 349-351<br/>dreload function, 499<br/>drop method, 136, 138<br/>dropna method, 192-193, 306, 315<br/>drop_duplicates method, 197<br/>DST (daylight saving time), 335<br/>dstack function, 456<br/>dtype (see data types)<br/>dtype attribute, 88, 92<br/>duck typing, 35<br/>dummy variables, 208-211, 372, 386, 391<br/>dumps function, 179<br/>duplicate data<br/></p>
<p>axis indexes with duplicate labels, 157<br/>removing, 197<br/>time series with duplicate indexes, 326<br/></p>
<p>duplicated method, 197<br/>dynamic references in Python, 33<br/></p>
<p>E<br/>edit-compile-run workflow, 6<br/>education, continuing, 401<br/>eig function, 118<br/>elif statement, 46<br/>else statement, 46<br/>empty function, 89-90<br/>empty namespace, 25<br/>empty_like function, 90<br/>encode method, 42<br/>end-of-line (EOL) markers, 80<br/>endswith method, 213, 218<br/>enumerate function, 59<br/>%env magic function, 486<br/>EOL (end-of-line) markers, 80<br/>equal function, 108<br/>error handling in Python, 77-80<br/>escape characters, 41<br/>ewm function, 358<br/>Excel files (Microsoft), 186-187<br/>ExcelFile class, 186<br/>exception handling in Python, 77-80<br/>exclamation point (!), 486<br/>execute-explore workflow, 6<br/></p>
<p>Index | 509</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>exit command, 16<br/>exp function, 107<br/>expanding function, 356<br/>exponentially-weighted functions, 358<br/>extend method, 56<br/>extract method, 218<br/>eye function, 90<br/></p>
<p>F<br/>%F datetime format, 46, 320<br/>fabs function, 107<br/>facet grids, 283<br/>FacetGrid class, 285<br/>factorplot built-in function, 283<br/>fancy indexing, 102, 459<br/>FDIC bank failures list, 180<br/>Feather binary file format, 168, 184<br/>feature engineering, 383<br/>Federal Election Commission database exam&#8208;<br/></p>
<p>ple, 440-448<br/>Figure object, 255<br/>file management<br/></p>
<p>binary data formats, 183-187<br/>commonly used file methods, 82<br/>design tips, 500<br/>file input and output with arrays, 115<br/>JSON data, 178-180<br/>memory-mapped files, 478<br/>opening files, 80<br/>Python file modes, 82<br/>reading and writing data in text format,<br/></p>
<p>167-176<br/>saving plots to files, 267<br/>Web scraping, 180-183<br/>working with delimited formats, 176-178<br/></p>
<p>filling in data<br/>arithmetic methods with fill values, 148<br/>filling in missing data, 195-197, 200<br/>with group-specific values, 306<br/></p>
<p>fillna method, 192, 195-197, 200, 306, 352<br/>fill_value method, 315<br/>filtering<br/></p>
<p>in pandas library, 140-145<br/>missing data, 193<br/>outliers, 205<br/></p>
<p>find method, 212-213<br/>findall method, 214, 216, 218<br/>finditer method, 216<br/>first method, 156, 296<br/></p>
<p>fit method, 395, 400<br/>fixed frequency, 317<br/>flags attribute, 481<br/>flatten method, 453<br/>float data type, 39, 43<br/>float function, 43<br/>float128 data type, 91<br/>float16 data type, 91<br/>float32 data type, 91<br/>float64 data type, 91<br/>floor function, 107<br/>floordiv method, 149<br/>floor_divide function, 107<br/>flow control in Python, 46-50<br/>flush method, 83, 479<br/>fmax function, 107<br/>fmin function, 107<br/>for loops, 47, 68<br/>format method, 41<br/>formatting<br/></p>
<p>dates and times, 319, 321<br/>strings, 41<br/></p>
<p>Fortran order (column major order), 454, 481<br/>frequencies<br/></p>
<p>base, 330<br/>basic for time series, 329<br/>converting between, 327, 348-354<br/>date offsets and, 330<br/>fixed, 317<br/>period conversion, 340<br/>quarterly period frequencies, 342<br/></p>
<p>fromfile function, 471<br/>frompyfunc function, 468<br/>from_codes method, 367<br/>full function, 90<br/>full_like function, 90<br/>functions, 69<br/></p>
<p>(see also universal functions)<br/>about, 69<br/>accessing variables, 70<br/>anonymous, 73<br/>as objects, 72-73<br/>currying, 74<br/>errors and exception handling, 77<br/>exponentially-weighted, 358<br/>generators and, 75-80<br/>grouping with, 295<br/>in Python, 32<br/>lambda, 73<br/></p>
<p>510 | Index</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>magic, 28-29<br/>namespaces and, 70<br/>object introspection, 23<br/>partial argument application, 74<br/>profiling line by line, 496-498<br/>returning multiple values, 71<br/>sequence, 59-61<br/>transforming data using, 198<br/>type inference in, 168<br/>writing fast NumPy functions with Numba,<br/></p>
<p>476-478<br/>functools module, 74<br/></p>
<p>G<br/>gamma function, 119<br/>generators<br/></p>
<p>about, 75<br/>generator expressions for, 76<br/>itertools module and, 76<br/></p>
<p>get method, 63, 218<br/>GET request (HTTP), 187<br/>getattr function, 35<br/>getroot method, 182<br/>get_chunk method, 175<br/>get_dummies function, 208, 372, 385<br/>get_indexer method, 164<br/>get_value method, 145<br/>GIL (global interpreter lock), 3<br/>global keyword, 71<br/>glue for code, Python as, 2<br/>greater function, 108<br/>greater_equal function, 108<br/>Greenwich Mean Time, 335<br/>group keys, suppressing, 304<br/>group operations<br/></p>
<p>about, 287, 373<br/>cross-tabulation, 315<br/>data aggregation, 296-302<br/>GroupBy mechanics, 288-296<br/>pivot tables, 287, 313-316<br/>split-apply-combine, 288, 302-312<br/>unwrapped, 376<br/></p>
<p>group weighted average, 310<br/>groupby function, 77<br/>groupby method, 368, 476<br/>GroupBy object<br/></p>
<p>about, 288-291<br/>grouping by index level, 295<br/>grouping with dicts, 294<br/></p>
<p>grouping with functions, 295<br/>grouping with Series, 294<br/>iterating over groups, 291<br/>optimized methods, 296<br/>selecting columns, 293<br/>selecting subset of columns, 293<br/></p>
<p>groups method, 215<br/></p>
<p>H<br/>%H datetime format, 46, 319<br/>h(elp) debugger command, 490<br/>hasattr function, 35<br/>hash function, 64<br/>hash maps (see dicts)<br/>hash mark (#), 31<br/>hashability, 64<br/>HDF5 (hierarchical data format 5), 184-186,<br/></p>
<p>480<br/>HDFStore class, 184<br/>head method, 129<br/>heapsort method, 474<br/>hierarchical data format (HDF5), 480<br/>hierarchical indexing<br/></p>
<p>about, 221-224<br/>in pandas, 170<br/>reordering and sorting levels, 224<br/>reshaping data with, 243<br/>summary statistics by level, 225<br/>with DataFrame columns, 225<br/></p>
<p>%hist magic function, 29<br/>hist method, 277<br/>histograms, 277-279<br/>hsplit function, 456<br/>hstack function, 455<br/>HTML files, 180-183<br/>HTTP requests, 187<br/>Hugunin, Jim, 86<br/>Hunter, John D., 5, 253<br/></p>
<p>I<br/>%I datetime format, 46, 319<br/>identity function, 90<br/>IDEs (Integrated Development Environments),<br/></p>
<p>11<br/>idxmax method, 160<br/>idxmin method, 160<br/>if statement, 46<br/>iloc operator, 143, 207<br/>immutable objects, 38, 367<br/></p>
<p>Index | 511</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>import conventions<br/>for matplotlib, 253<br/>for modules, 14, 36<br/>for Python, 14, 36, 88<br/></p>
<p>importlib module, 499<br/>imshow function, 109<br/>in keyword, 56, 212<br/>in-place sorts, 57, 471<br/>in1d method, 114, 115<br/>indentation in Python, 30<br/>index method, 212-213, 315<br/>Index objects, 134-136<br/>indexes and indexing<br/></p>
<p>axis indexes with duplicate labels, 157<br/>boolean indexing, 99-102<br/>fancy indexing, 102, 459<br/>for ndarrays, 94-98<br/>for pandas library, 140-145, 157<br/>grouping by index level, 295<br/>hierarchical indexing, 170, 221-226, 243<br/>Index objects, 134-136<br/>integer indexing, 145<br/>merging on index, 232-235<br/>renaming axis indexes, 201<br/>time series data, 323<br/>time series with duplicate indexes, 326<br/>timedeltas and, 318<br/></p>
<p>indexing operator, 58<br/>indicator variables, 208-211<br/>indirect sorts, 472<br/>inner join type, 229<br/>input variables, 484<br/>insert method, 55, 136<br/>insort function, 57<br/>int data type, 39, 43<br/>int function, 43<br/>int16 data type, 91<br/>int32 data type, 91<br/>int64 data type, 91<br/>int8 data type, 91<br/>integer arrays, indexing, 102, 459<br/>integer indexing, 145<br/>Integrated Development Environments (IDEs),<br/></p>
<p>11<br/>interactive debugger, 488-492<br/>interpreted languages, 2, 16<br/>interrupting running code, 26<br/>intersect1d method, 115<br/>intersection method, 65-66, 136<br/></p>
<p>intersection_update method, 66<br/>intervals of time, 317<br/>inv function, 118<br/>.ipynb file extension, 20<br/>IPython<br/></p>
<p>%run command and, 17<br/>%run command in, 25-26<br/>about, 6<br/>advanced features, 500-502<br/>bookmarking directories, 487<br/>code development tips, 498-500<br/>command history in, 483-485<br/>exception handling in, 79<br/>executing code from clipboard, 26<br/>figures and subplots, 255<br/>interacting with operating system, 485-487<br/>keyboard shortcuts for, 27<br/>magic commands in, 28-29<br/>matplotlib integration, 29<br/>object introspection, 23-24<br/>running Jupyter notebook, 18-20<br/>running shell, 17-18<br/>shell commands in, 486<br/>software development tools, 487-498<br/>tab completion in, 21-23<br/></p>
<p>ipython command, 17-18<br/>is keyword, 38<br/>is not keyword, 38<br/>isalnum method, 218<br/>isalpha method, 218<br/>isdecimal method, 218<br/>isdigit method, 218<br/>isdisjoint method, 66<br/>isfinite function, 107<br/>isin method, 136, 163<br/>isinf function, 107<br/>isinstance function, 34<br/>islower method, 218<br/>isnan function, 107<br/>isnull method, 126, 192<br/>isnumeric method, 218<br/>issubdtype function, 450<br/>issubset method, 66<br/>issuperset method, 66<br/>isupper method, 218<br/>is_monotonic property, 136<br/>is_unique property, 136, 157, 326<br/>iter function, 35<br/>__iter__ magic method, 35<br/></p>
<p>512 | Index</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>iterator protocol, 35, 75-77<br/>itertools module, 76<br/></p>
<p>J<br/>jit function, 477<br/>join method, 212-213, 218, 235<br/>join operations, 227-232<br/>JSON (JavaScript Object Notation), 178-180,<br/></p>
<p>403<br/>json method, 187<br/>Jupyter notebook<br/></p>
<p>%load magic function, 25<br/>about, 6<br/>plotting nuances, 256<br/>running, 18-20<br/></p>
<p>jupyter notebook command, 19<br/></p>
<p>K<br/>KDE (kernel density estimate) plots, 278<br/>kernels, defined, 6, 18<br/>key-value pairs, 61<br/>keyboard shortcuts for IPython, 27<br/>KeyboardInterrupt exception, 26<br/>KeyError exception, 66<br/>keys method, 62<br/>keyword arguments, 32, 70<br/>kurt method, 160<br/></p>
<p>L<br/>l(ist) debugger command, 490<br/>labels<br/></p>
<p>axis indexes with duplicate labels, 157<br/>selecting in matplotlib, 261-263<br/></p>
<p>lagging data, 332<br/>lambda (anonymous) functions, 73<br/>language semantics for Python<br/></p>
<p>about, 30<br/>attributes, 35<br/>binary operators and comparisons, 36, 65<br/>comments, 31<br/>duck typing, 35<br/>function and object method calls, 32<br/>import conventions, 36<br/>indentation not braces, 30<br/>methods, 35<br/>mutable and immutable objects, 38<br/>object model, 31<br/>references, 32-34<br/></p>
<p>strongly typed language, 33<br/>variables and argument passing, 32<br/></p>
<p>last method, 296<br/>leading data, 332<br/>left join type, 229<br/>legend method, 264<br/>legend selection in matplotlib, 261-265<br/>len function, 295<br/>len method, 218<br/>less function, 108<br/>less_equal function, 108<br/>level keyword, 296<br/>level method, 159<br/>levels<br/></p>
<p>grouping by index levels, 295<br/>sorting, 224<br/>summary statistics by, 225<br/></p>
<p>lexsort method, 473<br/>libraries (see specific libraries)<br/>line plots, 269-271<br/>line style selection in matplotlib, 260<br/>linear algebra, 116-118<br/>linear regression, 312, 393-396<br/>Linux, setting up Python on, 9<br/>list comprehensions, 67-69<br/>list function, 37, 54<br/>lists (data structures)<br/></p>
<p>about, 54<br/>adding and removing elements, 55<br/>combining, 56<br/>concatenating, 56<br/>maintaining sorted lists, 57<br/>slicing, 58<br/>sorting, 57<br/></p>
<p>lists (data structures)binary searches, 57<br/>ljust method, 213<br/>load function, 115, 478<br/>%load magic function, 25<br/>loads function, 179<br/>loc operator, 130, 143, 265, 385<br/>local namespace, 70, 123<br/>localizing data to time zones, 335<br/>log function, 107<br/>log10 function, 107<br/>log1p function, 107<br/>log2 function, 107<br/>logical_and function, 108, 466<br/>logical_not function, 107<br/>logical_or function, 108<br/></p>
<p>Index | 513</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>logical_xor function, 108<br/>LogisticRegression class, 399<br/>LogisticRegressionCV class, 400<br/>long format, 246<br/>lower method, 199, 213, 218<br/>%lprun magic function, 496<br/>lstrip method, 213, 219<br/>lstsq function, 118<br/>lxml library, 180-183<br/></p>
<p>M<br/>%m datetime format, 46, 319<br/>%M datetime format, 46, 319<br/>mad method, 160<br/>magic functions, 28-29<br/></p>
<p>(see also specific magic functions)<br/>%debug magic function, 29<br/>%magic magic function, 29<br/>many-to-many merge, 229<br/>many-to-one join, 228<br/>map built-in function, 68, 73<br/>map method, 153, 199, 202<br/>mapping<br/></p>
<p>transforming data using, 198<br/>universal functions, 151-156<br/></p>
<p>margins method, 315<br/>margins, defined, 313<br/>marker selection in matplotlib, 260<br/>match method, 164, 214, 216, 219<br/>Math Kernel Library (MKL), 117<br/>matplotlib library<br/></p>
<p>about, 5, 253<br/>annotations in, 265-267<br/>color selection in, 259<br/>configuring, 268<br/>creating image plots, 109<br/>figures in, 255-259<br/>import convention, 253<br/>integration with IPython, 29<br/>label selection in, 261-263<br/>legend selection in, 261-265<br/>line style selection in, 260<br/>marker selection in, 260<br/>saving plots to files, 267<br/>subplots in, 255-259, 265-267<br/>tick mark selection in, 261-263<br/></p>
<p>%matplotlib magic function, 30, 486<br/>matrix operations in NumPy, 104, 116<br/>max method, 112, 156, 160, 296<br/></p>
<p>maximum function, 107<br/>mean method, 112, 160, 289, 296<br/>median method, 160, 296<br/>melt method, 249<br/>memmap object, 478<br/>memory management<br/></p>
<p>C versus Fortran order, 454<br/>continguous memory, 480-482<br/>NumPy-based algorithms and, 87<br/></p>
<p>memory-mapped files, 478<br/>merge function, 227-232<br/>mergesort method, 474<br/>merging data<br/></p>
<p>combining data with overlap, 241<br/>concatenating along an axis, 236-241<br/>database-stye DataFrame joins, 227-232<br/>merging on index, 232-235<br/></p>
<p>meshgrid function, 108<br/>methods<br/></p>
<p>categorical, 370-372<br/>chaining, 378-380<br/>defined, 32<br/>for boolean arrays, 113<br/>for strings, 211-213<br/>for summary statistics, 162-165<br/>for tuples, 54<br/>hidden, 22<br/>in Python, 32, 35<br/>object introspection, 23<br/>optimized for GroupBy, 296<br/>statistical, 111-112<br/>ufunc instance methods, 466-468<br/>vectorized string methods in pandas,<br/></p>
<p>216-219<br/>Microsoft Excel files, 186-187<br/>min method, 112, 156, 160, 296<br/>minimum function, 107<br/>missing data<br/></p>
<p>about, 191<br/>filling in, 195-197, 200<br/>filling with group-specific values, 306<br/>filtering out, 193<br/>marked by sentinel values, 171, 191<br/>sorting considerations, 154<br/></p>
<p>mixture-of-normals estimate, 278<br/>MKL (Math Kernel Library), 117<br/>mod function, 107<br/>modf function, 106-107<br/>modules<br/></p>
<p>514 | Index</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>import conventions for, 14, 36<br/>reloading dependencies, 498<br/></p>
<p>MovieLens 1M dataset example, 413-419<br/>moving window functions<br/></p>
<p>about, 354-357<br/>binary, 359<br/>exponentially-weighted functions, 358<br/>user-defined, 361<br/></p>
<p>mro method, 450<br/>MSFT attribute, 161<br/>mul method, 149<br/>multiply function, 107<br/>munging (see data wrangling)<br/>mutable objects, 38<br/></p>
<p>N<br/>n(ext) debugger command, 490<br/>NA data type, 192<br/>name attribute, 127, 130<br/>names attribute, 100, 469<br/>namespaces<br/></p>
<p>empty, 25<br/>functions and, 70<br/>in Python, 34<br/>NumPy, 88<br/></p>
<p>NaN (Not a Number), 107, 126, 191<br/>NaT (Not a Time), 321<br/>ndarray object<br/></p>
<p>about, 85, 87-88<br/>advanced input and output, 478-480<br/>arithmetic with, 93<br/>array-oriented programming, 108-115<br/>as structured arrays, 469-471<br/>attributes for, 89, 453, 463, 481<br/>boolean indexing, 99-102<br/>broadcasting and, 94, 457, 460-465<br/>C versus Fortan order, 454<br/>C versus Fortran order, 481<br/>concatenating arrays, 454<br/>creating, 88-90<br/>creating PeriodIndex from arrays, 345<br/>data types for, 90-93<br/>fancy indexing, 102, 459<br/>file input and output, 115<br/>finding elements in sorted arrays, 475<br/>indexes for, 94-98<br/>internals overview, 449-451<br/>linear algebra and, 116-118<br/>partially sorting arrays, 474<br/></p>
<p>pseudorandom number generation, 118-119<br/>random walks example, 119-122<br/>repeating elements in, 457<br/>reshaping arrays, 103, 452<br/>slicing arrays, 94-98<br/>sorting considerations, 113, 471<br/>splitting arrays, 455<br/>storage options, 480<br/>swapping axes in, 103<br/>transposing arrays, 103<br/></p>
<p>ndim attribute, 89<br/>nested code, 500<br/>nested data types, 469<br/>nested list comprehensions, 68-69<br/>nested tuples, 53<br/>New York MTA (Metropolitan Transportation<br/></p>
<p>Authority), 181<br/>newaxis attribute, 463<br/>&#8220;no-op&#8221; statement, 48<br/>None data type, 39, 44, 192<br/>normal function, 119<br/>not keyword, 56<br/>notfull method, 192<br/>notnull method, 126<br/>not_equal function, 108<br/>.npy file extension, 115<br/>.npz file extension, 115<br/>null value, 39, 44, 178<br/>Numba<br/></p>
<p>creating custom ufunc objects with, 478<br/>writing fast NumPy functions with, 476-478<br/></p>
<p>numeric data types, 39<br/>NumPy library<br/></p>
<p>about, 4, 85-87<br/>advanced array input and output, 478-480<br/>advanced array manipulation, 451-459<br/>advanced ufunc usage, 466-469<br/>array-oriented programming, 108-115<br/>arrays and broadcasting, 460-465<br/>file input and output with arrays, 115<br/>linear algebra and, 116-118<br/>ndarray object internals, 449-451<br/>ndarray object overview, 87-105<br/>performance tips, 480-482<br/>pseudorandom number generation, 118-119<br/>random walks example, 119-122<br/>sorting considerations, 113, 471-476<br/>structured and record arrays, 469-471<br/>ufunc overview, 105-108<br/></p>
<p>Index | 515</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>writing fast functions with Numba, 476-478<br/></p>
<p>O<br/>object data type, 91<br/>object introspection, 23-24<br/>object model, 31<br/>objectify function, 181-183<br/>objects (see Python objects)<br/>OHLC (Open-High-Low-Close) resampling,<br/></p>
<p>351<br/>ohlc aggregate function, 351<br/>Oliphant, Travis, 86<br/>OLS (ordinary least squares) regression, 312,<br/></p>
<p>388<br/>OLS class, 395<br/>Olson database, 335<br/>ones function, 89-90<br/>ones_like function, 90<br/>open built-in function, 80, 83<br/>openpyxl package, 186<br/>operating system, IPython interacting with,<br/></p>
<p>485-487<br/>or keyword, 43, 101<br/>OS X, setting up Python on, 9<br/>outer method, 467<br/>outliers, detecting and filtering, 205<br/>output join type, 229<br/>output variables, 484<br/></p>
<p>P<br/>%p datetime format, 321<br/>packages, installing or updating, 10<br/>pad method, 219<br/>%page magic function, 29<br/>pairplot function, 281<br/>pairs plot, 281<br/>pandas library, 4<br/></p>
<p>(see also data wrangling)<br/>about, 4, 123<br/>arithmetic and data alignment, 146-151<br/>as time zone naive, 335<br/>binary data formats, 183-187<br/>categorical data and, 363-372<br/>data structures for, 124-136<br/>drop method, 138<br/>filtering in, 140-145<br/>function application and mapping, 151<br/>group operations and, 373-378<br/>indexes in, 140-145, 157<br/></p>
<p>integer indexing, 145<br/>interacting with databases, 188<br/>interacting with Web APIs, 187<br/>interfacing with model code, 383<br/>JSON data, 178-180<br/>method chaining, 378-380<br/>nested data types and, 470<br/>plotting with, 268-285<br/>ranking data in, 153-156<br/>reading and writing data in text format,<br/></p>
<p>167-176<br/>reductions in, 158-165<br/>reindex method, 136-138<br/>selecting data in, 140-145<br/>sorting considerations, 153-156, 473, 476<br/>summary statistics in, 158-165<br/>vectorized string methods in, 216-219<br/>Web scraping, 180-183<br/>working with delimited formats, 176-178<br/></p>
<p>pandas-datareader package, 160<br/>parentheses (), 32, 51<br/>parse method, 186, 320<br/>partial argument application, 74<br/>partial function, 74<br/>partition method, 474<br/>pass statement, 48<br/>%paste magic function, 26, 29<br/>patches, defined, 266<br/>Patsy library<br/></p>
<p>about, 386<br/>categorical data and, 390-393<br/>creating model descriptions with, 386-388<br/>data transformations in Patsy formulas, 389<br/></p>
<p>pct_change method, 160, 311<br/>%pdb magic function, 29, 80, 489<br/>percent sign (%), 28, 495<br/>percentileofscore function, 361<br/>P&#233;rez, Fernando, 6<br/>period (.), 21<br/>Period class, 339<br/>PeriodIndex class, 340, 345<br/>periods of dates and times<br/></p>
<p>about, 339<br/>converting frequencies, 340<br/>converting timestamps to/from, 344<br/>creating PeriodIndex from arrays, 345<br/>fixed periods, 317<br/>quarterly period frequencies, 342<br/>resampling with, 353<br/></p>
<p>516 | Index</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>period_range function, 340, 343<br/>Perktold, Josef, 8<br/>permutation function, 119, 206<br/>permutations function, 77<br/>pickle module, 183<br/>pinv function, 118<br/>pip tool, 10, 180<br/>pipe method, 380<br/>pivot method, 247<br/>pivot tables, 287, 313-316<br/>pivoting data, 246-250<br/>pivot_table method, 313<br/>plot function, 259<br/>plot method, 269-271<br/>Plotly tool, 285<br/>plotting<br/></p>
<p>with matplotlib, 253-268<br/>with pandas and seaborn, 268-285<br/></p>
<p>point plots, 280<br/>pop method, 55, 62-63, 66<br/>%popd magic function, 485<br/>positional arguments, 32, 70<br/>pound sign (#), 31<br/>pow method, 149<br/>power function, 107<br/>pprint module, 500<br/>predict method, 400<br/>preparation, data (see data wrangling)<br/>private attributes, 22<br/>private methods, 22<br/>prod method, 160, 296<br/>product function, 77<br/>profiles for IPython, 501-502<br/>profiling code in IPython, 494-496<br/>profiling functions line by line, 496-498<br/>%prun magic function, 29, 495-496<br/>pseudocode, 14, 30<br/>pseudorandom number generation, 118-119<br/>%pushd magic function, 485<br/>put method, 459<br/>%pwd magic function, 485<br/>.py file extension, 16, 36<br/>pyplot module, 261<br/>Python<br/></p>
<p>community and conferences, 12<br/>control flow, 46-50<br/>data analysis with, 2-3, 15-16<br/>essential libraries, 4-8<br/>historical background, 11<br/></p>
<p>import conventions, 14, 36, 88<br/>installation and setup, 8-12<br/>interpreter for, 16<br/>language semantics, 30-38<br/>scalar types, 38-46<br/></p>
<p>python command, 16<br/>Python objects<br/></p>
<p>attributes and methods, 35<br/>converting to strings, 40<br/>defined, 31<br/>formatting, 18<br/>functions as, 72-73<br/>key-value pairs, 61<br/></p>
<p>pytz library, 335<br/></p>
<p>Q<br/>q(uit) debugger command, 490<br/>qcut function, 204, 305, 368<br/>qr function, 118<br/>quantile analysis, 305<br/>quantile method, 160, 296<br/>quarterly period frequencies, 342<br/>question mark (?), 23-24<br/>%quickref magic function, 29<br/>quicksort method, 474<br/>quotation marks in strings, 39<br/></p>
<p>R<br/>r character prefacing quotes, 41<br/>R language, 5, 8, 192<br/>radd method, 149<br/>rand function, 119<br/>randint function, 119<br/>randn function, 99, 119<br/>random module, 118-122<br/>random number generation, 118-119<br/>random sampling and permutation, 308<br/>random walks example, 119-122<br/>RandomState class, 119<br/>range function, 48, 90<br/>rank method, 155<br/>ranking data in pandas library, 153-156<br/>ravel method, 453<br/>rc method, 268<br/>rdiv method, 149<br/>re module, 72, 213<br/>read method, 81-82<br/>read-and-write mode for files, 82<br/>read-only mode for files, 82<br/></p>
<p>Index | 517</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>reading data<br/>in Microsoft Excel files, 186-187<br/>in text format, 167-175<br/></p>
<p>readline functionality, 484<br/>readlines method, 82<br/>read_clipboard function, 167<br/>read_csv function, 80, 167, 172, 274, 298<br/>read_excel function, 167, 186<br/>read_feather function, 168<br/>read_fwf function, 167<br/>read_hdf function, 167, 185<br/>read_html function, 167, 180-183<br/>read_json function, 167, 179<br/>read_msgpack function, 167<br/>read_pickle function, 167, 183<br/>read_sas function, 168<br/>read_sql function, 168, 190<br/>read_stata function, 168<br/>read_table function, 167, 172, 176<br/>reduce method, 466<br/>reduceat method, 467<br/>reductions (aggregations), 111<br/>references in Python, 32-34<br/>regplot method, 281<br/>regress function, 312<br/>regular expressions<br/></p>
<p>passes as delimiters, 171<br/>string manipulation and, 213-216<br/></p>
<p>reindex method, 136-138, 145, 157, 352<br/>reload function, 499<br/>remove method, 56, 66<br/>remove_categories method, 372<br/>remove_unused_categories method, 372<br/>rename method, 202<br/>rename_categories method, 372<br/>reorder_categories method, 372<br/>repeat function, 457<br/>repeat method, 219<br/>replace method, 200, 212-213, 219<br/>requests package, 187<br/>resample method, 327, 348-351, 377<br/>resampling<br/></p>
<p>defined, 348<br/>downsampling and, 348-351<br/>OHLC, 351<br/>upsampling and, 348, 352<br/>with periods, 353<br/></p>
<p>%reset magic function, 29, 485<br/>reset_index method, 250, 302<br/></p>
<p>reshape method, 103, 452<br/>*rest syntax, 54<br/>return statement, 69<br/>reusing command history, 483<br/>reversed function, 61<br/>rfind method, 213<br/>rfloordiv method, 149<br/>right join type, 229<br/>rint function, 107<br/>rjust method, 213<br/>rmul method, 149<br/>rollback method, 333<br/>rollforward method, 333<br/>rolling function, 355, 357<br/>rolling_corr function, 360<br/>row major order (C order), 454, 481<br/>row_stack function, 456<br/>rpow method, 149<br/>rstrip method, 213, 219<br/>rsub method, 149<br/>%run magic function<br/></p>
<p>about, 29<br/>exceptions and, 79<br/>interactive debugger and, 489, 492<br/>IPython and, 17, 25-26<br/>reusing command history with, 483<br/></p>
<p>r_ object, 456<br/></p>
<p>S<br/>%S datetime format, 46, 319<br/>s(tep) debugger command, 490<br/>sample method, 207, 308<br/>save function, 115, 478<br/>savefig method, 267<br/>savez function, 115<br/>savez_compressed function, 116<br/>scalar types in Python, 38-46, 93<br/>scatter plot matrix, 281<br/>scatter plots, 280<br/>scikit-learn library, 7, 397-401<br/>SciPy library, 6<br/>scope of functions, 70<br/>scripting languages, 2<br/>Seabold, Skipper, 8<br/>seaborn library, 269<br/>search method, 214, 216<br/>searching<br/></p>
<p>binary searches of lists, 57<br/>command history, 483<br/></p>
<p>518 | Index</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>searchsorted method, 475<br/>seed function, 119<br/>seek method, 81, 83-84<br/>semantics, language (see language semantics for<br/></p>
<p>Python)<br/>semicolon (;), 31<br/>sentinel value, 171, 191<br/>sequence functions, 59-61<br/>serialization (see storing data)<br/>Series data structure<br/></p>
<p>about, 4, 124-128<br/>duplicate indexes example, 157<br/>grouping with, 294<br/>JSON data and, 180<br/>operations between DataFrame and, 149<br/>plot method arguments, 271<br/>ranking data in, 155<br/>sorting considerations, 154, 473<br/>summary statistics methods for, 161<br/></p>
<p>set comprehensions, 68<br/>set function, 65, 277<br/>set literals, 65<br/>set operations, 65-67, 114<br/>setattr function, 35<br/>setdefault method, 64<br/>setdiff1d method, 115<br/>sets (data structures), 65-67<br/>setxor1d method, 115<br/>set_categories method, 372<br/>set_index method, 248<br/>set_title method, 263, 266<br/>set_trace function, 491<br/>set_value method, 145<br/>set_xlabel method, 263<br/>set_xlim method, 266<br/>set_xticklabels method, 262<br/>set_xticks method, 262<br/>set_ylim method, 266<br/>shape attribute, 88-89, 453<br/>shell commands in IPython, 486<br/>shift method, 332, 351<br/>shifting time series data, 332-334<br/>shuffle function, 119<br/>side effects, 38<br/>sign function, 107, 206<br/>sin function, 107<br/>sinh function, 107<br/>size method, 291<br/>skew method, 160<br/></p>
<p>skipna method, 159<br/>slice method, 219<br/>slice notation, 58<br/>slicing<br/></p>
<p>lists, 58<br/>ndarrays, 94-98<br/>strings, 41<br/></p>
<p>Smith, Nathaniel, 8<br/>Social Security Administration (SSA), 419<br/>software development tools for IPython<br/></p>
<p>about, 487<br/>basic profiling, 494-496<br/>interactive debugger, 488-492<br/>profiling functions line by line, 496-498<br/>timing code, 492-493<br/></p>
<p>solve function, 118<br/>sort method, 57, 60, 74, 113<br/>sorted function, 57, 60<br/>sorting considerations<br/></p>
<p>finding elements in sorted arrays, 475<br/>hierarchical indexing, 224<br/>in-place sorts, 57, 471<br/>indirect sorts, 472<br/>missing data, 154<br/>NumPy library, 113, 471-476<br/>pandas library, 153-156, 473, 476<br/>partially sorting arrays, 474<br/>stable sorting, 474<br/></p>
<p>sort_index method, 153<br/>sort_values method, 154, 473<br/>spaces, structuring code with, 30<br/>split concatenation function, 456<br/>split function, 455<br/>split method, 178, 211, 213-214, 216, 219<br/>split-apply-combine<br/></p>
<p>about, 288<br/>applying, 302-312<br/>filling missing values with group-specific<br/></p>
<p>values, 306<br/>group weighted average and correlation, 310<br/>group-wise linear regression, 312<br/>quantile and bucket analysis, 305<br/>random sampling and permutation, 308<br/>suppressing group keys, 304<br/></p>
<p>SQL (structured query language), 287<br/>SQLAlchemy project, 190<br/>sqlite3 module, 188<br/>sqrt function, 107<br/>square brackets [], 52, 54<br/></p>
<p>Index | 519</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>square function, 107<br/>SSA (Social Security Administration), 419<br/>stable sorting, 474<br/>stack method, 243<br/>stacked format, 246<br/>stacking operation, 227, 236<br/>start index, 58<br/>startswith method, 213, 218<br/>Stata file format, 168<br/>statistical methods, 111-112<br/>statsmodels library<br/></p>
<p>about, 8, 393<br/>estimating linear models, 393-396<br/>estimating time series processes, 396<br/>OLS regression and, 312<br/></p>
<p>std method, 112, 160, 296<br/>step index, 59<br/>stop index, 58<br/>storing data<br/></p>
<p>in binary format, 183-187<br/>in databases, 247<br/>ndarray object, 480<br/></p>
<p>str data type, 39, 43<br/>str function, 40, 43, 319<br/>strftime method, 45, 319<br/>strides/strided view, 449<br/>strings<br/></p>
<p>concatenating, 41<br/>converting between datetime and, 319-321<br/>converting Python objects to, 40<br/>data types for, 39-42<br/>formatting, 41<br/>manipulating, 211-219<br/>methods for, 211-213<br/>regular expressions and, 213-216<br/>slicing, 41<br/>vectorized methods in pandas, 216-219<br/></p>
<p>string_ data type, 91<br/>strip method, 211, 213, 219<br/>strongly typed language, 33<br/>strptime function, 45, 320<br/>structured arrays, 469-471<br/>structured data, 1<br/>sub method, 149, 215, 216<br/>subn method, 216<br/>subplots<br/></p>
<p>about, 255-259<br/>drawing on, 265-267<br/></p>
<p>subplots method, 257<br/></p>
<p>subplots_adjust method, 258<br/>subsetting time series data, 323<br/>subtract function, 107<br/>sum method, 112, 158, 160, 296, 466<br/>summary method, 395<br/>summary statistics<br/></p>
<p>about, 158-160<br/>by level, 225<br/>correlation and covariance, 160-162<br/>methods for, 162-165<br/></p>
<p>svd function, 118<br/>swapaxes method, 105<br/>swapping axes in arrays, 103<br/>symmetric_difference method, 66<br/>symmetric_difference_update method, 66<br/>syntactic sugar, 14<br/>sys module, 81, 175<br/></p>
<p>T<br/>T attribute, 103<br/>tab completion in IPython, 21-23<br/>tabs, structuring code with, 30<br/>take method, 207, 364, 459<br/>tan function, 107<br/>tanh function, 107<br/>Taylor, Jonathan, 8<br/>tell method, 81, 83<br/>ternary expressions, 49<br/>text editors, 11<br/>text files<br/></p>
<p>reading, 167-175<br/>text mode for files, 82-83<br/>writing to, 167-176<br/></p>
<p>text function, 265<br/>TextParser class, 174<br/>tick mark selection in matplotlib, 261-263<br/>tile function, 457<br/>time data type, 44, 319<br/>%time magic function, 29, 492<br/>time module, 318<br/>time series data<br/></p>
<p>about, 317<br/>basics overview, 322-323<br/>date offsets and, 330, 333-334<br/>estimating time series processes, 396<br/>frequences and, 329<br/>frequencies and, 330, 348-354<br/>indexing and, 323<br/>moving window functions, 354-362<br/></p>
<p>520 | Index</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>periods in, 339-347<br/>resampling, 348-354<br/>selecting, 323<br/>shifting, 332-334<br/>subsetting, 323<br/>time zone handling, 335-339<br/>with duplicate indexes, 326<br/></p>
<p>time zones<br/>about, 335<br/>converting data to, 336<br/>localizing data to, 335<br/>operations between different, 339<br/>operations with timestamp objects, 338<br/>USA.gov dataset example, 404-413<br/></p>
<p>time, programmer versus CPU, 3<br/>timedelta data type, 318-319<br/>TimeGrouper object, 378<br/>%timeit magic function, 29, 481, 492<br/>Timestamp object, 322, 333, 338<br/>timestamps<br/></p>
<p>converting periods to/from, 344<br/>defined, 317<br/>operations with time-zone&#8211;aware objects,<br/></p>
<p>338<br/>timezone method, 335<br/>timing code, 492-493<br/>top function, 303<br/>to_csv method, 175<br/>to_datetime method, 320<br/>to_excel method, 187<br/>to_json method, 180<br/>to_period method, 344<br/>to_pickle method, 183<br/>to_timestamp method, 345<br/>trace function, 117<br/>transform method, 373-376<br/>transforming data<br/></p>
<p>about, 197<br/>computing indicator/dummy variables,<br/></p>
<p>208-211<br/>detecting and filtering outliers, 205<br/>discretization and binning, 203<br/>in Patsy formulas, 389<br/>permutation and random sampling, 206<br/>removing duplicates, 197<br/>renaming axis indexes, 201<br/>replacing values, 200<br/>using functions or mapping, 198<br/></p>
<p>transpose method, 103<br/></p>
<p>transposing arrays, 103<br/>truncate method, 325<br/>try/except blocks, 77-79<br/>tuples (data structures)<br/></p>
<p>about, 51<br/>methods for, 54<br/>nested, 53<br/>unpacking, 53<br/></p>
<p>&#8220;two-language&#8221; problem, 3<br/>type casting, 43<br/>type inference in functions, 168<br/>TypeError exception, 78<br/>tzinfo data type, 319<br/>tz_convert method, 336<br/></p>
<p>U<br/>%U datetime format, 46, 320<br/>u(p) debugger command, 490<br/>ufuncs (see universal functions)<br/>uint16 data type, 91<br/>uint32 data type, 91<br/>uint64 data type, 91<br/>uint8 data type, 91<br/>unary universal functions, 106, 107<br/>underscore (_), 22, 54, 451<br/>undescore (_), 484<br/>Unicode standard, 40, 42, 83<br/>unicode_ data type, 91<br/>uniform function, 119<br/>union method, 65-66, 136<br/>union1d method, 115<br/>unique method, 114-115, 136, 162, 164, 363<br/>universal functions<br/></p>
<p>applying and mapping, 151<br/>comprehensive overview, 105-108<br/>creating custom objects with Numba, 478<br/>instance methods, 466-468<br/>writing in Python, 468<br/></p>
<p>unpacking tuples, 53<br/>unstack method, 243<br/>unwrapped group operation, 376<br/>update method, 63, 66<br/>updating packages, 10<br/>upper method, 213, 218<br/>upsampling, 348, 352<br/>US baby names dataset example, 419-434<br/>US Federal Election Commission database<br/></p>
<p>example, 440-448<br/>USA.gov dataset example, 403-413<br/></p>
<p>Index | 521</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>USDA food database example, 434-439<br/>UTC (coordinated universal time), 335<br/>UTF-8 encoding, 83<br/></p>
<p>V<br/>ValueError exception, 77, 92<br/>values attribute, 133<br/>values method, 62, 315<br/>values property, 384<br/>value_count method, 203<br/>value_counts method, 162, 274, 363<br/>var method, 112, 160, 296<br/>variables<br/></p>
<p>dummy, 208-211, 372, 386, 391<br/>function scope and, 70<br/>in Python, 32-34<br/>indicator, 208-211<br/>input, 484<br/>output, 484<br/>shell commands and, 486<br/></p>
<p>vectorization, 93<br/>vectorize function, 468, 478<br/>vectorized string methods in pandas, 216-219<br/>visualization tools, 285<br/>vsplit function, 456<br/>vstack function, 455<br/></p>
<p>W<br/>%w datetime format, 46, 319<br/>%W datetime format, 46, 320<br/>w(here) debugger command, 490<br/>Waskom, Michael, 269<br/>Wattenberg, Laura, 430<br/>Web APIs, pandas interacting with, 187<br/>Web scraping, 180-183<br/>where function, 109, 241<br/>while loops, 48<br/>whitespace<br/></p>
<p>regular expression describing, 214<br/></p>
<p>structuring code with, 30<br/>trimming around figures, 267<br/></p>
<p>%who magic function, 29<br/>%whos magic function, 29<br/>%who_ls magic function, 29<br/>Wickham, Hadley, 184, 288, 419<br/>wildcard expressions, 24<br/>Williams, Ashley, 434<br/>Windows, setting up Python on, 9<br/>with statement, 81<br/>wrangling (see data wrangling)<br/>write method, 82<br/>write-only mode for files, 82<br/>writelines method, 82-83<br/>writing data in text format, 167-176<br/></p>
<p>X<br/>%x datetime format, 321<br/>%X datetime format, 321<br/>%xdel magic function, 29, 485<br/>xlim method, 262<br/>xlrd package, 186<br/>XLS files, 186<br/>XLSX files, 186<br/>XML files, 180-183<br/>%xmode magic function, 79<br/></p>
<p>Y<br/>%Y datetime format, 45, 319<br/>%y datetime format, 45, 319<br/>yield keyword, 75<br/></p>
<p>Z<br/>%z datetime format, 46, 320<br/>&quot;zero-copy&quot; array views, 450<br/>zeros function, 89-90<br/>zeros_like function, 90<br/>zip function, 60<br/></p>
<p>522 | Index</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>About the Author<br/><b>Wes McKinney</b> is a New York-based software developer and entrepreneur. After fin&#8208;<br/>ishing his undergraduate degree in mathematics at MIT in 2007, he went on to do<br/>quantitative finance work at AQR Capital Management in Greenwich, CT. Frustrated<br/>by cumbersome data analysis tools, he learned Python and started building what<br/>would later become the pandas project. He&#8217;s now an active member of the Python<br/>data community and is an advocate for the use of Python in data analysis, finance,<br/>and statistical computing applications.<br/>Wes was later the cofounder and CEO of DataPad, whose technology assets and team<br/>were acquired by Cloudera in 2014. He has since become involved in big data tech&#8208;<br/>nology, joining the Project Management Committees for the Apache Arrow and<br/>Apache Parquet projects in the Apache Software Foundation. In 2016, he joined Two<br/>Sigma Investments in New York City, where he continues working to make data anal&#8208;<br/>ysis faster and easier through open source software.<br/></p>
<p>Colophon<br/>The animal on the cover of <i>Python for Data Analysis</i> is a golden-tailed, or pen-tailed,<br/>tree shrew (<i>Ptilocercus lowii</i>). The golden-tailed tree shrew is the only one of its spe&#8208;<br/>cies in the genus <i>Ptilocercus</i> and family <i>Ptilocercidae</i>; all the other tree shrews are of<br/>the family <i>Tupaiidae</i>. Tree shrews are identified by their long tails and soft red-brown<br/>fur. As nicknamed, the golden-tailed tree shrew has a tail that resembles the feather<br/>on a quill pen. Tree shrews are omnivores, feeding primarily on insects, fruit, seeds,<br/>and small vertebrates.<br/>Found predominantly in Indonesia, Malaysia, and Thailand, these wild mammals are<br/>known for their chronic consumption of alcohol. Malaysian tree shrews were found<br/>to spend several hours consuming the naturally fermented nectar of the bertam palm,<br/>equalling about 10 to 12 glasses of wine with 3.8% alcohol content. Despite this, no<br/>golden-tailed tree shrew has ever been intoxicated, thanks largely to their impressive<br/>ability to break down ethanol, which includes metabolizing the alcohol in a way not<br/>used by humans. Also more impressive than any of their mammal counterparts,<br/>including humans? Brain-to-body mass ratio.<br/>Despite these mammals&#8217; name, the golden-tailed shrew is not a true shrew, instead<br/>more closely related to primates. Because of their close relation, tree shrews have<br/>become an alternative to primates in medical experimentation for myopia, psychoso&#8208;<br/>cial stress, and hepatitis.<br/>The cover image is from <i>Cassell&#8217;s Natural History</i>. The cover fonts are URW Type&#8208;<br/>writer and Guardian Sans. The text font is Adobe Minion Pro; the heading font is<br/>Adobe Myriad Condensed; and the code font is Dalton Maag&#8217;s Ubuntu Mono.</p>

</div></div>
</body></html>
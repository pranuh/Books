<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html><head><title>Fundamentals of Data Engineering</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
</head>
<body>
<div style="page-break-before:always; page-break-after:always"><div><p> <br/>    </p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><b>Fundamentals of Data<br/>Engineering<br/></b></p>
<p>Plan and Build Robust Data Systems<br/></p>
<p><b>Joe Reis and Matt Housley</b></p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><b>Fundamentals of Data Engineering<br/></b>by Joe Reis and Matt Housley<br/>Copyright &#169; 2022 Joseph Reis and Matthew Housley. All rights reserved.<br/>Printed in the United States of America.<br/>Published by O&#8217;Reilly Media, Inc., 1005 Gravenstein Highway North,<br/>Sebastopol, CA 95472.<br/>O&#8217;Reilly books may be purchased for educational, business, or sales<br/>promotional use. Online editions are also available for most titles<br/>(<i>http://oreilly.com</i>). For more information, contact our<br/>corporate/institutional sales department: 800-998-9938 or<br/><i>corporate@oreilly.com</i>.<br/></p>
<p>Acquisitions Editor: Jessica Haberman<br/></p>
<p>Development Editor: Michele Cronin<br/></p>
<p>Production Editor: Gregory Hyman<br/></p>
<p>Copyeditor: Sharon Wilkey<br/></p>
<p>Proofreader: Amnet Systems, LLC<br/></p>
<p>Indexer: Judith McConville<br/></p>
<p>Interior Designer: David Futato<br/></p>
<p>Cover Designer: Karen Montgomery<br/></p>
<p>Illustrator: Kate Dullea<br/>June 2022: First Edition<br/></p>
<p><b>Revision History for the First Edition</b></p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>2022-06-22: First Release<br/>See <i>http://oreilly.com/catalog/errata.csp?isbn=9781098108304</i> for release<br/>details.<br/>The O&#8217;Reilly logo is a registered trademark of O&#8217;Reilly Media, Inc.<br/><i>Fundamentals of Data Engineering</i>, the cover image, and related trade<br/>dress are trademarks of O&#8217;Reilly Media, Inc.<br/>The views expressed in this work are those of the authors, and do not<br/>represent the publisher&#8217;s views. While the publisher and the authors have<br/>used good faith efforts to ensure that the information and instructions<br/>contained in this work are accurate, the publisher and the authors disclaim<br/>all responsibility for errors or omissions, including without limitation<br/>responsibility for damages resulting from the use of or reliance on this<br/>work. Use of the information and instructions contained in this work is at<br/>your own risk. If any code samples or other technology this work contains<br/>or describes is subject to open source licenses or the intellectual property<br/>rights of others, it is your responsibility to ensure that your use thereof<br/>complies with such licenses and/or rights.<br/>978-1-098-10830-4<br/>[LSI]</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><b>Preface<br/></b>How did this book come about? The origin is deeply rooted in our journey<br/>from data science into data engineering. We often jokingly refer to<br/>ourselves as <i>recovering data scientists</i>. We both had the experience of being<br/>assigned to data science projects, then struggling to execute these projects<br/>due to a lack of proper foundations. Our journey into data engineering<br/>began when we undertook data engineering tasks to build foundations and<br/>infrastructure.<br/>With the rise of data science, companies splashed out lavishly on data<br/>science talent, hoping to reap rich rewards. Very often, data scientists<br/>struggled with basic problems that their background and training did not<br/>address&#8212;data collection, data cleansing, data access, data transformation,<br/>and data infrastructure. These are problems that data engineering aims to<br/>solve.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><b>What This Book Isn&#8217;t<br/></b>Before we cover what this book is about and what you&#8217;ll get out of it, let&#8217;s<br/>quickly cover what this book <i>isn&#8217;t</i>. This book isn&#8217;t about data engineering<br/>using a particular tool, technology, or platform. While many excellent<br/>books approach data engineering technologies from this perspective, these<br/>books have a short shelf life. Instead, we try to focus on the fundamental<br/>concepts behind data engineering.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><b>What This Book Is About<br/></b>This book aims to fill a gap in current data engineering content and<br/>materials. While there&#8217;s no shortage of technical resources that address<br/>specific data engineering tools and technologies, people struggle to<br/>understand how to assemble these components into a coherent whole that<br/>applies in the real world. This book connects the dots of the end-to-end data<br/>lifecycle. It shows you how to stitch together various technologies to serve<br/>the needs of downstream data consumers such as analysts, data scientists,<br/>and machine learning engineers. This book works as a complement to<br/>O&#8217;Reilly books that cover the details of particular technologies, platforms<br/>and programming languages.<br/>The big idea of this book is the <i>data engineering lifecycle</i>: data generation,<br/>storage, ingestion, transformation, and serving Since the dawn of data,<br/>we&#8217;ve seen the rise and fall of innumerable specific technologies and<br/>vendor products, but the data engineering life cycle stages have remained<br/>essentially unchanged. With this framework, the reader will come away<br/>with a sound understanding for applying technologies to real-world<br/>business problems.<br/>Our goal here is to map out principles that reach across two axes. First, we<br/>wish to distill data engineering into principles that can encompass <i>any<br/>relevant technology</i>. Second, we wish to present principles that will stand<br/>the test of <i>time</i>. We hope that these ideas reflect lessons learned across the<br/>data technology upheaval of the last twenty years and that our mental<br/>framework will remain useful for a decade or more into the future.<br/>One thing to note: we unapologetically take a cloud-first approach. We view<br/>the cloud as a fundamentally transformative development that will endure<br/>for decades; most on-premises data systems and workloads will eventually<br/>move to cloud hosting. We assume that infrastructure and systems are<br/><i>ephemeral</i> and <i>scalable</i>, and that data engineers will lean toward deploying<br/>managed services in the cloud. That said, most concepts in this book will<br/>translate to non-cloud environments.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><b>Who Should Read This Book<br/></b>Our primary intended audience for this book consists of technical<br/>practitioners, mid- to senior-level software engineers, data scientists, or<br/>analysts interested in moving into data engineering; or data engineers<br/>working in the guts of specific technologies, but wanting to develop a more<br/>comprehensive perspective. Our secondary target audience consists of data<br/>stakeholders who work adjacent to technical practitioners&#8212;e.g., a data team<br/>lead with a technical background overseeing a team of data engineers, or a<br/>director of data warehousing wanting to migrate from on-premises<br/>technology to a cloud-based solution.<br/>Ideally, you&#8217;re curious and want to learn&#8212;why else would you be reading<br/>this book? You stay current with data technologies and trends by reading<br/>books and articles on data warehousing/data lakes, batch and streaming<br/>systems, orchestration, modeling, management, analysis, developments in<br/>cloud technologies, etc. This book will help you weave what you&#8217;ve read<br/>into a complete picture of data engineering across technologies and<br/>paradigms.<br/></p>
<p><b>Prerequisites<br/></b>We assume a good deal of familiarity with the types of data systems found<br/>in a corporate setting. In addition, we assume that readers have some<br/>familiarity with SQL and Python (or some other programming language),<br/>and experience with cloud services.<br/>Numerous resources are available for aspiring data engineers to practice<br/>Python and SQL. Free online resources abound (blog posts, tutorial sites,<br/>YouTube videos), and many new Python books are published every year.<br/>The cloud provides unprecedented opportunities to get hands-on experience<br/>with data tools. We suggest that aspiring data engineers set up accounts<br/>with cloud services such as AWS, Azure, Google Cloud Platform,<br/>Snowflake, Databricks, etc. Note that many of these platforms have <i>free tier</i></p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>options, but readers should keep a close eye on costs, and work with small<br/>quantities of data and single node clusters as they study.<br/>Developing familiarity with corporate data systems outside of a corporate<br/>environment remains difficult and this creates certain barriers for aspiring<br/>data engineers who have yet to land their first data job. This book can help.<br/>We suggest that data novices read for high level ideas, and then look at<br/>materials in the <i>additional resources</i> section at the end of each chapter. On a<br/>second read through, note any unfamiliar terms and technologies. You can<br/>utilize Google, Wikipedia, blog posts, YouTube videos, and vendor sites to<br/>become familiar with new terms and fill gaps in your understanding.<br/></p>
<p><b>What You&#8217;ll Learn and How It Will Improve<br/>Your Abilities<br/></b>This book aims to help you build a solid foundation for solving real world<br/>data engineering problems.<br/>By the end of this book you will understand:<br/></p>
<p>How data engineering impacts your current role (data scientist,<br/>software engineer, or data team lead).<br/>How to cut through the marketing hype and choose the right<br/>technologies, data architecture, and processes.<br/>How to use the data engineering lifecycle to design and build a robust<br/>architecture.<br/>Best practices for each stage of the data lifecycle.<br/></p>
<p>And you will be able to:<br/>Incorporate data engineering principles in your current role (data<br/>scientist, analyst, software engineer, data team lead, etc.)<br/>Stitch together a variety of cloud technologies to serve the needs of<br/>downstream data consumers.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Assess data engineering problems with an end-to-end framework of<br/>best practices<br/>Incorporate data governance and security across the data engineering<br/>lifecycle.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><b>The Book Outline<br/></b>This book is composed of four parts:<br/></p>
<p>Part I, &#8220;Foundation and Building Blocks&#8221;<br/>Part II, &#8220;The Data Engineering Lifecycle in Depth&#8221;<br/>Part III, &#8220;Security, Privacy, and the Future of Data Engineering&#8221;<br/>Appendices A and B: cloud networking, serialization and compression<br/></p>
<p>In Part I, we begin by defining data engineering in Chapter 1, then map out<br/>the data engineering lifecycle in Chapter 2. In Chapter 3, we discuss <i>good<br/>architecture</i>. In Chapter 4, we introduce a framework for choosing the right<br/>technology&#8212;while we frequently see technology and architecture<br/>conflated, these are in fact very different topics.<br/>Part II builds on Chapter 2 to cover the data engineering lifecycle in depth;<br/>each lifecycle stage&#8212;data generation, storage, ingestion, transformation<br/>and serving&#8212;is covered in its own chapter. Part II is arguably the heart of<br/>the book, and the other chapters exist to support the core ideas covered<br/>here.<br/>Part III covers additional topics. In Chapter 10, we discuss <i>security and<br/>privacy</i>. While security has always been an important part of the data<br/>engineering profession, it has only become more critical with the rise of for<br/>profit hacking and state sponsored cyber attacks. And what can we say of<br/>privacy? The era of corporate privacy nihilism is over&#8212;no company wants<br/>to see its name appear in the headline of an article on sloppy privacy<br/>practices. Reckless handling of personal data can also have significant legal<br/>ramifications with the advent of GDPR, CCPA and other regulations. In<br/>short, security and privacy must be top priorities in any data engineering<br/>work.<br/>In the course of working in data engineering, doing research for this book<br/>and interviewing numerous experts, we thought a good deal about where<br/>the field is going in the near and long term. Chapter 11 outlines our highly</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>speculative ideas on the future of data engineering. By its nature, the future<br/>is a slippery thing. Time will tell if some of our ideas are correct. We would<br/>love to hear from our readers on how their visions of the future agree with<br/>or differ from our own.<br/>In the appendix, we cover a handful of technical topics that are extremely<br/>relevant to the day to day practice of data engineering, but didn&#8217;t fit into the<br/>main body of the text. Specifically, cloud networking is a critical topic as<br/>data engineering shifts into the cloud, and engineers need to understand<br/>serialization and compression both to work directly with data files, and to<br/>assess performance considerations in data systems.<br/></p>
<p><b>Conventions Used in This Book<br/></b>The following typographical conventions are used in this book:<br/><i>Italic<br/></i></p>
<p>Indicates new terms, URLs, email addresses, filenames, and file<br/>extensions.<br/></p>
<p><i>Constant width<br/></i>Used for program listings, as well as within paragraphs to refer to<br/>program elements such as variable or function names, databases, data<br/>types, environment variables, statements, and keywords.<br/></p>
<p><b><i>Constant width bold<br/></i></b></p>
<p>Shows commands or other text that should be typed literally by the user.<br/><i>Constant width italic<br/></i></p>
<p>Shows text that should be replaced with user-supplied values or by<br/>values determined by context.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><b>TIP<br/></b>This element signifies a tip or suggestion.<br/></p>
<p><b>NOTE<br/></b>This element signifies a general note.<br/></p>
<p><b>WARNING<br/></b>This element indicates a warning or caution.<br/></p>
<p><b>How to Contact Us<br/></b>Please address comments and questions concerning this book to the<br/>publisher:<br/></p>
<p>O&#8217;Reilly Media, Inc.<br/></p>
<p>1005 Gravenstein Highway North<br/></p>
<p>Sebastopol, CA 95472<br/></p>
<p>800-998-9938 (in the United States or Canada)<br/></p>
<p>707-829-0515 (international or local)<br/></p>
<p>707-829-0104 (fax)<br/>We have a web page for this book, where we list errata, examples, and any<br/>additional information. You can access this page at<br/>https://oreil.ly/fundamentals-of-data.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Email <i>bookquestions@oreilly.com</i> to comment or ask technical questions<br/>about this book.<br/>For news and information about our books and courses, visit<br/>https://oreilly.com.<br/>Find us on LinkedIn: https://linkedin.com/company/oreilly-media<br/>Follow us on Twitter: https://twitter.com/oreillymedia<br/>Watch us on YouTube: https://www.youtube.com/oreillymedia<br/></p>
<p><b>Acknowledgments<br/></b>When we started writing this book, we were warned by many people that<br/>we faced a hard task. A book like this has a lot of moving parts, and due to<br/>its comprehensive view of the field of data engineering, it required a ton of<br/>research, interviews, discussions, and deep thinking. We won&#8217;t claim to<br/>have captured every nuance of data engineering, but we hope that the<br/>results resonate with you. Numerous individuals contributed to our efforts,<br/>and we&#8217;re grateful for the support we received from many experts.<br/>First, thanks to our amazing crew of technical reviewers. They slogged<br/>through many readings, and gave invaluable (and often ruthlessly blunt)<br/>feedback. This book would be a fraction of itself without their efforts. In no<br/>particular order, we give endless thanks to Bill Inmon, Andy Petrella, Matt<br/>Sharp, Tod Hanseman, Chris Tabb, Danny Lebzyon, Martin Kleppman,<br/>Scott Lorimor, Nick Schrock, Lisa Steckman, and Alex Woolford.<br/>Second, we&#8217;ve had a unique opportunity to talk with the leading experts in<br/>the field of data on our live shows, podcasts, meetups, and endless private<br/>calls. Their ideas helped shape our book. There are too many people to<br/>name individually, but we&#8217;d like to give shoutouts to Bill Inmon, Jordan<br/>Tigani, Zhamak Dehghani, Shruti Bhat, Eric Tschetter, Benn Stancil, Kevin<br/>Hu, Michael Rogove, Ryan Wright, Egor Gryaznov, Chad Sanderson, Julie<br/>Price, Matt Turck, Monica Rogati, Mars Lan, Pardhu Gunnam, Brian Suk,<br/>Barr Moses, Lior Gavish, Bruno Aziza, Gian Merlino, DeVaris Brown,</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Todd Beauchene, Tudor Girba, Scott Taylor, Ori Rafael, Lee Edwards,<br/>Bryan Offutt, Ollie Hughes, Gilbert Eijkelenboom, Chris Bergh, Fabiana<br/>Clemente, Andreas Kretz, Ori Reshef, Nick Singh, Mark Balkenende,<br/>Kenten Danas, Brian Olsen, Lior Gavish, Rhaghu Murthy, Greg Coquillo,<br/>David Aponte, Demetrios Brinkmann, Sarah Catanzaro, Michel Tricot, Levi<br/>Davis, Ted Walker, Carlos Kemeny, Josh Benamram, Chanin<br/>Nantasenamat, George Firican, Jordan Goldmeir, Minhaaj Rehmam, Luigi<br/>Patruno, Vin Vashista, Danny Ma, Jesse Anderson, Alessya Visnjic, Vishal<br/>Singh, Dave Langer, Roy Hasson, Todd Odess, Che Sharma, Scott<br/>Breitenother, Ben Taylor, Thom Ives, John Thompson, Brent Dykes, Josh<br/>Tobin, Mark Kosiba, Tyler Pugliese, Douwe Maan, Martin Traverso, Curtis<br/>Kowalski, Bob Davis, Koo Ping Shung, Ed Chenard, Matt Sciorma, Tyler<br/>Folkman, Jeff Baird, Tejas Manohar, Paul Singman, Kevin Stumpf, Willem<br/>Pineaar, and Michael Del Balso from Tecton, Emma Dahl, Harpreet Sahota,<br/>Ken Jee, Scott Taylor, Kate Strachnyi, Kristen Kehrer, Taylor Miller, Abe<br/>Gong, Ben Castleton, Ben Rogojan, David Mertz, Emmanuel Raj, Andrew<br/>Jones, Avery Smith, Brock Cooper, Jeff Larson, Jon King, Holden<br/>Ackerman, Miriah Peterson, Felipe Hoffa, David Gonzalez, Richard<br/>Wellman, Susan Walsh, Ravit Jain, Lauren Balik, Mikiko Bazeley, Mark<br/>Freeman, Mike Wimmer, Alexey Shchedrin, Mary Clair Thompson, Julie<br/>Burroughs, Jason Pedley, Freddy Drennan, Jake Carter, Jason Pedley, Kelly<br/>and Matt Phillipps, Brian Campbell, Faris Chebib, Dylan Gregerson, Ken<br/>Myers, and many others.<br/>If you&#8217;re not mentioned specifically, don&#8217;t take it personally. You know<br/>who you are. Let us know and we&#8217;ll get you on the next edition.<br/>We&#8217;d also like to thank the Ternary Data team, our students, and the<br/>countless people around the world who&#8217;ve supported us. It&#8217;s a great<br/>reminder the world is a very small place.<br/>Working with the O&#8217;Reilly crew was amazing! Special thanks to Jess<br/>Haberman for having confidence in us during the book proposal process,<br/>our amazing and extremely patient development editors Nicole Tach&#233; and<br/>Michele Cronin for invaluable editing, feedback and support. Thank you<br/>also to the superb production crew at O&#8217;Reilly (Greg and crew).</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Joe would like to thank his family&#8212;Cassie, Milo, and Ethan&#8212;for letting<br/>him write a book. They had to endure a ton, and Joe promises to never write<br/>another book again ;)<br/>Matt would like to thank his friends and family for their enduring patience<br/>and support. He&#8217;s still hopeful that Seneca will deign to give a five star<br/>review after a good deal of toil and missed family time around the holidays.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><b>Part I. Foundation and Building<br/>Blocks</b></p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><b>Chapter 1. Data Engineering<br/>Described<br/></b>If you work in data or software, you may have noticed data engineering<br/>emerging from the shadows and now sharing the stage with data science.<br/>Data engineering is one of the hottest fields in data and technology, and for<br/>a good reason. It builds the foundation for data science and analytics in<br/>production. This chapter explores what data engineering is, how the field<br/>was born and its evolution, the skills of data engineers, and with whom they<br/>work.<br/></p>
<p><b>What Is Data Engineering?<br/></b>Despite the current popularity of data engineering, there&#8217;s a lot of confusion<br/>about what data engineering means and what data engineers do. Data<br/>engineering has existed in some form since companies started doing things<br/>with data&#8212;such as predictive analysis, descriptive analytics, and reports&#8212;<br/>and came into sharp focus alongside the rise of data science in the 2010s.<br/>For the purpose of this book, it&#8217;s critical to define what <i>data engineering<br/></i>and <i>data engineer</i> mean.<br/>First, let&#8217;s look at the landscape of how data engineering is described and<br/>develop some terminology we can use throughout this book. Endless<br/>definitions of <i>data engineering</i> exist. In early 2022, a Google exact-match<br/>search for &#8220;what is data engineering?&#8221; returns over 91,000 unique results.<br/>Before we give our definition, here are a few examples of how some<br/>experts in the field define data engineering:</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><i>Data engineering is a set of operations aimed at creating interfaces and<br/>mechanisms for the flow and access of information. It takes dedicated<br/>specialists&#8212;data engineers&#8212;to maintain data so that it remains<br/>available and usable by others. In short, data engineers set up and<br/>operate the organization&#8217;s data infrastructure, preparing it for further<br/>analysis by data analysts and scientists.<br/></i></p>
<p>&#8212;From &#8220;Data Engineering and Its Main Concepts&#8221; by<br/>AlexSoft<br/></p>
<p><i>The first type of data engineering is SQL-focused. The work and primary<br/>storage of the data is in relational databases. All of the data processing<br/>is done with SQL or a SQL-based language. Sometimes, this data<br/>processing is done with an ETL tool.  The second type of data<br/>engineering is Big Data&#8211;focused. The work and primary storage of the<br/>data is in Big Data technologies like Hadoop, Cassandra, and HBase. All<br/>of the data processing is done in Big Data frameworks like MapReduce,<br/>Spark, and Flink. While SQL is used, the primary processing is done with<br/>programming languages like Java, Scala, and Python.<br/></i></p>
<p>&#8212;Jesse Anderson<br/><i>In relation to previously existing roles, the data engineering field could<br/>be thought of as a superset of business intelligence and data warehousing<br/>that brings more elements from software engineering. This discipline also<br/>integrates specialization around the operation of so-called &#8220;big data&#8221;<br/>distributed systems, along with concepts around the extended Hadoop<br/>ecosystem, stream processing, and in computation at scale.<br/></i></p>
<p>&#8212;Maxime Beauchemin<br/><i>Data engineering is all about the movement, manipulation, and<br/>management of data.<br/></i></p>
<p>&#8212;Lewis Gavin<br/>Wow! It&#8217;s entirely understandable if you&#8217;ve been confused about data<br/>engineering. That&#8217;s only a handful of definitions, and they contain an<br/>enormous range of opinions about the meaning of <i>data engineering</i>.<br/></p>
<p>1<br/></p>
<p><i>2<br/></i></p>
<p>3<br/></p>
<p>4<br/></p>
<p>5</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><b>Data Engineering Defined<br/></b>When we unpack the common threads of how various people define data<br/>engineering, an obvious pattern emerges: a data engineer gets data, stores it,<br/>and prepares it for consumption by data scientists, analysts, and others. We<br/>define <i>data engineering</i> and <i>data engineer</i> as follows:<br/></p>
<p><i>Data engineering is the development, implementation, and maintenance<br/>of systems and processes that take in raw data and produce high-quality,<br/>consistent information that supports downstream use cases, such as<br/>analysis and machine learning. Data engineering is the intersection of<br/>security, data management, DataOps, data architecture, orchestration,<br/>and software engineering. A data engineer manages the data engineering<br/>lifecycle, beginning with getting data from source systems and ending<br/>with serving data for use cases, such as analysis or machine learning.<br/></i></p>
<p><b>The Data Engineering Lifecycle<br/></b>It is all too easy to fixate on technology and miss the bigger picture<br/>myopically. This book centers around a big idea called the <i>data engineering<br/>lifecycle</i> (Figure 1-1), which we believe gives data engineers the holistic<br/>context to view their role.<br/></p>
<p><i>Figure 1-1. The data engineering lifecycle</i></p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>The data engineering lifecycle shifts the conversation away from<br/>technology and toward the data itself and the end goals that it must serve.<br/>The stages of the data engineering lifecycle are as follows:<br/></p>
<p>Generation<br/>Storage<br/>Ingestion<br/>Transformation<br/>Serving<br/></p>
<p>The data engineering lifecycle also has a notion of <i>undercurrents&#8212;</i>critical<br/>ideas across the entire lifecycle. These include security, data management,<br/>DataOps, data architecture, orchestration, and software engineering. We<br/>cover the data engineering lifecycle and its undercurrents more extensively<br/>in Chapter 2. Still, we introduce it here because it is essential to our<br/>definition of data engineering and the discussion that follows in this<br/>chapter.<br/>Now that you have a working definition of data engineering and an<br/>introduction to its lifecycle, let&#8217;s take a step back and look at a bit of<br/>history.<br/></p>
<p><b>Evolution of the Data Engineer<br/></b><i>History doesn&#8217;t repeat itself, but it rhymes.<br/></i></p>
<p>&#8212;A famous adage often attributed to Mark Twain<br/>Understanding data engineering today and tomorrow requires a context of<br/>how the field evolved. This section is not a history lesson, but looking at the<br/>past is invaluable in understanding where we are today and where things are<br/>going. A common theme constantly reappears: what&#8217;s old is new again.<br/><b>The early days: 1980 to 2000, from data warehousing to the web</b></p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>The birth of the data engineer arguably has its roots in data warehousing,<br/>dating as far back as the 1970s, with the <i>business data warehouse</i> taking<br/>shape in the 1980s and Bill Inmon officially coining the term <i>data<br/>warehouse</i> in 1990. After engineers at IBM developed the relational<br/>database and Structured Query Language (SQL), Oracle popularized the<br/>technology. As nascent data systems grew, businesses needed dedicated<br/>tools and data pipelines for reporting and business intelligence (BI). To help<br/>people correctly model their business logic in the data warehouse, Ralph<br/>Kimball and Inmon developed their respective eponymous data-modeling<br/>techniques and approaches, which are still widely used today.<br/>Data warehousing ushered in the first age of scalable analytics, with new<br/>massively parallel processing (MPP) databases that use multiple processors<br/>to crunch large amounts of data coming on the market and supporting<br/>unprecedented volumes of data. Roles such as BI engineer, ETL developer,<br/>and data warehouse engineer addressed the various needs of the data<br/>warehouse. Data warehouse and BI engineering were a precursor to today&#8217;s<br/>data engineering and still play a central role in the discipline.<br/>The internet went mainstream around the mid-1990s, creating a whole new<br/>generation of web-first companies such as AOL, Yahoo, and Amazon. The<br/>dot-com boom spawned a ton of activity in web applications and the<br/>backend systems to support them&#8212;servers, databases, and storage. Much of<br/>the infrastructure was expensive, monolithic, and heavily licensed. The<br/>vendors selling these backend systems likely didn&#8217;t foresee the sheer scale<br/>of the data that web applications would produce.<br/><b>The early 2000s: The birth of contemporary data engineering<br/></b>Fast-forward to the early 2000s, when the dot-com boom of the late &#8217;90s<br/>went bust, leaving behind a tiny cluster of survivors. Some of these<br/>companies, such as Yahoo, Google, and Amazon, would grow into<br/>powerhouse tech companies. Initially, these companies continued to rely on<br/>the traditional monolithic, relational databases and data warehouses of the<br/>1990s, pushing these systems to the limit. As these systems buckled,<br/>updated approaches were needed to handle data growth. The new</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>generation of the systems must be cost-effective, scalable, available, and<br/>reliable.<br/>Coinciding with the explosion of data, commodity hardware&#8212;such as<br/>servers, RAM, disks, and flash drives&#8212;also became cheap and ubiquitous.<br/>Several innovations allowed distributed computation and storage on<br/>massive computing clusters at a vast scale. These innovations started<br/>decentralizing and breaking apart traditionally monolithic services. The<br/>&#8220;big data&#8221; era had begun.<br/>The <i>Oxford English Dictionary</i> defines big data as &#8220;extremely large data<br/>sets that may be analyzed computationally to reveal patterns, trends, and<br/>associations, especially relating to human behavior and interactions.&#8221;<br/>Another famous and succinct description of big data is the three <i>V</i>&#8217;s of data:<br/>velocity, variety, and volume.<br/>In 2003, Google published a paper on the Google File System, and shortly<br/>after that, in 2004, a paper on MapReduce, an ultra-scalable data-processing<br/>paradigm. In truth, big data has earlier antecedents in MPP data warehouses<br/>and data management for experimental physics projects, but Google&#8217;s<br/>publications constituted a &#8220;big bang&#8221; for data technologies and the cultural<br/>roots of data engineering as we know it today. You&#8217;ll learn more about MPP<br/>systems and MapReduce in Chapters 3 and 8, respectively.<br/>The Google papers inspired engineers at Yahoo to develop and later open<br/>source Apache Hadoop in 2006.  It&#8217;s hard to overstate the impact of<br/>Hadoop. Software engineers interested in large-scale data problems were<br/>drawn to the possibilities of this new open source technology ecosystem. As<br/>companies of all sizes and types saw their data grow into many terabytes<br/>and even petabytes, the era of the big data engineer was born.<br/>Around the same time, Amazon had to keep up with its own exploding data<br/>needs and created elastic computing environments (Amazon Elastic<br/>Compute Cloud, or EC2), infinitely scalable storage systems (Amazon<br/>Simple Storage Service, or S3), highly scalable NoSQL databases (Amazon<br/>DynamoDB), and many other core data building blocks.  Amazon elected<br/>to offer these services for internal and external consumption through<br/></p>
<p>6<br/></p>
<p>7</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><i>Amazon Web Services</i> (AWS), becoming the first popular public cloud.<br/>AWS created an ultra-flexible pay-as-you-go resource marketplace by<br/>virtualizing and reselling vast pools of commodity hardware. Instead of<br/>purchasing hardware for a data center, developers could simply rent<br/>compute and storage from AWS.<br/>As AWS became a highly profitable growth engine for Amazon, other<br/>public clouds would soon follow, such as Google Cloud, Microsoft Azure,<br/>and DigitalOcean. The public cloud is arguably one of the most significant<br/>innovations of the 21st century and spawned a revolution in the way<br/>software and data applications are developed and deployed.<br/>The early big data tools and public cloud laid the foundation for today&#8217;s<br/>data ecosystem. The modern data landscape&#8212;and data engineering as we<br/>know it now&#8212;would not exist without these innovations.<br/><b>The 2000s and 2010s: Big data engineering<br/></b>Open source big data tools in the Hadoop ecosystem rapidly matured and<br/>spread from Silicon Valley to tech-savvy companies worldwide. For the<br/>first time, any business had access to the same bleeding-edge data tools<br/>used by the top tech companies. Another revolution occurred with the<br/>transition from batch computing to event streaming, ushering in a new era<br/>of big &#8220;real-time&#8221; data. You&#8217;ll learn about batch and event streaming<br/>throughout this book.<br/>Engineers could choose the latest and greatest&#8212;Hadoop, Apache Pig,<br/>Apache Hive, Dremel, Apache HBase, Apache Storm, Apache Cassandra,<br/>Apache Spark, Presto, and numerous other new technologies that came on<br/>the scene. Traditional enterprise-oriented and GUI-based data tools<br/>suddenly felt outmoded, and code-first engineering was in vogue with the<br/>ascendance of MapReduce. We (the authors) were around during this time,<br/>and it felt like old dogmas died a sudden death upon the altar of big data.<br/>The explosion of data tools in the late 2000s and 2010s ushered in the <i>big<br/>data engineer</i>. To effectively use these tools and techniques&#8212;namely, the<br/>Hadoop ecosystem including Hadoop, YARN, Hadoop Distributed File</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>System (HDFS), and MapReduce&#8212;big data engineers had to be proficient<br/>in software development and low-level infrastructure hacking, but with a<br/>shifted emphasis. Big data engineers typically maintained massive clusters<br/>of commodity hardware to deliver data at scale. While they might<br/>occasionally submit pull requests to Hadoop core code, they shifted their<br/>focus from core technology development to data delivery.<br/>Big data quickly became a victim of its own success. As a buzzword, <i>big<br/>data</i> gained popularity during the early 2000s through the mid-2010s. Big<br/>data captured the imagination of companies trying to make sense of the<br/>ever-growing volumes of data and the endless barrage of shameless<br/>marketing from companies selling big data tools and services. Because of<br/>the immense hype, it was common to see companies using big data tools for<br/>small data problems, sometimes standing up a Hadoop cluster to process<br/>just a few gigabytes. It seemed like everyone wanted in on the big data<br/>action. Dan Ariely tweeted, &#8220;Big data is like teenage sex: everyone talks<br/>about it, nobody really knows how to do it, everyone thinks everyone else is<br/>doing it, so everyone claims they are doing it.&#8221;<br/>Figure 1-2 shows a snapshot of Google Trends for the search term &#8220;big<br/>data&#8221; to get an idea of the rise and fall of big data.<br/></p>
<p><i>Figure 1-2. Google Trends for &#8220;big data&#8221; (March 2022)</i></p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Despite the term&#8217;s popularity, big data has lost steam. What happened? One<br/>word: simplification. Despite the power and sophistication of open source<br/>big data tools, managing them was a lot of work and required constant<br/>attention. Often, companies employed entire teams of big data engineers,<br/>costing millions of dollars a year, to babysit these platforms. Big data<br/>engineers often spent excessive time maintaining complicated tooling and<br/>arguably not as much time delivering the business&#8217;s insights and value.<br/>Open source developers, clouds, and third parties started looking for ways<br/>to abstract, simplify, and make big data available without the high<br/>administrative overhead and cost of managing their clusters, and installing,<br/>configuring, and upgrading their open source code. The term <i>big data</i> is<br/>essentially a relic to describe a particular time and approach to handling<br/>large amounts of data.<br/>Today, data is moving faster than ever and growing ever larger, but big data<br/>processing has become so accessible that it no longer merits a separate<br/>term; every company aims to solve its data problems, regardless of actual<br/>data size. Big data engineers are now simply <i>data engineers</i>.<br/><b>The 2020s: Engineering for the data lifecycle<br/></b>At the time of this writing, the data engineering role is evolving rapidly. We<br/>expect this evolution to continue at a rapid clip for the foreseeable future.<br/>Whereas data engineers historically tended to the low-level details of<br/>monolithic frameworks such as Hadoop, Spark, or Informatica, the trend is<br/>moving toward decentralized, modularized, managed, and highly abstracted<br/>tools.<br/>Indeed, data tools have proliferated at an astonishing rate (see Figure 1-3).<br/>Popular trends in the early 2020s include the <i>modern data stack</i>,<br/>representing a collection of off-the-shelf open source and third-party<br/>products assembled to make analysts&#8217; lives easier. At the same time, data<br/>sources and data formats are growing both in variety and size. Data<br/>engineering is increasingly a discipline of interoperation, and connecting<br/>various technologies like LEGO bricks, to serve ultimate business goals.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><i>Figure 1-3. Matt Turck&#8217;s Data Landscape in 2012 versus 2021<br/></i></p>
<p>The data engineer we discuss in this book can be described more precisely<br/>as a <i>data lifecycle engineer.</i> With greater abstraction and simplification, a<br/>data lifecycle engineer is no longer encumbered by the gory details of<br/>yesterday&#8217;s big data frameworks. While data engineers maintain skills in<br/>low-level data programming and use these as required, they increasingly<br/>find their role focused on things higher in the value chain: security, data<br/>management, DataOps, data architecture, orchestration, and general data<br/>lifecycle management.<br/>As tools and workflows simplify, we&#8217;ve seen a noticeable shift in the<br/>attitudes of data engineers. Instead of focusing on who has the &#8220;biggest<br/>data,&#8221; open source projects and services are increasingly concerned with<br/>managing and governing data, making it easier to use and discover, and<br/>improving its quality. Data engineers are now conversant in acronyms such<br/>as <i>CCPA</i> and <i>GDPR</i>;  as they engineer pipelines, they concern themselves<br/>with privacy, anonymization, data garbage collection, and compliance with<br/>regulations.<br/>What&#8217;s old is new again. While &#8220;enterprisey&#8221; stuff like data management<br/>(including data quality and governance) was common for large enterprises<br/>in the pre-big-data era, it wasn&#8217;t widely adopted in smaller companies. Now<br/>that many of the challenging problems of yesterday&#8217;s data systems are<br/>solved, neatly productized, and packaged, technologists and entrepreneurs<br/>have shifted focus back to the &#8220;enterprisey&#8221; stuff, but with an emphasis on<br/></p>
<p>8<br/></p>
<p>9</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>decentralization and agility, that contrasts with the traditional enterprise<br/>command-and-control approach.<br/>We view the present as a golden age of data lifecycle management. Data<br/>engineers managing the data engineering lifecycle have better tools and<br/>techniques than ever before. We discuss the data engineering lifecycle and<br/>its undercurrents in greater detail in the next chapter.<br/></p>
<p><b>Data Engineering and Data Science<br/></b>Where does data engineering fit in with data science? There&#8217;s some debate,<br/>with some arguing data engineering is a subdiscipline of data science. We<br/>believe data engineering is <i>separate</i> from data science and analytics. They<br/>complement each other, but they are distinctly different. Data engineering<br/>sits upstream from data science (Figure 1-4), meaning data engineers<br/>provide the inputs used by data scientists (downstream from data<br/>engineering), who convert these inputs into something useful.<br/></p>
<p><i>Figure 1-4. Data engineering sits upstream from data science<br/></i></p>
<p>Consider the Data Science Hierarchy of Needs (Figure 1-5). In 2017,<br/>Monica Rogati published this hierarchy in an article that showed where AI<br/>and machine learning (ML) sat in proximity to more &#8220;mundane&#8221; areas such<br/>as data movement/storage, collection, and infrastructure.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><i>Figure 1-5. The Data Science Hierarchy of Needs<br/></i></p>
<p>Although many data scientists are eager to build and tune ML models, the<br/>reality is an estimated 70% to 80% of their time is spent toiling in the<br/>bottom three parts of the hierarchy&#8212;gathering data, cleaning data,<br/>processing data&#8212;and only a tiny slice of their time on analysis and ML.<br/>Rogati argues that companies need to build a solid data foundation (the<br/>bottom three levels of the hierarchy) before tackling areas such as AI and<br/>ML.<br/>Data scientists aren&#8217;t typically trained to engineer production-grade data<br/>systems, and they end up doing this work haphazardly because they lack the<br/>support and resources of a data engineer. In an ideal world, data scientists<br/>should spend more than 90% of their time focused on the top layers of the<br/>pyramid: analytics, experimentation, and ML. When data engineers focus<br/>on these bottom parts of the hierarchy, they build a solid foundation for data<br/>scientists to succeed.<br/>With data science driving advanced analytics and ML, data engineering<br/>straddles the divide between getting data and getting value from data (see<br/>Figure 1-6). We believe data engineering is of equal importance and</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>visibility to data science, with data engineers playing a vital role in making<br/>data science successful in production.<br/></p>
<p><i>Figure 1-6. A data engineer gets data and provides value from the data<br/></i></p>
<p><b>Data Engineering Skills and Activities<br/></b>The skill set of a data engineer encompasses the &#8220;undercurrents&#8221; of data<br/>engineering: security, data management, DataOps, data architecture, and<br/>software engineering. This skill set requires an understanding of how to<br/>evaluate data tools and how they fit together across the data engineering<br/>lifecycle. It&#8217;s also critical to know how data is produced in source systems<br/>and how analysts and data scientists will consume and create value after<br/>processing and curating data. Finally, a data engineer juggles a lot of<br/>complex moving parts and must constantly optimize along the axes of cost,<br/>agility, scalability, simplicity, reuse, and interoperability (Figure 1-7). We<br/>cover these topics in more detail in upcoming chapters.<br/></p>
<p><i>Figure 1-7. The balancing act of data engineering<br/></i></p>
<p>As we discussed, in the recent past, a data engineer was expected to know<br/>and understand how to use a small handful of powerful and monolithic<br/>technologies (Hadoop, Spark, Teradata, Hive, and many others) to create a<br/>data solution. Utilizing these technologies often requires a sophisticated<br/>understanding of software engineering, networking, distributed computing,<br/>storage, or other low-level details. Their work would be devoted to cluster<br/>administration and maintenance, managing overhead, and writing pipeline<br/>and transformation jobs, among other tasks.<br/>Nowadays, the data-tooling landscape is dramatically less complicated to<br/>manage and deploy. Modern data tools considerably abstract and simplify</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>workflows. As a result, data engineers are now focused on balancing the<br/>simplest and most cost-effective, best-of-breed services that deliver value to<br/>the business. The data engineer is also expected to create agile data<br/>architectures that evolve as new trends emerge.<br/>What are some things a data engineer does <i>not</i> do? A data engineer<br/>typically does not directly build ML models, create reports or dashboards,<br/>perform data analysis, build key performance indicators (KPIs), or develop<br/>software applications. A data engineer should have a good functioning<br/>understanding of these areas to serve stakeholders best.<br/></p>
<p><b>Data Maturity and the Data Engineer<br/></b>The level of data engineering complexity within a company depends a great<br/>deal on the company&#8217;s data maturity. This significantly impacts a data<br/>engineer&#8217;s day-to-day job responsibilities and career progression. What is<br/>data maturity, exactly?<br/><i>Data maturity</i> is the progression toward higher data utilization, capabilities,<br/>and integration across the organization, but data maturity does not simply<br/>depend on the age or revenue of a company. An early-stage startup can have<br/>greater data maturity than a 100-year-old company with annual revenues in<br/>the billions. What matters is the way data is leveraged as a competitive<br/>advantage.<br/>Data maturity models have many versions, such as Data Management<br/>Maturity (DMM) and others, and it&#8217;s hard to pick one that is both simple<br/>and useful for data engineering. So, we&#8217;ll create our own simplified data<br/>maturity model. Our data maturity model (Figure 1-8) has three stages:<br/>starting with data, scaling with data, and leading with data. Let&#8217;s look at<br/>each of these stages and at what a data engineer typically does at each stage.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><i>Figure 1-8. Our simplified data maturity model for a company<br/></i></p>
<p><b>Stage 1: Starting with data<br/></b>A company getting started with data is, by definition, in the very early<br/>stages of its data maturity. The company may have fuzzy, loosely defined<br/>goals or no goals. Data architecture and infrastructure are in the very early<br/>stages of planning and development. Adoption and utilization are likely low<br/>or nonexistent. The data team is small, often with a headcount in the single<br/>digits. At this stage, a data engineer is usually a generalist and will typically<br/>play several other roles, such as data scientist or software engineer. A data<br/>engineer&#8217;s goal is to move fast, get traction, and add value.<br/>The practicalities of getting value from data are typically poorly<br/>understood, but the desire exists. Reports or analyses lack formal structure,<br/>and most requests for data are ad hoc. While it&#8217;s tempting to jump headfirst<br/>into ML at this stage, we don&#8217;t recommend it. We&#8217;ve seen countless data<br/>teams get stuck and fall short when they try to jump to ML without building<br/>a solid data foundation.<br/>That&#8217;s not to say you can&#8217;t get wins from ML at this stage&#8212;it is rare but<br/>possible. Without a solid data foundation, you likely won&#8217;t have the data to<br/>train reliable ML models nor the means to deploy these models to<br/>production in a scalable and repeatable way. We half-jokingly call ourselves<br/>&#8220;recovering data scientists&#8221;, mainly from personal experience with being<br/>involved in premature data science projects without adequate data maturity<br/>or data engineering support.<br/>A data engineer should focus on the following in organizations getting<br/>started with data:</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Get buy-in from key stakeholders, including executive management.<br/>Ideally, the data engineer should have a sponsor for critical initiatives<br/>to design and build a data architecture to support the company&#8217;s goals.<br/>Define the right data architecture (usually solo, since a data architect<br/>likely isn&#8217;t available). This means determining business goals and the<br/>competitive advantage you&#8217;re aiming to achieve with your data<br/>initiative. Work toward a data architecture that supports these goals.<br/>See Chapter 3 for our advice on &#8220;good&#8221; data architecture.<br/>Identify and audit data that will support key initiatives and operate<br/>within the data architecture you designed.<br/>Build a solid data foundation for future data analysts and data<br/>scientists to generate reports and models that provide competitive<br/>value. In the meantime, you may also have to generate these reports<br/>and models until this team is hired.<br/></p>
<p>This is a delicate stage with lots of pitfalls. Here are some tips for this<br/>stage:<br/></p>
<p>Organizational willpower may wane if a lot of visible successes don&#8217;t<br/>occur with data. Getting quick wins will establish the importance of<br/>data within the organization. Just keep in mind that quick wins will<br/>likely create technical debt. Have a plan to reduce this debt, as it will<br/>otherwise add friction for future delivery.<br/>Get out and talk to people, and avoid working in silos. We often see<br/>the data team working in a bubble, not communicating with people<br/>outside their departments and getting perspectives and feedback from<br/>business stakeholders. The danger is you&#8217;ll spend a lot of time working<br/>on things of little use to people.<br/>Avoid undifferentiated heavy lifting. Don&#8217;t box yourself in with<br/>unnecessary technical complexity. Use off-the-shelf, turnkey solutions<br/>wherever possible.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Build custom solutions and code only where this creates a competitive<br/>advantage.<br/></p>
<p><b>Stage 2: Scaling with data<br/></b>At this point, a company has moved away from ad hoc data requests and<br/>has formal data practices. Now the challenge is creating scalable data<br/>architectures and planning for a future where the company is genuinely<br/>data-driven. Data engineering roles move from generalists to specialists,<br/>with people focusing on particular aspects of the data engineering lifecycle.<br/>In organizations that are in stage 2 of data maturity, a data engineer&#8217;s goals<br/>are to do the following:<br/></p>
<p>Establish formal data practices<br/>Create scalable and robust data architectures<br/>Adopt DevOps and DataOps practices<br/>Build systems that support ML<br/>Continue to avoid undifferentiated heavy lifting and customize only<br/>when a competitive advantage results<br/></p>
<p>We return to each of these goals later in the book.<br/>Issues to watch out for include the following:<br/></p>
<p>As we grow more sophisticated with data, there&#8217;s a temptation to adopt<br/>bleeding-edge technologies based on social proof from Silicon Valley<br/>companies. This is rarely a good use of your time and energy. Any<br/>technology decisions should be driven by the value they&#8217;ll deliver to<br/>your customers.<br/>The main bottleneck for scaling is not cluster nodes, storage, or<br/>technology but the data engineering team. Focus on solutions that are<br/>simple to deploy and manage to expand your team&#8217;s throughput.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>You&#8217;ll be tempted to frame yourself as a technologist, a data genius<br/>who can deliver magical products. Shift your focus instead to<br/>pragmatic leadership and begin transitioning to the next maturity stage;<br/>communicate with other teams about the practical utility of data. Teach<br/>the organization how to consume and leverage data.<br/></p>
<p><b>Stage 3: Leading with data<br/></b>At this stage, the company is data-driven. The automated pipelines and<br/>systems created by data engineers allow people within the company to do<br/>self-service analytics and ML. Introducing new data sources is seamless,<br/>and tangible value is derived. Data engineers implement proper controls and<br/>practices to ensure that data is always available to the people and systems.<br/>Data engineering roles continue to specialize more deeply than in stage 2.<br/>In organizations in stage 3 of data maturity, a data engineer will continue<br/>building on prior stages, plus they will do the following:<br/></p>
<p>Create automation for the seamless introduction and usage of new data<br/>Focus on building custom tools and systems that leverage data as a<br/>competitive advantage<br/>Focus on the &#8220;enterprisey&#8221; aspects of data, such as data management<br/>(including data governance and quality) and DataOps<br/>Deploy tools that expose and disseminate data throughout the<br/>organization, including data catalogs, data lineage tools, and metadata<br/>management systems<br/>Collaborate efficiently with software engineers, ML engineers,<br/>analysts, and others<br/>Create a community and environment where people can collaborate<br/>and speak openly, no matter their role or position<br/></p>
<p>Issues to watch out for include the following:</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>At this stage, complacency is a significant danger. Once organizations<br/>reach stage 3, they must constantly focus on maintenance and<br/>improvement or risk falling back to a lower stage.<br/>Technology distractions are a more significant danger here than in the<br/>other stages. There&#8217;s a temptation to pursue expensive hobby projects<br/>that don&#8217;t deliver value to the business. Utilize custom-built<br/>technology only where it provides a competitive advantage.<br/></p>
<p><b>The Background and Skills of a Data Engineer<br/></b>Data engineering is a fast-growing field, and a lot of questions remain about<br/>how to become a data engineer. Because data engineering is a relatively<br/>new discipline, little formal training is available to enter the field.<br/>Universities don&#8217;t have a standard data engineering path. Although a<br/>handful of data engineering boot camps and online tutorials cover random<br/>topics, a common curriculum for the subject doesn&#8217;t yet exist.<br/>People entering data engineering arrive with varying backgrounds in<br/>education, career, and skill set. Everyone entering the field should expect to<br/>invest a significant amount of time in self-study. Reading this book is a<br/>good starting point; one of the primary goals of this book is to give you a<br/>foundation for the knowledge and skills we think are necessary to succeed<br/>as a data engineer.<br/>If you&#8217;re pivoting your career into data engineering, we&#8217;ve found that the<br/>transition is easiest when moving from an adjacent field, such as software<br/>engineering, ETL development, database administration, data science, or<br/>data analysis. These disciplines tend to be &#8220;data aware&#8221; and provide good<br/>context for data roles in an organization. They also equip folks with the<br/>relevant technical skills and context to solve data engineering problems.<br/>Despite the lack of a formalized path, a requisite body of knowledge exists<br/>that we believe a data engineer should know to be successful. By definition,<br/>a data engineer must understand both data and technology. With respect to<br/>data, this entails knowing about various best practices around data<br/>management. On the technology end, a data engineer must be aware of</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>various options for tools, their interplay, and their trade-offs. This requires a<br/>good understanding of software engineering, DataOps, and data<br/>architecture.<br/>Zooming out, a data engineer must also understand the requirements of data<br/>consumers (data analysts and data scientists) and the broader implications<br/>of data across the organization. Data engineering is a holistic practice; the<br/>best data engineers view their responsibilities through business and<br/>technical lenses.<br/></p>
<p><b>Business Responsibilities<br/></b>The macro responsibilities we list in this section aren&#8217;t exclusive to data<br/>engineers, but are crucial for anyone working in a data or technology field.<br/>Because a simple Google search will yield tons of resources to learn about<br/>these areas, we will simply list them for brevity:<br/><i>Know how to communicate with nontechnical and technical people.<br/></i></p>
<p>Communication is key, and you need to be able to establish rapport and<br/>trust with people across the organization. We suggest paying close<br/>attention to organizational hierarchies, who reports to whom, how<br/>people interact, and which silos exist. These observations will be<br/>invaluable to your success.<br/></p>
<p><i>Understand how to scope and gather business and product requirements.<br/></i>You need to know what to build and ensure that your stakeholders agree<br/>with your assessment. In addition, develop a sense of how data and<br/>technology decisions impact the business.<br/></p>
<p><i>Understand the cultural foundations of Agile, DevOps, and DataOps.<br/></i>Many technologists mistakenly believe these practices are solved<br/>through technology. We feel this is dangerously wrong. Agile, DevOps,</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>and DataOps are fundamentally cultural, requiring buy-in across the<br/>organization.<br/></p>
<p><i>Control costs.<br/></i>You&#8217;ll be successful when you can keep costs low while providing<br/>outsized value. Know how to optimize for time to value, the total cost of<br/>ownership, and opportunity cost. Learn to monitor costs to avoid<br/>surprises.<br/></p>
<p><i>Learn continuously.<br/></i>The data field feels like it&#8217;s changing at light speed. People who succeed<br/>in it are great at picking up new things while sharpening their<br/>fundamental knowledge. They&#8217;re also good at filtering, determining<br/>which new developments are most relevant to their work, which are still<br/>immature, and which are just fads. Stay abreast of the field and learn<br/>how to learn.<br/></p>
<p>A successful data engineer always zooms out to understand the big picture<br/>and how to achieve outsized value for the business. Communication is vital,<br/>both for technical and nontechnical people. We often see data teams<br/>succeed based on their communication with other stakeholders; success or<br/>failure is rarely a technology issue. Knowing how to navigate an<br/>organization, scope and gather requirements, control costs, and<br/>continuously learn will set you apart from the data engineers who rely<br/>solely on their technical abilities to carry their career.<br/></p>
<p><b>Technical Responsibilities<br/></b>You must understand how to build architectures that optimize performance<br/>and cost at a high level, using prepackaged or homegrown components.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Ultimately, architectures and constituent technologies are building blocks to<br/>serve the data engineering lifecycle. Recall the stages of the data<br/>engineering lifecycle:<br/></p>
<p>Generation<br/>Storage<br/>Ingestion<br/>Transformation<br/>Serving<br/></p>
<p>The undercurrents of the data engineering lifecycle are the following:<br/>Security<br/>Data management<br/>DataOps<br/>Data architecture<br/>Software engineering<br/></p>
<p>Zooming in a bit, we discuss some of the tactical data and technology skills<br/>you&#8217;ll need as a data engineer in this section; we discuss these in more<br/>detail in subsequent chapters.<br/>People often ask, should a data engineer know how to code? Short answer:<br/>yes. A data engineer should have production-grade software engineering<br/>chops. We note that the nature of software development projects undertaken<br/>by data engineers has changed fundamentally in the last few years. Fully<br/>managed services now replace a great deal of low-level programming effort<br/>previously expected of engineers, who now use managed open source, and<br/>simple plug-and-play software-as-a-service (SaaS) offerings. For example,<br/>data engineers now focus on high-level abstractions or writing pipelines as<br/>code within an orchestration framework.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Even in a more abstract world, software engineering best practices provide<br/>a competitive advantage, and data engineers who can dive into the deep<br/>architectural details of a codebase give their companies an edge when<br/>specific technical needs arise. In short, a data engineer who can&#8217;t write<br/>production-grade code will be severely hindered, and we don&#8217;t see this<br/>changing anytime soon. Data engineers remain software engineers, in<br/>addition to their many other roles.<br/>What languages should a data engineer know? We divide data engineering<br/>programming languages into primary and secondary categories. At the time<br/>of this writing, the primary languages of data engineering are SQL, Python,<br/>a Java Virtual Machine (JVM) language (usually Java or Scala), and bash:<br/><i>SQL<br/></i></p>
<p>The most common interface for databases and data lakes. After briefly<br/>being sidelined by the need to write custom MapReduce code for big<br/>data processing, SQL (in various forms) has reemerged as the lingua<br/>franca of data.<br/></p>
<p><i>Python<br/></i>The bridge language between data engineering and data science. A<br/>growing number of data engineering tools are written in Python or have<br/>Python APIs. It&#8217;s known as &#8220;the second-best language at everything.&#8221;<br/>Python underlies popular data tools such as pandas, NumPy, Airflow,<br/>sci-kit learn, TensorFlow, PyTorch, and PySpark. Python is the glue<br/>between underlying components and is frequently a first-class API<br/>language for interfacing with a framework.<br/></p>
<p><i>JVM languages such as Java and Scala<br/></i>Prevalent for Apache open source projects such as Spark, Hive, and<br/>Druid. The JVM is generally more performant than Python and may</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>provide access to lower-level features than a Python API (for example,<br/>this is the case for Apache Spark and Beam). Understanding Java or<br/>Scala will be beneficial if you&#8217;re using a popular open source data<br/>framework.<br/></p>
<p><i>bash<br/></i>The command-line interface for Linux operating systems. Knowing<br/>bash commands and being comfortable using CLIs will significantly<br/>improve your productivity and workflow when you need to script or<br/>perform OS operations. Even today, data engineers frequently use<br/>command-line tools like awk or sed to process files in a data pipeline or<br/>call bash commands from orchestration frameworks. If you&#8217;re using<br/>Windows, feel free to substitute PowerShell for bash.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><b>THE UNREASONABLE EFFECTIVENESS OF SQL<br/></b>The advent of MapReduce and the big data era relegated SQL to pass&#233;<br/>status. Since then, various developments have dramatically enhanced<br/>the utility of SQL in the data engineering lifecycle. Spark SQL, Google<br/>BigQuery, Snowflake, Hive, and many other data tools can process<br/>massive amounts of data by using declarative, set-theoretic SQL<br/>semantics. SQL is also supported by many streaming frameworks, such<br/>as Apache Flink, Beam, and Kafka. We believe that competent data<br/>engineers should be highly proficient in SQL.<br/>Are we saying that SQL is a be-all and end-all language? Not at all.<br/>SQL is a powerful tool that can quickly solve complex analytics and<br/>data transformation problems. Given that time is a primary constraint<br/>for data engineering team throughput, engineers should embrace tools<br/>that combine simplicity and high productivity. Data engineers also do<br/>well to develop expertise in composing SQL with other operations,<br/>either within frameworks such as Spark and Flink or by using<br/>orchestration to combine multiple tools. Data engineers should also<br/>learn modern SQL semantics for dealing with JavaScript Object<br/>Notation (JSON) parsing and nested data and consider leveraging a<br/>SQL management framework such as dbt (Data Build Tool).<br/>A proficient data engineer also recognizes when SQL is not the right<br/>tool for the job and can choose and code in a suitable alternative. A<br/>SQL expert could likely write a query to stem and tokenize raw text in a<br/>natural language processing (NLP) pipeline but would also recognize<br/>that coding in native Spark is a far superior alternative to this<br/>masochistic exercise.<br/></p>
<p>Data engineers may also need to develop proficiency in secondary<br/>programming languages, including R, JavaScript, Go, Rust, C/C++, C#, and<br/>Julia. Developing in these languages is often necessary when popular across<br/>the company or used with domain-specific data tools. For instance,<br/>JavaScript has proven popular as a language for user-defined functions in</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>cloud data warehouses. At the same time, C# and Pow er Shell are essential<br/>in companies that leverage Azure and the Microsoft ecosystem.<br/></p>
<p><b>KEEPING PACE IN A FAST-MOVING FIELD<br/></b><i>Once a new technology rolls over you, if you&#8217;re not part of the<br/>steamroller, you&#8217;re part of the road.<br/></i></p>
<p>&#8212;Stewart Brand<br/>How do you keep your skills sharp in a rapidly changing field like data<br/>engineering? Should you focus on the latest tools or deep dive into<br/>fundamentals? Here&#8217;s our advice: focus on the fundamentals to<br/>understand what&#8217;s not going to change; pay attention to ongoing<br/>developments to know where the field is going. New paradigms and<br/>practices are introduced all the time, and it&#8217;s incumbent on you to stay<br/>current. Strive to understand how new technologies will be helpful in<br/>the lifecycle.<br/></p>
<p><b>The Continuum of Data Engineering Roles, from A to B<br/></b>Although job descriptions paint a data engineer as a &#8220;unicorn&#8221; who must<br/>possess every data skill imaginable, data engineers don&#8217;t all do the same<br/>type of work or have the same skill set. Data maturity is a helpful guide to<br/>understanding the types of data challenges a company will face as it grows<br/>its data capability. It&#8217;s beneficial to look at some critical distinctions in the<br/>kinds of work data engineers do. Though these distinctions are simplistic,<br/>they clarify what data scientists and data engineers do and avoid lumping<br/>either role into the unicorn bucket.<br/>In data science, there&#8217;s the notion of type A and type B data scientists.<br/><i>Type A data scientists</i>&#8212;where <i>A</i> stands for <i>analysis</i>&#8212;focus on<br/>understanding and deriving insight from data. <i>Type B data scientists</i>&#8212;<br/>where <i>B</i> stands for <i>building</i>&#8212;share similar backgrounds as type A data<br/>scientists and possess strong programming skills. The type B data scientist<br/>builds systems that make data science work in production. Borrowing from<br/></p>
<p>10</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>this data scientist continuum, we&#8217;ll create a similar distinction for two types<br/>of data engineers:<br/><i>Type A data engineers<br/></i></p>
<p><i>A</i> stands for <i>abstraction</i>. In this case, the data engineer avoids<br/>undifferentiated heavy lifting, keeping data architecture as abstract and<br/>straightforward as possible and not reinventing the wheel. Type A data<br/>engineers manage the data engineering lifecycle mainly by using<br/>entirely off-the-shelf products, managed services, and tools. Type A<br/>data engineers work at companies across industries and at all levels of<br/>data maturity.<br/></p>
<p><i>Type B data engineers<br/>B</i> stands for <i>build</i>. Type B data engineers build data tools and systems<br/>that scale and leverage a company&#8217;s core competency and competitive<br/>advantage. In the data maturity range, a type B data engineer is more<br/>commonly found at companies in stage 2 and 3 (scaling and leading<br/>with data), or when an initial data use case is so unique and mission-<br/>critical that custom data tools are required to get started.<br/></p>
<p>Type A and type B data engineers may work in the same company and may<br/>even be the same person! More commonly, a type A data engineer is first<br/>hired to set the foundation, with type B data engineer skill sets either<br/>learned or hired as the need arises within a company.<br/></p>
<p><b>Data Engineers Inside an Organization<br/></b>Data engineers don&#8217;t work in a vacuum. Depending on what they&#8217;re<br/>working on, they will interact with technical and nontechnical people and</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>face different directions (internal and external). Let&#8217;s explore what data<br/>engineers do inside an organization and with whom they interact.<br/></p>
<p><b>Internal-Facing Versus External-Facing Data Engineers<br/></b>A data engineer serves several end users and faces many internal and<br/>external directions (Figure 1-9). Since not all data engineering workloads<br/>and responsibilities are the same, it&#8217;s essential to understand whom the data<br/>engineer serves. Depending on the end-use cases, a data engineer&#8217;s primary<br/>responsibilities are external facing, internal facing, or a blend of the two.<br/></p>
<p><i>Figure 1-9. The directions a data engineer faces<br/></i></p>
<p>An <i>external-facing</i> data engineer typically aligns with the users of external-<br/>facing applications, such as social media apps, Internet of Things (IoT)<br/>devices, and ecommerce platforms. This data engineer architects, builds,<br/>and manages the systems that collect, store, and process transactional and<br/>event data from these applications. The systems built by these data<br/>engineers have a feedback loop from the application to the data pipeline,<br/>and then back to the application (Figure 1-10).</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><i>Figure 1-10. External-facing data engineer systems<br/></i></p>
<p>External-facing data engineering comes with a unique set of problems.<br/>External-facing query engines often handle much larger concurrency loads<br/>than internal-facing systems. Engineers also need to consider putting tight<br/>limits on queries that users can run to limit the infrastructure impact of any<br/>single user. In addition, security is a much more complex and sensitive<br/>problem for external queries, especially if the data being queried is<br/>multitenant (data from many customers and housed in a single table).<br/>An <i>internal-facing data engineer</i> typically focuses on activities crucial to<br/>the needs of the business and internal stakeholders (Figure 1-11). Examples<br/>include creating and maintaining data pipelines and data warehouses for BI<br/>dashboards, reports, business processes, data science, and ML models.<br/></p>
<p><i>Figure 1-11. Internal-facing data engineer</i></p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>External-facing and internal-facing responsibilities are often blended. In<br/>practice, internal-facing data is usually a prerequisite to external-facing<br/>data. The data engineer has two sets of users with very different<br/>requirements for query concurrency, security, and more.<br/></p>
<p><b>Data Engineers and Other Technical Roles<br/></b>In practice, the data engineering lifecycle cuts across many domains of<br/>responsibility. Data engineers sit at the nexus of various roles, directly or<br/>through managers, interacting with many organizational units.<br/>Let&#8217;s look at whom a data engineer may impact. In this section, we&#8217;ll<br/>discuss technical roles connected to data engineering (Figure 1-12).<br/></p>
<p><i>Figure 1-12. Key technical stakeholders of data engineering<br/></i></p>
<p>The data engineer is a hub between <i>data producers</i>, such as software<br/>engineers, data architects, and DevOps or site-reliability engineers (SREs),<br/>and <i>data consumers</i>, such as data analysts, data scientists, and ML<br/>engineers. In addition, data engineers will interact with those in operational<br/>roles, such as DevOps engineers.<br/>Given the pace at which new data roles come into vogue (analytics and ML<br/>engineers come to mind), this is by no means an exhaustive list.<br/><b>Upstream stakeholders</b></p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>To be successful as a data engineer, you need to understand the data<br/>architecture you&#8217;re using or designing and the source systems producing the<br/>data you&#8217;ll need. Next, we discuss a few familiar upstream stakeholders:<br/>data architects, software engineers, and DevOps engineers.<br/><i>Data architects<br/></i>Data architects function at a level of abstraction one step removed from<br/>data engineers. Data architects design the blueprint for organizational data<br/>management, mapping out processes and overall data architecture and<br/>systems.  They also serve as a bridge between an organization&#8217;s technical<br/>and nontechnical sides. Successful data architects generally have &#8220;battle<br/>scars&#8221; from extensive engineering experience, allowing them to guide and<br/>assist engineers while successfully communicating engineering challenges<br/>to nontechnical business stakeholders.<br/>Data architects implement policies for managing data across silos and<br/>business units, steer global strategies such as data management and data<br/>governance, and guide significant initiatives. Data architects often play a<br/>central role in cloud migrations and greenfield cloud design.<br/>The advent of the cloud has shifted the boundary between data architecture<br/>and data engineering. Cloud data architectures are much more fluid than on-<br/>premises systems, so architecture decisions that traditionally involved<br/>extensive study, long lead times, purchase contracts, and hardware<br/>installation are now often made during the implementation process, just one<br/>step in a larger strategy. Nevertheless, data architects will remain influential<br/>visionaries in enterprises, working hand in hand with data engineers to<br/>determine the big picture of architecture practices and data strategies.<br/>Depending on the company&#8217;s data maturity and size, a data engineer may<br/>overlap with or assume the responsibilities of a data architect. Therefore, a<br/>data engineer should have a good understanding of architecture best<br/>practices and approaches.<br/>Note that we have placed data architects in the <i>upstream stakeholders<br/></i>section. Data architects often help design application data layers that are<br/>source systems for data engineers. Architects may also interact with data<br/></p>
<p>11</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>engineers at various other stages of the data engineering lifecycle. We cover<br/>&#8220;good&#8221; data architecture in Chapter 3.<br/><i>Software engineers<br/></i>Software engineers build the software and systems that run a business; they<br/>are largely responsible for generating the <i>internal data</i> that data engineers<br/>will consume and process. The systems built by software engineers<br/>typically generate application event data and logs, which are significant<br/>assets in their own right. This internal data contrasts with <i>external data<br/></i>pulled from SaaS platforms or partner businesses. In well-run technical<br/>organizations, software engineers and data engineers coordinate from the<br/>inception of a new project to design application data for consumption by<br/>analytics and ML applications.<br/>A data engineer should work together with software engineers to<br/>understand the applications that generate data, the volume, frequency, and<br/>format of the generated data, and anything else that will impact the data<br/>engineering lifecycle, such as data security and regulatory compliance. For<br/>example, this might mean setting upstream expectations on what the data<br/>software engineers need to do their jobs. Data engineers must work closely<br/>with the software engineers.<br/><i>DevOps engineers and site-reliability engineers<br/></i>DevOps and SREs often produce data through operational monitoring. We<br/>classify them as upstream of data engineers, but they may also be<br/>downstream, consuming data through dashboards or interacting with data<br/>engineers directly in coordinating operations of data systems.<br/><b>Downstream stakeholders<br/></b>The modern data engineering profession exists to serve downstream data<br/>consumers and use cases. This section discusses how data engineers interact<br/>with various downstream roles. We&#8217;ll also introduce a few service models,<br/>including centralized data engineering teams and cross-functional teams.<br/><i>Data scientists</i></p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Data scientists build forward-looking models to make predictions and<br/>recommendations. These models are then evaluated on live data to provide<br/>value in various ways. For example, model scoring might determine<br/>automated actions in response to real-time conditions, recommend products<br/>to customers based on the browsing history in their current session, or make<br/>live economic predictions used by traders.<br/>According to common industry folklore, data scientists spend 70% to 80%<br/>of their time collecting, cleaning, and preparing data.  In our experience,<br/>these numbers often reflect immature data science and data engineering<br/>practices. In particular, many popular data science frameworks can become<br/>bottlenecks if they are not scaled up appropriately. Data scientists who work<br/>exclusively on a single workstation force themselves to downsample data,<br/>making data preparation significantly more complicated and potentially<br/>compromising the quality of the models they produce. Furthermore, locally<br/>developed code and environments are often difficult to deploy in<br/>production, and a lack of automation significantly hampers data science<br/>workflows. If data engineers do their job and collaborate successfully, data<br/>scientists shouldn&#8217;t spend their time collecting, cleaning, and preparing data<br/>after initial exploratory work. Data engineers should automate this work as<br/>much as possible.<br/>The need for production-ready data science is a significant driver behind the<br/>emergence of the data engineering profession. Data engineers should help<br/>data scientists to enable a path to production. In fact, we (the authors)<br/>moved from data science to data engineering after recognizing this<br/>fundamental need. Data engineers work to provide the data automation and<br/>scale that make data science more efficient.<br/><i>Data analysts<br/></i>Data analysts (or business analysts) seek to understand business<br/>performance and trends. Whereas data scientists are forward-looking, a data<br/>analyst typically focuses on the past or present. Data analysts usually run<br/>SQL queries in a data warehouse or a data lake. They may also utilize<br/>spreadsheets for computation and analysis and various BI tools such as<br/></p>
<p>12</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Microsoft Power BI, Looker, or Tableau. Data analysts are domain experts<br/>in the data they work with frequently and become intimately familiar with<br/>data definitions, characteristics, and quality problems. A data analyst&#8217;s<br/>typical downstream customers are business users, management, and<br/>executives.<br/>Data engineers work with data analysts to build pipelines for new data<br/>sources required by the business. Data analysts&#8217; subject-matter expertise is<br/>invaluable in improving data quality, and they frequently collaborate with<br/>data engineers in this capacity.<br/><i>Machine learning engineers and AI researchers<br/></i>Machine learning engineers (ML engineers) overlap with data engineers<br/>and data scientists. ML engineers develop advanced ML techniques, train<br/>models, and design and maintain the infrastructure running ML processes in<br/>a scaled production environment. ML engineers often have advanced<br/>working knowledge of ML and deep learning techniques, and frameworks<br/>such as PyTorch or TensorFlow.<br/>ML engineers also understand the hardware, services, and systems required<br/>to run these frameworks, both for model training and model deployment at<br/>a production scale. It&#8217;s common for ML flows to run in a cloud<br/>environment where ML engineers can spin up and scale infrastructure<br/>resources on demand or rely on managed services.<br/>As we&#8217;ve mentioned, the boundaries between ML engineering, data<br/>engineering, and data science are blurry. Data engineers may have some<br/>DevOps responsibilities over ML systems, and data scientists may work<br/>closely with ML engineering in designing advanced ML processes.<br/>The world of ML engineering is snowballing and parallels a lot of the same<br/>developments occurring in data engineering. Whereas several years ago, the<br/>attention of ML was focused on how to build models, ML engineering now<br/>increasingly emphasizes incorporating best practices of machine learning<br/>operations (MLOps) and other mature practices previously adopted in<br/>software engineering and DevOps.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>AI researchers work on new, advanced ML techniques. AI researchers may<br/>work inside large technology companies, specialized intellectual property<br/>startups (OpenAI, DeepMind), or academic institutions. Some practitioners<br/>are dedicated to part-time research in conjunction with ML engineering<br/>responsibilities inside a company. Those working inside specialized ML<br/>labs are often 100% dedicated to research. Research problems may target<br/>immediate practical applications or more abstract demonstrations of AI.<br/>AlphaGo and GPT-3/GPT-4 are great examples of ML research projects.<br/>We&#8217;ve provided some references in &#8220;Additional Resources&#8221;.<br/>AI researchers in well-funded organizations are highly specialized and<br/>operate with supporting teams of engineers to facilitate their work. ML<br/>engineers in academia usually have fewer resources but rely on teams of<br/>graduate students, postdocs, and university staff to provide engineering<br/>support. ML engineers who are partially dedicated to research often rely on<br/>the same support teams for research and production.<br/></p>
<p><b>Data Engineers and Business Leadership<br/></b>We&#8217;ve discussed technical roles with which a data engineer interacts. But<br/>data engineers also operate more broadly as organizational connectors,<br/>often in a nontechnical capacity. Businesses have come to rely increasingly<br/>on data as a core part of many products or a product in itself. Data<br/>engineers now participate in strategic planning and lead key initiatives that<br/>extend beyond the boundaries of IT. Data engineers often support data<br/>architects by acting as the glue between the business and data<br/>science/analytics.<br/><b>Data in the C-suite<br/></b>C-level executives are increasingly involved in data and analytics, as these<br/>are recognized as significant assets for modern businesses. CEOs now<br/>concern themselves with initiatives that were once the exclusive province of<br/>IT, such as cloud migrations or deployment of a new customer data<br/>platform.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><i>Chief executive officer<br/></i>Chief executive officers (CEOs) at nontech companies generally don&#8217;t<br/>concern themselves with the nitty-gritty of data frameworks and software.<br/>Instead, they define a vision in collaboration with technical C-suite roles<br/>and company data leadership. Data engineers provide a window into what&#8217;s<br/>possible with data. Data engineers and their managers maintain a map of<br/>what data is available to the organization&#8212;both internally and from third<br/>parties&#8212;in what time frame. They are also tasked to study primary data<br/>architectural changes in collaboration with other engineering roles. For<br/>example, data engineers are often heavily involved in cloud migrations,<br/>migrations to new data systems, or deployment of streaming technologies.<br/><i>Chief information officer<br/></i>A chief information officer (CIO) is the senior C-suite executive<br/>responsible for information technology within an organization; it is an<br/>internal-facing role. A CIO must possess deep knowledge of information<br/>technology and business processes&#8212;either alone is insufficient. CIOs direct<br/>the information technology organization, setting ongoing policies while also<br/>defining and executing significant initiatives under the direction of the<br/>CEO.<br/>CIOs often collaborate with data engineering leadership in organizations<br/>with a well-developed data culture. If an organization is not very high in its<br/>data maturity, a CIO will typically help shape its data culture. CIOs will<br/>work with engineers and architects to map out major initiatives and make<br/>strategic decisions on adopting major architectural elements, such as<br/>enterprise resource planning (ERP) and customer relationship management<br/>(CRM) systems, cloud migrations, data systems, and internal-facing IT.<br/><i>Chief technology officer<br/></i>A chief technology officer (CTO) is similar to a CIO but faces outward. A<br/>CTO owns the key technological strategy and architectures for external-<br/>facing applications, such as mobile, web apps, and IoT&#8212;all critical data<br/>sources for data engineers. The CTO is likely a skilled technologist and has</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>a good sense of software engineering fundamentals and system architecture.<br/>In some organizations without a CIO, the CTO or sometimes the chief<br/>operating officer (COO) plays the role of CIO. Data engineers often report<br/>directly or indirectly through a CTO.<br/><i>Chief data officer<br/></i>The chief data officer (CDO) was created in 2002 at Capital One to<br/>recognize the growing importance of data as a business asset. The CDO is<br/>responsible for a company&#8217;s data assets and strategy. CDOs are focused on<br/>data&#8217;s business utility but should have a strong technical grounding. CDOs<br/>oversee data products, strategy, initiatives, and core functions such as<br/>master data management and privacy. Occasionally, CDOs manage business<br/>analytics and data engineering.<br/><i>Chief analytics officer<br/></i>The chief analytics officer (CAO) is a variant of the CDO role. Where both<br/>roles exist, the CDO focuses on the technology and organization required to<br/>deliver data. The CAO is responsible for analytics, strategy, and decision<br/>making for the business. A CAO may oversee data science and ML, though<br/>this largely depends on whether the company has a CDO or CTO role.<br/><i>Chief algorithms officer<br/></i>A chief algorithms officer (CAO-2) is a recent innovation in the C-suite, a<br/>highly technical role focused specifically on data science and ML. CAO-2s<br/>typically have experience as individual contributors and team leads in data<br/>science or ML projects. Frequently, they have a background in ML research<br/>and a related advanced degree.<br/>CAO-2s are expected to be conversant in current ML research and have<br/>deep technical knowledge of their company&#8217;s ML initiatives. In addition to<br/>creating business initiatives, they provide technical leadership, set research<br/>and development agendas, and build research teams.<br/><b>Data engineers and project managers</b></p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Data engineers often work on significant initiatives, potentially spanning<br/>many years. As we write this book, many data engineers are working on<br/>cloud migrations, migrating pipelines and warehouses to the next<br/>generation of data tools. Other data engineers are starting greenfield<br/>projects, assembling new data architectures from scratch by selecting from<br/>an astonishing number of best-of-breed architecture and tooling options.<br/>These large initiatives often benefit from <i>project management</i> (in contrast<br/>to product management, discussed next). Whereas data engineers function<br/>in an infrastructure and service delivery capacity, project managers direct<br/>traffic and serve as gatekeepers. Most project managers operate according<br/>to some variation of Agile and Scrum, with Waterfall still appearing<br/>occasionally. Business never sleeps, and business stakeholders often have a<br/>significant backlog of things they want to address and new initiatives they<br/>want to launch. Project managers must filter a long list of requests and<br/>prioritize critical deliverables to keep projects on track and better serve the<br/>company.<br/>Data engineers interact with project managers, often planning sprints for<br/>projects and ensuing standups related to the sprint. Feedback goes both<br/>ways, with data engineers informing project managers and other<br/>stakeholders about progress and blockers, and project managers balancing<br/>the cadence of technology teams against the ever-changing needs of the<br/>business.<br/><b>Data engineers and product managers<br/></b>Product managers oversee product development, often owning product<br/>lines. In the context of data engineers, these products are called <i>data<br/>products</i>. Data products are either built from the ground up or are<br/>incremental improvements upon existing products. Data engineers interact<br/>more frequently with <i>product managers</i> as the corporate world has adopted<br/>a data-centric focus<i>.</i> Like project managers, product managers balance the<br/>activity of technology teams against the needs of the customer and business.<br/><b>Data engineers and other management roles</b></p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Data engineers interact with various managers beyond project and product<br/>managers. However, these interactions usually follow either the services or<br/>cross-functional models. Data engineers either serve a variety of incoming<br/>requests as a centralized team or work as a resource assigned to a particular<br/>manager, project, or product.<br/>For more information on data teams and how to structure them, we<br/>recommend John Thompson&#8217;s <i>Building Analytics Teams</i> (Packt) and Jesse<br/>Anderson&#8217;s <i>Data Teams</i> (Apress). Both books provide strong frameworks<br/>and perspectives on the roles of executives with data, who to hire, and how<br/>to construct the most effective data team for your company.<br/></p>
<p><b>NOTE<br/></b>Companies don&#8217;t hire engineers simply to hack on code in isolation. To be worthy of<br/>their title, engineers should develop a deep understanding of the problems they&#8217;re tasked<br/>with solving, the technology tools at their disposal, and the people they work with and<br/>serve.<br/></p>
<p><b>Conclusion<br/></b>This chapter provided you with a brief overview of the data engineering<br/>landscape, including the following:<br/></p>
<p>Defining data engineering and describing what data engineers do<br/>Describing the types of data maturity in a company<br/>Type A and type B data engineers<br/>Whom data engineers work with<br/></p>
<p>We hope that this first chapter has whetted your appetite, whether you are a<br/>software development practitioner, data scientist, ML engineer, business<br/>stakeholder, entrepreneur, or venture capitalist. Of course, a great deal still<br/>remains to elucidate in subsequent chapters. Chapter 2 covers the data<br/>engineering lifecycle, followed by architecture in Chapter 3. The following</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>chapters get into the nitty-gritty of technology decisions for each part of the<br/>lifecycle. The entire data field is in flux, and as much as possible, each<br/>chapter focuses on the <i>immutables</i>&#8212;perspectives that will be valid for<br/>many years amid relentless change.<br/></p>
<p><b>Additional Resources<br/></b>&#8220;On Complexity in Big Data&#8221; by Jesse Anderson (O&#8217;Reilly)<br/>&#8220;Which Profession Is More Complex to Become, a Data Engineer or a<br/>Data Scientist?&#8221; thread on Quora<br/>&#8220;The Future of Data Engineering Is the Convergence of Disciplines&#8221;<br/>by Liam Hausmann<br/>The Information Management Body of Knowledge website<br/>&#8220;Doing Data Science at Twitter&#8221; by Robert Chang<br/>&#8220;A Short History of Big Data&#8221; by Mark van Rijmenam<br/>&#8220;Data Engineering: A Quick and Simple Definition&#8221; by James Furbush<br/>(O&#8217;Reilly)<br/>&#8220;Big Data Will Be Dead in Five Years&#8221; by Lewis Gavin<br/>&#8220;The AI Hierarchy of Needs&#8221; by Monica Rogati<br/>Chapter 1 of<i>What Is Data Engineering?</i> by Lewis Gavin (O&#8217;Reilly)<br/>&#8220;The Three Levels of Data Analysis: A Framework for Assessing Data<br/>Organization Maturity&#8221; by Emilie Schario<br/>&#8220;Data as a Product vs. Data as a Service&#8221; by Justin Gage<br/>&#8220;The Downfall of the Data Engineer&#8221; by Maxime Beauchemin<br/>&#8220;The Rise of the Data Engineer&#8221; by Maxime Beauchemin<br/>&#8220;Skills of the Data Architect&#8221; by Bob Lambert</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>&#8220;What Is a Data Architect? IT&#8217;s Data Framework Visionary&#8221; by Thor<br/>Olavsrud<br/>&#8220;OpenAI&#8217;s New Language Generator GPT-3 Is Shockingly Good&#8212;and<br/>Completely Mindless&#8221; by Will Douglas Heaven<br/>The AlphaGo research web page<br/>&#8220;How CEOs Can Lead a Data-Driven Culture&#8221; by Thomas H.<br/>Davenport and Nitin Mittal<br/>&#8220;Why CEOs Must Lead Big Data Initiatives&#8221; by John Weathington<br/>&#8220;How Creating a Data-Driven Culture Can Drive Success&#8221; by Frederik<br/>Bussler<br/><i>Building Analytics Teams</i> by John K. Thompson (Packt)<br/><i>Data Teams</i> by Jesse Anderson (Apress)<br/>&#8220;Information Management Body of Knowledge&#8221; Wikipedia page<br/>&#8220;Information management&#8221; Wikipedia page<br/></p>
<p>1  &#8220;Data Engineering and Its Main Concepts,&#8221; AlexSoft, last updated August 26, 2021,<br/>https://oreil.ly/e94py.<br/></p>
<p>2  <i>ETL</i> stands for <i>extract, transform, load</i>, a common pattern we cover in the book.<br/>3  Jesse Anderson, &#8220;The Two Types of Data Engineering,&#8221; June 27, 2018, <i>https://oreil.ly/dxDt6</i>.<br/>4  Maxime Beauchemin, &#8220;The Rise of the Data Engineer,&#8221; January 20, 2017,<br/></p>
<p><i>https://oreil.ly/kNDmd</i>.<br/>5  Lewis Gavin, <i>What Is Data Engineering?</i> (Sebastapol, CA: O&#8217;Reilly, 2020),<br/></p>
<p><i>https://oreil.ly/ELxLi</i>.<br/>6  Cade Metz, &#8220;How Yahoo Spawned Hadoop, the Future of Big Data,&#8221; <i>Wired</i>, October 18,<br/></p>
<p>2011, <i>https://oreil.ly/iaD9G</i>.<br/>7  Ron Miller, &#8220;How AWS Came to Be,&#8221; <i>TechCrunch</i>, July 2, 2016, <i>https://oreil.ly/VJehv</i>.<br/>8  <i>DataOps</i> is an abbreviation for <i>data operations</i>. We cover this topic in Chapter 2. For more<br/></p>
<p>information, read the DataOps Manifesto.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>9  These acronyms stand for <i>California Consumer Privacy Act</i> and <i>General Data Protection<br/>Regulation</i>, respectively.<br/></p>
<p>10  Robert Chang, &#8220;Doing Data Science at Twitter,&#8221; <i>Medium</i>, June 20, 2015,<br/><i>https://oreil.ly/xqjAx</i>.<br/></p>
<p>11  Paramita (Guha) Ghosh, &#8220;Data Architect vs. Data Engineer,&#8221; Dataversity, November 12,<br/>2021, <i>https://oreil.ly/TlyZY</i>.<br/></p>
<p>12  A variety of references exist for this notion. Although this cliche is widely known, a healthy<br/>debate has arisen around its validity in different practical settings. For more details, see Leigh<br/>Dodds, &#8220;Do Data Scientists Spend 80% of Their Time Cleaning Data? Turns Out, No?&#8221; Lost<br/>Boy blog, January 31, 2020, <i>https://oreil.ly/szFww</i>; and Alex Woodie, &#8220;Data Prep Still<br/>Dominates Data Scientists&#8217; Time, Survey Finds,&#8221; <i>Datanami</i>, July 6, 2020,<br/><i>https://oreil.ly/jDVWF</i>.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><b>Chapter 2. The Data<br/>Engineering Lifecycle<br/></b>The major goal of this book is to encourage you to move beyond viewing<br/>data engineering as a specific collection of data technologies. The data<br/>landscape is undergoing an explosion of new data technologies and<br/>practices, with ever-increasing levels of abstraction and ease of use.<br/>Because of increased technical abstraction, data engineers will increasingly<br/>become <i>data lifecycle engineers</i>, thinking and operating in terms of the<br/><i>principles</i> of data lifecycle management.<br/>In this chapter, you&#8217;ll learn about the <i>data engineering lifecycle</i>, which is<br/>the central theme of this book<i>.</i> The data engineering lifecycle is our<br/>framework describing &#8220;cradle to grave&#8221; data engineering<i>.</i> You will also<br/>learn about the undercurrents of the data engineering lifecycle, which are<br/>key foundations that support all data engineering efforts.<br/></p>
<p><b>What Is the Data Engineering Lifecycle?<br/></b>The data engineering lifecycle comprises stages that turn raw data<br/>ingredients into a useful end product, ready for consumption by analysts,<br/>data scientists, ML engineers, and others. This chapter introduces the major<br/>stages of the data engineering lifecycle, focusing on each stage&#8217;s core<br/>concepts and saving details for later chapters.<br/>We divide the data engineering lifecycle into the following five stages<br/>(Figure 2-1, top):<br/></p>
<p>Generation<br/>Storage<br/>Ingestion</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Transformation<br/>Serving data<br/></p>
<p><i>Figure 2-1. Components and undercurrents of the data engineering lifecycle<br/></i></p>
<p>We begin the data engineering lifecycle by getting data from source systems<br/>and storing it. Next, we transform the data and then proceed to our central<br/>goal, serving data to analysts, data scientists, ML engineers, and others. In<br/>reality, storage occurs throughout the lifecycle as data flows from beginning<br/>to end&#8212;hence, the diagram shows the storage &#8220;stage&#8221; as a foundation that<br/>underpins other stages.<br/>In general, the middle stages&#8212;storage, ingestion, transformation&#8212;can get a<br/>bit jumbled. And that&#8217;s OK. Although we split out the distinct parts of the<br/>data engineering lifecycle, it&#8217;s not always a neat, continuous flow. Various<br/>stages of the lifecycle may repeat themselves, occur out of order, overlap, or<br/>weave together in interesting and unexpected ways.<br/>Acting as a bedrock are <i>undercurrents</i> (Figure 2-1, bottom) that cut across<br/>multiple stages of the data engineering lifecycle: security, data<br/>management, DataOps, data architecture, orchestration, and software<br/>engineering. No part of the data engineering lifecycle can adequately<br/>function without these undercurrents.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><b>The Data Lifecycle Versus the Data Engineering<br/>Lifecycle<br/></b>You may be wondering about the difference between the overall data<br/>lifecycle and the data engineering lifecycle. There&#8217;s a subtle distinction<br/>between the two. The data engineering lifecycle is a subset of the whole<br/>data lifecycle (Figure 2-2). Whereas the full data lifecycle encompasses<br/>data across its entire lifespan, the data engineering lifecycle focuses on the<br/>stages a data engineer controls.<br/></p>
<p><i>Figure 2-2. The data engineering lifecycle is a subset of the full data lifecycle<br/></i></p>
<p><b>Generation: Source Systems<br/></b>A <i>source system</i> is the origin of the data used in the data engineering<br/>lifecycle. For example, a source system could be an IoT device, an<br/>application message queue, or a transactional database. A data engineer<br/>consumes data from a source system, but doesn&#8217;t typically own or control<br/>the source system itself. The data engineer needs to have a working<br/>understanding of the way source systems work, the way they generate data,<br/>the frequency and velocity of the data, and the variety of data they generate.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Engineers also need to keep an open line of communication with source<br/>system owners on changes that could break pipelines and analytics.<br/>Application code might change the structure of data in a field, or the<br/>application team might even choose to migrate the backend to an entirely<br/>new database technology.<br/>A major challenge in data engineering is the dizzying array of data source<br/>systems engineers must work with and understand. As an illustration, let&#8217;s<br/>look at two common source systems, one very traditional (an application<br/>database) and the other a more recent example (IoT swarms).<br/>Figure 2-3 illustrates a traditional source system with several application<br/>servers supported by a database. This source system pattern became popular<br/>in the 1980s with the explosive success of relational database management<br/>systems (RDBMSs). The application + database pattern remains popular<br/>today with various modern evolutions of software development practices.<br/>For example, applications often consist of many small service/database<br/>pairs with microservices rather than a single monolith.<br/></p>
<p><i>Figure 2-3. Source system example: an application database</i></p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Let&#8217;s look at another example of a source system. Figure 2-4 illustrates an<br/>IoT swarm: a fleet of devices (circles) sends data messages (rectangles) to a<br/>central collection system. This IoT source system is increasingly common<br/>as IoT devices such as sensors, smart devices, and much more increase in<br/>the wild.<br/></p>
<p><i>Figure 2-4. Source system example: an IoT swarm and messaging queue<br/></i></p>
<p><b>Evaluating source systems: Key engineering considerations<br/></b>There are many things to consider when assessing source systems,<br/>including how the system handles ingestion, state, and data generation. The<br/>following is a starting set of evaluation questions of source systems that<br/>data engineers must consider:<br/></p>
<p>What are the essential characteristics of the data source? Is it an<br/>application? A swarm of IoT devices?<br/>How is data persisted in the source system? Is data persisted long term,<br/>or is it temporary and quickly deleted?<br/>At what rate is data generated? How many events per second? How<br/>many gigabytes per hour?</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>What level of consistency can data engineers expect from the output<br/>data? If you&#8217;re running data-quality checks against the output data,<br/>how often do data inconsistencies occur&#8212;nulls where they aren&#8217;t<br/>expected, lousy formatting, etc.?<br/>How often do errors occur?<br/>Will the data contain duplicates?<br/>Will some data values arrive late, possibly much later than other<br/>messages produced simultaneously?<br/>What is the schema of the ingested data? Will data engineers need to<br/>join across several tables or even several systems to get a complete<br/>picture of the data?<br/>If schema changes (say, a new column is added), how is this dealt with<br/>and communicated to downstream stakeholders?<br/>How frequently should data be pulled from the source system?<br/>For stateful systems (e.g., a database tracking customer account<br/>information), is data provided as periodic snapshots or update events<br/>from change data capture (CDC)? What&#8217;s the logic for how changes<br/>are performed, and how are these tracked in the source database?<br/>Who/what is the data provider that will transmit the data for<br/>downstream consumption?<br/>Will reading from a data source impact its performance?<br/>Does the source system have upstream data dependencies? What are<br/>the characteristics of these upstream systems?<br/>Are data-quality checks in place to check for late or missing data?<br/></p>
<p>Sources produce data consumed by downstream systems, including human-<br/>generated spreadsheets, IoT sensors, and web and mobile applications. Each<br/>source has its unique volume and cadence of data generation. A data<br/>engineer should know how the source generates data, including relevant</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>quirks or nuances. Data engineers also need to understand the limits of the<br/>source systems they interact with. For example, will analytical queries<br/>against a source application database cause resource contention and<br/>performance issues?<br/>One of the most challenging nuances of source data is the schema. The<br/><i>schema</i> defines the hierarchical organization of data. Logically, we can<br/>think of data at the level of a whole source system, drilling down into<br/>individual tables, all the way to the structure of respective fields. The<br/>schema of data shipped from source systems is handled in various ways.<br/>Two popular options are schemaless and fixed schema.<br/><i>Schemaless</i> doesn&#8217;t mean the absence of schema. Rather, it means that the<br/>application defines the schema as data is written, whether to a messaging<br/>queue, a flat file, a blob, or a document database such as MongoDB. A<br/>more traditional model built on relational database storage uses a <i>fixed<br/>schema</i> enforced in the database, to which application writes must conform.<br/>Either of these models presents challenges for data engineers. Schemas<br/>change over time; in fact, schema evolution is encouraged in the Agile<br/>approach to software development. A key part of the data engineer&#8217;s job is<br/>taking raw data input in the source system schema and transforming this<br/>into valuable output for analytics. This job becomes more challenging as<br/>the source schema evolves.<br/>We dive into source systems in greater detail in Chapter 5; we also cover<br/>schemas and data modeling in Chapters 6 and 8, respectively.<br/></p>
<p><b>Storage<br/></b>After ingesting data, you need a place to store it. Choosing a storage<br/>solution is key to success in the rest of the data lifecycle, and it&#8217;s also one of<br/>the most complicated stages of the data lifecycle for a variety of reasons.<br/>First, data architectures in the cloud often leverage <i>several</i> storage<br/>solutions. Second, few data storage solutions function purely as storage,<br/>with many supporting complex transformation queries; even object storage<br/>solutions may support powerful query capabilities&#8212;e.g., Amazon S3</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Select. Third, while storage is a stage of the data engineering lifecycle, it<br/>frequently touches on other stages, such as ingestion, transformation, and<br/>serving.<br/>Storage runs across the entire data engineering lifecycle, often occurring in<br/>multiple places in a data pipeline, with storage systems crossing over with<br/>source systems, ingestion, transformation, and serving. In many ways, the<br/>way data is stored impacts how it is used in all of the stages of the data<br/>engineering lifecycle. For example, cloud data warehouses can store data,<br/>process data in pipelines, and serve it to analysts. Streaming frameworks<br/>such as Apache Kafka and Pulsar can function simultaneously as ingestion,<br/>storage, and query systems for messages, with object storage being a<br/>standard layer for data transmission.<br/><b>Evaluating storage systems: Key engineering considerations<br/></b>Here are a few key engineering questions to ask when choosing a storage<br/>system for a data warehouse, data lakehouse, database, or object storage:<br/></p>
<p>Is this storage solution compatible with the architecture&#8217;s required<br/>write and read speeds?<br/>Will storage create a bottleneck for downstream processes?<br/>Do you understand how this storage technology works? Are you<br/>utilizing the storage system optimally or committing unnatural acts?<br/>For instance, are you applying a high rate of random access updates in<br/>an object storage system? (This is an antipattern with significant<br/>performance overhead.)<br/>Will this storage system handle anticipated future scale? You should<br/>consider all capacity limits on the storage system: total available<br/>storage, read operation rate, write volume, etc.<br/>Will downstream users and processes be able to retrieve data in the<br/>required service-level agreement (SLA)?</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Are you capturing metadata about schema evolution, data flows, data<br/>lineage, and so forth? Metadata has a significant impact on the utility<br/>of data. Metadata represents an investment in the future, dramatically<br/>enhancing discoverability and institutional knowledge to streamline<br/>future projects and architecture changes.<br/>Is this a pure storage solution (object storage), or does it support<br/>complex query patterns (i.e., a cloud data warehouse)?<br/>Is the storage system schema-agnostic (object storage)? Flexible<br/>schema (Cassandra)? Enforced schema (a cloud data warehouse)?<br/>How are you tracking master data, golden records data quality, and<br/>data lineage for data governance? (We have more to say on these in<br/>&#8220;Data Management&#8221;.)<br/>How are you handling regulatory compliance and data sovereignty?<br/>For example, can you store your data in certain geographical locations<br/>but not others?<br/></p>
<p><b>Understanding data access frequency<br/></b>Not all data is accessed in the same way. Retrieval patterns will greatly vary<br/>based on the data being stored and queried. This brings up the notion of the<br/>&#8220;temperatures&#8221; of data. Data access frequency will determine the<br/>temperature of your data.<br/>Data that is most frequently accessed is called <i>hot data</i>. Hot data is<br/>commonly retrieved many times per day, perhaps even several times per<br/>second, in systems that serve user requests. This data should be stored for<br/>fast retrieval, where &#8220;fast&#8221; is relative to the use case. <i>Lukewarm data</i> might<br/>be accessed every so often&#8212;say, every week or month.<br/><i>Cold data</i> is seldom queried and is appropriate for storing in an archival<br/>system. Cold data is often retained for compliance purposes or in case of a<br/>catastrophic failure in another system. In the &#8220;old days,&#8221; cold data would be<br/>stored on tapes and shipped to remote archival facilities. In cloud</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>environments, vendors offer specialized storage tiers with very cheap<br/>monthly storage costs but high prices for data retrieval.<br/><b>Selecting a storage system<br/></b>What type of storage solution should you use? This depends on your use<br/>cases, data volumes, frequency of ingestion, format, and size of the data<br/>being ingested&#8212;essentially, the key considerations listed in the preceding<br/>bulleted questions. There is no one-size-fits-all universal storage<br/>recommendation. Every storage technology has its trade-offs. Countless<br/>varieties of storage technologies exist, and it&#8217;s easy to be overwhelmed<br/>when deciding the best option for your data architecture.<br/>Chapter 6 covers storage best practices and approaches in greater detail, as<br/>well as the crossover between storage and other lifecycle stages.<br/></p>
<p><b>Ingestion<br/></b>After you understand the data source and the characteristics of the source<br/>system you&#8217;re using, you need to gather the data. The second stage of the<br/>data engineering lifecycle is data ingestion from source systems.<br/>In our experience, source systems and ingestion represent the most<br/>significant bottlenecks of the data engineering lifecycle. The source systems<br/>are normally outside your direct control and might randomly become<br/>unresponsive or provide data of poor quality. Or, your data ingestion service<br/>might mysteriously stop working for many reasons. As a result, data flow<br/>stops or delivers insufficient data for storage, processing, and serving.<br/>Unreliable source and ingestion systems have a ripple effect across the data<br/>engineering lifecycle. But you&#8217;re in good shape, assuming you&#8217;ve answered<br/>the big questions about source systems.<br/><b>Key engineering considerations for the ingestion phase<br/></b>When preparing to architect or build a system, here are some primary<br/>questions about the ingestion stage:</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>What are the use cases for the data I&#8217;m ingesting? Can I reuse this data<br/>rather than create multiple versions of the same dataset?<br/>Are the systems generating and ingesting this data reliably, and is the<br/>data available when I need it?<br/>What is the data destination after ingestion?<br/>How frequently will I need to access the data?<br/>In what volume will the data typically arrive?<br/>What format is the data in? Can my downstream storage and<br/>transformation systems handle this format?<br/>Is the source data in good shape for immediate downstream use? If so,<br/>for how long, and what may cause it to be unusable?<br/>If the data is from a streaming source, does it need to be transformed<br/>before reaching its destination? Would an in-flight transformation be<br/>appropriate, where the data is transformed within the stream itself?<br/></p>
<p>These are just a sample of the factors you&#8217;ll need to think about with<br/>ingestion, and we cover those questions and more in Chapter 7. Before we<br/>leave, let&#8217;s briefly turn our attention to two major data ingestion concepts:<br/>batch versus streaming and push versus pull.<br/><b>Batch versus streaming<br/></b>Virtually all data we deal with is inherently <i>streaming</i>. Data is nearly<br/>always produced and updated continually at its source. <i>Batch ingestion</i> is<br/>simply a specialized and convenient way of processing this stream in large<br/>chunks&#8212;for example, handling a full day&#8217;s worth of data in a single batch.<br/>Streaming ingestion allows us to provide data to downstream systems&#8212;<br/>whether other applications, databases, or analytics systems&#8212;in a<br/>continuous, real-time fashion. Here, <i>real-time</i> (or <i>near real-time</i>) means that<br/>the data is available to a downstream system a short time after it is</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>produced (e.g., less than one second later). The latency required to qualify<br/>as real-time varies by domain and requirements.<br/>Batch data is ingested either on a predetermined time interval or as data<br/>reaches a preset size threshold. Batch ingestion is a one-way door: once<br/>data is broken into batches, the latency for downstream consumers is<br/>inherently constrained. Because of limitations of legacy systems, batch was<br/>for a long time the default way to ingest data. Batch processing remains an<br/>extremely popular way to ingest data for downstream consumption,<br/>particularly in analytics and ML.<br/>However, the separation of storage and compute in many systems and the<br/>ubiquity of event-streaming and processing platforms make the continuous<br/>processing of data streams much more accessible and increasingly popular.<br/>The choice largely depends on the use case and expectations for data<br/>timeliness.<br/><b>Key considerations for batch versus stream ingestion<br/></b>Should you go streaming-first? Despite the attractiveness of a streaming-<br/>first approach, there are many trade-offs to understand and think about. The<br/>following are some questions to ask yourself when determining whether<br/>streaming ingestion is an appropriate choice over batch ingestion:<br/></p>
<p>If I ingest the data in real time, can downstream storage systems<br/>handle the rate of data flow?<br/>Do I need millisecond real-time data ingestion? Or would a micro-<br/>batch approach work, accumulating and ingesting data, say, every<br/>minute?<br/>What are my use cases for streaming ingestion? What specific benefits<br/>do I realize by implementing streaming? If I get data in real time, what<br/>actions can I take on that data that would be an improvement upon<br/>batch?<br/>Will my streaming-first approach cost more in terms of time, money,<br/>maintenance, downtime, and opportunity cost than simply doing</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>batch?<br/>Are my streaming pipeline and system reliable and redundant if<br/>infrastructure fails?<br/>What tools are most appropriate for the use case? Should I use a<br/>managed service (Amazon Kinesis, Google Cloud Pub/Sub, Google<br/>Cloud Dataflow) or stand up my own instances of Kafka, Flink, Spark,<br/>Pulsar, etc.? If I do the latter, who will manage it? What are the costs<br/>and trade-offs?<br/>If I&#8217;m deploying an ML model, what benefits do I have with online<br/>predictions and possibly continuous training?<br/>Am I getting data from a live production instance? If so, what&#8217;s the<br/>impact of my ingestion process on this source system?<br/></p>
<p>As you can see, streaming-first might seem like a good idea, but it&#8217;s not<br/>always straightforward; extra costs and complexities inherently occur.<br/>Many great ingestion frameworks do handle both batch and micro-batch<br/>ingestion styles. We think batch is an excellent approach for many common<br/>use cases, such as model training and weekly reporting. Adopt true real-<br/>time streaming only after identifying a business use case that justifies the<br/>trade-offs against using batch.<br/><b>Push versus pull<br/></b>In the <i>push</i> model of data ingestion, a source system writes data out to a<br/>target, whether a database, object store, or filesystem. In the <i>pull</i> model,<br/>data is retrieved from the source system. The line between the push and pull<br/>paradigms can be quite blurry; data is often pushed and pulled as it works<br/>its way through the various stages of a data pipeline.<br/>Consider, for example, the extract, transform, load (ETL) process,<br/>commonly used in batch-oriented ingestion workflows. ETL&#8217;s <i>extract</i> (<i>E</i>)<br/>part clarifies that we&#8217;re dealing with a pull ingestion model. In traditional<br/>ETL, the ingestion system queries a current source table snapshot on a fixed</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>schedule. You&#8217;ll learn more about ETL and extract, load, transform (ELT)<br/>throughout this book.<br/>In another example, consider continuous CDC, which is achieved in a few<br/>ways. One common method triggers a message every time a row is changed<br/>in the source database. This message is <i>pushed</i> to a queue, where the<br/>ingestion system picks it up. Another common CDC method uses binary<br/>logs, which record every commit to the database. The database <i>pushes</i> to its<br/>logs. The ingestion system reads the logs but doesn&#8217;t directly interact with<br/>the database otherwise. This adds little to no additional load to the source<br/>database. Some versions of batch CDC use the <i>pull</i> pattern. For example, in<br/>timestamp-based CDC, an ingestion system queries the source database and<br/>pulls the rows that have changed since the previous update.<br/>With streaming ingestion, data bypasses a backend database and is pushed<br/>directly to an endpoint, typically with data buffered by an event-streaming<br/>platform. This pattern is useful with fleets of IoT sensors emitting sensor<br/>data. Rather than relying on a database to maintain the current state, we<br/>simply think of each recorded reading as an event. This pattern is also<br/>growing in popularity in software applications as it simplifies real-time<br/>processing, allows app developers to tailor their messages for downstream<br/>analytics, and greatly simplifies the lives of data engineers.<br/>We discuss ingestion best practices and techniques in depth in Chapter 7.<br/>Next, let&#8217;s turn to the transformation stage of the data engineering lifecycle.<br/></p>
<p><b>Transformation<br/></b>After you&#8217;ve ingested and stored data, you need to do something with it.<br/>The next stage of the data engineering lifecycle is <i>transformation</i>, meaning<br/>data needs to be changed from its original form into something useful for<br/>downstream use cases. Without proper transformations, data will sit inert,<br/>and not be in a useful form for reports, analysis, or ML. Typically, the<br/>transformation stage is where data begins to create value for downstream<br/>user consumption.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Immediately after ingestion, basic transformations map data into correct<br/>types (changing ingested string data into numeric and date types, for<br/>example), putting records into standard formats, and removing bad ones.<br/>Later stages of transformation may transform the data schema and apply<br/>normalization. Downstream, we can apply large-scale aggregation for<br/>reporting or featurize data for ML processes.<br/><b>Key considerations for the transformation phase<br/></b>When considering data transformations within the data engineering<br/>lifecycle, it helps to consider the following:<br/></p>
<p>What&#8217;s the cost and return on investment (ROI) of the transformation?<br/>What is the associated business value?<br/>Is the transformation as simple and self-isolated as possible?<br/>What business rules do the transformations support?<br/>Am I minimizing data movement between the transformation and the<br/>storage system during transformation?<br/></p>
<p>You can transform data in batch or while streaming in flight. As mentioned<br/>in &#8220;Ingestion&#8221;, virtually all data starts life as a continuous stream; batch is<br/>just a specialized way of processing a data stream. Batch transformations<br/>are overwhelmingly popular, but given the growing popularity of stream-<br/>processing solutions and the general increase in the amount of streaming<br/>data, we expect the popularity of streaming transformations to continue<br/>growing, perhaps entirely replacing batch processing in certain domains<br/>soon.<br/>Logically, we treat transformation as a standalone area of the data<br/>engineering lifecycle, but the realities of the lifecycle can be much more<br/>complicated in practice. Transformation is often entangled in other phases<br/>of the lifecycle. Typically, data is transformed in source systems or in flight<br/>during ingestion. For example, a source system may add an event<br/>timestamp to a record before forwarding it to an ingestion process. Or a<br/>record within a streaming pipeline may be &#8220;enriched&#8221; with additional fields</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>and calculations before it&#8217;s sent to a data warehouse. Transformations are<br/>ubiquitous in various parts of the lifecycle. Data preparation, data<br/>wrangling, and cleaning&#8212;these transformative tasks add value for end<br/>consumers of data.<br/>Business logic is a major driver of data transformation, often in data<br/>modeling. Data translates business logic into reusable elements (e.g., a sale<br/>means &#8220;somebody bought 12 picture frames from me for $30 each, or $360<br/>in total&#8221;). In this case, somebody bought 12 picture frames for $30 each.<br/>Data modeling is critical for obtaining a clear and current picture of<br/>business processes. A simple view of raw retail transactions might not be<br/>useful without adding the logic of accounting rules so that the CFO has a<br/>clear picture of financial health. Ensure a standard approach for<br/>implementing business logic across your transformations.<br/>Data featurization for ML is another data transformation process.<br/>Featurization intends to extract and enhance data features useful for training<br/>ML models. Featurization can be a dark art, combining domain expertise (to<br/>identify which features might be important for prediction) with extensive<br/>experience in data science. For this book, the main point is that once data<br/>scientists determine how to featurize data, featurization processes can be<br/>automated by data engineers in the transformation stage of a data pipeline.<br/>Transformation is a profound subject, and we cannot do it justice in this<br/>brief introduction. Chapter 8 delves into queries, data modeling, and various<br/>transformation practices and nuances.<br/></p>
<p><b>Serving Data<br/></b>You&#8217;ve reached the last stage of the data engineering lifecycle. Now that the<br/>data has been ingested, stored, and transformed into coherent and useful<br/>structures, it&#8217;s time to get value from your data. &#8220;Getting value&#8221; from data<br/>means different things to different users.<br/>Data has <i>value</i> when it&#8217;s used for practical purposes. Data that is not<br/>consumed or queried is simply inert. Data vanity projects are a major risk<br/>for companies. Many companies pursued vanity projects in the big data era,</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>gathering massive datasets in data lakes that were never consumed in any<br/>useful way. The cloud era is triggering a new wave of vanity projects built<br/>on the latest data warehouses, object storage systems, and streaming<br/>technologies. Data projects must be intentional across the lifecycle. What is<br/>the ultimate business purpose of the data so carefully collected, cleaned,<br/>and stored?<br/>Data serving is perhaps the most exciting part of the data engineering<br/>lifecycle. This is where the magic happens. This is where ML engineers can<br/>apply the most advanced techniques. Let&#8217;s look at some of the popular uses<br/>of data: analytics, ML, and reverse ETL.<br/><b>Analytics<br/></b>Analytics is the core of most data endeavors. Once your data is stored and<br/>transformed, you&#8217;re ready to generate reports or dashboards, and do ad hoc<br/>analysis on the data. Whereas the bulk of analytics used to encompass BI, it<br/>now includes other facets such as operational analytics and customer-facing<br/>analytics (Figure 2-5). Let&#8217;s briefly touch on these variations of analytics.<br/></p>
<p><i>Figure 2-5. Types of analytics<br/></i></p>
<p><i>Business intelligence<br/></i>BI marshals collected data to describe a business&#8217;s past and current state. BI<br/>requires using business logic to process raw data. Note that data serving for<br/>analytics is yet another area where the stages of the data engineering<br/>lifecycle can get tangled. As we mentioned earlier, business logic is often<br/>applied to data in the transformation stage of the data engineering lifecycle,<br/>but a logic-on-read approach has become increasingly popular. Data is</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>stored in a clean but fairly raw form, with minimal postprocessing business<br/>logic. A BI system maintains a repository of business logic and definitions.<br/>This business logic is used to query the data warehouse so that reports and<br/>dashboards align with business definitions.<br/>As a company grows its data maturity, it will move from ad hoc data<br/>analysis to self-service analytics, allowing democratized data access to<br/>business users without needing IT to intervene. The capability to do self-<br/>service analytics assumes that data is good enough that people across the<br/>organization can simply access it themselves, slice and dice it however they<br/>choose, and get immediate insights. Although self-service analytics is<br/>simple in theory, it&#8217;s tough to pull off in practice. The main reason is that<br/>poor data quality, organizational silos, and a lack of adequate data skills get<br/>in the way of allowing widespread use of analytics.<br/><i>Operational analytics<br/></i>Operational analytics focuses on the fine-grained details of operations,<br/>promoting actions that a user of the reports can act upon immediately.<br/>Operational analytics could be a live view of inventory or real-time<br/>dashboarding of website health. In this case, data is consumed in real time,<br/>either directly from a source system or from a streaming data pipeline. The<br/>types of insights in operational analytics differ from traditional BI since<br/>operational analytics is focused on the present and doesn&#8217;t necessarily<br/>concern historical trends.<br/><i>Embedded analytics<br/></i>You may wonder why we&#8217;ve broken out embedded analytics (customer-<br/>facing analytics) separately from BI. In practice, analytics provided to<br/>customers on a SaaS platform come with a separate set of requirements and<br/>complications. Internal BI faces a limited audience and generally presents a<br/>limited number of unified views. Access controls are critical but not<br/>particularly complicated. Access is managed using a handful of roles and<br/>access tiers.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>With customer-facing analytics, the request rate for reports, and the<br/>corresponding burden on analytics systems, go up dramatically; access<br/>control is significantly more complicated and critical. Businesses may be<br/>serving separate analytics and data to thousands or more customers. Each<br/>customer must see their data and only their data. An internal data-access<br/>error at a company would likely lead to a procedural review. A data leak<br/>between customers would be considered a massive breach of trust, leading<br/>to media attention and a significant loss of customers. Minimize your blast<br/>radius related to data leaks and security vulnerabilities. Apply tenant- or<br/>data-level security within your storage, and anywhere there&#8217;s a possibility<br/>of data leakage.<br/></p>
<p><b>MULTITENANCY<br/></b>Many current storage and analytics systems support multitenancy in<br/>various ways. Data engineers may choose to house data for many<br/>customers in common tables to allow a unified view for internal<br/>analytics and ML. This data is presented externally to individual<br/>customers through logical views with appropriately defined controls<br/>and filters. It is incumbent on data engineers to understand the minutiae<br/>of multitenancy in the systems they deploy to ensure absolute data<br/>security and isolation.<br/></p>
<p><b>Machine learning<br/></b>The emergence and success of ML is one of the most exciting technology<br/>revolutions. Once organizations reach a high level of data maturity, they can<br/>begin to identify problems amenable to ML and start organizing a practice<br/>around it.<br/>The responsibilities of data engineers overlap significantly in analytics and<br/>ML, and the boundaries between data engineering, ML engineering, and<br/>analytics engineering can be fuzzy. For example, a data engineer may need<br/>to support Spark clusters that facilitate analytics pipelines and ML model<br/>training. They may also need to provide a system that orchestrates tasks</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>across teams and support metadata and cataloging systems that track data<br/>history and lineage. Setting these domains of responsibility and the relevant<br/>reporting structures is a critical organizational decision.<br/>The feature store is a recently developed tool that combines data<br/>engineering and ML engineering. Feature stores are designed to reduce the<br/>operational burden for ML engineers by maintaining feature history and<br/>versions, supporting feature sharing among teams, and providing basic<br/>operational and orchestration capabilities, such as backfilling. In practice,<br/>data engineers are part of the core support team for feature stores to support<br/>ML engineering.<br/>Should a data engineer be familiar with ML? It certainly helps. Regardless<br/>of the operational boundary between data engineering, ML engineering,<br/>business analytics, and so forth, data engineers should maintain operational<br/>knowledge about their teams. A good data engineer is conversant in the<br/>fundamental ML techniques and related data-processing requirements, the<br/>use cases for models within their company, and the responsibilities of the<br/>organization&#8217;s various analytics teams. This helps maintain efficient<br/>communication and facilitate collaboration. Ideally, data engineers will<br/>build tools in partnership with other teams that neither team can make<br/>independently.<br/>This book cannot possibly cover ML in depth. A growing ecosystem of<br/>books, videos, articles, and communities is available if you&#8217;re interested in<br/>learning more; we include a few suggestions in &#8220;Additional Resources&#8221;.<br/>The following are some considerations for the serving data phase specific to<br/>ML:<br/></p>
<p>Is the data of sufficient quality to perform reliable feature engineering?<br/>Quality requirements and assessments are developed in close<br/>collaboration with teams consuming the data.<br/>Is the data discoverable? Can data scientists and ML engineers easily<br/>find valuable data?</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Where are the technical and organizational boundaries between data<br/>engineering and ML engineering? This organizational question has<br/>significant architectural implications.<br/>Does the dataset properly represent ground truth? Is it unfairly biased?<br/></p>
<p>While ML is exciting, our experience is that companies often prematurely<br/>dive into it. Before investing a ton of resources into ML, take the time to<br/>build a solid data foundation. This means setting up the best systems and<br/>architecture across the data engineering and ML lifecycle. It&#8217;s generally<br/>best to develop competence in analytics before moving to ML. Many<br/>companies have dashed their ML dreams because they undertook initiatives<br/>without appropriate foundations.<br/><b>Reverse ETL<br/></b>Reverse ETL has long been a practical reality in data, viewed as an<br/>antipattern that we didn&#8217;t like to talk about or dignify with a name. <i>Reverse<br/>ETL</i> takes processed data from the output side of the data engineering<br/>lifecycle and feeds it back into source systems, as shown in Figure 2-6. In<br/>reality, this flow is beneficial and often necessary; reverse ETL allows us to<br/>take analytics, scored models, etc., and feed these back into production<br/>systems or SaaS platforms.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><i>Figure 2-6. Reverse ETL<br/></i></p>
<p>Marketing analysts might calculate bids in Microsoft Excel by using the<br/>data in their data warehouse, and then upload these bids to Google Ads.<br/>This process was often entirely manual and primitive.<br/>As we&#8217;ve written this book, several vendors have embraced the concept of<br/>reverse ETL and built products around it, such as Hightouch and Census.<br/>Reverse ETL remains nascent as a field, but we suspect that it is here to<br/>stay.<br/>Reverse ETL has become especially important as businesses rely<br/>increasingly on SaaS and external platforms. For example, companies may<br/>want to push specific metrics from their data warehouse to a customer data<br/>platform or CRM system. Advertising platforms are another everyday use<br/>case, as in the Google Ads example. Expect to see more activity in reverse<br/>ETL, with an overlap in both data engineering and ML engineering.<br/>The jury is out on whether the term <i>reverse ETL</i> will stick. And the practice<br/>may evolve. Some engineers claim that we can eliminate reverse ETL by<br/>handling data transformations in an event stream and sending those events<br/>back to source systems as needed. Realizing widespread adoption of this</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>pattern across businesses is another matter. The gist is that transformed data<br/>will need to be returned to source systems in some manner, ideally with the<br/>correct lineage and business process associated with the source system.<br/></p>
<p><b>Major Undercurrents Across the Data<br/>Engineering Lifecycle<br/></b>Data engineering is rapidly maturing. Whereas prior cycles of data<br/>engineering simply focused on the technology layer, the continued<br/>abstraction and simplification of tools and practices have shifted this focus.<br/>Data engineering now encompasses far more than tools and technology. The<br/>field is now moving up the value chain, incorporating traditional enterprise<br/>practices such as data management and cost optimization, and newer<br/>practices like DataOps.<br/>We&#8217;ve termed these practices <i>undercurrents</i>&#8212;security, data management,<br/>DataOps, data architecture, orchestration, and software engineering&#8212;that<br/>support every aspect of the data engineering lifecycle (Figure 2-7). In this<br/>section, we give a brief overview of these undercurrents and their major<br/>components, which you&#8217;ll see in more detail throughout the book.<br/></p>
<p><i>Figure 2-7. The major undercurrents of data engineering<br/></i></p>
<p><b>Security<br/></b>Security must be top of mind for data engineers, and those who ignore it do<br/>so at their peril. That&#8217;s why security is the first undercurrent. Data engineers<br/>must understand both data and access security, exercising the principle of</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>least privilege. The principle of least privilege means giving a user or<br/>system access to only the essential data and resources to perform an<br/>intended function. A common antipattern we see with data engineers with<br/>little security experience is to give admin access to all users. This is a<br/>catastrophe waiting to happen!<br/>Give users only the access they need to do their jobs today, nothing more.<br/>Don&#8217;t operate from a root shell when you&#8217;re just looking for visible files<br/>with standard user access. When querying tables with a lesser role, don&#8217;t<br/>use the superuser role in a database. Imposing the principle of least<br/>privilege on ourselves can prevent accidental damage and keep you in a<br/>security-first mindset.<br/>People and organizational structure are always the biggest security<br/>vulnerabilities in any company. When we hear about major security<br/>breaches in the media, it often turns out that someone in the company<br/>ignored basic precautions, fell victim to a phishing attack, or otherwise<br/>acted irresponsibly. The first line of defense for data security is to create a<br/>culture of security that permeates the organization. All individuals who<br/>have access to data must understand their responsibility in protecting the<br/>company&#8217;s sensitive data and its customers.<br/>Data security is also about timing&#8212;providing data access to exactly the<br/>people and systems that need to access it and <i>only for the duration<br/>necessary to perform their work</i>. Data should be protected from unwanted<br/>visibility, both in flight and at rest, by using encryption, tokenization, data<br/>masking, obfuscation, and simple, robust access controls.<br/>Data engineers must be competent security administrators, as security falls<br/>in their domain. A data engineer should understand security best practices<br/>for the cloud and on prem. Knowledge of user and identity access<br/>management (IAM) roles, policies, groups, network security, password<br/>policies, and encryption are good places to start.<br/>Throughout the book, we highlight areas where security should be top of<br/>mind in the data engineering lifecycle. You can also gain more detailed<br/>insights into security in Chapter 10.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><b>Data Management<br/></b>You probably think that data management sounds very&#8230;corporate. &#8220;Old<br/>school&#8221; data management practices make their way into data and ML<br/>engineering. What&#8217;s old is new again. Data management has been around<br/>for decades but didn&#8217;t get a lot of traction in data engineering until recently.<br/>Data tools are becoming simpler, and there is less complexity for data<br/>engineers to manage. As a result, the data engineer moves up the value<br/>chain toward the next rung of best practices. Data best practices once<br/>reserved for huge companies&#8212;data governance, master data management,<br/>data-quality management, metadata management&#8212;are now filtering down<br/>to companies of all sizes and maturity levels. As we like to say, data<br/>engineering is becoming &#8220;enterprisey.&#8221; This is ultimately a great thing!<br/>The Data Management Association International (DAMA) <i>Data<br/>Management Body of Knowledge</i> (<i>DMBOK</i>), which we consider to be the<br/>definitive book for enterprise data management, offers this definition:<br/></p>
<p><i>Data management is the development, execution, and supervision of<br/>plans, policies, programs, and practices that deliver, control, protect, and<br/>enhance the value of data and information assets throughout their<br/>lifecycle.<br/></i></p>
<p>That&#8217;s a bit lengthy, so let&#8217;s look at how it ties to data engineering. Data<br/>engineers manage the data lifecycle, and data management encompasses the<br/>set of best practices that data engineers will use to accomplish this task,<br/>both technically and strategically. Without a framework for managing data,<br/>data engineers are simply technicians operating in a vacuum. Data<br/>engineers need a broader perspective of data&#8217;s utility across the<br/>organization, from the source systems to the C-suite, and everywhere in<br/>between.<br/>Why is data management important? Data management demonstrates that<br/>data is vital to daily operations, just as businesses view financial resources,<br/>finished goods, or real estate as assets. Data management practices form a<br/>cohesive framework that everyone can adopt to ensure that the organization<br/>gets value from data and handles it appropriately.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Data management has quite a few facets, including the following:<br/>Data governance, including discoverability and accountability<br/>Data modeling and design<br/>Data lineage<br/>Storage and operations<br/>Data integration and interoperability<br/>Data lifecycle management<br/>Data systems for advanced analytics and ML<br/>Ethics and privacy<br/></p>
<p>While this book is in no way an exhaustive resource on data management,<br/>let&#8217;s briefly cover some salient points from each area as they relate to data<br/>engineering.<br/><b>Data governance<br/></b>According to <i>Data Governance: The Definitive Guide,</i> &#8220;Data governance is,<br/>first and foremost, a data management function to ensure the quality,<br/>integrity, security, and usability of the data collected by an organization.&#8221;<br/>We can expand on that definition and say that data governance engages<br/>people, processes, and technologies to maximize data value across an<br/>organization while protecting data with appropriate security controls.<br/>Effective data governance is developed with intention and supported by the<br/>organization. When data governance is accidental and haphazard, the side<br/>effects can range from untrusted data to security breaches and everything in<br/>between. Being intentional about data governance will maximize the<br/>organization&#8217;s data capabilities and the value generated from data. It will<br/>also (hopefully) keep a company out of the headlines for questionable or<br/>downright reckless data practices.<br/></p>
<p>1</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Think of the typical example of data governance being done poorly. A<br/>business analyst gets a request for a report but doesn&#8217;t know what data to<br/>use to answer the question. They may spend hours digging through dozens<br/>of tables in a transactional database, wildly guessing at which fields might<br/>be useful. The analyst compiles a &#8220;directionally correct&#8221; report but isn&#8217;t<br/>entirely sure that the report&#8217;s underlying data is accurate or sound. The<br/>recipient of the report also questions the validity of the data. The integrity<br/>of the analyst&#8212;and of all data in the company&#8217;s systems&#8212;is called into<br/>question. The company is confused about its performance, making business<br/>planning impossible.<br/>Data governance is a foundation for data-driven business practices and a<br/>mission-critical part of the data engineering lifecycle. When data<br/>governance is practiced well, people, processes, and technologies align to<br/>treat data as a key business driver; if data issues occur, they are promptly<br/>handled.<br/>The core categories of data governance are discoverability, security, and<br/>accountability.  Within these core categories are subcategories, such as data<br/>quality, metadata, and privacy. Let&#8217;s look at each core category in turn.<br/><i>Discoverability<br/></i>In a data-driven company, data must be available and discoverable. End<br/>users should have quick and reliable access to the data they need to do their<br/>jobs. They should know where the data comes from, how it relates to other<br/>data, and what the data means.<br/>Some key areas of data discoverability include metadata management and<br/>master data management. Let&#8217;s briefly describe these areas.<br/><i>Metadata<br/>Metadata</i> is &#8220;data about data,&#8221; and it underpins every section of the data<br/>engineering lifecycle. Metadata is exactly the data needed to make data<br/>discoverable and governable.<br/>We divide metadata into two major categories: autogenerated and human<br/>generated. Modern data engineering revolves around automation, but<br/></p>
<p>2</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>metadata collection is often manual and error prone.<br/>Technology can assist with this process, removing much of the error-prone<br/>work of manual metadata collection. We&#8217;re seeing a proliferation of data<br/>catalogs, data-lineage tracking systems, and metadata management tools.<br/>Tools can crawl databases to look for relationships and monitor data<br/>pipelines to track where data comes from and where it goes. A low-fidelity<br/>manual approach uses an internally led effort where various stakeholders<br/>crowdsource metadata collection within the organization. These data<br/>management tools are covered in depth throughout the book, as they<br/>undercut much of the data engineering lifecycle.<br/>Metadata becomes a byproduct of data and data processes. However, key<br/>challenges remain. In particular, interoperability and standards are still<br/>lacking. Metadata tools are only as good as their connectors to data systems<br/>and their ability to share metadata. In addition, automated metadata tools<br/>should not entirely take humans out of the loop.<br/>Data has a social element; each organization accumulates social capital and<br/>knowledge around processes, datasets, and pipelines. Human-oriented<br/>metadata systems focus on the social aspect of metadata. This is something<br/>that Airbnb has emphasized in its various blog posts on data tools,<br/>particularly its original Dataportal concept.  Such tools should provide a<br/>place to disclose data owners, data consumers, and domain experts.<br/>Documentation and internal wiki tools provide a key foundation for<br/>metadata management, but these tools should also integrate with automated<br/>data cataloging. For example, data-scanning tools can generate wiki pages<br/>with links to relevant data objects.<br/>Once metadata systems and processes exist, data engineers can consume<br/>metadata in useful ways. Metadata becomes a foundation for designing<br/>pipelines and managing data throughout the lifecycle.<br/><i>DMBOK</i> identifies four main categories of metadata that are useful to data<br/>engineers:<br/></p>
<p>Business metadata<br/></p>
<p>3</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Technical metadata<br/>Operational metadata<br/>Reference metadata<br/></p>
<p>Let&#8217;s briefly describe each category of metadata.<br/><i>Business metadata</i> relates to the way data is used in the business, including<br/>business and data definitions, data rules and logic, how and where data is<br/>used, and the data owner(s).<br/>A data engineer uses business metadata to answer nontechnical questions<br/>about who, what, where, and how. For example, a data engineer may be<br/>tasked with creating a data pipeline for customer sales analysis. But what is<br/>a customer? Is it someone who&#8217;s purchased in the last 90 days? Or someone<br/>who&#8217;s purchased at any time the business has been open? A data engineer<br/>would use the correct data to refer to business metadata (data dictionary or<br/>data catalog) to look up how a &#8220;customer&#8221; is defined. Business metadata<br/>provides a data engineer with the right context and definitions to properly<br/>use data.<br/><i>Technical metadata</i> describes the data created and used by systems across<br/>the data engineering lifecycle. It includes the data model and schema, data<br/>lineage, field mappings, and pipeline workflows. A data engineer uses<br/>technical metadata to create, connect, and monitor various systems across<br/>the data engineering lifecycle.<br/>Here are some common types of technical metadata that a data engineer<br/>will use:<br/></p>
<p>Pipeline metadata (often produced in orchestration systems)<br/>Data lineage<br/>Schema<br/></p>
<p>Orchestration is a central hub that coordinates workflow across various<br/>systems. <i>Pipeline metadata</i> captured in orchestration systems provides</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>details of the workflow schedule, system and data dependencies,<br/>configurations, connection details, and much more.<br/><i>Data-lineage metadata</i> tracks the origin and changes to data, and its<br/>dependencies, over time. As data flows through the data engineering<br/>lifecycle, it evolves through transformations and combinations with other<br/>data. Data lineage provides an audit trail of data&#8217;s evolution as it moves<br/>through various systems and workflows.<br/><i>Schema metadata</i> describes the structure of data stored in a system such as<br/>a database, a data warehouse, a data lake, or a filesystem; it is one of the<br/>key differentiators across different storage systems. Object stores, for<br/>example, don&#8217;t manage schema metadata; instead, this must be managed in<br/>a <i>metastore</i>. On the other hand, cloud data warehouses manage schema<br/>metadata internally.<br/>These are just a few examples of technical metadata that a data engineer<br/>should know about. This is not a complete list, and we cover additional<br/>aspects of technical metadata throughout the book.<br/><i>Operational metadata</i> describes the operational results of various systems<br/>and includes statistics about processes, job IDs, application runtime logs,<br/>data used in a process, and error logs. A data engineer uses operational<br/>metadata to determine whether a process succeeded or failed and the data<br/>involved in the process.<br/>Orchestration systems can provide a limited picture of operational<br/>metadata, but the latter still tends to be scattered across many systems. A<br/>need for better-quality operational metadata, and better metadata<br/>management, is a major motivation for next-generation orchestration and<br/>metadata management systems.<br/><i>Reference metadata</i> is data used to classify other data. This is also referred<br/>to as <i>lookup data</i>. Standard examples of reference data are internal codes,<br/>geographic codes, units of measurement, and internal calendar standards.<br/>Note that much of reference data is fully managed internally, but items such<br/>as geographic codes might come from standard external references.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Reference data is essentially a standard for interpreting other data, so if it<br/>changes, this change happens slowly over time.<br/><i>Data accountability<br/>Data accountability</i> means assigning an individual to govern a portion of<br/>data. The responsible person then coordinates the governance activities of<br/>other stakeholders. Managing data quality is tough if no one is accountable<br/>for the data in question.<br/>Note that people accountable for data need not be data engineers. The<br/>accountable person might be a software engineer or product manager, or<br/>serve in another role. In addition, the responsible person generally doesn&#8217;t<br/>have all the resources necessary to maintain data quality. Instead, they<br/>coordinate with all people who touch the data, including data engineers.<br/>Data accountability can happen at various levels; accountability can happen<br/>at the level of a table or a log stream but could be as fine-grained as a single<br/>field entity that occurs across many tables. An individual may be<br/>accountable for managing a customer ID across many systems. For<br/>enterprise data management, a data domain is the set of all possible values<br/>that can occur for a given field type, such as in this ID example. This may<br/>seem excessively bureaucratic and meticulous, but it can significantly affect<br/>data quality.<br/><i>Data quality<br/></i></p>
<p><i>Can I trust this data?<br/></i>&#8212;Everyone in the business<br/></p>
<p><i>Data quality</i> is the optimization of data toward the desired state and orbits<br/>the question, &#8220;What do you get compared with what you expect?&#8221; Data<br/>should conform to the expectations in the business metadata. Does the data<br/>match the definition agreed upon by the business?<br/>A data engineer ensures data quality across the entire data engineering<br/>lifecycle. This involves performing data-quality tests, and ensuring data<br/>conformance to schema expectations, data completeness, and precision.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>According to <i>Data Governance: The Definitive Guide</i>, data quality is<br/>defined by three main characteristics:<br/><i>Accuracy<br/></i></p>
<p>Is the collected data factually correct? Are there duplicate values? Are<br/>the numeric values accurate?<br/></p>
<p><i>Completeness<br/></i>Are the records complete? Do all required fields contain valid values?<br/></p>
<p><i>Timeliness<br/></i>Are records available in a timely fashion?<br/></p>
<p>Each of these characteristics is quite nuanced. For example, how do we<br/>think about bots and web scrapers when dealing with web event data? If we<br/>intend to analyze the customer journey, we must have a process that lets us<br/>separate humans from machine-generated traffic. Any bot-generated events<br/>misclassified as <i>human</i> present data accuracy issues, and vice versa.<br/>A variety of interesting problems arise concerning completeness and<br/>timeliness. In the Google paper introducing the Dataflow model, the authors<br/>give the example of an offline video platform that displays ads.  The<br/>platform downloads video and ads while a connection is present, allows the<br/>user to watch these while offline, and then uploads ad view data once a<br/>connection is present again. This data may arrive late, well after the ads are<br/>watched. How does the platform handle billing for the ads?<br/>Fundamentally, this problem can&#8217;t be solved by purely technical means.<br/>Rather, engineers will need to determine their standards for late-arriving<br/>data and enforce these uniformly, possibly with the help of various<br/>technology tools.<br/>Data quality sits across the boundary of human and technology problems.<br/>Data engineers need robust processes to collect actionable human feedback<br/>on data quality and use technology tools to detect quality issues<br/></p>
<p>4<br/></p>
<p>5</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>preemptively before downstream users ever see them. We cover these<br/>collection processes in the appropriate chapters throughout this book.<br/></p>
<p><b>MASTER DATA MANAGEMENT<br/></b><i>Master data</i> is data about business entities such as employees,<br/>customers, products, and locations. As organizations grow larger and<br/>more complex through organic growth and acquisitions, and collaborate<br/>with other businesses, maintaining a consistent picture of entities and<br/>identities becomes more and more challenging.<br/><i>Master data management</i> (MDM) is the practice of building consistent<br/>entity definitions known as <i>golden records</i>. Golden records harmonize<br/>entity data across an organization and with its partners. MDM is a<br/>business operations process facilitated by building and deploying<br/>technology tools. For example, an MDM team might determine a<br/>standard format for addresses, and then work with data engineers to<br/>build an API to return consistent addresses and a system that uses<br/>address data to match customer records across company divisions.<br/>MDM reaches across the full data cycle into operational databases. It<br/>may fall directly under the purview of data engineering, but is often the<br/>assigned responsibility of a dedicated team that works across the<br/>organization. Even if they don&#8217;t own MDM, data engineers must always<br/>be aware of it, as they will collaborate on MDM initiatives.<br/></p>
<p><b>Data modeling and design<br/></b>To derive business insights from data, through business analytics and data<br/>science, the data must be in a usable form. The process for converting data<br/>into a usable form is known as <i>data modeling and design</i>. Whereas we<br/>traditionally think of data modeling as a problem for database<br/>administrators (DBAs) and ETL developers, data modeling can happen<br/>almost anywhere in an organization. Firmware engineers develop the data<br/>format of a record for an IoT device, or web application developers design</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>the JSON response to an API call or a MySQL table schema&#8212;these are all<br/>instances of data modeling and design.<br/>Data modeling has become more challenging because of the variety of new<br/>data sources and use cases. For instance, strict normalization doesn&#8217;t work<br/>well with event data. Fortunately, a new generation of data tools increases<br/>the flexibility of data models, while retaining logical separations of<br/>measures, dimensions, attributes, and hierarchies. Cloud data warehouses<br/>support the ingestion of enormous quantities of denormalized and<br/>semistructured data, while still supporting common data modeling patterns,<br/>such as Kimball, Inmon, and data vault. Data processing frameworks such<br/>as Spark can ingest a whole spectrum of data, from flat structured relational<br/>records to raw unstructured text. We discuss these data modeling and<br/>transformation patterns in greater detail in Chapter 8.<br/>With the wide variety of data that engineers must cope with, there is a<br/>temptation to throw up our hands and give up on data modeling. This is a<br/>terrible idea with harrowing consequences, made evident when people<br/>murmur of the write once, read never (WORN) access pattern or refer to a<br/><i>data swamp</i>. Data engineers need to understand modeling best practices as<br/>well as develop the flexibility to apply the appropriate level and type of<br/>modeling to the data source and use case.<br/><b>Data lineage<br/></b>As data moves through its lifecycle, how do you know what system affected<br/>the data or what the data is composed of as it gets passed around and<br/>transformed? <i>Data lineage</i> describes the recording of an audit trail of data<br/>through its lifecycle, tracking both the systems that process the data and the<br/>upstream data it depends on.<br/>Data lineage helps with error tracking, accountability, and debugging of<br/>data and the systems that process it. It has the obvious benefit of giving an<br/>audit trail for the data lifecycle and helps with compliance. For example, if<br/>a user would like their data deleted from your systems, having lineage for<br/>that data lets you know where that data is stored and its dependencies.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Data lineage has been around for a long time in larger companies with strict<br/>compliance standards. However, it&#8217;s now being more widely adopted in<br/>smaller companies as data management becomes mainstream. We also note<br/>that Andy Petrella&#8217;s concept of Data Observability Driven Development<br/>(DODD) is closely related to data lineage. DODD observes data all along<br/>its lineage. This process is applied during development, testing, and finally<br/>production to deliver quality and conformity to expectations.<br/><b>Data integration and interoperability<br/></b><i>Data integration and interoperability</i> is the process of integrating data<br/>across tools and processes. As we move away from a single-stack approach<br/>to analytics and toward a heterogeneous cloud environment in which<br/>various tools process data on demand, integration and interoperability<br/>occupy an ever-widening swath of the data engineer&#8217;s job.<br/>Increasingly, integration happens through general-purpose APIs rather than<br/>custom database connections. For example, a data pipeline might pull data<br/>from the Salesforce API, store it to Amazon S3, call the Snowflake API to<br/>load it into a table, call the API again to run a query, and then export the<br/>results to S3 where Spark can consume them.<br/>All of this activity can be managed with relatively simple Python code that<br/>talks to data systems rather than handling data directly. While the<br/>complexity of interacting with data systems has decreased, the number of<br/>systems and the complexity of pipelines has dramatically increased.<br/>Engineers starting from scratch quickly outgrow the capabilities of bespoke<br/>scripting and stumble into the need for <i>orchestration.</i> Orchestration is one<br/>of our undercurrents, and we discuss it in detail in &#8220;Orchestration&#8221;.<br/><b>Data lifecycle management<br/></b>The advent of data lakes encouraged organizations to ignore data archival<br/>and destruction. Why discard data when you can simply add more storage<br/>ad infinitum? Two changes have encouraged engineers to pay more<br/>attention to what happens at the end of the data engineering lifecycle.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>First, data is increasingly stored in the cloud. This means we have pay-as-<br/>you-go storage costs instead of large up-front capital expenditures for an<br/>on-premises data lake. When every byte shows up on a monthly AWS<br/>statement, CFOs see opportunities for savings. Cloud environments make<br/>data archival a relatively straightforward process. Major cloud vendors offer<br/>archival-specific object storage classes that allow long-term data retention<br/>at an extremely low cost, assuming very infrequent access (it should be<br/>noted that data retrieval isn&#8217;t so cheap, but that&#8217;s for another conversation).<br/>These storage classes also support extra policy controls to prevent<br/>accidental or deliberate deletion of critical archives.<br/>Second, privacy and data retention laws such as the GDPR and the CCPA<br/>require data engineers to actively manage data destruction to respect users&#8217;<br/>&#8220;right to be forgotten.&#8221; Data engineers must know what consumer data they<br/>retain and must have procedures to destroy data in response to requests and<br/>compliance requirements.<br/>Data destruction is straightforward in a cloud data warehouse. SQL<br/>semantics allow deletion of rows conforming to a where clause. Data<br/>destruction was more challenging in data lakes, where write-once, read-<br/>many was the default storage pattern. Tools such as Hive ACID and Delta<br/>Lake allow easy management of deletion transactions at scale. New<br/>generations of metadata management, data lineage, and cataloging tools<br/>will also streamline the end of the data engineering lifecycle.<br/><b>Ethics and privacy<br/></b></p>
<p><i>Ethical behavior is doing the right thing when no one else is watching.<br/></i>&#8212;Aldo Leopold<br/></p>
<p>The last several years of data breaches, misinformation, and mishandling of<br/>data make one thing clear: data impacts people. Data used to live in the<br/>Wild West, freely collected and traded like baseball cards. Those days are<br/>long gone. Whereas data&#8217;s ethical and privacy implications were once<br/>considered nice to have, like security, they&#8217;re now central to the general<br/>data lifecycle. Data engineers need to do the right thing when no one else is</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>watching, because everyone will be watching someday. We hope that more<br/>organizations will encourage a culture of good data ethics and privacy.<br/>How do ethics and privacy impact the data engineering lifecycle? Data<br/>engineers need to ensure that datasets mask personally identifiable<br/>information (PII) and other sensitive information; bias can be identified and<br/>tracked in datasets as they are transformed. Regulatory requirements and<br/>compliance penalties are only growing. Ensure that your data assets are<br/>compliant with a growing number of data regulations, such as GDPR and<br/>CCPA. Please take this seriously. We offer tips throughout the book to<br/>ensure that you&#8217;re baking ethics and privacy into the data engineering<br/>lifecycle.<br/></p>
<p><b>Orchestration<br/></b><i>We think that orchestration matters because we view it as really the<br/>center of gravity of both the data platform as well as the data lifecycle,<br/>the software development lifecycle as it comes to data.<br/></i></p>
<p>&#8212;Nick Schrock, founder of Elementl<br/>Orchestration is not only a central DataOps process, but also a critical part<br/>of the engineering and deployment flow for data jobs. So, what is<br/>orchestration?<br/><i>Orchestration</i> is the process of coordinating many jobs to run as quickly<br/>and efficiently as possible on a scheduled cadence. For instance, people<br/>often refer to orchestration tools like Apache Airflow as <i>schedulers</i>. This<br/>isn&#8217;t quite accurate. A pure scheduler, such as cron, is aware only of time;<br/>an orchestration engine builds in metadata on job dependencies, generally<br/>in the form of a directed acyclic graph (DAG). The DAG can be run once or<br/>scheduled to run at a fixed interval of daily, weekly, every hour, every five<br/>minutes, etc.<br/>As we discuss orchestration throughout this book, we assume that an<br/>orchestration system stays online with high availability. This allows the<br/>orchestration system to sense and monitor constantly without human<br/>intervention and run new jobs anytime they are deployed. An orchestration</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>system monitors jobs that it manages and kicks off new tasks as internal<br/>DAG dependencies are completed. It can also monitor external systems and<br/>tools to watch for data to arrive and criteria to be met. When certain<br/>conditions go out of bounds, the system also sets error conditions and sends<br/>alerts through email or other channels. You might set an expected<br/>completion time of 10 a.m. for overnight daily data pipelines. If jobs are not<br/>done by this time, alerts go out to data engineers and consumers.<br/>Orchestration systems also build job history capabilities, visualization, and<br/>alerting. Advanced orchestration engines can backfill new DAGs or<br/>individual tasks as they are added to a DAG. They also support<br/>dependencies over a time range. For example, a monthly reporting job<br/>might check that an ETL job has been completed for the full month before<br/>starting.<br/>Orchestration has long been a key capability for data processing but was not<br/>often top of mind nor accessible to anyone except the largest companies.<br/>Enterprises used various tools to manage job flows, but these were<br/>expensive, out of reach of small startups, and generally not extensible.<br/>Apache Oozie was extremely popular in the 2010s, but it was designed to<br/>work within a Hadoop cluster and was difficult to use in a more<br/>heterogeneous environment. Facebook developed Dataswarm for internal<br/>use in the late 2000s; this inspired popular tools such as Airflow, introduced<br/>by Airbnb in 2014.<br/>Airflow was open source from its inception, and was widely adopted. It was<br/>written in Python, making it highly extensible to almost any use case<br/>imaginable. While many other interesting open source orchestration<br/>projects exist, such as Luigi and Conductor, Airflow is arguably the<br/>mindshare leader for the time being. Airflow arrived just as data processing<br/>was becoming more abstract and accessible, and engineers were<br/>increasingly interested in coordinating complex flows across multiple<br/>processors and storage systems, especially in cloud environments.<br/>At this writing, several nascent open source projects aim to mimic the best<br/>elements of Airflow&#8217;s core design while improving on it in key areas. Some</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>of the most interesting examples are Prefect and Dagster, which aim to<br/>improve the portability and testability of DAGs to allow engineers to move<br/>from local development to production more easily. Argo is an orchestration<br/>engine built around Kubernetes primitives; Metaflow is an open source<br/>project out of Netflix that aims to improve data science orchestration.<br/>We must point out that orchestration is strictly a batch concept. The<br/>streaming alternative to orchestrated task DAGs is the streaming DAG.<br/>Streaming DAGs remain challenging to build and maintain, but next-<br/>generation streaming platforms such as Pulsar aim to dramatically reduce<br/>the engineering and operational burden. We talk more about these<br/>developments Chapter 8.<br/></p>
<p><b>DataOps<br/></b>DataOps maps the best practices of Agile methodology, DevOps, and<br/>statistical process control (SPC) to data. Whereas DevOps aims to improve<br/>the release and quality of software products, DataOps does the same thing<br/>for data products.<br/>Data products differ from software products because of the way data is<br/>used. A software product provides specific functionality and technical<br/>features for end users. By contrast, a data product is built around sound<br/>business logic and metrics, whose users make decisions or build models<br/>that perform automated actions. A data engineer must understand both the<br/>technical aspects of building software products, and the business logic,<br/>quality, and metrics that will create excellent data products.<br/>Like DevOps, DataOps borrows much from lean manufacturing and supply<br/>chain management, mixing people, processes, and technology to reduce<br/>time to value. As Data Kitchen (experts in DataOps) describes it:6</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><i>DataOps is a collection of technical practices, workflows, cultural<br/>norms, and architectural patterns that enable:<br/></i></p>
<p><i>Rapid innovation and experimentation delivering new insights to<br/>customers with increasing velocity<br/>Extremely high data quality and very low error rates<br/>Collaboration across complex arrays of people, technology, and<br/>environments<br/>Clear measurement, monitoring, and transparency of results<br/></i></p>
<p>Lean practices (such as lead time reduction and minimizing defects) and the<br/>resulting improvements to quality and productivity are things we are glad to<br/>see gaining momentum both in software and data operations.<br/>First and foremost, DataOps is a set of cultural habits; the data engineering<br/>team needs to adopt a cycle of communicating and collaborating with the<br/>business, breaking down silos, continuously learning from successes and<br/>mistakes, and rapid iteration. Only when these cultural habits are set in<br/>place can the team get the best results from technology and tools.<br/>Depending on a company&#8217;s data maturity, a data engineer has some options<br/>to build DataOps into the fabric of the overall data engineering lifecycle. If<br/>the company has no preexisting data infrastructure or practices, DataOps is<br/>very much a greenfield opportunity that can be baked in from day one. With<br/>an existing project or infrastructure that lacks DataOps, a data engineer can<br/>begin adding DataOps into workflows. We suggest first starting with<br/>observability and monitoring to get a window into the performance of a<br/>system, then adding in automation and incident response. A data engineer<br/>may work alongside an existing DataOps team to improve the data<br/>engineering lifecycle in a data-mature company. In all cases, a data engineer<br/>must be aware of the philosophy and technical aspects of DataOps.<br/>DataOps has three core technical elements: automation, monitoring and<br/>observability, and incident response (Figure 2-8). Let&#8217;s look at each of these<br/>pieces and how they relate to the data engineering lifecycle.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><i>Figure 2-8. The three pillars of DataOps<br/></i></p>
<p><b>Automation<br/></b>Automation enables reliability and consistency in the DataOps process and<br/>allows data engineers to quickly deploy new product features, and<br/>improvements to existing workflows. DataOps automation has a similar<br/>framework and workflow to DevOps, consisting of change management<br/>(environment, code, and data version control), continuous<br/>integration/continuous deployment (CI/CD), and configuration as code.<br/>Like DevOps, DataOps practices monitor and maintain the reliability of<br/>technology and systems (data pipelines, orchestration, etc.), with the added<br/>dimension of checking for data quality, data/model drift, metadata integrity,<br/>and more.<br/>Let&#8217;s briefly discuss the evolution of DataOps automation within a<br/>hypothetical organization. An organization with a low level of DataOps<br/>maturity often attempts to schedule multiple stages of data transformation<br/>processes using cron jobs. This works well for a while. As data pipelines<br/>become more complicated, several things are likely to happen. If the cron<br/>jobs are hosted on a cloud instance, the instance may have an operational<br/>problem, causing the jobs to stop running unexpectedly. As the spacing<br/>between jobs becomes tighter, a job will eventually run long, causing a<br/>subsequent job to fail or produce stale data. Engineers may not be aware of<br/>job failures until they hear from analysts that their reports are out-of-date.<br/>As the organization&#8217;s data maturity grows, data engineers will typically<br/>adopt an orchestration framework, perhaps Airflow or Dagster. Data<br/>engineers are aware that Airflow presents an operational burden, but the<br/>benefits of orchestration eventually outweigh the complexity. Engineers<br/>will gradually migrate their cron jobs to Airflow jobs. Now, dependencies<br/>are checked before jobs run. More transformation jobs can be packed into a<br/>given time because each job can start as soon as upstream data is ready<br/>rather than at a fixed, predetermined time.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>The data engineering team still has room for operational improvements. A<br/>data scientist eventually deploys a broken DAG, bringing down the Airflow<br/>web server and leaving the data team operationally blind. After enough<br/>such headaches, the data engineering team members realize that they need<br/>to stop allowing manual DAG deployments. In their next phase of<br/>operational maturity, they adopt automated DAG deployment. DAGs are<br/>tested before deployment, and monitoring processes ensure that the new<br/>DAGs start running properly. In addition, data engineers block the<br/>deployment of new Python dependencies until installation is validated.<br/>After automation is adopted, the data team is much happier and experiences<br/>far fewer headaches.<br/>One of the tenets of the DataOps Manifesto is &#8220;Embrace change.&#8221; This does<br/>not mean change for the sake of change, but goal-oriented change. At each<br/>stage of our automation journey, opportunities exist for operational<br/>improvement. Even at the high level of maturity that we&#8217;ve described here,<br/>further room for improvement remains. Engineers might embrace a next-<br/>generation orchestration framework that builds in better metadata<br/>capabilities. Or they might try to develop a framework that builds DAGs<br/>automatically based on data-lineage specifications. The main point is that<br/>engineers constantly seek to implement improvements in automation that<br/>will reduce their workload and increase the value that they deliver to the<br/>business.<br/><b>Observability and monitoring<br/></b>As we tell our clients, &#8220;Data is a silent killer.&#8221; We&#8217;ve seen countless<br/>examples of bad data lingering in reports for months or years. Executives<br/>may make key decisions from this bad data, discovering the error only<br/>much later. The outcomes are usually bad and sometimes catastrophic for<br/>the business. Initiatives are undermined and destroyed, years of work<br/>wasted. In some of the worst cases, bad data may lead companies to<br/>financial ruin.<br/>Another horror story occurs when the systems that create the data for<br/>reports randomly stop working, resulting in reports being delayed by</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>several days. The data team doesn&#8217;t know until they&#8217;re asked by<br/>stakeholders why reports are late or producing stale information.<br/>Eventually, various stakeholders lose trust in the capabilities of the core<br/>data team and start their own splinter teams. The result is many different<br/>unstable systems, inconsistent reports, and silos.<br/>If you&#8217;re not observing and monitoring your data and the systems that<br/>produce the data, you&#8217;re inevitably going to experience your own data<br/>horror story. Observability, monitoring, logging, alerting, and tracing are all<br/>critical to getting ahead of any problems along the data engineering<br/>lifecycle. We recommend you incorporate SPC to understand whether<br/>events being monitored are out of line and which incidents are worth<br/>responding to.<br/>Petrella&#8217;s DODD method mentioned previously in this chapter provides an<br/>excellent framework for thinking about data observability. DODD is much<br/>like test-driven development (TDD) in software engineering:<br/></p>
<p><i>The purpose of DODD is to give everyone involved in the data chain<br/>visibility into the data and data applications so that everyone involved in<br/>the data value chain has the ability to identify changes to the data or<br/>data applications at every step&#8212;from ingestion to transformation to<br/>analysis&#8212;to help troubleshoot or prevent data issues. DODD focuses on<br/>making data observability a first-class consideration in the data<br/>engineering lifecycle.<br/></i></p>
<p>We cover many aspects of monitoring and observability throughout the data<br/>engineering lifecycle in later chapters.<br/><b>Incident response<br/></b>A high-functioning data team using DataOps will be able to ship new data<br/>products quickly. But mistakes will inevitably happen. A system may have<br/>downtime, a new data model may break downstream reports, an ML model<br/>may become stale and provide bad predictions&#8212;countless problems can<br/>interrupt the data engineering lifecycle. <i>Incident response</i> is about using the<br/>automation and observability capabilities mentioned previously to rapidly<br/></p>
<p><i>7</i></p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>identify root causes of an incident and resolve it as reliably and quickly as<br/>possible.<br/>Incident response isn&#8217;t just about technology and tools, though these are<br/>beneficial; it&#8217;s also about open and blameless communication, both on the<br/>data engineering team and across the organization. As Werner Vogels is<br/>famous for saying, &#8220;Everything breaks all the time.&#8221; Data engineers must be<br/>prepared for a disaster and ready to respond as swiftly and efficiently as<br/>possible.<br/>Data engineers should proactively find issues before the business reports<br/>them. Failure happens, and when the stakeholders or end users see<br/>problems, they will present them. They will be unhappy to do so. The<br/>feeling is different when they go to raise those issues to a team and see that<br/>they are actively being worked on to resolve already. Which team&#8217;s state<br/>would you trust more as an end user? Trust takes a long time to build and<br/>can be lost in minutes. Incident response is as much about retroactively<br/>responding to incidents as proactively addressing them before they happen.<br/><b>DataOps summary<br/></b>At this point, DataOps is still a work in progress. Practitioners have done a<br/>good job of adapting DevOps principles to the data domain and mapping<br/>out an initial vision through the DataOps Manifesto and other resources.<br/>Data engineers would do well to make DataOps practices a high priority in<br/>all of their work. The up-front effort will see a significant long-term payoff<br/>through faster delivery of products, better reliability and accuracy of data,<br/>and greater overall value for the business.<br/>The state of operations in data engineering is still quite immature compared<br/>with software engineering. Many data engineering tools, especially legacy<br/>monoliths, are not automation-first. A recent movement has arisen to adopt<br/>automation best practices across the data engineering lifecycle. Tools like<br/>Airflow have paved the way for a new generation of automation and data<br/>management tools. The general practices we describe for DataOps are<br/>aspirational, and we suggest companies try to adopt them to the fullest<br/>extent possible, given the tools and knowledge available today.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><b>Data Architecture<br/></b>A data architecture reflects the current and future state of data systems that<br/>support an organization&#8217;s long-term data needs and strategy. Because an<br/>organization&#8217;s data requirements will likely change rapidly, and new tools<br/>and practices seem to arrive on a near-daily basis, data engineers must<br/>understand good data architecture. Chapter 3 covers data architecture in<br/>depth, but we want to highlight here that data architecture is an<br/>undercurrent of the data engineering lifecycle.<br/>A data engineer should first understand the needs of the business and gather<br/>requirements for new use cases. Next, a data engineer needs to translate<br/>those requirements to design new ways to capture and serve data, balanced<br/>for cost and operational simplicity. This means knowing the trade-offs with<br/>design patterns, technologies, and tools in source systems, ingestion,<br/>storage, transformation, and serving data.<br/>This doesn&#8217;t imply that a data engineer is a data architect, as these are<br/>typically two separate roles. If a data engineer works alongside a data<br/>architect, the data engineer should be able to deliver on the data architect&#8217;s<br/>designs and provide architectural feedback.<br/></p>
<p><b>Software Engineering<br/></b>Software engineering has always been a central skill for data engineers. In<br/>the early days of contemporary data engineering (2000&#8211;-2010), data<br/>engineers worked on low-level frameworks and wrote MapReduce jobs in<br/>C, C++, and Java. At the peak of the big data era (the mid-2010s), engineers<br/>started using frameworks that abstracted away these low-level details.<br/>This abstraction continues today. Cloud data warehouses support powerful<br/>transformations using SQL semantics; tools like Spark have become more<br/>user-friendly, transitioning away from low-level coding details and toward<br/>easy-to-use dataframes. Despite this abstraction, software engineering is<br/>still critical to data engineering. We want to briefly discuss a few common<br/>areas of software engineering that apply to the data engineering lifecycle.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><b>Core data processing code<br/></b>Though it has become more abstract and easier to manage, core data<br/>processing code still needs to be written, and it appears throughout the data<br/>engineering lifecycle. Whether in ingestion, transformation, or data serving,<br/>data engineers need to be highly proficient and productive in frameworks<br/>and languages such as Spark, SQL, or Beam; we reject the notion that SQL<br/>is not code.<br/>It&#8217;s also imperative that a data engineer understand proper code-testing<br/>methodologies, such as unit, regression, integration, end-to-end, and smoke.<br/><b>Development of open source frameworks<br/></b>Many data engineers are heavily involved in developing open source<br/>frameworks. They adopt these frameworks to solve specific problems in the<br/>data engineering lifecycle, and then continue developing the framework<br/>code to improve the tools for their use cases and contribute back to the<br/>community.<br/>In the big data era, we saw a Cambrian explosion of data-processing<br/>frameworks inside the Hadoop ecosystem. These tools primarily focused on<br/>transforming and serving parts of the data engineering lifecycle. Data<br/>engineering tool speciation has not ceased or slowed down, but the<br/>emphasis has shifted up the ladder of abstraction, away from direct data<br/>processing. This new generation of open source tools assists engineers in<br/>managing, enhancing, connecting, optimizing, and monitoring data.<br/>For example, Airflow dominated the orchestration space from 2015 until<br/>the early 2020s. Now, a new batch of open source competitors (including<br/>Prefect, Dagster, and Metaflow) has sprung up to fix perceived limitations<br/>of Airflow, providing better metadata handling, portability, and dependency<br/>management. Where the future of orchestration goes is anyone&#8217;s guess.<br/>Before data engineers begin engineering new internal tools, they would do<br/>well to survey the landscape of publicly available tools. Keep an eye on the<br/>total cost of ownership (TCO) and opportunity cost associated with<br/>implementing a tool. There is a good chance that an open source project</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>already exists to address the problem they&#8217;re looking to solve, and they<br/>would do well to collaborate rather than reinventing the wheel.<br/><b>Streaming<br/></b>Streaming data processing is inherently more complicated than batch, and<br/>the tools and paradigms are arguably less mature. As streaming data<br/>becomes more pervasive in every stage of the data engineering lifecycle,<br/>data engineers face interesting software engineering problems.<br/>For instance, data processing tasks such as joins that we take for granted in<br/>the batch processing world often become more complicated in real time,<br/>requiring more complex software engineering. Engineers must also write<br/>code to apply a variety of <i>windowing</i> methods. Windowing allows real-time<br/>systems to calculate valuable metrics such as trailing statistics. Engineers<br/>have many frameworks to choose from, including various function<br/>platforms (OpenFaaS, AWS Lambda, Google Cloud Functions) for<br/>handling individual events or dedicated stream processors (Spark, Beam,<br/>Flink or Pulsar) for analyzing streams to support reporting and real-time<br/>actions.<br/><b>Infrastructure as code<br/></b><i>Infrastructure as code</i> (IaC) applies software engineering practices to the<br/>configuration and management of infrastructure. The infrastructure<br/>management burden of the big data era has decreased as companies have<br/>migrated to managed big data systems (such as Databricks and Amazon<br/>EMR) and cloud data warehouses. When data engineers have to manage<br/>their infrastructure in a cloud environment, they increasingly do this<br/>through IaC frameworks rather than manually spinning up instances and<br/>installing software. Several general-purpose and cloud-platform-specific<br/>frameworks allow automated infrastructure deployment based on a set of<br/>specifications. Many of these frameworks can manage cloud services as<br/>well as infrastructure. There is also a notion of IaC with containers and<br/>Kubernetes, using tools like Helm.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>These practices are a vital part of DevOps, allowing version control and<br/>repeatability of deployments. Naturally, these capabilities are precious<br/>throughout the data engineering lifecycle, especially as we adopt DataOps<br/>practices.<br/><b>Pipelines as code<br/></b><i>Pipelines as code</i> is the core concept of present-day orchestration systems,<br/>which touch every stage of the data engineering lifecycle. Data engineers<br/>use code (typically Python) to declare data tasks and dependencies among<br/>them. The orchestration engine interprets these instructions to run steps<br/>using available resources.<br/><b>General-purpose problem solving<br/></b>In practice, regardless of which high-level tools they adopt, data engineers<br/>will run into corner cases throughout the data engineering lifecycle that<br/>require them to solve problems outside the boundaries of their chosen tools<br/>and to write custom code. When using frameworks like Fivetran, Airbyte,<br/>or Singer, data engineers will encounter data sources without existing<br/>connectors and need to write something custom. They should be proficient<br/>in software engineering to understand APIs, pull and transform data, handle<br/>exceptions, and so forth.<br/></p>
<p><b>Conclusion<br/></b>Most discussions we&#8217;ve seen in the past about data engineering involve<br/>technologies but miss the bigger picture of data lifecycle management. As<br/>technologies become more abstract and do more heavy lifting, a data<br/>engineer has the opportunity to think and act on a higher level. The data<br/>engineering lifecycle, supported by its undercurrents, is an extremely useful<br/>mental model for organizing the work of data engineering.<br/>We break the data engineering lifecycle into the following stages:<br/></p>
<p>Generation</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Storage<br/>Ingestion<br/>Transformation<br/>Serving data<br/></p>
<p>Several themes cut across the data engineering lifecycle as well. These are<br/>the undercurrents of the data engineering lifecycle. At a high level, the<br/>undercurrents are as follows:<br/></p>
<p>Security<br/>Data management<br/>DataOps<br/>Data architecture<br/>Orchestration<br/>Software engineering<br/></p>
<p>A data engineer has several top-level goals across the data lifecycle:<br/>produce optimum ROI and reduce costs (financial and opportunity), reduce<br/>risk (security, data quality), and maximize data value and utility.<br/>The next two chapters discuss how these elements impact good architecture<br/>design, along with choosing the right technologies. If you feel comfortable<br/>with these two topics, feel free to skip ahead to Part II, where we cover each<br/>of the stages of the data engineering lifecycle.<br/></p>
<p><b>Additional Resources<br/></b><i>Managing the data engineering lifecycle:<br/></i></p>
<p>&#8220;Staying Ahead of Debt&#8221; by Etai Mizrahi<br/><i>Data transformation and processing:</i></p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>&#8220;Data transformation&#8221; Wikipedia page<br/>&#8220;Data processing&#8221; Wikipedia page<br/>&#8220;A Comparison of Data Processing Frameworks&#8221; by Ludovic Santos<br/></p>
<p><i>Undercurrents:<br/></i>&#8220;The Dataflow Model: A Practical Approach to Balancing<br/>Correctness, Latency, and Cost in Massive-Scale, Unbounded, Out-of-<br/>Order Data Processing&#8221; by Tyler Akidau et al.<br/></p>
<p><i>DataOps:<br/></i>&#8220;Getting Started with DevOps Automation&#8221; by Jared Murrell<br/>&#8220;Incident Management in the Age of DevOps&#8221; Atlassian web page<br/>&#8220;The Seven Stages of Effective Incident Response&#8221; Atlassian web<br/>page<br/>&#8220;Is DevOps Related to DataOps?&#8221; by Carol Jang and Jove Kuang<br/></p>
<p><i>Data management:<br/></i>DAMA International website<br/></p>
<p><i>Data management and metadata:<br/></i>&#8220;What Is Metadata&#8221; by Michelle Knight<br/></p>
<p><i>Airbnb Data Portal:<br/></i>&#8220;Democratizing Data at Airbnb&#8221; by Chris Williams et al.<br/>&#8220;Five Steps to Begin Collecting the Value of Your Data&#8221; Lean-Data<br/>web page<br/></p>
<p><i>Orchestration:<br/></i>&#8220;An Introduction to Dagster: The Orchestrator for the Full Data<br/>Lifecycle&#8221; video by Nick Schrock</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>1  Evren Eryurek et al., <i>Data Governance: The Definitive Guide</i> (Sebastopol, CA: O&#8217;Reilly,<br/>2021), 1, <i>https://oreil.ly/LFT4d</i>.<br/></p>
<p>2  Eryurek, <i>Data Governance</i>, 5.<br/>3  Chris Williams et al., &#8220;Democratizing Data at Airbnb,&#8221; <i>The Airbnb Tech Blog</i>, May 12, 2017,<br/></p>
<p><i>https://oreil.ly/dM332</i>.<br/>4  Eryurek, <i>Data Governance</i>, 113.<br/>5  Tyler Akidau et al., &#8220;The Dataflow Model: A Practical Approach to Balancing Correctness,<br/></p>
<p>Latency, and Cost in Massive-Scale, Unbounded, Out-of-Order Data Processing,&#8221; <i>Proceedings<br/>of the VLDB Endowment</i> 8 (2015): 1792&#8211;1803, https://oreil.ly/Z6XYy.<br/></p>
<p>6  &#8220;What Is DataOps,&#8221; DataKitchen FAQ page, accessed May 5, 2022, <i>https://oreil.ly/Ns06w</i>.<br/>7  Andy Petrella, &#8220;Data Observability Driven Development: The Perfect Analogy for<br/></p>
<p>Beginners,&#8221; Kensu, accessed May 5, 2022, <i>https://oreil.ly/MxvSX</i>.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><b>Chapter 3. Designing Good<br/>Data Architecture<br/></b>Good data architecture provides seamless capabilities across every step of<br/>the data lifecycle and undercurrent. We&#8217;ll begin by defining <i>data<br/>architecture</i> and then discuss components and considerations. We&#8217;ll then<br/>touch on specific batch patterns (data warehouses, data lakes), streaming<br/>patterns, and patterns that unify batch and streaming. Throughout, we&#8217;ll<br/>emphasize leveraging the capabilities of the cloud to deliver scalability,<br/>availability, and reliability.<br/></p>
<p><b>What Is Data Architecture?<br/></b>Successful data engineering is built upon rock-solid data architecture. This<br/>chapter aims to review a few popular architecture approaches and<br/>frameworks, and then craft our opinionated definition of what makes<br/>&#8220;good&#8221; data architecture. Indeed, we won&#8217;t make everyone happy. Still, we<br/>will lay out a pragmatic, domain-specific, working definition for <i>data<br/>architecture</i> that we think will work for companies of vastly different<br/>scales, business processes, and needs.<br/>What is data architecture? When you stop to unpack it, the topic becomes a<br/>bit murky; researching data architecture yields many inconsistent and often<br/>outdated definitions. It&#8217;s a lot like when we defined <i>data engineering</i> in<br/>Chapter 1&#8212;there&#8217;s no consensus. In a field that is constantly changing, this<br/>is to be expected. So what do we mean by <i>data architecture</i> for the<br/>purposes of this book? Before defining the term, it&#8217;s essential to understand<br/>the context in which it sits. Let&#8217;s briefly cover enterprise architecture, which<br/>will frame our definition of data architecture.<br/></p>
<p><b>Enterprise Architecture, Defined</b></p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Enterprise architecture has many subsets, including business, technical,<br/>application, and data (Figure 3-1). As such, many frameworks and<br/>resources are devoted to enterprise architecture. In truth, architecture is a<br/>surprisingly controversial topic.<br/></p>
<p><i>Figure 3-1. Data architecture is a subset of enterprise architecture<br/></i></p>
<p>The term <i>enterprise</i> gets mixed reactions. It brings to mind sterile corporate<br/>offices, command-and-control/waterfall planning, stagnant business<br/>cultures, and empty catchphrases. Even so, we can learn some things here.<br/>Before we define and describe <i>enterprise architecture</i>, let&#8217;s unpack this<br/>term. Let&#8217;s look at how enterprise architecture is defined by some<br/>significant thought leaders: TOGAF, Gartner, and EABOK.<br/><b>TOGAF&#8217;s definition<br/></b><i>TOGAF</i> is <i>The Open Group Architecture Framework</i>, a standard of The<br/>Open Group. It&#8217;s touted as the most widely used architecture framework<br/>today. Here&#8217;s the TOGAF definition:<br/></p>
<p><i>The term &#8220;enterprise&#8221; in the context of &#8220;enterprise architecture&#8221; can<br/>denote an entire enterprise&#8212;encompassing all of its information and<br/>technology services, processes, and infrastructure&#8212;or a specific domain<br/>within the enterprise. In both cases, the architecture crosses multiple<br/>systems, and multiple functional groups within the enterprise.<br/></i></p>
<p><b>Gartner&#8217;s definition<br/></b><i>Gartner</i> is a global research and advisory company that produces research<br/>articles and reports on trends related to enterprises. Among other things, it<br/>is responsible for the (in)famous Gartner Hype Cycle. Gartner&#8217;s definition<br/>is as follows:</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><i>Enterprise architecture (EA) is a discipline for proactively and<br/>holistically leading enterprise responses to disruptive forces by<br/>identifying and analyzing the execution of change toward desired<br/>business vision and outcomes. EA delivers value by presenting business<br/>and IT leaders with signature-ready recommendations for adjusting<br/>policies and projects to achieve targeted business outcomes that<br/>capitalize on relevant business disruptions.<br/></i></p>
<p><b>EABOK&#8217;s definition<br/></b><i>EABOK</i> is the <i>Enterprise Architecture Book of Knowledge</i>, an enterprise<br/>architecture reference produced by the MITRE Corporation. EABOK was<br/>released as an incomplete draft in 2004 and has not been updated since.<br/>Though seemingly obsolete, EABOK is still frequently referenced in<br/>descriptions of enterprise architecture; we found many of its ideas helpful<br/>while writing this book. Here&#8217;s the EABOK definition:<br/></p>
<p><i>Enterprise Architecture (EA) is an organizational model; an abstract<br/>representation of an Enterprise that aligns strategy, operations, and<br/>technology to create a roadmap for success.<br/></i></p>
<p><b>Our definition<br/></b>We extract a few common threads in these definitions of enterprise<br/>architecture: change, alignment, organization, opportunities, problem-<br/>solving, and migration. Here is our definition of <i>enterprise architecture</i>, one<br/>that we feel is more relevant to today&#8217;s fast-moving data landscape:<br/></p>
<p><i>Enterprise architecture is the design of systems to support change in the<br/>enterprise, achieved by flexible and reversible decisions reached through<br/>careful evaluation of trade-offs.<br/></i></p>
<p>Here, we touch on some key areas we&#8217;ll return to throughout the book:<br/>flexible and reversible decisions, change management, and evaluation of<br/>trade-offs. We discuss each theme at length in this section and then make<br/>the definition more concrete in the latter part of the chapter by giving<br/>various examples of data architecture.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Flexible and reversible decisions are essential for two reasons. First, the<br/>world is constantly changing, and predicting the future is impossible.<br/>Reversible decisions allow you to adjust course as the world changes and<br/>you gather new information. Second, there is a natural tendency toward<br/>enterprise ossification as organizations grow. As organizations grow, they<br/>become increasingly risk averse and cumbersome. Adopting a culture of<br/>reversible decisions helps overcome this tendency by reducing the risk<br/>attached to a decision.<br/>Jeff Bezos is credited with the idea of one-way and two-way doors.  A <i>one-<br/>way door</i> is a decision that is almost impossible to reverse. For example,<br/>Amazon could have decided to sell AWS or shut it down. It would be nearly<br/>impossible for Amazon to rebuild a public cloud with the same market<br/>position after such an action.<br/>On the other hand, a <i>two-way door</i> is an easily reversible decision: you<br/>walk through and proceed if you like what you see in the room or step back<br/>through the door if you don&#8217;t. Amazon might decide to require the use of<br/>DynamoDB for a new microservices database. If this policy doesn&#8217;t work,<br/>Amazon has the option of reversing it and refactoring some services to use<br/>other databases. Since the stakes attached to each reversible decision (two-<br/>way door) are low, organizations can make more decisions, iterating,<br/>improving, and collecting data rapidly.<br/>Change management is closely related to reversible decisions and is a<br/>central theme of enterprise architecture frameworks. Even with an emphasis<br/>on reversible decisions, enterprises often need to undertake large initiatives.<br/>These are ideally broken into smaller changes, each one a reversible<br/>decision in itself. Returning to Amazon, we note a five-year gap (2007 to<br/>2012) from the publication of a paper on the DynamoDB concept to Werner<br/>Vogels&#8217;s announcement of the DynamoDB service on AWS. Behind the<br/>scenes, teams took numerous small actions to make DynamoDB a concrete<br/>reality for AWS customers. Managing such small actions is at the heart of<br/>change management.<br/></p>
<p>1</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Architects are not simply mapping out IT processes and vaguely looking<br/>toward a distant, utopian future; they actively solve business problems and<br/>create new opportunities. Technical solutions exist not for their own sake<br/>but in support of business goals. Architects identify problems in the current<br/>state (poor data quality, scalability limits, money-losing lines of business),<br/>define desired future states (agile data-quality improvement, scalable cloud<br/>data solutions, improved business processes), and realize initiatives through<br/>execution of small, concrete steps.<br/></p>
<p><i>Technical solutions exist not for their own sake but in support of business<br/>goals.<br/></i></p>
<p>We found significant inspiration in <i>Fundamentals of Software Architecture<br/></i>by Mark Richards and Neal Ford (O&#8217;Reilly). They emphasize that trade-offs<br/>are inevitable and ubiquitous in the engineering space. Sometimes the<br/>relatively fluid nature of software and data leads us to believe that we are<br/>freed from the constraints that engineers face in the hard, cold physical<br/>world. Indeed, this is partially true; patching a software bug is much easier<br/>than redesigning and replacing an airplane wing. However, digital systems<br/>are ultimately constrained by physical limits such as latency, reliability,<br/>density, and energy consumption. Engineers also confront various<br/>nonphysical limits, such as characteristics of programming languages and<br/>frameworks, and practical constraints in managing complexity, budgets, etc.<br/>Magical thinking culminates in poor engineering. Data engineers must<br/>account for trade-offs at every step to design an optimal system while<br/>minimizing high-interest technical debt.<br/>Let&#8217;s reiterate one central point in our enterprise architecture definition:<br/>enterprise architecture balances flexibility and trade-offs. This isn&#8217;t always<br/>an easy balance, and architects must constantly assess and reevaluate with<br/>the recognition that the world is dynamic. Given the pace of change that<br/>enterprises are faced with, organizations&#8212;and their architecture&#8212;cannot<br/>afford to stand still.<br/></p>
<p><b>Data Architecture Defined</b></p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Now that you understand enterprise architecture, let&#8217;s dive into data<br/>architecture by establishing a working definition that will set the stage for<br/>the rest of the book. <i>Data architecture</i> is a subset of enterprise architecture,<br/>inheriting its properties: processes, strategy, change management, and<br/>technology. Here are a couple of definitions of data architecture that<br/>influence our definition.<br/><b>TOGAF&#8217;s definition<br/></b>TOGAF defines data architecture as follows:<br/></p>
<p><i>A description of the structure and interaction of the enterprise&#8217;s major<br/>types and sources of data, logical data assets, physical data assets, and<br/>data management resources.<br/></i></p>
<p><b>DAMA&#8217;s definition<br/></b>The DAMA <i>DMBOK</i> defines data architecture as follows:<br/></p>
<p><i>Identifying the data needs of the enterprise (regardless of structure) and<br/>designing and maintaining the master blueprints to meet those needs.<br/>Using master blueprints to guide data integration, control data assets,<br/>and align data investments with business strategy.<br/></i></p>
<p><b>Our definition<br/></b>Considering the preceding two definitions and our experience, we have<br/>crafted our definition of <i>data architecture</i>:<br/></p>
<p><i>Data architecture is the design of systems to support the evolving data<br/>needs of an enterprise, achieved by flexible and reversible decisions<br/>reached through a careful evaluation of trade-offs.<br/></i></p>
<p>How does data architecture fit into data engineering? Just as the data<br/>engineering lifecycle is a subset of the data lifecycle, data engineering<br/>architecture is a subset of general data architecture. <i>Data engineering<br/>architecture</i> is the systems and frameworks that make up the key sections of<br/>the data engineering lifecycle. We&#8217;ll use <i>data architecture</i> interchangeably<br/>with <i>data engineering architecture</i> throughout this book.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Other aspects of data architecture that you should be aware of are<br/>operational and technical (Figure 3-2). <i>Operational architecture<br/></i>encompasses the functional requirements of what needs to happen related to<br/>people, processes, and technology. For example, what business processes<br/>does the data serve? How does the organization manage data quality? What<br/>is the latency requirement from when the data is produced to when it<br/>becomes available to query? <i>Technical architecture</i> outlines how data is<br/>ingested, stored, transformed, and served along the data engineering<br/>lifecycle. For instance, how will you move 10 TB of data every hour from a<br/>source database to your data lake? In short, operational architecture<br/>describes <i>what</i> needs to be done, and technical architecture details <i>how</i> it<br/>will happen.<br/></p>
<p><i>Figure 3-2. Operational and technical data architecture<br/></i></p>
<p>Now that we have a working definition of data architecture, let&#8217;s cover the<br/>elements of &#8220;good&#8221; data architecture.<br/></p>
<p><b>&#8220;Good&#8221; Data Architecture<br/></b><i>Never shoot for the best architecture, but rather the least worst<br/>architecture.<br/></i></p>
<p>&#8212;Neal Ford, Mark Richards<br/>According to Grady Booch, &#8220;Architecture represents the significant design<br/>decisions that shape a system, where <i>significant</i> is measured by cost of<br/>change.&#8221; Data architects aim to make significant decisions that will lead to<br/>good architecture at a basic level.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>What do we mean by &#8220;good&#8221; data architecture? To paraphrase an old cliche,<br/>you know good when you see it. <i>Good data architecture</i> serves business<br/>requirements with a common, widely reusable set of building blocks while<br/>maintaining flexibility and making appropriate trade-offs. Bad architecture<br/>is authoritarian and tries to cram a bunch of one-size-fits-all decisions into a<br/>big ball of mud.<br/>Agility is the foundation for good data architecture; it acknowledges that<br/>the world is fluid. <i>Good data architecture is flexible and easily<br/>maintainable</i>. It evolves in response to changes within the business and new<br/>technologies and practices that may unlock even more value in the future.<br/>Businesses and their use cases for data are always evolving. The world is<br/>dynamic, and the pace of change in the data space is accelerating. Last<br/>year&#8217;s data architecture that served you well might not be sufficient for<br/>today, let alone next year.<br/>Bad data architecture is tightly coupled, rigid, overly centralized, or uses<br/>the wrong tools for the job, hampering development and change<br/>management. Ideally, by designing architecture with reversibility in mind,<br/>changes will be less costly.<br/>The undercurrents of the data engineering lifecycle form the foundation of<br/>good data architecture for companies at any stage of data maturity. Again,<br/>these undercurrents are security, data management, DataOps, data<br/>architecture, orchestration, and software engineering.<br/>Good data architecture is a living, breathing thing. It&#8217;s never finished. In<br/>fact, per our definition, change and evolution are central to the meaning and<br/>purpose of data architecture. Let&#8217;s now look at the principles of good data<br/>architecture.<br/></p>
<p><b>Principles of Good Data Architecture<br/></b>This section takes a 10,000-foot view of good architecture by focusing on<br/>principles&#8212;key ideas useful in evaluating major architectural decisions and<br/>practices. We borrow inspiration for our architecture principles from several</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>sources, especially the AWS Well-Architected Framework and Google<br/>Cloud&#8217;s Five Principles for Cloud-Native Architecture.<br/>The AWS Well-Architected Framework consists of six pillars:<br/></p>
<p>Operational excellence<br/>Security<br/>Reliability<br/>Performance efficiency<br/>Cost optimization<br/>Sustainability<br/></p>
<p>Google Cloud&#8217;s Five Principles for Cloud-Native Architecture are as<br/>follows:<br/></p>
<p>Design for automation.<br/>Be smart with state.<br/>Favor managed services.<br/>Practice defense in depth.<br/>Always be architecting.<br/></p>
<p>We advise you to carefully study both frameworks, identify valuable ideas,<br/>and determine points of disagreement. We&#8217;d like to expand or elaborate on<br/>these pillars with these principles of data engineering architecture:<br/></p>
<p>1. Choose common components wisely.<br/>2. Plan for failure.<br/>3. Architect for scalability.<br/>4. Architecture is leadership.<br/>5. Always be architecting.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>6. Build loosely coupled systems.<br/>7. Make reversible decisions.<br/>8. Prioritize security.<br/>9. Embrace FinOps.<br/></p>
<p><b>Principle 1: Choose Common Components Wisely<br/></b>One of the primary jobs of a data engineer is to choose common<br/>components and practices that can be used widely across an organization.<br/>When architects choose well and lead effectively, common components<br/>become a fabric facilitating team collaboration and breaking down silos.<br/>Common components enable agility within and across teams in conjunction<br/>with shared knowledge and skills.<br/>Common components can be anything that has broad applicability within an<br/>organization. Common components include object storage, version-control<br/>systems, observability, monitoring and orchestration systems, and<br/>processing engines. Common components should be accessible to everyone<br/>with an appropriate use case, and teams are encouraged to rely on common<br/>components already in use rather than reinventing the wheel. Common<br/>components must support robust permissions and security to enable sharing<br/>of assets among teams while preventing unauthorized access.<br/>Cloud platforms are an ideal place to adopt common components. For<br/>example, compute and storage separation in cloud data systems allows users<br/>to access a shared storage layer (most commonly object storage) using<br/>specialized tools to access and query the data needed for specific use cases.<br/>Choosing common components is a balancing act. On the one hand, you<br/>need to focus on needs across the data engineering lifecycle and teams,<br/>utilize common components that will be useful for individual projects, and<br/>simultaneously facilitate interoperation and collaboration. On the other<br/>hand, architects should avoid decisions that will hamper the productivity of<br/>engineers working on domain-specific problems by forcing them into one-<br/>size-fits-all technology solutions. Chapter 4 provides additional details.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><b>Principle 2: Plan for Failure<br/></b><i>Everything fails, all the time.<br/></i></p>
<p>&#8212;Werner Vogels<br/>Modern hardware is highly robust and durable. Even so, any hardware<br/>component will fail, given enough time. To build highly robust data<br/>systems, you must consider failures in your designs. Here are a few key<br/>terms for evaluating failure scenarios; we describe these in greater detail in<br/>this chapter and throughout the book:<br/><i>Availability<br/></i></p>
<p>The percentage of time an IT service or component is in an operable<br/>state.<br/></p>
<p><i>Reliability<br/></i>The system&#8217;s probability of meeting defined standards in performing its<br/>intended function during a specified interval.<br/></p>
<p><i>Recovery time objective<br/></i>The maximum acceptable time for a service or system outage. The<br/>recovery time objective (RTO) is generally set by determining the<br/>business impact of an outage. An RTO of one day might be fine for an<br/>internal reporting system. A website outage of just five minutes could<br/>have a significant adverse business impact on an online retailer.<br/></p>
<p><i>Recovery point objective<br/></i>The acceptable state after recovery. In data systems, data is often lost<br/>during an outage. In this setting, the recovery point objective (RPO)<br/>refers to the maximum acceptable data loss.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Engineers need to consider acceptable reliability, availability, RTO, and<br/>RPO in designing for failure. These will guide their architecture decisions<br/>as they assess possible failure scenarios.<br/></p>
<p><b>Principle 3: Architect for Scalability<br/></b>Scalability in data systems encompasses two main capabilities. First,<br/>scalable systems can <i>scale up</i> to handle significant quantities of data. We<br/>might need to spin up a large cluster to train a model on a petabyte of<br/>customer data or scale out a streaming ingestion system to handle a<br/>transient load spike. Our ability to scale up allows us to handle extreme<br/>loads temporarily. Second, scalable systems can <i>scale down</i>. Once the load<br/>spike ebbs, we should automatically remove capacity to cut costs. (This is<br/>related to principle 9.) An <i>elastic system</i> can scale dynamically in response<br/>to load, ideally in an automated fashion.<br/>Some scalable systems can also <i>scale to zero</i>: they shut down completely<br/>when not in use. Once the large model-training job completes, we can<br/>delete the cluster. Many serverless systems (e.g., serverless functions and<br/>serverless online analytical processing, or OLAP, databases) can<br/>automatically scale to zero.<br/>Note that deploying inappropriate scaling strategies can result in<br/>overcomplicated systems and high costs. A straightforward relational<br/>database with one failover node may be appropriate for an application<br/>instead of a complex cluster arrangement. Measure your current load,<br/>approximate load spikes, and estimate load over the next several years to<br/>determine if your database architecture is appropriate. If your startup grows<br/>much faster than anticipated, this growth should also lead to more available<br/>resources to rearchitect for scalability.<br/></p>
<p><b>Principle 4: Architecture Is Leadership<br/></b>Data architects are responsible for technology decisions and architecture<br/>descriptions and disseminating these choices through effective leadership<br/>and training. Data architects should be highly technically competent but</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>delegate most individual contributor work to others. Strong leadership skills<br/>combined with high technical competence are rare and extremely valuable.<br/>The best data architects take this duality seriously.<br/>Note that leadership does not imply a command-and-control approach to<br/>technology. It was not uncommon in the past for architects to choose one<br/>proprietary database technology and force every team to house their data<br/>there. We oppose this approach because it can significantly hinder current<br/>data projects. Cloud environments allow architects to balance common<br/>component choices with flexibility that enables innovation within projects.<br/>Returning to the notion of technical leadership, Martin Fowler describes a<br/>specific archetype of an ideal software architect, well embodied in his<br/>colleague Dave Rice:<br/></p>
<p><i>In many ways, the most important activity of Architectus Oryzus is to<br/>mentor the development team, to raise their level so they can take on<br/>more complex issues. Improving the development team&#8217;s ability gives an<br/>architect much greater leverage than being the sole decision-maker and<br/>thus running the risk of being an architectural bottleneck.<br/></i></p>
<p>An ideal data architect manifests similar characteristics. They possess the<br/>technical skills of a data engineer but no longer practice data engineering<br/>day to day; they mentor current data engineers, make careful technology<br/>choices in consultation with their organization, and disseminate expertise<br/>through training and leadership. They train engineers in best practices and<br/>bring the company&#8217;s engineering resources together to pursue common<br/>goals in both technology and business.<br/>As a data engineer, you should practice architecture leadership and seek<br/>mentorship from architects. Eventually, you may well occupy the architect<br/>role yourself.<br/></p>
<p><b>Principle 5: Always Be Architecting<br/></b>We borrow this principle directly from Google Cloud&#8217;s Five Principles for<br/>Cloud-Native Architecture. Data architects don&#8217;t serve in their role simply<br/></p>
<p><i>2</i></p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>to maintain the existing state; instead, they constantly design new and<br/>exciting things in response to changes in business and technology. Per the<br/>EABOK, an architect&#8217;s job is to develop deep knowledge of the <i>baseline<br/>architecture</i> (current state), develop a <i>target architecture</i>, and map out a<br/><i>sequencing plan</i> to determine priorities and the order of architecture<br/>changes.<br/>We add that modern architecture should not be command-and-control or<br/>waterfall but collaborative and agile. The data architect maintains a target<br/>architecture and sequencing plans that change over time. The target<br/>architecture becomes a moving target, adjusted in response to business and<br/>technology changes internally and worldwide. The sequencing plan<br/>determines immediate priorities for delivery.<br/></p>
<p><b>Principle 6: Build Loosely Coupled Systems<br/></b><i>When the architecture of the system is designed to enable teams to test,<br/>deploy, and change systems without dependencies on other teams, teams<br/>require little communication to get work done. In other words, both the<br/>architecture and the teams are loosely coupled.<br/></i></p>
<p>&#8212;Google DevOps tech Architecture Guide<br/>In 2002, Bezos wrote an email to Amazon employees that became known as<br/>the Bezos API Mandate:<br/></p>
<p>1. All teams will henceforth expose their data and functionality through<br/>service interfaces.<br/></p>
<p>2. Teams must communicate with each other through these interfaces.<br/>3. There will be no other form of interprocess communication allowed:<br/></p>
<p>no direct linking, no direct reads of another team&#8217;s data store, no<br/>shared-memory model, no back-doors whatsoever. The only<br/>communication allowed is via service interface calls over the network.<br/></p>
<p>4. It doesn&#8217;t matter what technology they use. HTTP, Corba, Pubsub,<br/>custom protocols&#8212;doesn&#8217;t matter.<br/></p>
<p>3</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>5. All service interfaces, without exception, must be designed from the<br/>ground up to be externalizable. That is to say, the team must plan and<br/>design to be able to expose the interface to developers in the outside<br/>world. No exceptions.<br/></p>
<p>The advent of Bezos&#8217;s API Mandate is widely viewed as a watershed<br/>moment for Amazon. Putting data and services behind APIs enabled the<br/>loose coupling and eventually resulted in AWS as we know it now.<br/>Google&#8217;s pursuit of loose coupling allowed it to grow its systems to an<br/>extraordinary scale.<br/>For software architecture, a loosely coupled system has the following<br/>properties:<br/></p>
<p>1. Systems are broken into many small components.<br/>2. These systems interface with other services through abstraction layers,<br/></p>
<p>such as a messaging bus or an API. These abstraction layers hide and<br/>protect internal details of the service, such as a database backend or<br/>internal classes and method calls.<br/></p>
<p>3. As a consequence of property 2, internal changes to a system<br/>component don&#8217;t require changes in other parts. Details of code<br/>updates are hidden behind stable APIs. Each piece can evolve and<br/>improve separately.<br/></p>
<p>4. As a consequence of property 3, there is no waterfall, global release<br/>cycle for the whole system. Instead, each component is updated<br/>separately as changes and improvements are made.<br/></p>
<p>Notice that we are talking about <i>technical systems</i>. We need to think bigger.<br/>Let&#8217;s translate these technical characteristics into organizational<br/>characteristics:<br/></p>
<p>1. Many small teams engineer a large, complex system. Each team is<br/>tasked with engineering, maintaining, and improving some system<br/>components.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>2. These teams publish the abstract details of their components to other<br/>teams via API definitions, message schemas, etc. Teams need not<br/>concern themselves with other teams&#8217; components; they simply use the<br/>published API or message specifications to call these components.<br/>They iterate their part to improve their performance and capabilities<br/>over time. They might also publish new capabilities as they are added<br/>or request new stuff from other teams. Again, the latter happens<br/>without teams needing to worry about the internal technical details of<br/>the requested features. Teams work together through <i>loosely coupled<br/>communication</i>.<br/></p>
<p>3. As a consequence of characteristic 2, each team can rapidly evolve and<br/>improve its component independently of the work of other teams.<br/></p>
<p>4. Specifically, characteristic 3 implies that teams can release updates to<br/>their components with minimal downtime. Teams release continuously<br/>during regular working hours to make code changes and test them.<br/></p>
<p>Loose coupling of both technology and human systems will allow your data<br/>engineering teams to more efficiently collaborate with one another and with<br/>other parts of the company. This principle also directly facilitates principle<br/>7.<br/></p>
<p><b>Principle 7: Make Reversible Decisions<br/></b>The data landscape is changing rapidly. Today&#8217;s hot technology or stack is<br/>tomorrow&#8217;s afterthought. Popular opinion shifts quickly. You should aim for<br/>reversible decisions, as these tend to simplify your architecture and keep it<br/>agile.<br/>As Fowler wrote, &#8220;One of an architect&#8217;s most important tasks is to remove<br/>architecture by finding ways to eliminate irreversibility in software<br/>designs.&#8221;  What was true when Fowler wrote this in 2003 is just as accurate<br/>today.<br/>As we said previously, Bezos refers to reversible decisions as &#8220;two-way<br/>doors.&#8221; As he says, &#8220;If you walk through and don&#8217;t like what you see on the<br/></p>
<p>4</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>other side, you can&#8217;t get back to before. We can call these Type 1 decisions.<br/>But most decisions aren&#8217;t like that&#8212;they are changeable, reversible&#8212;<br/>they&#8217;re two-way doors.&#8221; Aim for two-way doors whenever possible.<br/>Given the pace of change&#8212;and the decoupling/modularization of<br/>technologies across your data architecture&#8212;always strive to pick the best-<br/>of-breed solutions that work for today. Also, be prepared to upgrade or<br/>adopt better practices as the landscape evolves.<br/></p>
<p><b>Principle 8: Prioritize Security<br/></b>Every data engineer must assume responsibility for the security of the<br/>systems they build and maintain. We focus now on two main ideas: zero-<br/>trust security and the shared responsibility security model. These align<br/>closely to a cloud-native architecture.<br/><b>Hardened-perimeter and zero-trust security models<br/></b>To define <i>zero-trust security</i>, it&#8217;s helpful to start by understanding the<br/>traditional hard-perimeter security model and its limitations, as detailed in<br/>Google Cloud&#8217;s Five Principles:<br/></p>
<p><i>Traditional architectures place a lot of faith in perimeter security,<br/>crudely a hardened network perimeter with &#8220;trusted things&#8221; inside and<br/>&#8220;untrusted things&#8221; outside. Unfortunately, this approach has always<br/>been vulnerable to insider attacks, as well as external threats such as<br/>spear phishing.<br/></i></p>
<p>The 1996 film <i>Mission Impossible</i> presents a perfect example of the hard-<br/>perimeter security model and its limitations. In the movie, the CIA hosts<br/>highly sensitive data on a storage system inside a room with extremely tight<br/>physical security. Ethan Hunt infiltrates CIA headquarters and exploits a<br/>human target to gain physical access to the storage system. Once inside the<br/>secure room, he can exfiltrate data with relative ease.<br/>For at least a decade, alarming media reports have made us aware of the<br/>growing menace of security breaches that exploit human targets inside<br/>hardened organizational security perimeters. Even as employees work on</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>highly secure corporate networks, they remain connected to the outside<br/>world through email and mobile devices. External threats effectively<br/>become internal threats.<br/>In a cloud-native environment, the notion of a hardened perimeter erodes<br/>further. All assets are connected to the outside world to some degree. While<br/>virtual private cloud (VPC) networks can be defined with no external<br/>connectivity, the API control plane that engineers use to define these<br/>networks still faces the internet.<br/><b>The shared responsibility model<br/></b>Amazon emphasizes the shared responsibility model, which divides security<br/>into the security <i>of</i> the cloud and security <i>in</i> the cloud. AWS is responsible<br/>for the security of the cloud:<br/></p>
<p><i>AWS is responsible for protecting the infrastructure that runs AWS<br/>services in the AWS Cloud. AWS also provides you with services that you<br/>can use securely.<br/></i></p>
<p>AWS users are responsible for security in the cloud:<br/><i>Your responsibility is determined by the AWS service that you use. You<br/>are also responsible for other factors including the sensitivity of your<br/>data, your organization&#8217;s requirements, and applicable laws and<br/>regulations.<br/></i></p>
<p>In general, all cloud providers operate on this shared responsibility model.<br/>They secure their services according to published specifications. Still, it is<br/>ultimately the user&#8217;s responsibility to design a security model for their<br/>applications and data and leverage cloud capabilities to realize this model.<br/><b>Data engineers as security engineers<br/></b>In the corporate world today, a command-and-control approach to security<br/>is quite common, wherein security and networking teams manage<br/>perimeters and general security practices. The cloud pushes this<br/>responsibility out to engineers who are not explicitly in security roles.<br/>Because of this responsibility, in conjunction with more general erosion of</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>the hard security perimeter, all data engineers should consider themselves<br/>security engineers.<br/>Failure to assume these new implicit responsibilities can lead to dire<br/>consequences. Numerous data breaches have resulted from the simple error<br/>of configuring Amazon S3 buckets with public access.  Those who handle<br/>data must assume that they are ultimately responsible for securing it.<br/></p>
<p><b>Principle 9: Embrace FinOps<br/></b>Let&#8217;s start by considering a couple of definitions of FinOps. First, the<br/>FinOps Foundation offers this:<br/></p>
<p><i>FinOps is an evolving cloud financial management discipline and<br/>cultural practice that enables organizations to get maximum business<br/>value by helping engineering, finance, technology, and business teams to<br/>collaborate on data-driven spending decisions.<br/></i></p>
<p>In addition, J.R. Sorment and Mike Fuller provide the following definition<br/>in <i>Cloud FinOps</i> (O&#8217;Reilly):<br/></p>
<p><i>The term &#8220;FinOps&#8221; typically refers to the emerging professional<br/>movement that advocates a collaborative working relationship between<br/>DevOps and Finance, resulting in an iterative, data-driven management<br/>of infrastructure spending (i.e., lowering the unit economics of cloud)<br/>while simultaneously increasing the cost efficiency and, ultimately, the<br/>profitability of the cloud environment.<br/></i></p>
<p>The cost structure of data has evolved dramatically during the cloud era. In<br/>an on-premises setting, data systems are generally acquired with a capital<br/>expenditure (described more in Chapter 4) for a new system every few<br/>years in an on-premises setting. Responsible parties have to balance their<br/>budget against desired compute and storage capacity. Overbuying entails<br/>wasted money, while underbuying means hampering future data projects<br/>and driving significant personnel time to control system load and data size;<br/>underbuying may require faster technology refresh cycles, with associated<br/>extra costs.<br/></p>
<p>5</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In the cloud era, most data systems are pay-as-you-go and readily scalable.<br/>Systems can run on a cost per query model, cost per processing capacity<br/>model, or another variant of a pay-as-you-go model. This approach can be<br/>far more efficient than the capital expenditure approach. It is now possible<br/>to scale up for high performance, and then scale down to save money.<br/>However, the pay-as-you-go approach makes spending far more dynamic.<br/>The new challenge for data leaders is to manage budgets, priorities, and<br/>efficiency.<br/>Cloud tooling necessitates a set of processes for managing spending and<br/>resources. In the past, data engineers thought in terms of performance<br/>engineering&#8212;maximizing the performance for data processes on a fixed set<br/>of resources and buying adequate resources for future needs. With FinOps,<br/>engineers need to learn to think about the cost structures of cloud systems.<br/>For example, what is the appropriate mix of AWS spot instances when<br/>running a distributed cluster? What is the most appropriate approach for<br/>running a sizable daily job in terms of cost-effectiveness and performance?<br/>When should the company switch from a pay-per-query model to reserved<br/>capacity?<br/>FinOps evolves the operational monitoring model to monitor spending on<br/>an ongoing basis. Rather than simply monitor requests and CPU utilization<br/>for a web server, FinOps might monitor the ongoing cost of serverless<br/>functions handling traffic, as well as spikes in spending trigger alerts. Just<br/>as systems are designed to fail gracefully in excessive traffic, companies<br/>may consider adopting hard limits for spending, with graceful failure modes<br/>in response to spending spikes.<br/>Ops teams should also think in terms of cost attacks. Just as a distributed<br/>denial-of-service (DDoS) attack can block access to a web server, many<br/>companies have discovered to their chagrin that excessive downloads from<br/>S3 buckets can drive spending through the roof and threaten a small startup<br/>with bankruptcy. When sharing data publicly, data teams can address these<br/>issues by setting requester-pays policies, or simply monitoring for excessive<br/>data access spending and quickly removing access if spending begins to rise<br/>to unacceptable levels.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>As of this writing, FinOps is a recently formalized practice. The FinOps<br/>Foundation was started only in 2019.  However, we highly recommend you<br/>start thinking about FinOps early, before you encounter high cloud bills.<br/>Start your journey with the FinOps Foundation and O&#8217;Reilly&#8217;s <i>Cloud<br/>FinOps</i>. We also suggest that data engineers involve themselves in the<br/>community process of creating FinOps practices for data engineering&#8212; in<br/>such a new practice area, a good deal of territory is yet to be mapped out.<br/>Now that you have a high-level understanding of good data architecture<br/>principles, let&#8217;s dive a bit deeper into the major concepts you&#8217;ll need to<br/>design and build good data architecture.<br/></p>
<p><b>Major Architecture Concepts<br/></b>If you follow the current trends in data, it seems like new types of data tools<br/>and architectures are arriving on the scene every week. Amidst this flurry of<br/>activity, we must not lose sight of the main goal of all of these architectures:<br/>to take data and transform it into something useful for downstream<br/>consumption.<br/></p>
<p><b>Domains and Services<br/></b><i>A sphere of knowledge, influence, or activity. The subject area to which<br/>the user applies a program is the domain of the software.<br/></i></p>
<p>&#8212;Eric Evans<br/>Before diving into the components of the architecture, let&#8217;s briefly cover<br/>two terms you&#8217;ll see come up very often: domain and services. A <i>domain</i> is<br/>the real-world subject area for which you&#8217;re architecting. A <i>service</i> is a set<br/>of functionality whose goal is to accomplish a task. For example, you might<br/>have a sales order-processing service whose task is to process orders as they<br/>are created. The sales order-processing service&#8217;s only job is to process<br/>orders; it doesn&#8217;t provide other functionality, such as inventory management<br/>or updating user profiles.<br/></p>
<p>6</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>A domain can contain multiple services. For example, you might have a<br/>sales domain with three services: orders, invoicing, and products. Each<br/>service has particular tasks that support the sales domain. Other domains<br/>may also share services (Figure 3-3). In this case, the accounting domain is<br/>responsible for basic accounting functions: invoicing, payroll, and accounts<br/>receivable (AR). Notice the accounting domain shares the invoice service<br/>with the sales domain since a sale generates an invoice, and accounting<br/>must keep track of invoices to ensure that payment is received. Sales and<br/>accounting own their respective domains.<br/></p>
<p><i>Figure 3-3. Two domains (sales and accounting) share a common service (invoices), and sales and<br/>accounting own their respective domains<br/></i></p>
<p>When thinking about what constitutes a domain, focus on what the domain<br/>represents in the real world and work backward. In the preceding example,<br/>the sales domain should represent what happens with the sales function in<br/>your company. When architecting the sales domain, avoid cookie-cutter<br/>copying and pasting from what other companies do. Your company&#8217;s sales<br/>function likely has unique aspects that require specific services to make it<br/>work the way your sales team expects.<br/>Identify what should go in the domain. When determining what the domain<br/>should encompass and what services to include, the best advice is to simply<br/>go and talk with users and stakeholders, listen to what they&#8217;re saying, and<br/>build the services that will help them do their job. Avoid the classic trap of<br/>architecting in a vacuum.<br/></p>
<p><b>Distributed Systems, Scalability, and Designing for<br/>Failure</b></p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>The discussion in this section is related to our second and third principles of<br/>data engineering architecture discussed previously: plan for failure and<br/>architect for scalability. As data engineers, we&#8217;re interested in four closely<br/>related characteristics of data systems (availability and reliability were<br/>mentioned previously, but we reiterate them here for completeness):<br/><i>Scalability<br/></i></p>
<p>Allows us to increase the capacity of a system to improve performance<br/>and handle the demand. For example, we might want to scale a system<br/>to handle a high rate of queries or process a huge data set.<br/></p>
<p><i>Elasticity<br/></i>The ability of a scalable system to scale dynamically; a highly elastic<br/>system can automatically scale up and down based on the current<br/>workload. Scaling up is critical as demand increases, while scaling<br/>down saves money in a cloud environment. Modern systems sometimes<br/>scale to zero, meaning they can automatically shut down when idle.<br/></p>
<p><i>Availability<br/></i>The percentage of time an IT service or component is in an operable<br/>state.<br/></p>
<p><i>Reliability<br/></i>The system&#8217;s probability of meeting defined standards in performing its<br/>intended function during a specified interval.<br/></p>
<p><b>TIP<br/></b>See PagerDuty&#8217;s &#8220;Why Are Availability and Reliability Crucial?&#8221; web page for<br/>definitions and background on availability and reliability.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>How are these characteristics related? If a system fails to meet performance<br/>requirements during a specified interval, it may become unresponsive. Thus<br/>low reliability can lead to low availability. On the other hand, dynamic<br/>scaling helps ensure adequate performance without manual intervention<br/>from engineers&#8212;elasticity improves reliability.<br/>Scalability can be realized in a variety of ways. For your services and<br/>domains, does a single machine handle everything? A single machine can<br/>be scaled vertically; you can increase resources (CPU, disk, memory, I/O).<br/>But there are hard limits to possible resources on a single machine. Also,<br/>what happens if this machine dies? Given enough time, some components<br/>will eventually fail. What&#8217;s your plan for backup and failover? Single<br/>machines generally can&#8217;t offer high availability and reliability.<br/>We utilize a distributed system to realize higher overall scaling capacity and<br/>increased availability and reliability. <i>Horizontal scaling</i> allows you to add<br/>more machines to satisfy load and resource requirements (Figure 3-4).<br/>Common horizontally scaled systems have a leader node that acts as the<br/>main point of contact for the instantiation, progress, and completion of<br/>workloads. When a workload is started, the leader node distributes tasks to<br/>the worker nodes within its system, completing the tasks and returning the<br/>results to the leader node. Typical modern distributed architectures also<br/>build in redundancy. Data is replicated so that if a machine dies, the other<br/>machines can pick up where the missing server left off; the cluster may add<br/>more machines to restore capacity.<br/></p>
<p><i>Figure 3-4. A simple horizontal distributed system utilizing a leader-follower architecture, with one<br/>leader node and three worker nodes</i></p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Distributed systems are widespread in the various data technologies you&#8217;ll<br/>use across your architecture. Almost every cloud data warehouse object<br/>storage system you use has some notion of distribution under the hood.<br/>Management details of the distributed system are typically abstracted away,<br/>allowing you to focus on high-level architecture instead of low-level<br/>plumbing. However, we highly recommend that you learn more about<br/>distributed systems because these details can be extremely helpful in<br/>understanding and improving the performance of your pipelines; Martin<br/>Kleppmann&#8217;s <i>Designing Data-Intensive Applications</i> (O&#8217;Reilly) is an<br/>excellent resource.<br/></p>
<p><b>Tight Versus Loose Coupling: Tiers, Monoliths, and<br/>Microservices<br/></b>When designing a data architecture, you choose how much interdependence<br/>you want to include within your various domains, services, and resources.<br/>On one end of the spectrum, you can choose to have extremely centralized<br/>dependencies and workflows. Every part of a domain and service is vitally<br/>dependent upon every other domain and service. This pattern is known as<br/><i>tightly coupled</i>.<br/>On the other end of the spectrum, you have decentralized domains and<br/>services that do not have strict dependence on each other, in a pattern<br/>known as <i>loose coupling</i>. In a loosely coupled scenario, it&#8217;s easy for<br/>decentralized teams to build systems whose data may not be usable by their<br/>peers. Be sure to assign common standards, ownership, responsibility, and<br/>accountability to the teams owning their respective domains and services.<br/>Designing &#8220;good&#8221; data architecture relies on trade-offs between the tight<br/>and loose coupling of domains and services.<br/>It&#8217;s worth noting that many of the ideas in this section originate in software<br/>development. We&#8217;ll try to retain the context of these big ideas&#8217; original<br/>intent and spirit&#8212;keeping them agnostic of data&#8212;while later explaining<br/>some differences you should be aware of when applying these concepts to<br/>data specifically.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><b>Architecture tiers<br/></b>As you develop your architecture, it helps to be aware of architecture tiers.<br/>Your architecture has layers&#8212;data, application, business logic, presentation,<br/>and so forth&#8212;and you need to know how to decouple these layers. Because<br/>tight coupling of modalities presents obvious vulnerabilities, keep in mind<br/>how you structure the layers of your architecture to achieve maximum<br/>reliability and flexibility. Let&#8217;s look at single-tier and multitier architecture.<br/><i>Single tier<br/></i>In a <i>single-tier architecture</i>, your database and application are tightly<br/>coupled, residing on a single server (Figure 3-5). This server could be your<br/>laptop or a single virtual machine (VM) in the cloud. The tightly coupled<br/>nature means if the server, the database, or the application fails, the entire<br/>architecture fails. While single-tier architectures are good for prototyping<br/>and development, they are not advised for production environments because<br/>of the obvious failure risks.<br/></p>
<p><i>Figure 3-5. Single-tier architecture<br/></i></p>
<p>Even when single-tier architectures build in redundancy (for example, a<br/>failover replica), they present significant limitations in other ways. For<br/>instance, it is often impractical (and not advisable) to run analytics queries<br/>against production application databases. Doing so risks overwhelming the<br/>database and causing the application to become unavailable. A single-tier<br/>architecture is fine for testing systems on a local machine but is not advised<br/>for production uses.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><i>Multitier<br/></i>The challenges of a tightly coupled single-tier architecture are solved by<br/>decoupling the data and application. A <i>multitier</i> (also known as <i>n-tier</i>)<br/>architecture is composed of separate layers: data, application, business<br/>logic, presentation, etc. These layers are bottom-up and hierarchical,<br/>meaning the lower layer isn&#8217;t necessarily dependent on the upper layers; the<br/>upper layers depend on the lower layers. The notion is to separate data from<br/>the application, and application from the presentation.<br/>A common multitier architecture is a three-tier architecture, a widely used<br/>client-server design. A <i>three-tier architecture</i> consists of data, application<br/>logic, and presentation tiers (Figure 3-6). Each tier is isolated from the<br/>other, allowing for separation of concerns. With a three-tier architecture,<br/>you&#8217;re free to use whatever technologies you prefer within each tier without<br/>the need to be monolithically focused.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><i>Figure 3-6. A three-tier architecture<br/></i></p>
<p>We&#8217;ve seen many single-tier architectures in production. Single-tier<br/>architectures offer simplicity but also severe limitations. Eventually, an<br/>organization or application outgrows this arrangement; it works well until it<br/>doesn&#8217;t. For instance, in a single-tier architecture, the data and logic layers<br/>share and compete for resources (disk, CPU, and memory) in ways that are<br/>simply avoided in a multitier architecture. Resources are spread across<br/>various tiers. Data engineers should use tiers to evaluate their layered<br/>architecture and the way dependencies are handled. Again, start simple and<br/>bake in evolution to additional tiers as your architecture becomes more<br/>complex.<br/>In a multitier architecture, you need to consider separating your layers and<br/>the way resources are shared within layers when working with a distributed<br/>system. Distributed systems under the hood power many technologies</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>you&#8217;ll encounter across the data engineering lifecycle. First, think about<br/>whether you want resource contention with your nodes. If not, exercise a<br/><i>shared-nothing architecture</i>: a single node handles each request, meaning<br/>other nodes do not share resources such as memory, disk, or CPU with this<br/>node or with each other. Data and resources are isolated to the node.<br/>Alternatively, various nodes can handle multiple requests and share<br/>resources but at the risk of resource contention. Another consideration is<br/>whether nodes should share the same disk and memory accessible by all<br/>nodes. This is called a <i>shared disk architecture</i> and is common when you<br/>want shared resources if a random node failure occurs.<br/><b>Monoliths<br/></b>The general notion of a monolith includes as much as possible under one<br/>roof; in its most extreme version, a monolith consists of a single codebase<br/>running on a single machine that provides both the application logic and<br/>user interface.<br/>Coupling within monoliths can be viewed in two ways: technical coupling<br/>and domain coupling. <i>Technical coupling</i> refers to architectural tiers, while<br/><i>domain coupling</i> refers to the way domains are coupled together. A<br/>monolith has varying degrees of coupling among technologies and domains.<br/>You could have an application with various layers decoupled in a multitier<br/>architecture but still share multiple domains. Or, you could have a single-<br/>tier architecture serving a single domain.<br/>The tight coupling of a monolith implies a lack of modularity of its<br/>components. Swapping out or upgrading components in a monolith is often<br/>an exercise in trading one pain for another. Because of the tightly coupled<br/>nature, reusing components across the architecture is difficult or impossible.<br/>When evaluating how to improve a monolithic architecture, it&#8217;s often a<br/>game of whack-a-mole: one component is improved, often at the expense of<br/>unknown consequences with other areas of the monolith.<br/>Data teams will often ignore solving the growing complexity of their<br/>monolith, letting it devolve into a big ball of mud.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Chapter 4 provides a more extensive discussion comparing monoliths to<br/>distributed technologies. We also discuss the <i>distributed monolith</i>, a strange<br/>hybrid that emerges when engineers build distributed systems with<br/>excessive tight coupling.<br/><b>Microservices<br/></b>Compared with the attributes of a monolith&#8212;interwoven services,<br/>centralization, and tight coupling among services&#8212;microservices are the<br/>polar opposite. <i>Microservices architecture</i> comprises separate,<br/>decentralized, and loosely coupled services. Each service has a specific<br/>function and is decoupled from other services operating within its domain.<br/>If one service temporarily goes down, it won&#8217;t affect the ability of other<br/>services to continue functioning.<br/>A question that comes up often is how to convert your monolith into many<br/>microservices (Figure 3-7). This completely depends on how complex your<br/>monolith is and how much effort it will be to start extracting services out of<br/>it. It&#8217;s entirely possible that your monolith cannot be broken apart, in which<br/>case, you&#8217;ll want to start creating a new parallel architecture that has the<br/>services decoupled in a microservices-friendly manner. We don&#8217;t suggest an<br/>entire refactor but instead break out services. The monolith didn&#8217;t arrive<br/>overnight and is a technology issue as an organizational one. Be sure you<br/>get buy-in from stakeholders of the monolith if you plan to break it apart.<br/>If you&#8217;d like to learn more about breaking apart a monolith, we suggest<br/>reading the fantastic, pragmatic guide <i>Software Architecture: The Hard<br/>Parts</i> by Neal Ford et al. (O&#8217;Reilly).<br/><b>Considerations for data architecture<br/></b>As we mentioned at the start of this section, the concepts of tight versus<br/>loose coupling stem from software development, with some of these<br/>concepts dating back over 20 years. Though architectural practices in data<br/>are now adopting those from software development, it&#8217;s still common to see<br/>very monolithic, tightly coupled data architectures. Some of this is due to<br/>the nature of existing data technologies and the way they integrate.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>For example, data pipelines might consume data from many sources<br/>ingested into a central data warehouse. The central data warehouse is<br/>inherently monolithic. A move toward a microservices equivalent with a<br/>data warehouse is to decouple the workflow with domain-specific data<br/>pipelines connecting to corresponding domain-specific data warehouses.<br/>For example, the sales data pipeline connects to the sales-specific data<br/>warehouse, and the inventory and product domains follow a similar pattern.<br/></p>
<p><i>Figure 3-7. An extremely monolithic architecture runs all functionality inside a single codebase,<br/>potentially colocating a database on the same host server<br/></i></p>
<p>Rather than dogmatically preach microservices over monoliths (among<br/>other arguments), we suggest you pragmatically use loose coupling as an<br/>ideal, while recognizing the state and limitations of the data technologies<br/>you&#8217;re using within your data architecture. Incorporate reversible<br/>technology choices that allow for modularity and loose coupling whenever<br/>possible.<br/>As you can see in Figure 3-7, you separate the components of your<br/>architecture into different layers of concern in a vertical fashion. While a<br/>multitier architecture solves the technical challenges of decoupling shared<br/>resources, it does not address the complexity of sharing domains. Along the<br/>lines of single versus multitiered architecture, you should also consider how<br/>you separate the domains of your data architecture. For example, your<br/>analyst team might rely on data from sales and inventory. The sales and<br/>inventory domains are different and should be viewed as separate.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>One approach to this problem is centralization: a single team is responsible<br/>for gathering data from all domains and reconciling it for consumption<br/>across the organization. (This is a common approach in traditional data<br/>warehousing.) Another approach is the <i>data mesh</i>. With the data mesh, each<br/>software team is responsible for preparing its data for consumption across<br/>the rest of the organization. We&#8217;ll say more about the data mesh later in this<br/>chapter.<br/>Our advice: monoliths aren&#8217;t necessarily bad, and it might make sense to<br/>start with one under certain conditions. Sometimes you need to move fast,<br/>and it&#8217;s much simpler to start with a monolith. Just be prepared to break it<br/>into smaller pieces eventually; don&#8217;t get too comfortable.<br/></p>
<p><b>User Access: Single Versus Multitenant<br/></b>As a data engineer, you have to make decisions about sharing systems<br/>across multiple teams, organizations, and customers. In some sense, all<br/>cloud services are multitenant, although this multitenancy occurs at various<br/>grains. For example, a cloud compute instance is usually on a shared server,<br/>but the VM itself provides some degree of isolation. Object storage is a<br/>multitenant system, but cloud vendors guarantee security and isolation so<br/>long as customers configure their permissions correctly.<br/>Engineers frequently need to make decisions about multitenancy at a much<br/>smaller scale. For example, do multiple departments in a large company<br/>share the same data warehouse? Does the organization share data for<br/>multiple large customers within the same table?<br/>We have two factors to consider in multitenancy: performance and security.<br/>With multiple large tenants within a cloud system, will the system support<br/>consistent performance for all tenants, or will there be a noisy neighbor<br/>problem? (That is, will high usage from one tenant degrade performance for<br/>other tenants?) Regarding security, data from different tenants must be<br/>properly isolated. When a company has multiple external customer tenants,<br/>these tenants should not be aware of one another, and engineers must<br/>prevent data leakage. Strategies for data isolation vary by system. For</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>instance, it is often perfectly acceptable to use multitenant tables and isolate<br/>data through views. However, you must make certain that these views<br/>cannot leak data. Read vendor or project documentation to understand<br/>appropriate strategies and risks.<br/></p>
<p><b>Event-Driven Architecture<br/></b>Your business is rarely static. Things often happen in your business, such as<br/>getting a new customer, a new order from a customer, or an order for a<br/>product or service. These are all examples of <i>events</i> that are broadly defined<br/>as something that happened, typically a change in the <i>state</i> of something.<br/>For example, a new order might be created by a customer, or a customer<br/>might later make an update to this order.<br/>An event-driven workflow (Figure 3-8) encompasses the ability to create,<br/>update, and asynchronously move events across various parts of the data<br/>engineering lifecycle. This workflow boils down to three main areas: event<br/>production, routing, and consumption. An event must be produced and<br/>routed to something that consumes it without tightly coupled dependencies<br/>among the producer, event router, and consumer.<br/></p>
<p><i>Figure 3-8. In an event-driven workflow, an event is produced, routed, and then consumed<br/></i></p>
<p>An event-driven architecture (Figure 3-9) embraces the event-driven<br/>workflow and uses this to communicate across various services. The<br/>advantage of an event-driven architecture is that it distributes the state of an<br/>event across multiple services. This is helpful if a service goes offline, a<br/>node fails in a distributed system, or you&#8217;d like multiple consumers or<br/>services to access the same events. Anytime you have loosely coupled<br/>services, this is a candidate for event-driven architecture. Many of the<br/>examples we describe later in this chapter incorporate some form of event-<br/>driven architecture.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><i>Figure 3-9. In an event-driven architecture, events are passed between loosely coupled services<br/></i></p>
<p>You&#8217;ll learn more about event-driven streaming and messaging systems in<br/>Chapter 5.<br/></p>
<p><b>Brownfield Versus Greenfield Projects<br/></b>Before you design your data architecture project, you need to know whether<br/>you&#8217;re starting with a clean slate or redesigning an existing architecture.<br/>Each type of project requires assessing trade-offs, albeit with different<br/>considerations and approaches. Projects roughly fall into two buckets:<br/>brownfield and greenfield.<br/><b>Brownfield projects<br/></b><i>Brownfield projects</i> often involve refactoring and reorganizing an existing<br/>architecture and are constrained by the choices of the present and past.<br/>Because a key part of architecture is change management, you must figure<br/>out a way around these limitations and design a path forward to achieve<br/>your new business and technical objectives. Brownfield projects require a<br/>thorough understanding of the legacy architecture and the interplay of<br/>various old and new technologies. All too often, it&#8217;s easy to criticize a prior<br/>team&#8217;s work and decisions, but it is far better to dig deep, ask questions, and<br/>understand why decisions were made. Empathy and context go a long way<br/>in helping you diagnose problems with the existing architecture, identify<br/>opportunities, and recognize pitfalls.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>You&#8217;ll need to introduce your new architecture and technologies and<br/>deprecate the old stuff at some point. Let&#8217;s look at a couple of popular<br/>approaches. Many teams jump headfirst into an all-at-once or big-bang<br/>overhaul of the old architecture, often figuring out deprecation as they go.<br/>Though popular, we don&#8217;t advise this approach because of the associated<br/>risks and lack of a plan. This path often leads to disaster, with many<br/>irreversible and costly decisions. Your job is to make reversible, high-ROI<br/>decisions.<br/>A popular alternative to a direct rewrite is the strangler pattern: new<br/>systems slowly and incrementally replace a legacy architecture&#8217;s<br/>components.  Eventually, the legacy architecture is completely replaced.<br/>The attraction to the strangler pattern is its targeted and surgical approach of<br/>deprecating one piece of a system at a time. This allows for flexible and<br/>reversible decisions while assessing the impact of the deprecation on<br/>dependent systems.<br/>It&#8217;s important to note that deprecation might be &#8220;ivory tower&#8221; advice and<br/>not practical or achievable. Eradicating legacy technology or architecture<br/>might be impossible if you&#8217;re at a large organization. Someone, somewhere,<br/>is using these legacy components. As someone once said, &#8220;Legacy is a<br/>condescending way to describe something that makes money.&#8221;<br/>If you can deprecate, understand there are numerous ways to deprecate your<br/>old architecture. It is critical to demonstrate value on the new platform by<br/>gradually increasing its maturity to show evidence of success and then<br/>follow an exit plan to shut down old systems.<br/><b>Greenfield projects<br/></b>On the opposite end of the spectrum, a <i>greenfield project</i> allows you to<br/>pioneer a fresh start, unconstrained by the history or legacy of a prior<br/>architecture. Greenfield projects tend to be easier than brownfield projects,<br/>and many data architects and engineers find them more fun! You have the<br/>opportunity to try the newest and coolest tools and architectural patterns.<br/>What could be more exciting?<br/></p>
<p>7</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>You should watch out for some things before getting too carried away. We<br/>see teams get overly exuberant with shiny object syndrome. They feel<br/>compelled to reach for the latest and greatest technology fad without<br/>understanding how it will impact the value of the project. There&#8217;s also a<br/>temptation to do <i>resume-driven development</i>, stacking up impressive new<br/>technologies without prioritizing the project&#8217;s ultimate goals.  Always<br/>prioritize requirements over building something cool.<br/>Whether you&#8217;re working on a brownfield or greenfield project, always<br/>focus on the tenets of &#8220;good&#8221; data architecture. Assess trade-offs, make<br/>flexible and reversible decisions, and strive for positive ROI.<br/>Now, we&#8217;ll look at examples and types of architectures&#8212;some established<br/>for decades (the data warehouse), some brand-new (the data lakehouse),<br/>and some that quickly came and went but still influence current architecture<br/>patterns (Lambda architecture).<br/></p>
<p><b>Examples and Types of Data Architecture<br/></b>Because data architecture is an abstract discipline, it helps to reason by<br/>example. In this section, we outline prominent examples and types of data<br/>architecture that are popular today. Though this set of examples is by no<br/>means exhaustive, the intention is to expose you to some of the most<br/>common data architecture patterns and to get you thinking about the<br/>requisite flexibility and trade-off analysis needed when designing a good<br/>architecture for your use case.<br/></p>
<p><b>Data Warehouse<br/></b>A <i>data warehouse</i> is a central data hub used for reporting and analysis. Data<br/>in a data warehouse is typically highly formatted and structured for<br/>analytics use cases. It&#8217;s among the oldest and most well-established data<br/>architectures.<br/>In 1990, Bill Inmon originated the notion of the data warehouse, which he<br/>described as &#8220;a subject-oriented, integrated, nonvolatile, and time-variant<br/></p>
<p>8</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>collection of data in support of management&#8217;s decisions.&#8221;  Though<br/>technical aspects of the data warehouse have evolved significantly, we feel<br/>this original definition still holds its weight today.<br/>In the past, data warehouses were widely used at enterprises with<br/>significant budgets (often in the millions of dollars) to acquire data systems<br/>and pay internal teams to provide ongoing support to maintain the data<br/>warehouse. This was expensive and labor-intensive. Since then, the<br/>scalable, pay-as-you-go model has made cloud data warehouses accessible<br/>even to tiny companies. Because a third-party provider manages the data<br/>warehouse infrastructure, companies can do a lot more with fewer people,<br/>even as the complexity of their data grows.<br/>It&#8217;s worth noting two types of data warehouse architecture: organizational<br/>and technical. The <i>organizational data warehouse architecture</i> organizes<br/>data associated with certain business team structures and processes. The<br/><i>technical data warehouse architecture</i> reflects the technical nature of the<br/>data warehouse, such as MPP. A company can have a data warehouse<br/>without an MPP system or run an MPP system that is not organized as a<br/>data warehouse. However, the technical and organizational architectures<br/>have existed in a virtuous cycle and are frequently identified with each<br/>other.<br/>The organizational data warehouse architecture has two main<br/>characteristics:<br/><i>Separates analytics processes (OLAP) from production databases (online<br/>transaction processing)<br/></i></p>
<p>This separation is critical as businesses grow. Moving data into a<br/>separate physical system directs load away from production systems and<br/>improves analytics performance.<br/></p>
<p><i>Centralizes and organizes data<br/></i>Traditionally, a data warehouse pulls data from application systems by<br/>using ETL. The extract phase pulls data from source systems. The<br/>transformation phase cleans and standardizes data, organizing and<br/></p>
<p>9</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>imposing business logic in a highly modeled form. (Chapter 8 covers<br/>transformations and data models.) The load phase pushes data into the<br/>data warehouse target database system. Data is loaded into multiple data<br/>marts that serve the analytical needs for specific lines or business and<br/>departments. Figure 3-10 shows the general workflow. The data<br/>warehouse and ETL go hand in hand with specific business structures,<br/>including DBA and ETL developer teams that implement the direction<br/>of business leaders to ensure that data for reporting and analytics<br/>corresponds to business processes. An ETL system is not a prerequisite<br/>for a data warehouse, as you will learn when we discuss another load<br/>and transformation pattern, ELT.<br/></p>
<p><i>Figure 3-10. Basic data warehouse with ETL<br/></i></p>
<p>Regarding the technical data warehouse architecture, the first MPP systems<br/>in the late 1970s became popular in the 1980s. MPPs support essentially the<br/>same SQL semantics used in relational application databases. Still, they are<br/>optimized to scan massive amounts of data in parallel and thus allow high-<br/>performance aggregation and statistical calculations. In recent years, MPP<br/>systems have increasingly shifted from a row-based to a columnar<br/>architecture to facilitate even larger data and queries, especially in cloud<br/>data warehouses. MPPs are indispensable for running performant queries<br/>for large enterprises as data and reporting needs grow.<br/>One variation on ETL is ELT. With the ELT data warehouse architecture,<br/>data gets moved more or less directly from production systems into a</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>staging area in the data warehouse. Staging in this setting indicates that the<br/>data is in a raw form. Rather than using an external system, transformations<br/>are handled directly in the data warehouse. The intention is to take<br/>advantage of the massive computational power of cloud data warehouses<br/>and data processing tools. Data is processed in batches, and transformed<br/>output is written into tables and views for analytics. Figure 3-11 shows the<br/>general process. ELT is also popular in a streaming arrangement, as events<br/>are streamed from a CDC process, stored in a staging area, and then<br/>subsequently transformed within the data warehouse.<br/></p>
<p><i>Figure 3-11. ELT&#8212;extract, load, and transform<br/></i></p>
<p>A second version of ELT was popularized during big data growth in the<br/>Hadoop ecosystem. This is <i>transform-on-read ELT</i>, which we discuss in<br/>&#8220;Data Lake&#8221;.<br/><b>The cloud data warehouse<br/></b><i>Cloud data warehouses</i> represent a significant evolution of the on-premises<br/>data warehouse architecture and have thus led to significant changes to the<br/>organizational architecture. Amazon Redshift kicked off the cloud data<br/>warehouse revolution. Instead of needing to appropriately size an MPP<br/>system for the next several years and sign a multimillion-dollar contract to<br/>procure the system, companies had the option of spinning up a Redshift<br/>cluster on demand, scaling it up over time as data and analytics demand<br/>grew. They could even spin up new Redshift clusters on demand to serve<br/>specific workloads and quickly delete clusters when they were no longer<br/>needed.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Google BigQuery, Snowflake, and other competitors popularized the idea of<br/>separating compute from storage. In this architecture, data is housed in<br/>object storage, allowing virtually limitless storage. This also gives users the<br/>option to spin up computing power on demand, providing ad hoc big data<br/>capabilities without the long-term cost of thousands of nodes.<br/>Cloud data warehouses expand the capabilities of MPP systems to cover<br/>many big data use cases that required a Hadoop cluster in the very recent<br/>past. They can readily process petabytes of data in a single query. They<br/>typically support data structures that allow the storage of tens of megabytes<br/>of raw text data per row or extremely rich and complex JSON documents.<br/>As cloud data warehouses (and data lakes) mature, the line between the data<br/>warehouse and the data lake will continue to blur.<br/>So significant is the impact of the new capabilities offered by cloud data<br/>warehouses that we might consider jettisoning the term <i>data warehouse<br/></i>altogether. Instead, these services are evolving into a new data platform<br/>with much broader capabilities than those offered by a traditional MPP<br/>system.<br/><b>Data marts<br/></b>A <i>data mart</i> is a more refined subset of a warehouse designed to serve<br/>analytics and reporting, focused on a single suborganization, department, or<br/>line of business; every department has its own data mart, specific to its<br/>needs. This is in contrast to the full data warehouse that serves the broader<br/>organization or business.<br/>Data marts exist for two reasons. First, a data mart makes data more easily<br/>accessible to analysts and report developers. Second, data marts provide an<br/>additional stage of transformation beyond that provided by the initial ETL<br/>or ELT pipelines. This can significantly improve performance if reports or<br/>analytics queries require complex joins and aggregations of data, especially<br/>when the raw data is large. Transform processes can populate the data mart<br/>with joined and aggregated data to improve performance for live queries.<br/>Figure 3-12 shows the general workflow. We discuss data marts, and<br/>modeling data for data marts, in Chapter 8.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><i>Figure 3-12. ETL or ELT plus data marts<br/></i></p>
<p><b>Data Lake<br/></b>Among the most popular architectures that appeared during the big data era<br/>is the <i>data lake</i>. Instead of imposing tight structural limitations on data,<br/>why not simply dump all of your data&#8212;structured and unstructured&#8212;into a<br/>central location? The data lake promised to be a democratizing force,<br/>liberating the business to drink from a fountain of limitless data. The first-<br/>generation data lake, &#8220;data lake 1.0,&#8221; made solid contributions but generally<br/>failed to deliver on its promise.<br/>Data lake 1.0 started with HDFS. As the cloud grew in popularity, these<br/>data lakes moved to cloud-based object storage, with extremely cheap<br/>storage costs and virtually limitless storage capacity. Instead of relying on a<br/>monolithic data warehouse where storage and compute are tightly coupled,<br/>the data lake allows an immense amount of data of any size and type to be<br/>stored. When this data needs to be queried or transformed, you have access<br/>to nearly unlimited computing power by spinning up a cluster on demand,<br/>and you can pick your favorite data-processing technology for the task at<br/>hand&#8212;MapReduce, Spark, Ray, Presto, Hive, etc.<br/>Despite the promise and hype, data lake 1.0 had serious shortcomings. The<br/>data lake became a dumping ground; terms such as <i>data swamp</i>, <i>dark data</i>,<br/>and <i>WORN</i> were coined as once-promising data projects failed. Data grew<br/>to unmanageable sizes, with little in the way of schema management, data<br/>cataloging, and discovery tools. In addition, the original data lake concept<br/>was essentially write-only, creating huge headaches with the arrival of<br/>regulations such as GDPR that required targeted deletion of user records.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Processing data was also challenging. Relatively banal data transformations<br/>such as joins were a huge headache to code as MapReduce jobs. Later<br/>frameworks such as Pig and Hive somewhat improved the situation for data<br/>processing but did little to address the basic problems of data management.<br/>Simple data manipulation language (DML) operations common in SQL&#8212;<br/>deleting or updating rows&#8212;were painful to implement, generally achieved<br/>by creating entirely new tables. While big data engineers radiated a<br/>particular disdain for their counterparts in data warehousing, the latter could<br/>point out that data warehouses provided basic data management capabilities<br/>out of the box, and that SQL was an efficient tool for writing complex,<br/>performant queries and transformations.<br/>Data lake 1.0 also failed to deliver on another core promise of the big data<br/>movement. Open source software in the Apache ecosystem was touted as a<br/>means to avoid multimillion-dollar contracts for proprietary MPP systems.<br/>Cheap, off-the-shelf hardware would replace custom vendor solutions. In<br/>reality, big data costs ballooned as the complexities of managing Hadoop<br/>clusters forced companies to hire large teams of engineers at high salaries.<br/>Companies often chose to purchase licensed, customized versions of<br/>Hadoop from vendors to avoid the exposed wires and sharp edges of the<br/>raw Apache codebase and acquire a set of scaffolding tools to make Hadoop<br/>more user-friendly. Even companies that avoided managing Hadoop<br/>clusters using cloud storage had to spend big on talent to write MapReduce<br/>jobs.<br/>We should be careful not to understate the utility and power of first-<br/>generation data lakes. Many organizations found significant value in data<br/>lakes&#8212;especially huge, heavily data-focused Silicon Valley tech companies<br/>like Netflix and Facebook. These companies had the resources to build<br/>successful data practices and create their custom Hadoop-based tools and<br/>enhancements. But for many organizations, data lakes turned into an<br/>internal superfund site of waste, disappointment, and spiraling costs.<br/></p>
<p><b>Convergence, Next-Generation Data Lakes, and the Data<br/>Platform</b></p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In response to the limitations of first-generation data lakes, various players<br/>have sought to enhance the concept to fully realize its promise. For<br/>example, Databricks introduced the notion of a <i>data lakehouse</i>. The<br/>lakehouse incorporates the controls, data management, and data structures<br/>found in a data warehouse while still housing data in object storage and<br/>supporting a variety of query and transformation engines. In particular, the<br/>data lakehouse supports atomicity, consistency, isolation, and durability<br/>(ACID) transactions, a big departure from the original data lake, where you<br/>simply pour in data and never update or delete it. The term <i>data lakehouse<br/></i>suggests a convergence between data lakes and data warehouses.<br/>The technical architecture of cloud data warehouses has evolved to be very<br/>similar to a data lake architecture. Cloud data warehouses separate compute<br/>from storage, support petabyte-scale queries, store unstructured text and<br/>semistructured objects, and integrate with advanced processing<br/>technologies such as Spark or Beam.<br/>We believe that the trend of convergence will only continue. The data lake<br/>and the data warehouse will still exist as different architectures. In practice,<br/>their capabilities will converge so that few users will notice a boundary<br/>between them in their day-to-day work. We now see several vendors<br/>offering <i>data platforms</i> that combine data lake and data warehouse<br/>capabilities. From our perspective, AWS, Azure, Google Cloud, Snowflake,<br/>and Databricks are class leaders, each offering a constellation of tightly<br/>integrated tools for working with data, running the gamut from relational to<br/>completely unstructured. Instead of choosing between a data lake or data<br/>warehouse architecture, future data engineers will have the option to choose<br/>a converged data platform based on a variety of factors, including vendor,<br/>ecosystem, and relative openness.<br/></p>
<p><b>Modern Data Stack<br/></b>The <i>modern data stack</i> (Figure 3-13) is currently a trendy analytics<br/>architecture that highlights the type of abstraction we expect to see more<br/>widely used over the next several years. Whereas past data stacks relied on<br/>expensive, monolithic toolsets, the main objective of the modern data stack</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>is to use cloud-based, plug-and-play, easy-to-use, off-the-shelf components<br/>to create a modular and cost-effective data architecture. These components<br/>include data pipelines, storage, transformation, data<br/>management/governance, monitoring, visualization, and exploration. The<br/>domain is still in flux, and the specific tools are changing and evolving<br/>rapidly, but the core aim will remain the same: to reduce complexity and<br/>increase modularization. Note that the notion of a modern data stack<br/>integrates nicely with the converged data platform idea from the previous<br/>section.<br/></p>
<p><i>Figure 3-13. Basic components of the modern data stack<br/></i></p>
<p>Key outcomes of the modern data stack are self-service (analytics and<br/>pipelines), agile data management, and using open source tools or simple<br/>proprietary tools with clear pricing structures. Community is a central<br/>aspect of the modern data stack as well. Unlike products of the past that had<br/>releases and roadmaps largely hidden from users, projects and companies<br/>operating in the modern data stack space typically have strong user bases<br/>and active communities that participate in the development by using the<br/>product early, suggesting features, and submitting pull requests to improve<br/>the code.<br/>Regardless of where &#8220;modern&#8221; goes (we share our ideas in Chapter 11), we<br/>think the key concept of plug-and-play modularity with easy-to-understand<br/>pricing and implementation is the way of the future. Especially in analytics<br/>engineering, the modern data stack is and will continue to be the default<br/>choice of data architecture. Throughout the book, the architecture we<br/>reference contains pieces of the modern data stack, such as cloud-based and<br/>plug-and-play modular components.<br/></p>
<p><b>Lambda Architecture</b></p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In the &#8220;old days&#8221; (the early to mid-2010s), the popularity of working with<br/>streaming data exploded with the emergence of Kafka as a highly scalable<br/>message queue and frameworks such as Apache Storm and Samza for<br/>streaming/real-time analytics. These technologies allowed companies to<br/>perform new types of analytics and modeling on large amounts of data, user<br/>aggregation and ranking, and product recommendations. Data engineers<br/>needed to figure out how to reconcile batch and streaming data into a single<br/>architecture. The Lambda architecture was one of the early popular<br/>responses to this problem.<br/>In a <i>Lambda architecture</i> (Figure 3-14), you have systems operating<br/>independently of each other&#8212;batch, streaming, and serving. The source<br/>system is ideally immutable and append-only, sending data to two<br/>destinations for processing: stream, and batch. In-stream processing intends<br/>to serve the data with the lowest possible latency in a &#8220;speed&#8221; layer, usually<br/>a NoSQL database. In the batch layer, data is processed and transformed in<br/>a system such as a data warehouse, creating precomputed and aggregated<br/>views of the data. The serving layer provides a combined view by<br/>aggregating query results from the two layers.<br/></p>
<p><i>Figure 3-14. Lambda architecture<br/></i></p>
<p>Lambda architecture has its share of challenges and criticisms. Managing<br/>multiple systems with different codebases is as difficult as it sounds,<br/>creating error-prone systems with code and data that are extremely difficult<br/>to reconcile.<br/>We mention Lambda architecture because it still gets attention and is<br/>popular in search-engine results for data architecture. Lambda isn&#8217;t our first</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>recommendation if you&#8217;re trying to combine streaming and batch data for<br/>analytics. Technology and practices have moved on.<br/>Next, let&#8217;s look at a reaction to Lambda architecture, the Kappa<br/>architecture.<br/></p>
<p><b>Kappa Architecture<br/></b>As a response to the shortcomings of Lambda architecture, Jay Kreps<br/>proposed an alternative called <i>Kappa architecture</i> (Figure 3-15).  The<br/>central thesis is this: why not just use a stream-processing platform as the<br/>backbone for all data handling&#8212;ingestion, storage, and serving? This<br/>facilitates a true event-based architecture. Real-time and batch processing<br/>can be applied seamlessly to the same data by reading the live event stream<br/>directly and replaying large chunks of data for batch processing.<br/></p>
<p><i>Figure 3-15. Kappa architecture<br/></i></p>
<p>Though the original Kappa architecture article came out in 2014, we<br/>haven&#8217;t seen it widely adopted. There may be a couple of reasons for this.<br/>First, streaming itself is still a bit of a mystery for many companies.<br/>Second, Kappa architecture turns out to be complicated and expensive in<br/>practice. While some streaming systems can scale to huge data volumes,<br/>they are complex and expensive; batch storage and processing remain much<br/>more efficient and cost-effective for enormous historical datasets.<br/></p>
<p><b>The Dataflow Model and Unified Batch and Streaming<br/></b>Both Lambda and Kappa sought to address limitations of the Hadoop<br/>ecosystem of the 2010s by trying to duct-tape together complicated tools<br/>that were likely not natural fits in the first place. The central challenge of<br/></p>
<p>10</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>unifying batch and streaming data remained, and Lambda and Kappa both<br/>provided inspiration and groundwork for continued progress in this pursuit.<br/>One of the central problems of managing batch and stream processing is<br/>unifying multiple code paths. While the Kappa architecture relies on a<br/>unified queuing and storage layer, one still has to confront using different<br/>tools for collecting real-time statistics or running batch aggregation jobs.<br/>Today, engineers seek to solve this in several ways. Google made its mark<br/>by developing the Dataflow model and the Apache Beam framework that<br/>implements this model.<br/>The core idea in the Dataflow model is to view all data as events, as the<br/>aggregation is performed over various types of windows. Ongoing real-time<br/>event streams are <i>unbounded data</i>. Data batches are simply bounded event<br/>streams, and the boundaries provide a natural window. Engineers can<br/>choose from various windows for real-time aggregation, such as sliding or<br/>tumbling. Real-time and batch processing happens in the same system using<br/>nearly identical code.<br/>The philosophy of &#8220;batch as a special case of streaming&#8221; is now more<br/>pervasive. Various frameworks such as Flink and Spark have adopted a<br/>similar approach.<br/></p>
<p><b>Architecture for IoT<br/></b>The <i>Internet of Things</i> (IoT) is the distributed collection of devices, aka<br/><i>things</i>&#8212;computers, sensors, mobile devices, smart home devices, and<br/>anything else with an internet connection. Rather than generating data from<br/>direct human input (think data entry from a keyboard), IoT data is generated<br/>from devices that collect data periodically or continuously from the<br/>surrounding environment and transmit it to a destination. IoT devices are<br/>often low-powered and operate in low-resource/low bandwidth<br/>environments.<br/>While the concept of IoT devices dates back at least a few decades, the<br/>smartphone revolution created a massive IoT swarm virtually overnight.<br/>Since then, numerous new IoT categories have emerged, such as smart</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>thermostats, car entertainment systems, smart TVs, and smart speakers. The<br/>IoT has evolved from a futurist fantasy to a massive data engineering<br/>domain. We expect IoT to become one of the dominant ways data is<br/>generated and consumed, and this section goes a bit deeper than the others<br/>you&#8217;ve read.<br/>Having a cursory understanding of IoT architecture will help you<br/>understand broader data architecture trends. Let&#8217;s briefly look at some IoT<br/>architecture concepts.<br/><b>Devices<br/></b><i>Devices</i> (also known as <i>things</i>) are the physical hardware connected to the<br/>internet, sensing the environment around them and collecting and<br/>transmitting data to a downstream destination. These devices might be used<br/>in consumer applications like a doorbell camera, smartwatch, or thermostat.<br/>The device might be an AI-powered camera that monitors an assembly line<br/>for defective components, a GPS tracker to record vehicle locations, or a<br/>Raspberry Pi programmed to download the latest tweets and brew your<br/>coffee. Any device capable of collecting data from its environment is an<br/>IoT device.<br/>Devices should be minimally capable of collecting and transmitting data.<br/>However, the device might also crunch data or run ML on the data it<br/>collects before sending it downstream&#8212;edge computing and edge machine<br/>learning, respectively.<br/>A data engineer doesn&#8217;t necessarily need to know the inner details of IoT<br/>devices but should know what the device does, the data it collects, any edge<br/>computations or ML it runs before transmitting the data, and how often it<br/>sends data. It also helps to know the consequences of a device or internet<br/>outage, environmental or other external factors affecting data collection,<br/>and how these may impact the downstream collection of data from the<br/>device.<br/><b>Interfacing with devices</b></p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>A device isn&#8217;t beneficial unless you can get its data. This section covers<br/>some of the key components necessary to interface with IoT devices in the<br/>wild.<br/><i>IoT gateway<br/></i>An <i>IoT gateway</i> is a hub for connecting devices and securely routing<br/>devices to the appropriate destinations on the internet. While you can<br/>connect a device directly to the internet without an IoT gateway, the<br/>gateway allows devices to connect using extremely little power. It acts as a<br/>way station for data retention and manages an internet connection to the<br/>final data destination.<br/>New low-power WiFi standards are designed to make IoT gateways less<br/>critical in the future, but these are just rolling out now. Typically, a swarm<br/>of devices will utilize many IoT gateways, one at each physical location<br/>where devices are present (Figure 3-16).<br/></p>
<p><i>Figure 3-16. A device swarm (circles), IoT gateways, and message queue with messages (rectangles<br/>within the queue)<br/></i></p>
<p><i>Ingestion</i></p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><i>Ingestion</i> begins with an IoT gateway, as discussed previously. From there,<br/>events and measurements can flow into an event ingestion architecture.<br/>Of course, other patterns are possible. For instance, the gateway may<br/>accumulate data and upload it in batches for later analytics processing. In<br/>remote physical environments, gateways may not have connectivity to a<br/>network much of the time. They may upload all data only when they are<br/>brought into the range of a cellular or WiFi network. The point is that the<br/>diversity of IoT systems and environments presents complications&#8212;e.g.,<br/>late-arriving data, data structure and schema disparities, data corruption,<br/>and connection disruption&#8212;that engineers must account for in their<br/>architectures and downstream analytics.<br/><i>Storage<br/></i>Storage requirements will depend a great deal on the latency requirement<br/>for the IoT devices in the system. For example, for remote sensors<br/>collecting scientific data for analysis at a later time, batch object storage<br/>may be perfectly acceptable. However, near real-time responses may be<br/>expected from a system backend that constantly analyzes data in a home<br/>monitoring and automation solution. In this case, a message queue or time-<br/>series database is more appropriate. We discuss storage systems in more<br/>detail in Chapter 6.<br/><i>Serving<br/></i>Serving patterns are incredibly diverse. In a batch scientific application,<br/>data might be analyzed using a cloud data warehouse and then served in a<br/>report. Data will be presented and served in numerous ways in a home-<br/>monitoring application. Data will be analyzed in the near time using a<br/>stream-processing engine or queries in a time-series database to look for<br/>critical events such as a fire, electrical outage, or break-in. Detection of an<br/>anomaly will trigger alerts to the homeowner, the fire department, or other<br/>entity. A batch analytics component also exists&#8212;for example, a monthly<br/>report on the state of the home.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>One significant serving pattern for IoT looks like reverse ETL (Figure 3-<br/>17), although we tend not to use this term in the IoT context. Think of this<br/>scenario: data from sensors on manufacturing devices is collected and<br/>analyzed. The results of these measurements are processed to look for<br/>optimizations that will allow equipment to operate more efficiently. Data is<br/>sent back to reconfigure the devices and optimize them.<br/></p>
<p><i>Figure 3-17. IoT serving pattern for downstream use cases<br/></i></p>
<p><b>Scratching the surface of the IoT<br/></b>IoT scenarios are incredibly complex, and IoT architecture and systems are<br/>also less familiar to data engineers who may have spent their careers<br/>working with business data. We hope that this introduction will encourage<br/>interested data engineers to learn more about this fascinating and rapidly<br/>evolving specialization.<br/></p>
<p><b>Data Mesh<br/></b>The <i>data mesh</i> is a recent response to sprawling monolithic data platforms,<br/>such as centralized data lakes and data warehouses, and &#8220;the great divide of<br/>data,&#8221; wherein the landscape is divided between operational data and<br/>analytical data.  The data mesh attempts to invert the challenges of<br/>centralized data architecture, taking the concepts of domain-driven design<br/>(commonly used in software architectures) and applying them to data<br/>architecture. Because the data mesh has captured much recent attention, you<br/>should be aware of it.<br/>A big part of the data mesh is decentralization, as Zhamak Dehghani noted<br/>in her groundbreaking article on the topic:<br/></p>
<p>11</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><i>In order to decentralize the monolithic data platform, we need to reverse<br/>how we think about data, its locality, and ownership. Instead of flowing<br/>the data from domains into a centrally owned data lake or platform,<br/>domains need to host and serve their domain datasets in an easily<br/>consumable way.<br/></i></p>
<p>Dehghani later identified four key components of the data mesh:<br/>Domain-oriented decentralized data ownership and architecture<br/>Data as a product<br/>Self-serve data infrastructure as a platform<br/>Federated computational governance<br/></p>
<p>Figure 3-18 shows a simplified version of a data mesh architecture, with the<br/>three domains interoperating.<br/></p>
<p><i>12<br/></i>13</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><i>Figure 3-18. Simplified example of a data mesh architecture. Source: From Data Mesh, by Zhamak<br/>Dehghani. Copyright &#169; 2022 Zhamak Dehghani. Published by O&#8217;Reilly Media, Inc. Used with<br/></i></p>
<p><i>permission.<br/></i></p>
<p>You can learn more about data mesh in Dehghani&#8217;s book <i>Data Mesh<br/></i>(O&#8217;Reilly).<br/></p>
<p><b>Other Data Architecture Examples<br/></b>Data architectures have countless other variations, such as data fabric, data<br/>hub, scaled architecture, metadata-first architecture, event-driven<br/>architecture, live data stack (Chapter 11), and many more. And new<br/>architectures will continue to emerge as practices consolidate and mature,<br/>and tooling simplifies and improves. We&#8217;ve focused on a handful of the</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>most critical data architecture patterns that are extremely well established,<br/>evolving rapidly, or both.<br/>As a data engineer, pay attention to how new architectures may help your<br/>organization. Stay abreast of new developments by cultivating a high-level<br/>awareness of the data engineering ecosystem developments. Be open-<br/>minded and don&#8217;t get emotionally attached to one approach. Once you&#8217;ve<br/>identified potential value, deepen your learning and make concrete<br/>decisions. When done right, minor tweaks&#8212;or major overhauls&#8212;in your<br/>data architecture can positively impact the business.<br/></p>
<p><b>Who&#8217;s Involved with Designing a Data<br/>Architecture?<br/></b>Data architecture isn&#8217;t designed in a vacuum. Bigger companies may still<br/>employ data architects, but those architects will need to be heavily in tune<br/>and current with the state of technology and data. Gone are the days of<br/>ivory tower data architecture. In the past, architecture was largely<br/>orthogonal to engineering. We expect this distinction will disappear as data<br/>engineering, and engineering in general, quickly evolves, becoming more<br/>agile, with less separation between engineering and architecture.<br/>Ideally, a data engineer will work alongside a dedicated data architect.<br/>However, if a company is small or low in its level of data maturity, a data<br/>engineer might do double duty as an architect. Because data architecture is<br/>an undercurrent of the data engineering lifecycle, a data engineer should<br/>understand &#8220;good&#8221; architecture and the various types of data architecture.<br/>When designing architecture, you&#8217;ll work alongside business stakeholders<br/>to evaluate trade-offs. What are the trade-offs inherent in adopting a cloud<br/>data warehouse versus a data lake? What are the trade-offs of various cloud<br/>platforms? When might a unified batch/streaming framework (Beam, Flink)<br/>be an appropriate choice? Studying these choices in the abstract will<br/>prepare you to make concrete, valuable decisions.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><b>Conclusion<br/></b>You&#8217;ve learned how data architecture fits into the data engineering lifecycle<br/>and what makes for &#8220;good&#8221; data architecture, and you&#8217;ve seen several<br/>examples of data architectures. Because architecture is such a key<br/>foundation for success, we encourage you to invest the time to study it<br/>deeply and understand the trade-offs inherent in any architecture. You will<br/>be prepared to map out architecture that corresponds to your organization&#8217;s<br/>unique requirements.<br/>Next up, let&#8217;s look at some approaches to choosing the right technologies to<br/>be used in data architecture and across the data engineering lifecycle.<br/></p>
<p><b>Additional Resources<br/></b>&#8220;Separating Utility from Value Add&#8221; by Ross Pettit<br/>&#8220;Tactics vs. Strategy: SOA and the Tarpit of Irrelevancy&#8221; by Neal Ford<br/>The Information Management Body of Knowledge website<br/>&#8220;The Modern Data Stack: Past, Present, and Future&#8221; by Tristan Handy<br/>&#8220;Potemkin Data Science&#8221; by Michael Correll<br/>&#8220;A Comparison of Data Processing Frameworks&#8221; by Ludovik Santos<br/>&#8220;Modern CI Is Too Complex and Misdirected&#8221; by Gregory Szorc<br/>&#8220;Questioning the Lambda Architecture&#8221; by Jay Kreps<br/>&#8220;End-to-End Serverless ETL Orchestration in AWS: A Guide&#8221; by<br/>Rittika Jindal<br/>&#8220;A Brief Introduction to Two Data Processing Architectures&#8212;Lambda<br/>and Kappa for Big Data&#8221; by Iman Samizadeh<br/>&#8220;How to Beat the Cap Theorem&#8221; by Nathan Marz</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>&#8220;The Log: What Every Software Engineer Should Know About Real-<br/>Time Data&#8217;s Unifying Abstraction&#8221; by Jay Kreps<br/>&#8220;Run Your Data Team Like a Product Team&#8221; by Emilie Schario and<br/>Taylor A. Murphy<br/>&#8220;Data as a Product vs. Data as a Service&#8221; by Justin Gage<br/></p>
<p><i>Martin Fowler articles:<br/></i>&#8220;EagerReadDerivation&#8221;<br/>&#8220;AnemicDomainModel&#8221;<br/>&#8220;DomainDrivenDesign&#8221;<br/>&#8220;Event Sourcing&#8221;<br/>&#8220;ReportingDatabase&#8221;<br/>&#8220;BoundedContext&#8221;<br/>&#8220;Focusing on Events&#8221;<br/>&#8220;Polyglot Persistence&#8221;<br/>&#8220;UtilityVsStrategicDichotomy&#8221;<br/></p>
<p><i>Data applications:<br/></i>&#8220;Introducing Dagster: An Open Source Python Library for Building<br/>Data Applications&#8221; by Nick Schrock<br/>&#8220;Down with Pipeline Debt: Introducing Great Expectations,&#8221; by the<br/>Great Expectations project<br/>&#8220;Functional Data Engineering: A Modern Paradigm for Batch Data<br/>Processing&#8221; by Maxime Beauchemin<br/>&#8220;What Is the Open Data Ecosystem and Why It&#8217;s Here to Stay&#8221; by<br/>Casber Wang</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>&#8220;Staying Ahead of Data Debt&#8221; by Etai Mizrahi<br/>&#8220;Choosing Open Wisely&#8221; by Benoit Dageville et al.<br/>&#8220;The Ultimate Data Observability Checklist&#8221; by Molly Vorwerck<br/>&#8220;Moving Beyond Batch vs. Streaming&#8221; by David Yaffe<br/>&#8220;Disasters I&#8217;ve Seen in a Microservices World&#8221; by Joao Alves<br/></p>
<p><i>Naming conventions:<br/></i>&#8220;240 Tables and No Documentation?&#8221; by Alexey Makhotkin<br/>&#8220;Data Warehouse Architecture: Overview&#8221; by Roelant Vos<br/>&#8220;The Design and Implementation of Modern Column-Oriented<br/>Database Systems&#8221; by Daniel Abadi et al.<br/>&#8220;Column-oriented DBMS&#8221; Wikipedia page<br/>&#8220;The Data Dichotomy: Rethinking the Way We Treat Data and<br/>Services&#8221; by Ben Stopford<br/>&#8220;The Curse of the Data Lake Monster&#8221; by Kiran Prakash and Lucy<br/>Chambers<br/>&#8220;Software Infrastructure 2.0: A Wishlist&#8221; by Erik Bernhardsson<br/>&#8220;Data Team Platform&#8221; by GitLab Data<br/>&#8220;Falling Back in Love with Data Pipelines&#8221; by Sean Knapp<br/></p>
<p><i>Build versus buy fallacy:<br/></i>&#8220;What&#8217;s Wrong with MLOps?&#8221; by Laszlo Sragner<br/>&#8220;Zachman Framework&#8221; Wikipedia page<br/>&#8220;How TOGAF Defines Enterprise Architecture (EA)&#8221; by Avancier<br/>Limited<br/>EABOK Draft, edited by Paula Hagan</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>&#8220;What Is Data Architecture? A Framework for Managing Data&#8221; by<br/>Thor Olavsrud<br/>&#8220;Google Cloud Architecture Framework&#8221; Google Cloud Architecture<br/>web page<br/>Microsoft&#8217;s &#8220;Azure Architecture Center&#8221;<br/>&#8220;Five Principles for Cloud-Native Architecture: What It Is and How to<br/>Master It&#8221; by Tom Grey<br/>&#8220;Choosing the Right Architecture for Global Data Distribution&#8221;<br/>Google Cloud Architecture web page<br/>&#8220;Principled Data Engineering, Part I: Architectural Overview&#8221; by<br/>Hussein Danish<br/>&#8220;A Personal Implementation of Modern Data Architecture: Getting<br/>Strava Data into Google Cloud Platform&#8221; by Matthew Reeve<br/>&#8220;The Cost of Cloud, a Trillion Dollar Paradox&#8221; by Sarah Wang and<br/>Martin Casado<br/><i>Data Architecture: A Primer for the Data Scientist</i> by W.H. Inmon et<br/>al. (Academic Press)<br/>&#8220;Enterprise Architecture&#8221; Gartner Glossary definition<br/>EABOK website<br/>TOGAF framework website<br/>&#8220;Defining Architecture&#8221; ISO/IEC/IEEE 42010 web page<br/>&#8220;The Six Principles of Modern Data Architecture&#8221; by Joshua Klahr<br/>&#8220;The Rise of the Metadata Lake&#8221; by Prukalpa<br/>&#8220;The Top 5 Data Trends for CDOs to Watch Out for in 2021&#8221; by<br/>Prukalpa<br/>&#8220;Test Data Quality at Scale with Deequ&#8221; by Dustin Lange et al.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>&#8220;What the Heck Is Data Mesh&#8221; by Chris Riccomini<br/>&#8220;Reliable Microservices Data Exchange with the Outbox Pattern&#8221; by<br/>Gunnar Morling<br/>&#8220;Who Needs an Architect&#8221; by Martin Fowler<br/>&#8220;Enterprise Architecture&#8217;s Role in Building a Data-Driven<br/>Organization&#8221; by Ashutosh Gupta<br/>&#8220;How to Build a Data Architecture to Drive Innovation&#8212;Today and<br/>Tomorrow&#8221; by Antonio Castro et al.<br/>&#8220;Data Warehouse Architecture&#8221; tutorial at Javatpoint<br/>Snowflake&#8217;s &#8220;What Is Data Warehouse Architecture&#8221; web page<br/>&#8220;Three-Tier Architecture&#8221; by IBM Education<br/>&#8220;The Building Blocks of a Modern Data Platform&#8221; by Prukalpa<br/>&#8220;Data Architecture: Complex vs. Complicated&#8221; by Dave Wells<br/>&#8220;Data Fabric Defined&#8221; by James Serra<br/>&#8220;Data Fabric Architecture Is Key to Modernizing Data Management<br/>and Integration&#8221; by Ashutosh Gupta<br/>&#8220;Unified Analytics: Where Batch and Streaming Come Together; SQL<br/>and Beyond&#8221; Apache Flink Roadmap<br/>&#8220;What Is a Data Lakehouse?&#8221; by Ben Lorica et al.<br/>&#8220;Big Data Architectures&#8221; Azure documentation<br/>&#8220;Microsoft Azure IoT Reference Architecture&#8221; documentation<br/></p>
<p>1  Jeff Haden, &#8220;Amazon Founder Jeff Bezos: This Is How Successful People Make Such Smart<br/>Decisions,&#8221; <i>Inc.</i>, December 3, 2018, <i>https://oreil.ly/QwIm0</i>.<br/></p>
<p>2  Martin Fowler, &#8220;Who Needs an Architect?&#8221; <i>IEEE Software</i>, July/August 2003,<br/><i>https://oreil.ly/wAMmZ</i>.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>3  &#8220;The Bezos API Mandate: Amazon&#8217;s Manifesto for Externalization,&#8221; Nordic APIs, January<br/>19, 2021, <i>https://oreil.ly/vIs8m</i>.<br/></p>
<p>4  Fowler, &#8220;Who Needs an Architect?&#8221;<br/>5  Ericka Chickowski, &#8220;Leaky Buckets: 10 Worst Amazon S3 Breaches,&#8221; Bitdefender <i>Business<br/></i></p>
<p><i>Insights</i> blog, Jan 24, 2018, <i>https://oreil.ly/pFEFO</i>.<br/>6  &#8220;FinOps Foundation Soars to 300 Members and Introduces New Partner Tiers for Cloud<br/></p>
<p>Service Providers and Vendors,&#8221; Business Wire, June 17, 2019, <i>https://oreil.ly/XcwYO</i>.<br/>7  Martin Fowler, &#8220;StranglerFigApplication,&#8221; June 29, 2004, <i>https://oreil.ly/PmqxB</i>.<br/>8  Mike Loukides, &#8220;Resume Driven Development,&#8221; <i>O&#8217;Reilly Radar</i>, October 13, 2004,<br/></p>
<p><i>https://oreil.ly/BUHa8</i>.<br/>9  H.W. Inmon, <i>Building the Data Warehouse</i> (Hoboken: Wiley, 2005).<br/>10  Jay Kreps, &#8220;Questioning the Lambda Architecture,&#8221; O&#8217;Reilly, July 2, 2014,<br/></p>
<p><i>https://oreil.ly/wWR3n</i>.<br/>11  Martin Fowler, &#8220;Data Mesh Principles and Logical Architecture,&#8221; MartinFowler.com,<br/></p>
<p>December 3, 2020, <i>https://oreil.ly/ezWE7</i>.<br/>12  Zhamak Dehghani, &#8220;How to Move Beyond a Monolithic Data Lake to a Distributed Data<br/></p>
<p>Mesh,&#8221; MartinFowler.com, May 20, 2019, <i>https://oreil.ly/SqMe8</i>.<br/>13  Zhamak Dehghani, &#8220;Data Mesh Principles,&#8221; MartinFowler.com, December 3, 2020,<br/></p>
<p><i>https://oreil.ly/RymDi</i>.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><b>Chapter 4. Choosing<br/>Technologies Across the Data<br/>Engineering Lifecycle<br/></b>Data engineering nowadays suffers from an embarrassment of riches. We<br/>have no shortage of technologies to solve various types of data problems.<br/>Data technologies are available as turnkey offerings consumable in almost<br/>every way&#8212;open source, managed open source, proprietary software,<br/>proprietary service, and more. However, it&#8217;s easy to get caught up in<br/>chasing bleeding-edge technology while losing sight of the core purpose of<br/>data engineering: designing robust and reliable systems to carry data<br/>through the full lifecycle and serve it according to the needs of end users.<br/>Just as structural engineers carefully choose technologies and materials to<br/>realize an architect&#8217;s vision for a building, data engineers are tasked with<br/>making appropriate technology choices to shepherd data through the<br/>lifecycle to serve data applications and users.<br/>Chapter 3 discussed &#8220;good&#8221; data architecture and why it matters. We now<br/>explain how to choose the right technologies to serve this architecture. Data<br/>engineers must choose good technologies to make the best possible data<br/>product. We feel the criteria to choose a good data technology is simple:<br/>does it add value to a data product and the broader business?<br/>A lot of people confuse architecture and tools. Architecture is <i>strategic</i>;<br/>tools are <i>tactical</i>. We sometimes hear, &#8220;Our data architecture are tools <i>X</i>, <i>Y</i>,<br/>and <i>Z</i>.&#8221; This is the wrong way to think about architecture. Architecture is<br/>the top-level design, roadmap, and blueprint of data systems that satisfy the<br/>strategic aims for the business. Architecture is the <i>what</i>, <i>why</i>, and <i>when</i>.<br/>Tools are used to make the architecture a reality; tools are the <i>how</i>.<br/>We often see teams going &#8220;off the rails&#8221; and choosing technologies before<br/>mapping out an architecture. The reasons vary: shiny object syndrome,</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>resume-driven development, and a lack of expertise in architecture. In<br/>practice, this prioritization of technology often means they cobble together<br/>a kind of Dr. Suess fantasy machine rather than a true data architecture. We<br/>strongly advise against choosing technology before getting your<br/>architecture right. Architecture first, technology second.<br/>This chapter discusses our tactical plan for making technology choices once<br/>we have a strategic architecture blueprint. The following are some<br/>considerations for choosing data technologies across the data engineering<br/>lifecycle:<br/></p>
<p>Team size and capabilities<br/>Speed to market<br/>Interoperability<br/>Cost optimization and business value<br/>Today versus the future: immutable versus transitory technologies<br/>Location (cloud, on prem, hybrid cloud, multicloud)<br/>Build versus buy<br/>Monolith versus modular<br/>Serverless versus servers<br/>Optimization, performance and the benchmark wars.<br/>The undercurrents of the data engineering lifecycle<br/></p>
<p><b>Team Size and Capabilities<br/></b>The first thing you need to assess is your team&#8217;s size and its capabilities<br/>with technology. Are you on a small team (perhaps a team of one) of people<br/>who are expected to wear many hats, or is the team large enough that<br/>people work in specialized roles? Will a handful of people be responsible<br/>for multiple stages of the data engineering lifecycle, or do people cover</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>particular niches? Your team&#8217;s size will influence the types of technologies<br/>you adopt.<br/>There is a continuum of simple to complex technologies, and a team&#8217;s size<br/>roughly determines the amount of bandwidth your team can dedicate to<br/>complex solutions. We sometimes see small data teams read blog posts<br/>about a new cutting-edge technology at a giant tech company and then try<br/>to emulate these same extremely complex technologies and practices. We<br/>call this <i>cargo-cult engineering</i>, and it&#8217;s generally a big mistake that<br/>consumes a lot of valuable time and money, often with little to nothing to<br/>show in return. Especially for small teams or teams with weaker technical<br/>chops, use as many managed and SaaS tools as possible, and dedicate your<br/>limited bandwidth to solving the complex problems that directly add value<br/>to the business.<br/>Take an inventory of your team&#8217;s skills. Do people lean toward low-code<br/>tools, or do they favor code-first approaches? Are people strong in certain<br/>languages like Java, Python, or Go? Technologies are available to cater to<br/>every preference on the low-code to code-heavy spectrum. Again, we<br/>suggest sticking with technologies and workflows with which the team is<br/>familiar. We&#8217;ve seen data teams invest a lot of time in learning the shiny<br/>new data framework, only to never use it in production. Learning new<br/>technologies, languages, and tools is a considerable time investment, so<br/>make these investments wisely.<br/></p>
<p><b>Speed to Market<br/></b>In technology, speed to market wins. This means choosing the right<br/>technologies that help you deliver features and data faster while<br/>maintaining high-quality standards and security. It also means working in a<br/>tight feedback loop of launching, learning, iterating, and making<br/>improvements.<br/>Perfect is the enemy of good. Some data teams will deliberate on<br/>technology choices for months or years without reaching any decisions.<br/>Slow decisions and output are the kiss of death to data teams. We&#8217;ve seen</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>more than a few data teams dissolve for moving too slow and failing to<br/>deliver the value they were hired to produce.<br/>Deliver value early and often. As we&#8217;ve mentioned, use what works. Your<br/>team members will likely get better leverage with tools they already know.<br/>Avoid undifferentiated heavy lifting that engages your team in<br/>unnecessarily complex work that adds little to no value. Choose tools that<br/>help you move quickly, reliably, safely, and securely.<br/></p>
<p><b>Interoperability<br/></b>Rarely will you use only one technology or system. When choosing a<br/>technology or system, you&#8217;ll need to ensure that it interacts and operates<br/>with other technologies. <i>Interoperability</i> describes how various<br/>technologies or systems connect, exchange information, and interact.<br/>Let&#8217;s say you&#8217;re evaluating two technologies, A and B. How easily does<br/>technology A integrate with technology B when thinking about<br/>interoperability? This is often a spectrum of difficulty, ranging from<br/>seamless to time-intensive. Is seamless integration already baked into each<br/>product, making setup a breeze? Or do you need to do a lot of manual<br/>configuration to integrate these technologies?<br/>Often, vendors and open source projects will target specific platforms and<br/>systems to interoperate. Most data ingestion and visualization tools have<br/>built-in integrations with popular data warehouses and data lakes.<br/>Furthermore, popular data-ingestion tools will integrate with common APIs<br/>and services, such as CRMs, accounting software, and more.<br/>Sometimes standards are in place for interoperability. Almost all databases<br/>allow connections via Java Database Connectivity (JDBC) or Open<br/>Database Connectivity (ODBC), meaning that you can easily connect to a<br/>database by using these standards. In other cases, interoperability occurs in<br/>the absence of standards. Representational state transfer (REST) is not truly<br/>a standard for APIs; every REST API has its quirks. In these cases, it&#8217;s up</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>to the vendor or open source software (OSS) project to ensure smooth<br/>integration with other technologies and systems.<br/>Always be aware of how simple it will be to connect your various<br/>technologies across the data engineering lifecycle. As mentioned in other<br/>chapters, we suggest designing for modularity and giving yourself the<br/>ability to easily swap out technologies as new practices and alternatives<br/>become available.<br/></p>
<p><b>Cost Optimization and Business Value<br/></b>In a perfect world, you&#8217;d get to experiment with all the latest, coolest<br/>technologies without considering cost, time investment, or value added to<br/>the business. In reality, budgets and time are finite, and the cost is a major<br/>constraint for choosing the right data architectures and technologies. Your<br/>organization expects a positive ROI from your data projects, so you must<br/>understand the basic costs you can control. Technology is a major cost<br/>driver, so your technology choices and management strategies will<br/>significantly impact your budget. We look at costs through three main<br/>lenses: total cost of ownership, opportunity cost, and FinOps.<br/></p>
<p><b>Total Cost of Ownership<br/></b><i>Total cost of ownership</i> (TCO) is the total estimated cost of an initiative,<br/>including the direct and indirect costs of products and services utilized.<br/><i>Direct costs</i> can be directly attributed to an initiative. Examples are the<br/>salaries of a team working on the initiative or the AWS bill for all services<br/>consumed. <i>Indirect costs</i>, also known as <i>overhead</i>, are independent of the<br/>initiative and must be paid regardless of where they&#8217;re attributed.<br/>Apart from direct and indirect costs, <i>how something is purchased</i> impacts<br/>the way costs are accounted for. Expenses fall into two big groups: capital<br/>expenses and operational expenses.<br/><i>Capital expenses</i>, also known as <i>capex</i>, require an up-front investment.<br/>Payment is required <i>today</i>. Before the cloud existed, companies would</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>typically purchase hardware and software up front through large acquisition<br/>contracts. In addition, significant investments were required to host<br/>hardware in server rooms, data centers, and colocation facilities. These up-<br/>front investments&#8212;commonly hundreds of thousands to millions of dollars<br/>or more&#8212;would be treated as assets and slowly depreciate over time. From<br/>a budget perspective, capital was required to fund the entire purchase. This<br/>is capex, a significant capital outlay with a long-term plan to achieve a<br/>positive ROI on the effort and expense put forth.<br/><i>Operational expenses</i>, also known as <i>opex</i>, are the opposite of capex in<br/>certain respects. Opex is gradual and spread out over time. Whereas capex<br/>is long-term focused, opex is short-term. Opex can be pay-as-you-go or<br/>similar and allows a lot of flexibility. Opex is closer to a direct cost, making<br/>it easier to attribute to a data project.<br/>Until recently, opex wasn&#8217;t an option for large data projects. Data systems<br/>often required multimillion-dollar contracts. This has changed with the<br/>advent of the cloud, as data platform services allow engineers to pay on a<br/>consumption-based model. In general, opex allows for a far greater ability<br/>for engineering teams to choose their software and hardware. Cloud-based<br/>services let data engineers iterate quickly with various software and<br/>technology configurations, often inexpensively.<br/>Data engineers need to be pragmatic about flexibility. The data landscape is<br/>changing too quickly to invest in long-term hardware that inevitably goes<br/>stale, can&#8217;t easily scale, and potentially hampers a data engineer&#8217;s flexibility<br/>to try new things. Given the upside for flexibility and low initial costs, we<br/>urge data engineers to take an opex-first approach centered on the cloud and<br/>flexible, pay-as-you-go technologies.<br/></p>
<p><b>Total Opportunity Cost of Ownership<br/></b>Any choice inherently excludes other possibilities. <i>Total opportunity cost of<br/>ownership</i> (TOCO) is the cost of lost opportunities that we incur in<br/>choosing a technology, an architecture, or a process.  Note that ownership<br/>in this setting doesn&#8217;t require long-term purchases of hardware or licenses.<br/></p>
<p>1</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Even in a cloud environment, we effectively own a technology, a stack, or a<br/>pipeline once it becomes a core part of our production data processes and is<br/>difficult to move away from. Data engineers often fail to evaluate TOCO<br/>when undertaking a new project; in our opinion, this is a massive blind spot.<br/>If you choose data stack A, you&#8217;ve chosen the benefits of data stack A over<br/>all other options, effectively excluding data stacks B, C, and D. You&#8217;re<br/>committed to data stack A and everything it entails&#8212;the team to support it,<br/>training, setup, and maintenance. What happens if data stack A was a poor<br/>choice? What happens when data stack A becomes obsolete? Can you still<br/>move to data stack B?<br/>How quickly and cheaply can you move to something newer and better?<br/>This is a critical question in the data space, where new technologies and<br/>products seem to appear at an ever-faster rate. Does the expertise you&#8217;ve<br/>built up on data stack A translate to the next wave? Or are you able to swap<br/>out components of data stack A and buy yourself some time and options?<br/>The first step to minimizing opportunity cost is evaluating it with eyes wide<br/>open. We&#8217;ve seen countless data teams get stuck with technologies that<br/>seemed good at the time and are either not flexible for future growth or<br/>simply obsolete. Inflexible data technologies are a lot like bear traps.<br/>They&#8217;re easy to get into and extremely painful to escape.<br/></p>
<p><b>FinOps<br/></b>We already touched on FinOps in &#8220;Principle 9: Embrace FinOps&#8221;. As we&#8217;ve<br/>discussed, typical cloud spending is inherently opex: companies pay for<br/>services to run critical data processes rather than making up-front purchases<br/>and clawing back value over time. The goal of FinOps is to fully<br/>operationalize financial accountability and business value by applying the<br/>DevOps-like practices of monitoring and dynamically adjusting systems.<br/>In this chapter, we want to emphasize one thing about FinOps that is well<br/>embodied in this quote:</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><i>If it seems that FinOps is about saving money, then think again. FinOps<br/>is about making money. Cloud spend can drive more revenue, signal<br/>customer base growth, enable more product and feature release velocity,<br/>or even help shut down a data center.<br/></i></p>
<p>In our setting of data engineering, the ability to iterate quickly and scale<br/>dynamically is invaluable for creating business value. This is one the major<br/>motivations for shifting data workloads to the cloud.<br/></p>
<p><b>Today Versus the Future: Immutable Versus<br/>Transitory Technologies<br/></b>In an exciting domain like data engineering, it&#8217;s all too easy to focus on a<br/>rapidly evolving future while ignoring the concrete needs of the present.<br/>The intention to build a better future is noble but often leads to<br/>overarchitecting and overengineering. Tooling chosen for the future may be<br/>stale and out-of-date when this future arrives; the future frequently looks<br/>little like what we envisioned years before.<br/>As many life coaches would tell you, focus on the present. You should<br/>choose the best technology for the moment and near future, but in a way<br/>that supports future unknowns and evolution. Ask yourself: where are you<br/>today, and what are your goals for the future? Your answers to these<br/>questions should inform your decisions about your architecture and thus the<br/>technologies used within that architecture. This is done by understanding<br/>what is likely to change and what tends to stay the same.<br/>We have two classes of tools to consider: immutable and transitory.<br/><i>Immutable technologies</i> might be components that underpin the cloud or<br/>languages and paradigms that have stood the test of time. In the cloud,<br/>examples of immutable technologies are object storage, networking,<br/>servers, and security. Object storage such as Amazon S3 and Azure Blob<br/>Storage will be around from today until the end of the decade, and probably<br/>much longer. Storing your data in object storage is a wise choice. Object<br/>storage continues to improve in various ways and constantly offers new<br/></p>
<p><i>2</i></p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>options, but your data will be safe and usable in object storage regardless of<br/>the rapid evolution of technology.<br/>For languages, SQL and bash have been around for many decades, and we<br/>don&#8217;t see them disappearing anytime soon. Immutable technologies benefit<br/>from the Lindy effect: the longer a technology has been established, the<br/>longer it will be used. Think of the power grid, relational databases, the C<br/>programming language, or the x86 processor architecture. We suggest<br/>applying the Lindy effect as a litmus test to determine whether a technology<br/>is potentially immutable.<br/><i>Transitory technologies</i> are those that come and go. The typical trajectory<br/>begins with a lot of hype, followed by meteoric growth in popularity, then a<br/>slow descent into obscurity. The JavaScript frontend landscape is a classic<br/>example. How many JavaScript frontend frameworks have come and gone<br/>between 2010 and 2020? Backbone.js, Ember.js, and Knockout were<br/>popular in the early 2010s, AngularJS in the mid-2010s, and React and<br/>Vue.js have massive mindshare today. What&#8217;s the popular frontend<br/>framework three years from now? Who knows.<br/>New well-funded entrants and open source projects arrive on the data front<br/>every day. Every vendor will say their product will change the industry and<br/>&#8220;make the world a better place&#8221;. Most of these companies and projects<br/>don&#8217;t get long-term traction and fade slowly into obscurity. Top VCs are<br/>making big-money bets, knowing that most of their data-tooling<br/>investments will fail. How can you possibly know which technologies to<br/>invest in for your data architecture? It&#8217;s hard. Just consider the number of<br/>technologies in Matt Turck&#8217;s (in)famous depictions of the ML, AI, and data<br/>(MAD) landscape that we introduced in Chapter 1 (Figure 4-1).</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><i>Figure 4-1. Matt Turck&#8217;s 2021 MAD data landscape<br/></i></p>
<p>Even relatively successful technologies often fade into obscurity quickly,<br/>after a few years of rapid adoption, a victim of their success. For instance,<br/>in the early 2010s, Hive was met with rapid uptake because it allowed both<br/>analysts and engineers to query massive datasets without coding complex<br/>MapReduce jobs by hand. Inspired by the success of Hive but wishing to<br/>improve on its shortcomings, engineers developed Presto and other<br/>technologies. Hive now appears primarily in legacy deployments.<br/></p>
<p><b>Our Advice<br/></b>Given the rapid pace of tooling and best-practice changes, we suggest<br/>evaluating tools every two years (Figure 4-2). Whenever possible, find the<br/>immutable technologies along the data engineering lifecycle, and use those<br/>as your base. Build transitory tools around the immutables.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><i>Figure 4-2. Use a two-year time horizon to reevaluate your technology choices<br/></i></p>
<p>Given the reasonable probability of failure for many data technologies, you<br/>need to consider how easy it is to transition from a chosen technology.<br/>What are the barriers to leaving? As mentioned previously in our discussion<br/>about opportunity cost, avoid &#8220;bear traps.&#8221; Go into a new technology with<br/>eyes wide open, knowing the project might get abandoned, the company<br/>may not be viable, or the technology simply isn&#8217;t a good fit any longer.<br/></p>
<p><b>Location<br/></b>Companies now have numerous options when deciding where to run their<br/>technology stacks. A slow shift toward the cloud culminates in a veritable<br/>stampede of companies spinning up workloads on AWS, Azure, and Google<br/>Cloud Platform (GCP). In the last decade, many CTOs have come to view<br/>their decisions around technology hosting as having existential significance<br/>for their organizations. If they move too slowly, they risk being left behind<br/>by their more agile competition; on the other hand, a poorly planned cloud<br/>migration could lead to technological failure and catastrophic costs.<br/>Let&#8217;s look at the principal places to run your technology stack: on premises,<br/>the cloud, hybrid cloud, and multicloud.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><b>On Premises<br/></b>While new startups are increasingly born in the cloud, on-premises systems<br/>are still the default for established companies. Essentially, these companies<br/>own their hardware, which may live in data centers they own or in leased<br/>colocation space. In either case, companies are operationally responsible for<br/>their hardware and the software that runs on it. If hardware fails, they have<br/>to repair or replace it. They also have to manage upgrade cycles every few<br/>years as new, updated hardware is released and older hardware ages and<br/>becomes less reliable. They must ensure that they have enough hardware to<br/>handle peaks; for an online retailer, this means hosting enough capacity to<br/>handle the load spikes of Black Friday. For data engineers in charge of on-<br/>premises systems, this means buying large-enough systems to allow good<br/>performance for peak load and large jobs without overbuying and<br/>overspending.<br/>On the one hand, established companies have established operational<br/>practices that have served them well. Suppose a company that relies on<br/>information technology has been in business for some time. This means it<br/>has managed to juggle the cost and personnel requirements of running its<br/>hardware, managing software environments, deploying code from dev<br/>teams, and running databases and big data systems.<br/>On the other hand, established companies see their younger, more agile<br/>competition scaling rapidly and taking advantage of cloud-managed<br/>services. They also see established competitors making forays into the<br/>cloud, allowing them to temporarily scale up enormous computing power<br/>for massive data jobs or the Black Friday shopping spike.<br/>Companies in competitive sectors generally don&#8217;t have the option to stand<br/>still. Competition is fierce, and there&#8217;s always the threat of being<br/>&#8220;disrupted&#8221; by more agile competition, backed by a large pile of venture<br/>capital dollars. Every company must keep its existing systems running<br/>efficiently while deciding what moves to make next. This could involve<br/>adopting newer DevOps practices, such as containers, Kubernetes,<br/>microservices, and continuous deployment while keeping their hardware</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>running on premises. It could involve a complete migration to the cloud, as<br/>discussed next.<br/></p>
<p><b>Cloud<br/></b>The cloud flips the on-premises model on its head. Instead of purchasing<br/>hardware, you simply rent hardware and managed services from a cloud<br/>provider (such as AWS, Azure, or Google Cloud). These resources can<br/>often be reserved on an extremely short-term basis; VMs spin up in less<br/>than a minute, and subsequent usage is billed in per-second increments.<br/>This allows cloud users to dynamically scale resources that were<br/>inconceivable with on-premises servers.<br/>In a cloud environment, engineers can quickly launch projects and<br/>experiment without worrying about long lead time hardware planning. They<br/>can begin running servers as soon as their code is ready to deploy. This<br/>makes the cloud model extremely appealing to startups that are tight on<br/>budget and time.<br/>The early cloud era was dominated by infrastructure as a service (IaaS)<br/>offerings&#8212;products such as VMs and virtual disks that are essentially<br/>rented slices of hardware. Slowly, we&#8217;ve seen a shift toward platform as a<br/>service (PaaS), while SaaS products continue to grow at a rapid clip.<br/>PaaS includes IaaS products but adds more sophisticated managed services<br/>to support applications. Examples are managed databases such as Amazon<br/>Relational Database Service (RDS) and Google Cloud SQL, managed<br/>streaming platforms such as Amazon Kinesis and Simple Queue Service<br/>(SQS), and managed Kubernetes such as Google Kubernetes Engine (GKE)<br/>and Azure Kubernetes Service (AKS). PaaS services allow engineers to<br/>ignore the operational details of managing individual machines and<br/>deploying frameworks across distributed systems. They provide turnkey<br/>access to complex, autoscaling systems with minimal operational overhead.<br/>SaaS offerings move one additional step up the ladder of abstraction. SaaS<br/>typically provides a fully functioning enterprise software platform with<br/>little operational management. Examples of SaaS include Salesforce,</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Google Workspace, Microsoft 365, Zoom, and Fivetran. Both the major<br/>public clouds and third parties offer SaaS platforms. SaaS covers a whole<br/>spectrum of enterprise domains, including video conferencing, data<br/>management, ad tech, office applications, and CRM systems.<br/>This chapter also discusses serverless, increasingly important in PaaS and<br/>SaaS offerings. Serverless products generally offer automated scaling from<br/>zero to extremely high usage rates. They are billed on a pay-as-you-go basis<br/>and allow engineers to operate without operational awareness of underlying<br/>servers. Many people quibble with the term <i>serverless</i>; after all, the code<br/>must run somewhere. In practice, serverless usually means <i>many invisible<br/>servers</i>.<br/>Cloud services have become increasingly appealing to established<br/>businesses with existing data centers and IT infrastructure. Dynamic,<br/>seamless scaling is extremely valuable to businesses that deal with<br/>seasonality (e.g., retail businesses coping with Black Friday load) and web<br/>traffic load spikes. The advent of COVID-19 in 2020 was a major driver of<br/>cloud adoption, as companies recognized the value of rapidly scaling up<br/>data processes to gain insights in a highly uncertain business climate;<br/>businesses also had to cope with substantially increased load due to a spike<br/>in online shopping, web app usage, and remote work.<br/>Before we discuss the nuances of choosing technologies in the cloud, let&#8217;s<br/>first discuss why migration to the cloud requires a dramatic shift in<br/>thinking, specifically on the pricing front; this is closely related to FinOps,<br/>introduced in &#8220;FinOps&#8221;. Enterprises that migrate to the cloud often make<br/>major deployment errors by not appropriately adapting their practices to the<br/>cloud pricing model.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><b>A BRIEF DETOUR ON CLOUD ECONOMICS<br/></b>To understand how to use cloud services efficiently through cloud<br/>native architecture, you need to know how clouds make money. This is<br/>an extremely complex concept and one on which cloud providers offer<br/>little transparency. Consider this sidebar a starting point for your<br/>research, discovery, and process development.<br/><b>Cloud Services and Credit Default Swaps<br/></b>Let&#8217;s go on a little tangent about credit default swaps. Don&#8217;t worry, this<br/>will make sense in a bit. Recall that credit default swaps rose to infamy<br/>after the 2007 global financial crisis. A credit default swap was a<br/>mechanism for selling different tiers of risk attached to an asset (i.e., a<br/>mortgage.) It is not our intention to present this idea in any detail, but<br/>rather to offer an analogy wherein many cloud services are similar to<br/>financial derivatives; cloud providers not only slice hardware assets into<br/>small pieces through virtualization, but also sell these pieces with<br/>varying technical characteristics and risks attached. While providers are<br/>extremely tight-lipped about details of their internal systems, there are<br/>massive opportunities for optimization and scaling by understanding<br/>cloud pricing and exchanging notes with other users.<br/>Look at the example of archival cloud storage. At the time of this<br/>writing, GCP openly admits that its archival class storage runs on the<br/>same clusters as standard cloud storage, yet the price per gigabyte per<br/>month of archival storage is roughly 1/17 that of standard storage. How<br/>is this possible?<br/>Here&#8217;s our educated guess. When purchasing cloud storage, each disk in<br/>a storage cluster has three assets that cloud providers and consumers<br/>use. First, it has a certain storage capacity&#8212;say, 10 TB. Second, it<br/>supports a certain number of input/output operations (IOPs) per second<br/>&#8212;say, 100. Third, disks support a certain maximum bandwidth, the<br/>maximum read speed for optimally organized files. A magnetic drive<br/>might be capable of reading at 200 MB/s.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Any of these limits (IOPs, storage capacity, bandwidth) is a potential<br/>bottleneck for a cloud provider. For instance, the cloud provider might<br/>have a disk storing 3 TB of data but hitting maximum IOPs. An<br/>alternative to leaving the remaining 7 TB empty is to sell the empty<br/>space without selling IOPs. Or, more specifically, sell cheap storage<br/>space and expensive IOPs to discourage reads.<br/>Much like traders of financial derivatives, cloud vendors also deal in<br/>risk. In the case of archival storage, vendors are selling a type of<br/>insurance, but one that pays out for the insurer rather than the policy<br/>buyer in the event of a catastrophe. While data storage costs per month<br/>are extremely cheap, I risk paying a high price if I ever need to retrieve<br/>data. But this is a price that I will happily pay in a true emergency.<br/>Similar considerations apply to nearly any cloud service. While on-<br/>premises servers are essentially sold as commodity hardware, the cost<br/>model in the cloud is more subtle. Rather than just charging for CPU<br/>cores, memory, and features, cloud vendors monetize characteristics<br/>such as durability, reliability, longevity, and predictability; a variety of<br/>compute platforms discount their offerings for workloads that are<br/>ephemeral or can be arbitrarily interrupted when capacity is needed<br/>elsewhere.<br/><b>Cloud &#8800; On Premises<br/></b>This heading may seem like a silly tautology, but the belief that cloud<br/>services are just like familiar on-premises servers is a widespread<br/>cognitive error that plagues cloud migrations and leads to horrifying<br/>bills. This demonstrates a broader issue in tech that we refer to as <i>the<br/>curse of familiarity</i>. Many new technology products are intentionally<br/>designed to look like something familiar to facilitate ease of use and<br/>accelerate adoption. But, any new technology product has subtleties and<br/>wrinkles that users must learn to identify, accommodate, and optimize.<br/>Moving on-premises servers one by one to VMs in the cloud&#8212;known<br/>as simple <i>lift and shift</i>&#8212;is a perfectly reasonable strategy for the initial<br/>phase of cloud migration, especially when a company is facing some</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>kind of financial cliff, such as the need to sign a significant new lease or<br/>hardware contract if existing hardware is not shut down. However,<br/>companies that leave their cloud assets in this initial state are in for a<br/>rude shock. On a direct comparison basis, long-running servers in the<br/>cloud are significantly more expensive than their on-premises<br/>counterparts.<br/>The key to finding value in the cloud is understanding and optimizing<br/>the cloud pricing model. Rather than deploying a set of long-running<br/>servers capable of handling full peak load, use autoscaling to allow<br/>workloads to scale down to minimal infrastructure when loads are light,<br/>and up to massive clusters during peak times. To realize discounts<br/>through more ephemeral, less durable workloads, use reserved or spot<br/>instances, or use serverless functions in place of servers.<br/>We often think of this optimization as leading to lower costs, but we<br/>should also strive to <i>increase business value</i> by exploiting the dynamic<br/>nature of the cloud.  Data engineers can create new value in the cloud<br/>by accomplishing things that were impossible in their on-premises<br/>environment. For example, it is possible to quickly spin up massive<br/>compute clusters to run complex transformations at scales that were<br/>unaffordable for on-premises hardware.<br/><b>Data Gravity<br/></b>In addition to basic errors such as following on-premises operational<br/>practices in the cloud, data engineers need to watch out for other<br/>aspects of cloud pricing and incentives that frequently catch users<br/>unawares.<br/>Vendors want to lock you into their offerings. Getting data onto the<br/>platform is cheap or free on most cloud platforms, but getting data out<br/>can be extremely expensive. Be aware of data egress fees and their<br/>long-term impacts on your business before getting blindsided by a large<br/>bill. <i>Data gravity</i> is real: once data lands in a cloud, the cost to extract it<br/>and migrate processes can be very high.<br/></p>
<p>3</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><b>Hybrid Cloud<br/></b>As more established businesses migrate into the cloud, the hybrid cloud<br/>model is growing in importance. Virtually no business can migrate all of its<br/>workloads overnight. The hybrid cloud model assumes that an organization<br/>will indefinitely maintain some workloads outside the cloud.<br/>There are many reasons to consider a hybrid cloud model. Organizations<br/>may believe that they have achieved operational excellence in certain areas,<br/>such as their application stack and associated hardware. Thus, they may<br/>migrate only to specific workloads where they see immediate benefits in the<br/>cloud environment. For example, an on-premises Spark stack is migrated to<br/>ephemeral cloud clusters, reducing the operational burden of managing<br/>software and hardware for the data engineering team and allowing rapid<br/>scaling for large data jobs.<br/>This pattern of putting analytics in the cloud is beautiful because data flows<br/>primarily in one direction, minimizing data egress costs (Figure 4-3). That<br/>is, on-premises applications generate event data that can be pushed to the<br/>cloud essentially for free. The bulk of data remains in the cloud where it is<br/>analyzed, while smaller amounts of data are pushed back to on premises for<br/>deploying models to applications, reverse ETL, etc.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><i>Figure 4-3. A hybrid cloud data flow model minimizing egress costs<br/></i></p>
<p>A new generation of managed hybrid cloud service offerings also allows<br/>customers to locate cloud-managed servers in their data centers.  This gives<br/>users the ability to incorporate the best features in each cloud alongside on-<br/>premises infrastructure.<br/></p>
<p><b>Multicloud<br/></b><i>Multicloud</i> simply refers to deploying workloads to multiple public clouds.<br/>Companies may have several motivations for multicloud deployments. SaaS<br/>platforms often wish to offer services close to existing customer cloud<br/>workloads. Snowflake and Databricks provide their SaaS offerings across<br/>multiple clouds for this reason. This is especially critical for data-intensive<br/>applications, where network latency and bandwidth limitations hamper<br/>performance, and data egress costs can be prohibitive.<br/></p>
<p>4</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Another common motivation for employing a multicloud approach is to<br/>take advantage of the best services across several clouds. For example, a<br/>company might want to handle its Google Ads and Analytics data on<br/>Google Cloud and deploy Kubernetes through GKE. And the company<br/>might also adopt Azure specifically for Microsoft workloads. Also, the<br/>company may like AWS because it has several best-in-class services (e.g.,<br/>AWS Lambda) and enjoys huge mindshare, making it relatively easy to hire<br/>AWS-proficient engineers. Any mix of various cloud provider services is<br/>possible. Given the intense competition among the major cloud providers,<br/>expect them to offer more best-of-breed services, making multicloud more<br/>compelling.<br/>A multicloud methodology has several disadvantages. As we just<br/>mentioned, data egress costs and networking bottlenecks are critical. Going<br/>multicloud can introduce significant complexity. Companies must now<br/>manage a dizzying array of services across several clouds; cross-cloud<br/>integration and security present a considerable challenge; multicloud<br/>networking can be diabolically complicated.<br/>A new generation of &#8220;cloud of clouds&#8221; services aims to facilitate multicloud<br/>with reduced complexity by offering services across clouds and seamlessly<br/>replicating data between clouds or managing workloads on several clouds<br/>through a single pane of glass. To cite one example, a Snowflake account<br/>runs in a single cloud region, but customers can readily spin up other<br/>accounts in GCP, AWS, or Azure. Snowflake provides simple scheduled<br/>data replication between these various cloud accounts. The Snowflake<br/>interface is essentially the same in all of these accounts, removing the<br/>training burden of switching between cloud-native data services.<br/>The &#8220;cloud of clouds&#8221; space is evolving quickly; within a few years of this<br/>book&#8217;s publication, many more of these services will be available. Data<br/>engineers and architects would do well to maintain awareness of this<br/>quickly changing landscape.<br/></p>
<p><b>Decentralized: Blockchain and the Edge</b></p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Though not widely used now, it&#8217;s worth briefly mentioning a new trend that<br/>might become popular over the next decade: decentralized computing.<br/>Whereas today&#8217;s applications mainly run on premises and in the cloud, the<br/>rise of blockchain, Web 3.0, and edge computing may invert this paradigm.<br/>For the moment, decentralized platforms have proven extremely popular<br/>but have not had a significant impact in the data space; even so, keeping an<br/>eye on these platforms is worthwhile as you assess technology decisions.<br/></p>
<p><b>Our Advice<br/></b>From our perspective, we are still at the beginning of the transition to the<br/>cloud. Thus the evidence and arguments around workload placement and<br/>migration are in flux. The cloud itself is changing, with a shift from the<br/>IaaS model built around Amazon EC2 that drove the early growth of AWS,<br/>toward more managed service offerings such as AWS Glue, Google<br/>BigQuery, and Snowflake.<br/>We&#8217;ve also seen the emergence of new workload placement abstractions.<br/>On-premises services are becoming more cloud-like and abstracted. Hybrid<br/>cloud services allow customers to run fully managed services within their<br/>walls while facilitating tight integration between local and remote<br/>environments. Further, the &#8220;cloud of clouds&#8221; is beginning to take shape,<br/>fueled by third-party services and public cloud vendors.<br/><b>Choose technologies for the present, but look toward the future<br/></b>As we mentioned in &#8220;Today Versus the Future: Immutable Versus<br/>Transitory Technologies&#8221;, you need to keep one eye on the present while<br/>planning for unknowns. Right now is a tough time to plan workload<br/>placements and migrations. Because of the fast pace of competition and<br/>change in the cloud industry, the decision space will look very different in<br/>five to ten years. It is tempting to take into account every possible future<br/>architecture permutation.<br/>We believe that it is critical to avoid this endless trap of analysis. Instead,<br/>plan for the present. Choose the best technologies for your current needs</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>and concrete plans for the near future. Choose your deployment platform<br/>based on real business needs while focusing on simplicity and flexibility.<br/>In particular, don&#8217;t choose a complex multicloud or hybrid-cloud strategy<br/>unless there&#8217;s a compelling reason. Do you need to serve data near<br/>customers on multiple clouds? Do industry regulations require you to house<br/>certain data in your data centers? Do you have a compelling technology<br/>need for specific services on two different clouds? Choose a single-cloud<br/>deployment strategy if these scenarios don&#8217;t apply to you.<br/>On the other hand, have an escape plan. As we&#8217;ve emphasized before, every<br/>technology&#8212;even open source software&#8212;comes with some degree of lock-<br/>in. A single-cloud strategy has significant advantages of simplicity and<br/>integration but comes with significant lock-in attached. In this instance,<br/>we&#8217;re talking about mental flexibility, the flexibility to evaluate the current<br/>state of the world and imagine alternatives. Ideally, your escape plan will<br/>remain locked behind glass, but preparing this plan will help you to make<br/>better decisions in the present and give you a way out if things go wrong in<br/>the future.<br/></p>
<p><b>Cloud Repatriation Arguments<br/></b>As we wrote this book, Sarah Wang and Martin Casado published &#8220;The<br/>Cost of Cloud, A Trillion Dollar Paradox&#8221;, an article that generated<br/>significant sound and fury in the tech space. Readers widely interpreted the<br/>article as a call for the repatriation of cloud workloads to on-premises<br/>servers. They make a somewhat more subtle argument that companies<br/>should expend significant resources to control cloud spending and should<br/>consider repatriation as a possible option.<br/>We want to take a moment to dissect one part of their discussion. Wang and<br/>Casado cite Dropbox&#8217;s repatriation of significant workloads from AWS to<br/>Dropbox-owned servers as a case study for companies considering similar<br/>repatriation moves.<br/><b>You are not Dropbox, nor are you Cloudflare</b></p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>We believe that this case study is frequently used without appropriate<br/>context and is a compelling example of the <i>false equivalence</i> logical fallacy.<br/>Dropbox provides particular services where ownership of hardware and<br/>data centers can offer a competitive advantage. Companies should not rely<br/>excessively on Dropbox&#8217;s example when assessing cloud and on-premises<br/>deployment options.<br/>First, it&#8217;s important to understand that Dropbox stores enormous amounts of<br/>data. The company is tight-lipped about exactly how much data it hosts, but<br/>says it is many exabytes and continues to grow.<br/>Second, Dropbox handles a vast amount of network traffic. We know that<br/>its bandwidth consumption in 2017 was significant enough for the company<br/>to add &#8220;hundreds of gigabits of internet connectivity with transit providers<br/>(regional and global ISPs), and hundreds of new peering partners (where we<br/>exchange traffic directly rather than through an ISP).&#8221;  The data egress<br/>costs would be extremely high in a public cloud environment.<br/>Third, Dropbox is essentially a cloud storage vendor, but one with a highly<br/>specialized storage product that combines object and block storage<br/>characteristics. Dropbox&#8217;s core competence is a differential file-update<br/>system that can efficiently synchronize actively edited files among users<br/>while minimizing network and CPU usage. The product is not a good fit for<br/>object storage, block storage, or other standard cloud offerings. Dropbox<br/>has instead benefited from building a custom, highly integrated software<br/>and hardware stack.<br/>Fourth, while Dropbox moved its core product to its hardware, it continued<br/>building out other AWS workloads. This allows Dropbox to focus on<br/>building one highly tuned cloud service at an extraordinary scale rather than<br/>trying to replace multiple services. Dropbox can focus on its core<br/>competence in cloud storage and data synchronization while offloading<br/>software and hardware management in other areas, such as data analytics.<br/>Other frequently cited success stories that companies have built outside the<br/>cloud include Backblaze and Cloudflare, but these offer similar lessons.<br/>Backblaze began life as a personal cloud data backup product but has since<br/></p>
<p>5<br/></p>
<p>6<br/></p>
<p>7</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>begun to offer B2, an object storage service similar to Amazon S3.<br/>Backblaze currently stores over an exabyte of data. Cloudflare claims to<br/>provide services for over 25 million internet properties, with points of<br/>presence in over 200 cities and 51 terabits per second (Tbps) of total<br/>network capacity.<br/>Netflix offers yet another useful example. The company is famous for<br/>running its tech stack on AWS, but this is only partially true. Netflix does<br/>run video transcoding on AWS, accounting for roughly 70% of its compute<br/>needs in 2017.  Netflix also runs its application backend and data analytics<br/>on AWS. However, rather than using the AWS content distribution network,<br/>Netflix has built a custom CDN in collaboration with internet service<br/>providers, utilizing a highly specialized combination of software and<br/>hardware. For a company that consumes a substantial slice of all internet<br/>traffic,  building out this critical infrastructure allowed it to deliver high-<br/>quality video to a huge customer base cost-effectively.<br/>These case studies suggest that it makes sense for companies to manage<br/>their own hardware and network connections in particular circumstances.<br/>The biggest modern success stories of companies building and maintaining<br/>hardware involve extraordinary scale (exabytes of data, terabits per second<br/>of bandwidth, etc.) and limited use cases where companies can realize a<br/>competitive advantage by engineering highly integrated hardware and<br/>software stacks. In addition, all of these companies consume massive<br/>network bandwidth, suggesting that data egress charges would be a major<br/>cost if they chose to operate fully from a public cloud.<br/>Consider continuing to run workloads on premises or repatriating cloud<br/>workloads if you run a truly cloud-scale service. What is cloud scale? You<br/>might be at cloud scale if you are storing an exabyte of data or handling<br/>terabits per second of traffic <i>to and from the internet</i>. (Achieving a terabit<br/>per second of <i>internal</i> network traffic is fairly easy.) In addition, consider<br/>owning your servers if data egress costs are a major factor for your<br/>business. To give a concrete example of cloud scale workloads that could<br/>benefit from repatriation, Apple might gain a significant financial and<br/>performance advantage by migrating iCloud storage to its own servers.<br/></p>
<p>8<br/></p>
<p>9<br/></p>
<p>10</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><b>Build Versus Buy<br/></b>Build versus buy is an age-old debate in technology. The argument for<br/>building is that you have end-to-end control over the solution and are not at<br/>the mercy of a vendor or open source community. The argument supporting<br/>buying comes down to resource constraints and expertise; do you have the<br/>expertise to build a better solution than something already available? Either<br/>decision comes down to TCO, TOCO, and whether the solution provides a<br/>competitive advantage to your organization.<br/>If you&#8217;ve caught on to a theme in the book so far, it&#8217;s that we suggest<br/>investing in building and customizing <i>when doing so will provide a<br/>competitive advantage</i> for your business. Otherwise, stand on the shoulders<br/>of giants and <i>use what&#8217;s already available</i> in the market. Given the number<br/>of open source and paid services&#8212;both of which may have communities of<br/>volunteers or highly paid teams of amazing engineers&#8212;you&#8217;re foolish to<br/>build everything yourself.<br/>As we often ask, &#8220;When you need new tires for your car, do you get the raw<br/>materials, build the tires from scratch, and install them yourself?&#8221; Like most<br/>people, you&#8217;re probably buying tires and having someone install them. The<br/>same argument applies to build versus buy. We&#8217;ve seen teams that have<br/>built their databases from scratch. A simple open source RDBMS would<br/>have served their needs much better upon closer inspection. Imagine the<br/>amount of time and money invested in this homegrown database. Talk about<br/>low ROI for TCO and opportunity cost.<br/>This is where the distinction between the type A and type B data engineer<br/>comes in handy. As we pointed out earlier, type A and type B roles are often<br/>embodied in the same engineer, especially in a small organization.<br/>Whenever possible, lean toward type A behavior; avoid undifferentiated<br/>heavy lifting and embrace abstraction. Use open source frameworks, or if<br/>this is too much trouble, look at buying a suitable managed or proprietary<br/>solution. Plenty of great modular services are available to choose from in<br/>either case.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>The shifting reality of how companies adopt software is worth mentioning.<br/>Whereas in the past, IT used to make most of the software purchase and<br/>adoption decisions in a top-down manner, these days, the trend is for<br/>bottom-up software adoption in a company, driven by developers, data<br/>engineers, data scientists, and other technical roles. Technology adoption<br/>within companies is becoming an organic, continuous process.<br/>Let&#8217;s look at some options for open source and proprietary solutions.<br/></p>
<p><b>Open Source Software<br/></b><i>Open source software</i> (OSS) is a software distribution model in which<br/>software, and the underlying codebase, is made available for general use,<br/>typically under specific licensing terms. Often OSS is created and<br/>maintained by a distributed team of collaborators. OSS is free to use,<br/>change, and distribute most of the time, but with specific caveats. For<br/>example, many licenses require that the source code of open source&#8211;derived<br/>software be included when the software is distributed.<br/>The motivations for creating and maintaining OSS vary. Sometimes OSS is<br/>organic, springing from the mind of an individual or a small team that<br/>creates a novel solution and chooses to release it into the wild for public<br/>use. Other times, a company may make a specific tool or technology<br/>available to the public under an OSS license.<br/>OSS has two main flavors: community managed and commercial OSS.<br/><b>Community-managed OSS<br/></b>OSS projects succeed with a strong community and vibrant user base.<br/><i>Community-managed OSS</i> is a prevalent path for OSS projects. The<br/>community opens up high rates of innovations and contributions from<br/>developers worldwide with popular OSS projects.<br/>The following are factors to consider with a community-managed OSS<br/>project:<br/><i>Mindshare</i></p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Avoid adopting OSS projects that don&#8217;t have traction and popularity.<br/>Look at the number of GitHub stars, forks, and commit volume and<br/>recency. Another thing to pay attention to is community activity on<br/>related chat groups and forums. Does the project have a strong sense of<br/>community? A strong community creates a virtuous cycle of strong<br/>adoption. It also means that you&#8217;ll have an easier time getting technical<br/>assistance and finding talent qualified to work with the framework.<br/></p>
<p><i>Maturity<br/></i>How long has the project been around, how active is it today, and how<br/>usable are people finding it in production? A project&#8217;s maturity<br/>indicates that people find it useful and are willing to incorporate it into<br/>their production workflows.<br/></p>
<p><i>Troubleshooting<br/></i>How will you have to handle problems if they arise? Are you on your<br/>own to troubleshoot issues, or can the community help you solve your<br/>problem?<br/></p>
<p><i>Project management<br/></i>Look at Git issues and the way they&#8217;re addressed. Are they addressed<br/>quickly? If so, what&#8217;s the process to submit an issue and get it resolved?<br/></p>
<p><i>Team<br/></i>Is a company sponsoring the OSS project? Who are the core<br/>contributors?<br/></p>
<p><i>Developer relations and community management</i></p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>What is the project doing to encourage uptake and adoption? Is there a<br/>vibrant Slack community that provides encouragement and support?<br/></p>
<p><i>Contributing<br/></i>Does the project encourage and accept pull requests?<br/></p>
<p><i>Roadmap<br/></i>Is there a project roadmap? If so, is it clear and transparent?<br/></p>
<p><i>Self-hosting and maintenance<br/></i>Do you have the resources to host and maintain the OSS solution? If so,<br/>what&#8217;s the TCO and TOCO versus buying a managed service from the<br/>OSS vendor?<br/></p>
<p><i>Giving back to the community<br/></i>If you like the project and are actively using it, consider investing in it.<br/>You can contribute to the codebase, help fix issues, and give advice in<br/>the community forums and chats. If the project allows donations,<br/>consider making one. Many OSS projects are essentially community-<br/>service projects, and the maintainers often have full-time jobs in<br/>addition to helping with the OSS project. Sadly, it&#8217;s often a labor of love<br/>that doesn&#8217;t afford the maintainer a living wage. If you can afford to<br/>donate, please do so.<br/></p>
<p><b>Commercial OSS<br/></b>Sometimes OSS has some drawbacks. Namely, you have to host and<br/>maintain the solution in your environment. This may be trivial or extremely<br/>complicated and cumbersome, depending on the OSS application.<br/>Commercial vendors try to solve this management headache by hosting and</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>managing the OSS solution for you, typically as a cloud SaaS offering.<br/>Examples of such vendors include Databricks (Spark), Confluent (Kafka),<br/>DBT Labs (dbt), and there are many, many others.<br/>This model is called <i>commercial OSS</i> (COSS). Typically, a vendor will<br/>offer the &#8220;core&#8221; of the OSS for free while charging for enhancements,<br/>curated code distributions, or fully managed services.<br/>A vendor is often affiliated with the community OSS project. As an OSS<br/>project becomes more popular, the maintainers may create a separate<br/>business for a managed version of the OSS. This typically becomes a cloud<br/>SaaS platform built around a managed version of the open source code.<br/>This is a widespread trend: an OSS project becomes popular, an affiliated<br/>company raises truckloads of venture capital (VC) money to commercialize<br/>the OSS project, and the company scales as a fast-moving rocket ship.<br/>At this point, the data engineer has two options. You can continue using the<br/>community-managed OSS version, which you need to continue maintaining<br/>on your own (updates, server/container maintenance, pull requests for bug<br/>fixes, etc.). Or, you can pay the vendor and let it take care of the<br/>administrative management of the COSS product.<br/>The following are factors to consider with a commercial OSS project:<br/><i>Value<br/></i></p>
<p>Is the vendor offering a better value than if you managed the OSS<br/>technology yourself? Some vendors will add many bells and whistles to<br/>their managed offerings that aren&#8217;t available in the community OSS<br/>version. Are these additions compelling to you?<br/></p>
<p><i>Delivery model<br/></i>How do you access the service? Is the product available via download,<br/>API, or web/mobile UI? Be sure you can easily access the initial version<br/>and subsequent releases.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><i>Support<br/></i>Support cannot be understated, and it&#8217;s often opaque to the buyer. What<br/>is the support model for the product, and is there an extra cost for<br/>support? Frequently, vendors will sell support for an additional fee. Be<br/>sure you clearly understand the costs of obtaining support. Also,<br/>understand what is covered in support, and what is not covered.<br/>Anything that&#8217;s not covered by support will be your responsibility to<br/>own and manage.<br/></p>
<p><i>Releases and bug fixes<br/></i>Is the vendor transparent about the release schedule, improvements, and<br/>bug fixes? Are these updates easily available to you?<br/></p>
<p><i>Sales cycle and pricing<br/></i>Often a vendor will offer on-demand pricing, especially for a SaaS<br/>product, and offer you a discount if you commit to an extended<br/>agreement. Be sure to understand the trade-offs of paying as you go<br/>versus paying up front. Is it worth paying a lump sum, or is your money<br/>better spent elsewhere?<br/></p>
<p><i>Company finances<br/></i>Is the company viable? If the company has raised VC funds, you can<br/>check their funding on sites like Crunchbase. How much runway does<br/>the company have, and will it still be in business in a couple of years?<br/></p>
<p><i>Logos versus revenue</i></p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Is the company focused on growing the number of customers (logos), or<br/>is it trying to grow revenue? You may be surprised by the number of<br/>companies primarily concerned with growing their customer count,<br/>GitHub stars, or Slack channel membership without the revenue to<br/>establish sound finances.<br/></p>
<p><i>Community support<br/></i>Is the company truly supporting the community version of the OSS<br/>project? How much is the company contributing to the community OSS<br/>codebase? Controversies have arisen with certain vendors co-opting<br/>OSS projects and subsequently providing little value back to the<br/>community. How likely will the product remain viable as a community-<br/>supported open source if the company shuts down?<br/></p>
<p>Note also that clouds offer their own managed open source products. If a<br/>cloud vendor sees traction with a particular product or project, expect that<br/>vendor to offer its version. This can range from simple examples (open<br/>source Linux offered on VMs) to extremely complex managed services<br/>(fully managed Kafka). The motivation for these offerings is simple: clouds<br/>make their money through consumption. More offerings in a cloud<br/>ecosystem mean a greater chance of &#8220;stickiness&#8221; and increased customer<br/>spending.<br/></p>
<p><b>Proprietary Walled Gardens<br/></b>While OSS is ubiquitous, a big market also exists for non-OSS<br/>technologies. Some of the biggest companies in the data industry sell closed<br/>source products. Let&#8217;s look at two major types of <i>proprietary walled<br/>gardens</i>, independent companies and cloud-platform offerings.<br/><b>Independent offerings</b></p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>The data-tool landscape has seen exponential growth over the last several<br/>years. Every day, new independent offerings arise for data tools. With the<br/>ability to raise funds from VCs flush with capital, these data companies can<br/>scale and hire great engineering, sales, and marketing teams. This presents a<br/>situation where users have some great product choices in the marketplace<br/>while having to wade through endless sales and marketing clutter.<br/>Often a company selling a data tool will not release it as OSS, instead<br/>offering a proprietary solution. Although you won&#8217;t have the transparency<br/>of a pure OSS solution, a proprietary independent solution can work quite<br/>well, especially as a fully managed service in the cloud.<br/>The following are things to consider with an independent offering:<br/><i>Interoperability<br/></i></p>
<p>Make sure that the tool interoperates with other tools you&#8217;ve chosen<br/>(OSS, other independents, cloud offerings, etc.) Interoperability is key,<br/>so make sure you can try it before you buy.<br/></p>
<p><i>Mindshare and market share<br/></i>Is the solution popular? Does it command a presence in the<br/>marketplace? Does it enjoy positive customer reviews?<br/></p>
<p><i>Documentation and support<br/></i>Problems and questions will inevitably arise. Is it clear how to solve<br/>your problem, either through documentation or support?<br/></p>
<p><i>Pricing<br/></i>Is the pricing understandable? Map out low-, medium-, and high-<br/>probability usage scenarios, with respective costs. Are you able to<br/>negotiate a contract, along with a discount? Is it worth it? How much<br/>flexibility do you lose if you sign a contract, both in negotiation and the</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>ability to try new options? Are you able to obtain contractual<br/>commitments on future pricing?<br/></p>
<p><i>Longevity<br/></i>Will the company survive long enough for you to get value from its<br/>product? If the company has raised money, search around for its funding<br/>situation. Look at user reviews. Ask friends and post questions on social<br/>networks about other users&#8217; experiences with the product. Make sure<br/>you know what you&#8217;re getting into.<br/></p>
<p><b>Cloud platform proprietary service offerings<br/></b>Cloud vendors develop and sell their proprietary services for storage,<br/>databases, and more. Many of these solutions are internal tools used by<br/>respective sibling companies. For example, Amazon created the database<br/>DynamoDB to overcome the limitations of traditional relational databases<br/>and handle the large amounts of user and order data as Amazon.com grew<br/>into a behemoth. Amazon later offered the DynamoDB service solely on<br/>AWS; it&#8217;s now a top-rated product used by companies of all sizes and<br/>maturity levels. Cloud vendors will often bundle their products to work well<br/>together. Each cloud can create stickiness with its user base by creating a<br/>strong integrated ecosystem.<br/>The following are factors to consider with a proprietary cloud offering:<br/><i>Performance versus price comparisons<br/></i></p>
<p>Is the cloud offering substantially better than an independent or OSS<br/>version? What&#8217;s the TCO of choosing a cloud&#8217;s offering?<br/></p>
<p><i>Purchase considerations<br/></i>On-demand pricing can be expensive. Can you lower your cost by<br/>purchasing reserved capacity or entering into a long-term commitment</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>agreement?<br/></p>
<p><b>Our Advice<br/></b>Build versus buy comes back to knowing your competitive advantage and<br/>where it makes sense to invest resources toward customization. In general,<br/>we favor OSS and COSS by default, which frees you to focus on improving<br/>those areas where these options are insufficient. Focus on a few areas where<br/>building something will add significant value or reduce friction<br/>substantially.<br/>Don&#8217;t treat internal operational overhead as a sunk cost. There&#8217;s excellent<br/>value in upskilling your existing data team to build sophisticated systems on<br/>managed platforms rather than babysitting on-premises servers. In addition,<br/>think about how a company makes money, especially its sales and customer<br/>experience teams, which will generally indicate how you&#8217;re treated during<br/>the sales cycle and when you&#8217;re a paying customer.<br/>Finally, who is responsible for the budget at your company? How does this<br/>person decide the projects and technologies that get funded? Before making<br/>the business case for COSS or managed services, does it make sense to try<br/>to use OSS first? The last thing you want is for your technology choice to<br/>be stuck in limbo while waiting for budget approval. As the old saying<br/>goes, <i>time kills deals</i>. In your case, more time spent in limbo means a<br/>higher likelihood your budget approval will die. Know beforehand who<br/>controls the budget and what will successfully get approved.<br/></p>
<p><b>Monolith Versus Modular<br/></b>Monoliths versus modular systems is another longtime debate in the<br/>software architecture space. Monolithic systems are self-contained, often<br/>performing multiple functions under a single system. The monolith camp<br/>favors the simplicity of having everything in one place. It&#8217;s easier to reason<br/>about a single entity, and you can move faster because there are fewer<br/>moving parts. The <i>modular</i> camp leans toward decoupled, best-of-breed</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>technologies performing tasks at which they are uniquely great. Especially<br/>given the rate of change in products in the data world, the argument is you<br/>should aim for interoperability among an ever-changing array of solutions.<br/>What approach should you take in your data engineering stack? Let&#8217;s<br/>explore the trade-offs.<br/></p>
<p><b>Monolith<br/></b>The <i>monolith</i> (Figure 4-4) has been a technology mainstay for decades. The<br/>old days of waterfall meant that software releases were huge, tightly<br/>coupled, and moved at a slow cadence. Large teams worked together to<br/>deliver a single working codebase. Monolithic data systems continue to this<br/>day, with older software vendors such as Informatica and open source<br/>frameworks such as Spark.<br/></p>
<p><i>Figure 4-4. The monolith tightly couples its services<br/></i></p>
<p>The pros of the monolith are it&#8217;s easy to reason about, and it requires a<br/>lower cognitive burden and context switching since everything is self-<br/>contained. Instead of dealing with dozens of technologies, you deal with<br/>&#8220;one&#8221; technology and typically one principal programming language.<br/>Monoliths are an excellent option if you want simplicity in reasoning about<br/>your architecture and processes.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Of course, the monolith has cons. For one thing, monoliths are brittle.<br/>Because of the vast number of moving parts, updates and releases take<br/>longer and tend to bake in &#8220;the kitchen sink.&#8221; If the system has a bug&#8212;<br/>hopefully, the software&#8217;s been thoroughly tested before release!&#8212;it can<br/>harm the entire system.<br/>User-induced problems also happen with monoliths. For example, we saw a<br/>monolithic ETL pipeline that took 48 hours to run. If anything broke<br/>anywhere in the pipeline, the entire process had to restart. Meanwhile,<br/>anxious business users were waiting for their reports, which were already<br/>two days late by default. Breakages were common enough that the<br/>monolithic system was eventually thrown out.<br/>Multitenancy in a monolithic system can also be a significant problem. It<br/>can be challenging to isolate the workloads of multiple users. In an on-prem<br/>data warehouse, one user-defined function might consume enough CPU to<br/>slow the system for other users. Conflicts between dependencies and<br/>resource contention are frequent sources of headaches.<br/>Another con of monoliths is that switching to a new system will be painful<br/>if the vendor or open source project dies. Because all of your processes are<br/>contained in the monolith, extracting yourself out of that system, and onto a<br/>new platform, will be costly in both time and money.<br/></p>
<p><b>Modularity<br/></b><i>Modularity</i> (Figure 4-5) is an old concept in software engineering, but<br/>modular distributed systems truly came into vogue with the rise of<br/>microservices. Instead of relying on a massive monolith to handle your<br/>needs, why not break apart systems and processes into their self-contained<br/>areas of concern? Microservices can communicate via APIs, allowing<br/>developers to focus on their domains while making their applications<br/>accessible to other microservices. This is the trend in software engineering<br/>and is increasingly seen in modern data systems.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><i>Figure 4-5. With modularity, each service is decoupled from another<br/></i></p>
<p>Major tech companies have been key drivers in the microservices<br/>movement. The famous Bezos API mandate decreases coupling between<br/>applications, allowing refactoring and decomposition. Bezos also imposed<br/>the two-pizza rule (no team should be so large that two pizzas can&#8217;t feed the<br/>whole group). Effectively, this means that a team will have at most five<br/>members. This cap also limits the complexity of a team&#8217;s domain of<br/>responsibility&#8212;in particular, the codebase that it can manage. Whereas an<br/>extensive monolithic application might entail a group of one hundred<br/>people, dividing developers into small groups of five requires that this<br/>application be broken into small, manageable, loosely coupled pieces.<br/>In a modular microservice environment, components are swappable, and it&#8217;s<br/>possible to create a <i>polyglot</i> (multiprogramming language) application; a<br/>Java service can replace a service written in Python. Service customers<br/>need worry only about the technical specifications of the service API, not<br/>behind-the-scenes details of implementation.<br/>Data-processing technologies have shifted toward a modular model by<br/>providing strong support for interoperability. Data is stored in object storage<br/>in a standard format such as Parquet in data lakes and lakehouses. Any<br/>processing tool that supports the format can read the data and write<br/>processed results back into the lake for processing by another tool. Cloud<br/>data warehouses support interoperation with object storage through<br/>import/export using standard formats and external tables&#8212;i.e., queries run<br/>directly on data in a data lake.<br/>New technologies arrive on the scene at a dizzying rate in today&#8217;s data<br/>ecosystem, and most get stale and outmoded quickly. Rinse and repeat. The<br/>ability to swap out tools as technology changes is invaluable. We view data<br/>modularity as a more powerful paradigm than monolithic data engineering.<br/>Modularity allows engineers to choose the best technology for each job or<br/>step along the pipeline.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>The cons of modularity are that there&#8217;s more to reason about. Instead of<br/>handling a single system of concern, now you potentially have countless<br/>systems to understand and operate. Interoperability is a potential headache;<br/>hopefully, these systems all play nicely together.<br/>This very problem led us to break out orchestration as a separate<br/>undercurrent instead of placing it under data management. Orchestration is<br/>also important for monolithic data architectures; witness the success of<br/>tools like BMC Software&#8217;s Control-M in the traditional data warehousing<br/>space. But orchestrating five or ten tools is dramatically more complex than<br/>orchestrating one. Orchestration becomes the glue that binds data stack<br/>modules together.<br/></p>
<p><b>The Distributed Monolith Pattern<br/></b>The <i>distributed monolith pattern</i> is a distributed architecture that still<br/>suffers from many of the limitations of monolithic architecture. The basic<br/>idea is that one runs a distributed system with different services to perform<br/>different tasks. Still, services and nodes share a common set of<br/>dependencies or a common codebase.<br/>One standard example is a traditional Hadoop cluster. A Hadoop cluster can<br/>simultaneously host several frameworks, such as Hive, Pig, or Spark. The<br/>cluster also has many internal dependencies. In addition, the cluster runs<br/>core Hadoop components: Hadoop common libraries, HDFS, YARN, and<br/>Java. In practice, a cluster often has one version of each component<br/>installed.<br/>A standard on-prem Hadoop system entails managing a common<br/>environment that works for all users and all jobs. Managing upgrades and<br/>installations is a significant challenge. Forcing jobs to upgrade<br/>dependencies risks breaking them; maintaining two versions of a<br/>framework entails extra complexity.<br/>Some modern Python-based orchestration technologies also suffer from this<br/>problem. While they utilize a highly decoupled and asynchronous<br/>architecture, every service runs the same codebase with the same</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>dependencies. Any executor can execute any task, so a client library for a<br/>single task run in one DAG must be installed on the whole cluster.<br/>Orchestrating many tools entails installing client libraries for a host of<br/>APIs. Dependency conflicts are a constant problem.<br/>One solution to the problems of the distributed monolith is ephemeral<br/>infrastructure in a cloud setting. Each job gets its own temporary server or<br/>cluster installed with dependencies. Each cluster remains highly monolithic,<br/>but separating jobs dramatically reduces conflicts. For example, this pattern<br/>is now quite common for Spark with services like Amazon EMR and<br/>Google Cloud Dataproc.<br/>A second solution is to properly decompose the distributed monolith into<br/>multiple software environments using containers. We have more to say on<br/>containers in &#8220;Serverless Versus Servers&#8221;.<br/></p>
<p><b>Our Advice<br/></b>While monoliths are attractive because of ease of understanding and<br/>reduced complexity, this comes at a high cost. The cost is the potential loss<br/>of flexibility, opportunity cost, and high-friction development cycles.<br/>Here are some things to consider when evaluating monoliths versus<br/>modular options:<br/><i>Interoperability<br/></i></p>
<p>Architect for sharing and interoperability.<br/><i>Avoiding the &#8220;bear trap&#8221;<br/></i></p>
<p>Something that is easy to get into might be painful or impossible to<br/>escape.<br/></p>
<p><i>Flexibility<br/></i>Things are moving so fast in the data space right now. Committing to a<br/>monolith reduces flexibility and reversible decisions.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><b>Serverless Versus Servers<br/></b>A big trend for cloud providers is <i>serverless</i>, allowing developers and data<br/>engineers to run applications without managing servers behind the scenes.<br/>Serverless provides a quick time to value for the right use cases. For other<br/>cases, it might not be a good fit. Let&#8217;s look at how to evaluate whether<br/>serverless is right for you.<br/></p>
<p><b>Serverless<br/></b>Though serverless has been around for quite some time, the serverless trend<br/>kicked off in full force with AWS Lambda in 2014. With the promise of<br/>executing small chunks of code on an as-needed basis without having to<br/>manage a server, serverless exploded in popularity. The main reasons for its<br/>popularity are cost and convenience. Instead of paying the cost of a server,<br/>why not just pay when your code is evoked?<br/>Serverless has many flavors. Though function as a service (FaaS) is wildly<br/>popular, serverless systems predate the advent of AWS Lambda. Google<br/>Cloud&#8217;s BigQuery is serverless in that data engineers don&#8217;t need to manage<br/>backend infrastructure, and the system scales to zero and scales up<br/>automatically to handle large queries. Just load data into the system and<br/>start querying. You pay for the amount of data your query consumes and a<br/>small cost to store your data. This payment model&#8212;paying for consumption<br/>and storage&#8212;is becoming more prevalent.<br/>When does serverless make sense? As with many other cloud services, it<br/>depends; and data engineers would do well to understand the details of<br/>cloud pricing to predict when serverless deployments will become<br/>expensive. Looking specifically at the case of AWS Lambda, various<br/>engineers have found hacks to run batch workloads at meager costs.  On<br/>the other hand, serverless functions suffer from an inherent overhead<br/>inefficiency. Handling one event per function call at a high event rate can be<br/>catastrophically expensive.<br/></p>
<p>11</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>As with other areas of ops, it&#8217;s critical to monitor and model. <i>Monitor</i> to<br/>determine cost per event in a real-world environment, and <i>model</i> using this<br/>cost per event to determine overall costs as event rates grow. Modeling<br/>should also include worst-case scenarios&#8212;what happens if my site gets hit<br/>by a bot swarm or DDoS attack?<br/></p>
<p><b>Containers<br/></b>In conjunction with serverless and microservices, <i>containers</i> are one of the<br/>most powerful trending operational technologies as of this writing.<br/>Containers play a role in both serverless and microservices.<br/>Containers are often referred to as <i>lightweight virtual machines</i>. Whereas a<br/>traditional VM wraps up an entire operating system, a container packages<br/>an isolated user space (such as a filesystem and a few processes); many<br/>such containers can coexist on a single host operating system. This provides<br/>some of the principal benefits of virtualization (i.e., dependency and code<br/>isolation) without the overhead of carrying around an entire operating<br/>system kernel.<br/>A single hardware node can host numerous containers with fine-grained<br/>resource allocations. At the time of this writing, containers continue to grow<br/>in popularity, along with Kubernetes, a container management system.<br/>Serverless environments typically run on containers behind the scenes.<br/>Indeed, Kubernetes is a kind of serverless environment because it allows<br/>developers and ops teams to deploy microservices without worrying about<br/>the details of the machines where they are deployed.<br/>Containers provide a partial solution to problems of the distributed<br/>monolith mentioned earlier in this chapter. For example, Hadoop now<br/>supports containers, allowing each job to have its own isolated<br/>dependencies.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><b>WARNING<br/></b>Container clusters do not provide the same security and isolation that full VMs offer.<br/><i>Container escape</i>&#8212;broadly, a class of exploits whereby code in a container gains<br/>privileges outside the container at the OS level&#8212;is common enough to be considered a<br/>risk for multitenancy. While Amazon EC2 is a truly multitenant environment with VMs<br/>from many customers hosted on the same hardware, a Kubernetes cluster should host<br/>code only within an environment of mutual trust (e.g., inside the walls of a single<br/>company). In addition, code review processes and vulnerability scanning are critical to<br/>ensure that a developer doesn&#8217;t introduce a security hole.<br/></p>
<p>Various flavors of container platforms add additional serverless features.<br/>Containerized function platforms run containers as ephemeral units<br/>triggered by events rather than persistent services.  This gives users the<br/>simplicity of AWS Lambda with the full flexibility of a container<br/>environment instead of the highly restrictive Lambda runtime. And services<br/>such as AWS Fargate and Google App Engine run containers without<br/>managing a compute cluster required for Kubernetes. These services also<br/>fully isolate containers, preventing the security issues associated with<br/>multitenancy.<br/>Abstraction will continue working its way across the data stack. Consider<br/>the impact of Kubernetes on cluster management. While you can manage<br/>your Kubernetes cluster&#8212;and many engineering teams do so&#8212;even<br/>Kubernetes is widely available as a managed service.<br/></p>
<p><b>When Infrastructure Makes Sense<br/></b>Why would you want to run your own servers instead of using serverless?<br/>There are a few reasons. Cost is a big factor. Serverless makes less sense<br/>when the usage and cost exceed the ongoing cost of running and<br/>maintaining a server (Figure 4-6). However, at a certain scale, the economic<br/>benefits of serverless may diminish, and running servers becomes more<br/>attractive.<br/></p>
<p>12</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><i>Figure 4-6. Cost of serverless versus utilizing a server<br/></i></p>
<p>Customization, power, and control are other major reasons to favor servers<br/>over serverless. Some serverless frameworks can be underpowered or<br/>limited for certain use cases. Here are some things to consider when using<br/>servers, particularly in the cloud, where server resources are ephemeral:<br/><i>Expect servers to fail.<br/></i></p>
<p>Server failure will happen. Avoid using a &#8220;special snowflake&#8221; server<br/>that is overly customized and brittle, as this introduces a glaring<br/>vulnerability in your architecture. Instead, treat servers as ephemeral<br/>resources that you can create as needed and then delete. If your<br/>application requires specific code to be installed on the server, use a<br/>boot script or build an image. Deploy code to the server through a<br/>CI/CD pipeline.<br/></p>
<p><i>Use clusters and autoscaling.</i></p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Take advantage of the cloud&#8217;s ability to grow and shrink compute<br/>resources on demand. As your application increases its usage, cluster<br/>your application servers, and use autoscaling capabilities to<br/>automatically horizontally scale your application as demand grows.<br/></p>
<p><i>Treat your infrastructure as code.<br/></i>Automation doesn&#8217;t apply to just servers and should extend to your<br/>infrastructure whenever possible. Deploy your infrastructure (servers or<br/>otherwise) using deployment managers such as Terraform, AWS<br/>CloudFormation, and Google Cloud Deployment Manager.<br/></p>
<p><i>Use containers.<br/></i>For more sophisticated or heavy-duty workloads with complex installed<br/>dependencies, consider using containers on either a single server or<br/>Kubernetes.<br/></p>
<p><b>Our Advice<br/></b>Here are some key considerations to help you determine whether serverless<br/>is right for you:<br/><i>Workload size and complexity<br/></i></p>
<p>Serverless works best for simple, discrete tasks and workloads. It&#8217;s not<br/>as suitable if you have many moving parts or require a lot of compute or<br/>memory horsepower. In that case, consider using containers and a<br/>container workflow orchestration framework like Kubernetes.<br/></p>
<p><i>Execution frequency and duration</i></p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>How many requests per second will your serverless application process?<br/>How long will each request take to process? Cloud serverless platforms<br/>have limits on execution frequency, concurrency, and duration. If your<br/>application can&#8217;t function neatly within these limits, it is time to<br/>consider a container-oriented approach.<br/></p>
<p><i>Requests and networking<br/></i>Serverless platforms often utilize some form of simplified networking<br/>and don&#8217;t support all cloud virtual networking features, such as VPCs<br/>and firewalls.<br/></p>
<p><i>Language<br/></i>What language do you typically use? If it&#8217;s not one of the officially<br/>supported languages supported by the serverless platform, you should<br/>consider containers instead.<br/></p>
<p><i>Runtime limitations<br/></i>Serverless platforms don&#8217;t give you complete operating system<br/>abstractions. Instead, you&#8217;re limited to a specific runtime image.<br/></p>
<p><i>Cost<br/></i>Serverless functions are incredibly convenient but potentially<br/>expensive. When your serverless function processes only a few events,<br/>your costs are low; costs rise rapidly as the event count increases. This<br/>scenario is a frequent source of surprise cloud bills.<br/></p>
<p>In the end, abstraction tends to win. We suggest looking at using serverless<br/>first and then servers&#8212;with containers and orchestration if possible&#8212;once</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>you&#8217;ve outgrown serverless options.<br/></p>
<p><b>Optimization, Performance, and the<br/>Benchmark Wars<br/></b>Imagine that you are a billionaire shopping for new transportation. You&#8217;ve<br/>narrowed your choice to two options:<br/></p>
<p>787 Business Jet<br/>Range: 9,945 nautical miles (with 25 passengers)<br/>Maximum speed: 0.90 Mach<br/>Cruise speed: 0.85 Mach<br/>Fuel capacity: 101,323 kilograms<br/>Maximum takeoff weight: 227,930 kilograms<br/>Maximum thrust: 128,000 pounds<br/></p>
<p>Tesla Model S Plaid<br/>Range: 560 kilometers<br/>Maximum speed: 322 kilometers/hour<br/>0&#8211;-100 kilometers/hour: 2.1 seconds<br/>Battery capacity: 100 kilowatt hours<br/>Nurburgring lap time: 7 minutes, 30.9 seconds<br/>Horsepower: 1020<br/>Torque: 1050 lb-ft<br/></p>
<p>Which of these options offers better performance? You don&#8217;t have to know<br/>much about cars or aircraft to recognize that this is an idiotic comparison.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>One option is a wide-body private jet designed for intercontinental<br/>operation, while the other is an electric supercar.<br/>We see such apples-to-oranges comparisons made all the time in the<br/>database space. Benchmarks either compare databases that are optimized<br/>for completely different use cases, or use test scenarios that bear no<br/>resemblance to real-world needs.<br/>Recently, we saw a new round of benchmark wars flare up among major<br/>vendors in the data space. We applaud benchmarks and are glad to see many<br/>database vendors finally dropping DeWitt clauses from their customer<br/>contracts.  Even so, let the buyer beware: the data space is full of<br/>nonsensical benchmarks.  Here are a few common tricks used to place a<br/>thumb on the benchmark scale.<br/></p>
<p><b>Big Data...for the 1990s<br/></b>Products that claim to support &#8220;big data&#8221; at petabyte scale will often use<br/>benchmark datasets small enough to easily fit in the storage on your<br/>smartphone. For systems that rely on caching layers to deliver performance,<br/>test datasets fully reside in solid-state drive (SSD) or memory, and<br/>benchmarks can show ultra-high performance by repeatedly querying the<br/>same data. A small test dataset also minimizes RAM and SSD costs when<br/>comparing pricing.<br/>To benchmark for real-world use cases, you must simulate anticipated real-<br/>world data and query size. Evaluate query performance and resource costs<br/>based on a detailed evaluation of your needs.<br/></p>
<p><b>Nonsensical Cost Comparisons<br/></b>Nonsensical cost comparisons are a standard trick when analyzing a<br/>price/performance or TCO. For instance, many MPP systems can&#8217;t be<br/>readily created and deleted even when they reside in a cloud environment;<br/>these systems run for years on end once they&#8217;ve been configured. Other<br/>databases support a dynamic compute model and charge either per query or<br/>per second of use. Comparing ephemeral and non-ephemeral systems on a<br/></p>
<p>13<br/>14</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>cost-per-second basis is nonsensical, but we see this all the time in<br/>benchmarks.<br/></p>
<p><b>Asymmetric Optimization<br/></b>The deceit of asymmetric optimization appears in many guises, but here&#8217;s<br/>one example. Often a vendor will compare a row-based MPP system<br/>against a columnar database by using a benchmark that runs complex join<br/>queries on highly normalized data. The normalized data model is optimal<br/>for the row-based system, but the columnar system would realize its full<br/>potential only with some schema changes. To make matters worse, vendors<br/>juice their systems with an extra shot of join optimization (e.g., pre-<br/>indexing joins) without applying comparable tuning in the competing<br/>database (e.g., putting joins in a materialized view).<br/></p>
<p><b>Caveat Emptor<br/></b>As with all things in data technology, let the buyer beware. Do your<br/>homework before blindly relying on vendor benchmarks to evaluate and<br/>choose technology.<br/></p>
<p><b>Undercurrents and Their Impacts on<br/>Choosing Technologies<br/></b>As seen in this chapter, a data engineer has a lot to consider when<br/>evaluating technologies. Whatever technology you choose, be sure to<br/>understand how it supports the undercurrents of the data engineering<br/>lifecycle. Let&#8217;s briefly review them again.<br/></p>
<p><b>Data Management<br/></b>Data management is a broad area, and concerning technologies, it isn&#8217;t<br/>always apparent whether a technology adopts data management as a<br/>principal concern. For example, behind the scenes, a third-party vendor may</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>use data management best practices&#8212;regulatory compliance, security,<br/>privacy, data quality, and governance&#8212;but hide these details behind a<br/>limited UI layer. In this case, while evaluating the product, it helps to ask<br/>the company about its data management practices. Here are some sample<br/>questions you should ask:<br/></p>
<p>How are you protecting data against breaches, both from the outside<br/>and within?<br/>What is your product&#8217;s compliance with GDPR, CCPA, and other data<br/>privacy regulations?<br/>Do you allow me to host my data to comply with these regulations?<br/>How do you ensure data quality and that I&#8217;m viewing the correct data<br/>in your solution?<br/></p>
<p>There are many other questions to ask, and these are just a few of the ways<br/>to think about data management as it relates to choosing the right<br/>technologies. These same questions should also apply to the OSS solutions<br/>you&#8217;re considering.<br/></p>
<p><b>DataOps<br/></b>Problems will happen. They just will. A server or database may die, a<br/>cloud&#8217;s region may have an outage, you might deploy buggy code, bad data<br/>might be introduced into your data warehouse, and other unforeseen<br/>problems may occur.<br/>When evaluating a new technology, how much control do you have over<br/>deploying new code, how will you be alerted if there&#8217;s a problem, and how<br/>will you respond when there&#8217;s a problem? The answer largely depends on<br/>the type of technology you&#8217;re considering. If the technology is OSS, you&#8217;re<br/>likely responsible for setting up monitoring, hosting, and code deployment.<br/>How will you handle issues? What&#8217;s your incident response?<br/>Much of the operations are outside your control if you&#8217;re using a managed<br/>offering. Consider the vendor&#8217;s SLA, the way they alert you to issues, and</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>whether they&#8217;re transparent about how they&#8217;re addressing the case,<br/>including providing an ETA to a fix.<br/></p>
<p><b>Data Architecture<br/></b>As discussed in Chapter 3, good data architecture means assessing trade-<br/>offs and choosing the best tools for the job while keeping your decisions<br/>reversible. With the data landscape morphing at warp speed, the <i>best tool<br/></i>for the job is a moving target. The main goals are to avoid unnecessary<br/>lock-in, ensure interoperability across the data stack, and produce high ROI.<br/>Choose your technologies accordingly.<br/></p>
<p><b>Orchestration Example: Airflow<br/></b>Throughout most of this chapter, we have actively avoided discussing any<br/>particular technology too extensively. We make an exception for<br/>orchestration because the space is currently dominated by one open source<br/>technology, Apache Airflow.<br/>Maxime Beauchemin kicked off the Airflow project at Airbnb in 2014.<br/>Airflow was developed from the beginning as a noncommercial open<br/>source project. The framework quickly grew significant mindshare outside<br/>Airbnb, becoming an Apache Incubator project in 2016 and a full Apache-<br/>sponsored project in 2019.<br/>Airflow enjoys many advantages, largely because of its dominant position<br/>in the open source marketplace. First, the Airflow open source project is<br/>extremely active, with a high rate of commits and a quick response time for<br/>bugs and security issues, and the project recently released Airflow 2, a<br/>major refactor of the codebase. Second, Airflow enjoys massive mindshare.<br/>Airflow has a vibrant, active community on many communications<br/>platforms, including Slack, Stack Overflow, and GitHub. Users can easily<br/>find answers to questions and problems. Third, Airflow is available<br/>commercially as a managed service or software distribution through many<br/>vendors, including GCP, AWS, and Astronomer.io.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Airflow also has some downsides. Airflow relies on a few core nonscalable<br/>components (the scheduler and backend database) that can become<br/>bottlenecks for performance, scale, and reliability; the scalable parts of<br/>Airflow still follow a distributed monolith pattern. (See &#8220;Monolith Versus<br/>Modular&#8221;.) Finally, Airflow lacks support for many data-native constructs,<br/>such as schema management, lineage, and cataloging; and it is challenging<br/>to develop and test Airflow workflows.<br/>We do not attempt an exhaustive discussion of Airflow alternatives here but<br/>just mention a couple of the key orchestration contenders at the time of<br/>writing. Prefect and Dagster aim to solve some of the problems discussed<br/>previously by rethinking components of the Airflow architecture. Will there<br/>be other orchestration frameworks and technologies not discussed here?<br/>Plan on it.<br/>We highly recommend that anyone choosing an orchestration technology<br/>study the options discussed here. They should also acquaint themselves<br/>with activity in the space, as new developments will certainly occur by the<br/>time you read this.<br/></p>
<p><b>Software Engineering<br/></b>As a data engineer, you should strive for simplification and abstraction<br/>across the data stack. Buy or use prebuilt open source solutions whenever<br/>possible. Eliminating undifferentiated heavy lifting should be your big goal.<br/>Focus your resources&#8212;custom coding and tooling&#8212;on areas that give you<br/>a solid competitive advantage. For example, is hand-coding a database<br/>connection between your production database and your cloud data<br/>warehouse a competitive advantage for you? Probably not. This is very<br/>much a solved problem. Pick an off-the-shelf solution (open source or<br/>managed SaaS) instead. The world doesn&#8217;t need the millionth +1 database-<br/>to-cloud data warehouse connector.<br/>On the other hand, why do customers buy from you? Your business likely<br/>has something special about the way it does things. Maybe it&#8217;s a particular<br/>algorithm that powers your fintech platform. By abstracting away a lot of</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>the redundant workflows and processes, you can continue chipping away,<br/>refining, and customizing the things that move the needle for the business.<br/></p>
<p><b>Conclusion<br/></b>Choosing the right technologies is no easy task, especially when new<br/>technologies and patterns emerge daily. Today is possibly the most<br/>confusing time in history for evaluating and selecting technologies.<br/>Choosing technologies is a balance of use case, cost, build versus buy, and<br/>modularization. Always approach technology the same way as architecture:<br/>assess trade-offs and aim for reversible decisions.<br/></p>
<p>1  For more details, see &#8220;Total Opportunity Cost of Ownership&#8221; by Joseph Reis in <i>97 Things<br/>Every Data Engineer Should Know</i> (O&#8217;Reilly).<br/></p>
<p>2  J.R. Storment and Mike Fuller, <i>Cloud FinOps</i> (Sebastopol, CA: O&#8217;Reilly, 2019), 6,<br/><i>https://oreil.ly/RvRvX</i>.<br/></p>
<p>3  This is a major point of emphasis in Storment and Fuller, <i>Cloud FinOps</i>.<br/>4  Examples include Google Cloud Anthos and AWS Outposts.<br/>5  Raghav Bhargava, &#8220;Evolution of Dropbox&#8217;s Edge Network,&#8221; Dropbox.Tech, June 19, 2017,<br/></p>
<p><i>https://oreil.ly/RAwPf</i>.<br/>6  Akhil Gupta, &#8220;Scaling to Exabytes and Beyond,&#8221; Dropbox.Tech, March 14, 2016,<br/></p>
<p><i>https://oreil.ly/5XPKv</i>.<br/>7  &#8220;Dropbox Migrates 34 PB of Data to an Amazon S3 Data Lake for Analytics,&#8221; AWS website,<br/></p>
<p>2020, <i>https://oreil.ly/wpVoM</i>.<br/>8  Todd Hoff, &#8220;The Eternal Cost Savings of Netflix&#8217;s Internal Spot Market,&#8221; High Scalability,<br/></p>
<p>December 4, 2017, <i>https://oreil.ly/LLoFt</i>.<br/>9  Todd Spangler, &#8220;Netflix Bandwidth Consumption Eclipsed by Web Media Streaming<br/></p>
<p>Applications,&#8221; <i>Variety</i>, September 10, 2019, <i>https://oreil.ly/tTm3k</i>.<br/>10  Amir Efrati and Kevin McLaughlin, &#8220;Apple&#8217;s Spending on Google Cloud Storage on Track to<br/></p>
<p>Soar 50% This Year,&#8221; <i>The Information</i>, June 29, 2021, <i>https://oreil.ly/OlFyR</i>.<br/>11  Evan Sangaline, &#8220;Running FFmpeg on AWS Lambda for 1.9% the Cost of AWS Elastic<br/></p>
<p>Transcoder,&#8221; Intoli blog, May 2, 2018, <i>https://oreil.ly/myzOv</i>.<br/>12  Examples include OpenFaaS, Knative, and Google Cloud Run.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>13  Justin Olsson and Reynold Xin, &#8220;Eliminating the Anti-competitive DeWitt Clause for<br/>Database Benchmarking,&#8221; Databricks, November 8, 2021, <i>https://oreil.ly/3iFOE</i>.<br/></p>
<p>14  For a classic of the genre, see William McKnight and Jake Dolezal, &#8220;Data Warehouse in the<br/>Cloud Benchmark,&#8221; GigaOm, February 7, 2019, <i>https://oreil.ly/QjCmA</i>.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><b>Part II. The Data Engineering<br/>Lifecycle in Depth</b></p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><b>Chapter 5. Data Generation in<br/>Source Systems<br/></b>Welcome to the first stage of the data engineering lifecycle: data generation<br/>in source systems. As we described earlier, the job of a data engineer is to<br/>take data from source systems, do something with it, and make it helpful in<br/>serving downstream use cases. But before you get raw data, you must<br/>understand where the data exists, how it is generated, and its characteristics<br/>and quirks.<br/>This chapter covers some popular operational source system patterns and<br/>the significant types of source systems. Many source systems exist for data<br/>generation, and we&#8217;re not exhaustively covering them all. We&#8217;ll consider<br/>the data these systems generate and things you should consider when<br/>working with source systems. We also discuss how the undercurrents of<br/>data engineering apply to this first phase of the data engineering lifecycle<br/>(Figure 5-1).<br/></p>
<p><i>Figure 5-1. Source systems generate the data for the rest of the data engineering lifecycle</i></p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>As data proliferates, especially with the rise of data sharing (discussed<br/>next), we expect that a data engineer&#8217;s role will shift heavily toward<br/>understanding the interplay between data sources and destinations. The<br/>basic plumbing tasks of data engineering&#8212;moving data from A to B&#8212;will<br/>simplify dramatically. On the other hand, it will remain critical to<br/>understand the nature of data as it&#8217;s created in source systems.<br/></p>
<p><b>Sources of Data: How Is Data Created?<br/></b>As you learn about the various underlying operational patterns of the<br/>systems that generate data, it&#8217;s essential to understand how data is created.<br/>Data is an unorganized, context-less collection of facts and figures. It can be<br/>created in many ways, both analog and digital.<br/><i>Analog data</i> creation occurs in the real world, such as vocal speech, sign<br/>language, writing on paper, or playing an instrument. This analog data is<br/>often transient; how often have you had a verbal conversation whose<br/>contents are lost to the ether after the conversation ends?<br/><i>Digital data</i> is either created by converting analog data to digital form or is<br/>the native product of a digital system. An example of analog to digital is a<br/>mobile texting app that converts analog speech into digital text. An example<br/>of digital data creation is a credit card transaction on an ecommerce<br/>platform. A customer places an order, the transaction is charged to their<br/>credit card, and the information for the transaction is saved to various<br/>databases.<br/>We&#8217;ll utilize a few common examples in this chapter, such as data created<br/>when interacting with a website or mobile application. But in truth, data is<br/>everywhere in the world around us. We capture data from IoT devices,<br/>credit card terminals, telescope sensors, stock trades, and more.<br/>Get familiar with your source system and how it generates data. Put in the<br/>effort to read the source system documentation and understand its patterns<br/>and quirks. If your source system is an RDBMS, learn how it operates</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>(writes, commits, queries, etc.); learn the ins and outs of the source system<br/>that might affect your ability to ingest from it.<br/></p>
<p><b>Source Systems: Main Ideas<br/></b>Source systems produce data in various ways. This section discusses the<br/>main ideas you&#8217;ll frequently encounter as you work with source systems.<br/></p>
<p><b>Files and Unstructured Data<br/></b>A <i>file</i> is a sequence of bytes, typically stored on a disk. Applications often<br/>write data to files. Files may store local parameters, events, logs, images,<br/>and audio.<br/>In addition, files are a universal medium of data exchange. As much as data<br/>engineers wish that they could get data programmatically, much of the<br/>world still sends and receives files. For example, if you&#8217;re getting data from<br/>a government agency, there&#8217;s an excellent chance you&#8217;ll download the data<br/>as an Excel or CSV file or receive the file in an email.<br/>The main types of source file formats you&#8217;ll run into as a data engineer&#8212;<br/>files that originate either manually or as an output from a source system<br/>process&#8212;are Excel, CSV, TXT, JSON, and XML. These files have their<br/>quirks and can be structured (Excel, CSV), semistructured (JSON, XML,<br/>CSV), or unstructured (TXT, CSV). Although you&#8217;ll use certain formats<br/>heavily as a data engineer (such as Parquet, ORC, and Avro), we&#8217;ll cover<br/>these later and put the spotlight here on source system files. Chapter 6<br/>covers the technical details of files.<br/></p>
<p><b>APIs<br/></b><i>Application programming interfaces</i> (APIs) are a standard way of<br/>exchanging data between systems. In theory, APIs simplify the data<br/>ingestion task for data engineers. In practice, many APIs still expose a good<br/>deal of data complexity for engineers to manage. Even with the rise of<br/>various services and frameworks, and services for automating API data</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>ingestion, data engineers must often invest a good deal of energy into<br/>maintaining custom API connections. We discuss APIs in greater detail<br/>later in this chapter.<br/></p>
<p><b>Application Databases (OLTP systems)<br/></b>An <i>application database</i> stores the state of an application. A standard<br/>example is a database that stores account balances for bank accounts. As<br/>customer transactions and payments happen, the application updates bank<br/>account balances.<br/>Typically, an application database is an <i>online transaction processing<br/></i>(OLTP) system&#8212;a database that reads and writes individual data records at<br/>a high rate. OLTP systems are often referred to as <i>transactional databases</i>,<br/>but this does not necessarily imply that the system in question supports<br/><i>atomic transactions</i>.<br/>More generally, OLTP databases support low latency and high concurrency.<br/>An RDBMS database can select or update a row in less than a millisecond<br/>(not accounting for network latency) and handle thousands of reads and<br/>writes per second. A document database cluster can manage even higher<br/>document commit rates at the expense of potential inconsistency. Some<br/>graph databases can also handle transactional use cases.<br/>Fundamentally, OLTP databases work well as application backends when<br/>thousands or even millions of users might be interacting with the<br/>application simultaneously, updating and writing data concurrently. OLTP<br/>systems are less suited to use cases driven by analytics at scale, where a<br/>single query must scan a vast amount of data.<br/><b>ACID<br/></b>Support for atomic transactions is one of a critical set of database<br/>characteristics known together as ACID (as you may recall from Chapter 3,<br/>this stands for <i>atomicity, consistency, isolation, durability</i>). <i>Consistency<br/></i>means that any database read will return the last written version of the<br/>retrieved item. <i>Isolation</i> entails that if two updates are in flight concurrently</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>for the same thing, the end database state will be consistent with the<br/>sequential execution of these updates in the order they were submitted.<br/><i>Durability</i> indicates that committed data will never be lost, even in the<br/>event of power loss.<br/>Note that ACID characteristics are not required to support application<br/>backends, and relaxing these constraints can be a considerable boon to<br/>performance and scale. However, ACID characteristics guarantee that the<br/>database will maintain a consistent picture of the world, dramatically<br/>simplifying the app developer&#8217;s task.<br/>All engineers (data or otherwise) must understand operating with and<br/>without ACID. For instance, to improve performance, some distributed<br/>databases use relaxed consistency constraints, such as <i>eventual consistency</i>,<br/>to improve performance. Understanding the consistency model you&#8217;re<br/>working with helps you prevent disasters.<br/><b>Atomic transactions<br/></b>An <i>atomic transaction</i> is a set of several changes that are committed as a<br/>unit. In the example in Figure 5-2, a traditional banking application running<br/>on an RDBMS executes a SQL statement that checks two account balances,<br/>one in Account A (the source) and another in Account B (the destination).<br/>Money is then moved from Account A to Account B if sufficient funds are<br/>in Account A. The entire transaction should run with updates to both<br/>account balances or fail without updating either account balance. That is,<br/>the whole operation should happen as a <i>transaction</i>.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><i>Figure 5-2. Example of an atomic transaction: a bank account transfer using OLTP<br/></i></p>
<p><b>OLTP and analytics<br/></b>Often, small companies run analytics directly on an OLTP. This pattern<br/>works in the short term but is ultimately not scalable. At some point,<br/>running analytical queries on OLTP runs into performance issues due to<br/>structural limitations of OLTP or resource contention with competing<br/>transactional workloads. Data engineers must understand the inner<br/>workings of OLTP and application backends to set up appropriate<br/>integrations with analytics systems without degrading production<br/>application performance.<br/>As companies offer more analytics capabilities in SaaS applications, the<br/>need for hybrid capabilities&#8212;quick updates with combined analytics<br/>capabilities&#8212;has created new challenges for data engineers. We&#8217;ll use the<br/>term <i>data application</i> to refer to applications that hybridize transactional<br/>and analytics workloads.<br/></p>
<p><b>Online Analytical Processing System<br/></b>In contrast to an OLTP system, an <i>online analytical processing</i> (OLAP)<br/>system is built to run large analytics queries and is typically inefficient at<br/>handling lookups of individual records. For example, modern column<br/>databases are optimized to scan large volumes of data, dispensing with<br/>indexes to improve scalability and scan performance. Any query typically<br/>involves scanning a minimal data block, often 100 MB or more in size.<br/>Trying to look up thousands of individual items per second in such a system</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>will bring it to its knees unless it is combined with a caching layer designed<br/>for this use case.<br/>Note that we&#8217;re using the term <i>OLAP</i> to refer to any database system that<br/>supports high-scale interactive analytics queries; we are not limiting<br/>ourselves to systems that support OLAP cubes (multidimensional arrays of<br/>data). The <i>online</i> part of OLAP implies that the system constantly listens<br/>for incoming queries, making OLAP systems suitable for interactive<br/>analytics.<br/>Although this chapter covers source systems, OLAPs are typically storage<br/>and query systems for analytics. Why are we talking about them in our<br/>chapter on source systems? In practical use cases, engineers often need to<br/>read data from an OLAP system. For example, a data warehouse might<br/>serve data used to train an ML model. Or, an OLAP system might serve a<br/>reverse ETL workflow, where derived data in an analytics system is sent<br/>back to a source system, such as a CRM, SaaS platform, or transactional<br/>application.<br/></p>
<p><b>Change Data Capture<br/></b><i>Change data capture</i> (CDC) is a method for extracting each change event<br/>(insert, update, delete) that occurs in a database. CDC is frequently<br/>leveraged to replicate between databases in near real time or create an event<br/>stream for downstream processing.<br/>CDC is handled differently depending on the database technology.<br/>Relational databases often generate an event log stored directly on the<br/>database server that can be processed to create a stream. (See &#8220;Database<br/>Logs&#8221;.) Many cloud NoSQL databases can send a log or event stream to a<br/>target storage location.<br/></p>
<p><b>Logs<br/></b>A <i>log</i> captures information about events that occur in systems. For example,<br/>a log may capture traffic and usage patterns on a web server. Your desktop</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>computer&#8217;s operating system (Windows, macOS, Linux) logs events as the<br/>system boots and when applications start or crash, for example.<br/>Logs are a rich data source, potentially valuable for downstream data<br/>analysis, ML, and automation. Here are a few familiar sources of logs:<br/></p>
<p>Operating systems<br/>Applications<br/>Servers<br/>Containers<br/>Networks<br/>IoT devices<br/></p>
<p>All logs track events and event metadata. At a minimum, a log should<br/>capture who, what, and when:<br/><i>Who<br/></i></p>
<p>The human, system, or service account associated with the event (e.g., a<br/>web browser user agent or a user ID)<br/></p>
<p><i>What happened<br/></i>The event and related metadata<br/></p>
<p><i>When<br/></i>The timestamp of the event<br/></p>
<p><b>Log encoding<br/></b>Logs are encoded in a few ways:<br/><i>Binary-encoded logs</i></p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>These encode data in a custom compact format for space efficiency and<br/>fast I/O. Database logs, discussed in &#8220;Database Logs&#8221;, are a standard<br/>example.<br/></p>
<p><i>Semistructured logs<br/></i>These are encoded as text in an object serialization format (JSON, more<br/>often than not). Semistructured logs are machine-readable and portable.<br/>However, they are much less efficient than binary logs. And though they<br/>are nominally machine-readable, extracting value from them often<br/>requires significant custom code.<br/></p>
<p><i>Plain-text (unstructured) logs<br/></i>These essentially store the console output from software. As such, no<br/>general-purpose standards exist. These logs can provide helpful<br/>information for data scientists and ML engineers, though extracting<br/>useful information from the raw text data might be complicated.<br/></p>
<p><b>Log resolution<br/></b>Logs are created at various resolutions and log levels. The log <i>resolution<br/></i>refers to the amount of event data captured in a log. For example, database<br/>logs capture enough information from database events to allow<br/>reconstructing the database state at any point in time.<br/>On the other hand, capturing all data changes in logs for a big data system<br/>often isn&#8217;t practical. Instead, these logs may note only that a particular type<br/>of commit event has occurred. The <i>log level</i> refers to the conditions<br/>required to record a log entry, specifically concerning errors and debugging.<br/>Software is often configurable to log every event or to log only errors, for<br/>example.<br/><b>Log latency: Batch or real time</b></p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Batch logs are often written continuously to a file. Individual log entries can<br/>be written to a messaging system such as Kafka or Pulsar for real-time<br/>applications.<br/></p>
<p><b>Database Logs<br/></b><i>Database logs</i> are essential enough that they deserve more detailed<br/>coverage. Write-ahead logs&#8212;typically, binary files stored in a specific<br/>database-native format&#8212;play a crucial role in database guarantees and<br/>recoverability. The database server receives write and update requests to a<br/>database table (see Figure 5-3), storing each operation in the log before<br/>acknowledging the request. The acknowledgment comes with a log<br/>associated guarantee: even if the server fails, it can recover its state on<br/>reboot by completing the unfinished work from the logs.<br/></p>
<p><i>Figure 5-3. Log operations on a database table<br/></i></p>
<p>Database logs are extremely useful in data engineering, especially for CDC<br/>to generate event streams from database changes.<br/></p>
<p><b>CRUD</b></p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><i>CRUD</i>, which stands for <i>create</i>, <i>read</i>, <i>update</i>, and <i>delete</i>, is a transactional<br/>pattern commonly used in programming and represents the four basic<br/>operations of persistent storage. CRUD is the most common pattern for<br/>storing application state in a database. A basic tenet of CRUD is that data<br/>must be created before being used. After the data has been created, the data<br/>can be read and updated. Finally, the data may need to be destroyed. CRUD<br/>guarantees these four operations will occur on data, regardless of its<br/>storage.<br/>CRUD is a widely used pattern in software applications, and you&#8217;ll<br/>commonly find CRUD used in APIs and databases. For example, a web<br/>application will make heavy use of CRUD for RESTful HTTP requests and<br/>storing and retrieving data from a database.<br/>As with any database, we can use snapshot-based extraction to get data<br/>from a database where our application applies CRUD operations. On the<br/>other hand, event extraction with CDC gives us a complete history of<br/>operations and potentially allows for near real-time analytics.<br/></p>
<p><b>Insert-Only<br/></b>The <i>insert-only pattern</i> retains history directly in a table containing data.<br/>Rather than updating records, new records get inserted with a timestamp<br/>indicating when they were created (Table 5-1). For example, suppose you<br/>have a table of customer addresses. Following a CRUD pattern, you would<br/>simply update the record if the customer changed their address. With the<br/>insert-only pattern, a new address record is inserted with the same customer<br/>ID. To read the current customer address by customer ID, you would look<br/>up the latest record under that ID.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p> <br/><i>T<br/>a<br/>b<br/>l<br/>e <br/>5<br/>-<br/>1<br/>. <br/>A<br/>n <br/>i<br/>n<br/>s<br/>e<br/>r<br/>t<br/>-<br/>o<br/>n<br/>l<br/>y <br/>p<br/>a<br/>t<br/>t<br/>e<br/>r<br/>n <br/>p<br/>r<br/>o<br/>d<br/>u</i></p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><i>c<br/>e<br/>s <br/>m<br/>u<br/>l<br/>t<br/>i<br/>p<br/>l<br/>e <br/>v<br/>e<br/>r<br/>s<br/>i<br/>o<br/>n<br/>s <br/>o<br/>f <br/>a <br/>r<br/>e<br/>c<br/>o<br/>r<br/>d<br/></i> <br/><b>Record ID Value Timestamp<br/></b> <br/>1 40 2021-09-19T00:10:23+00:00<br/>1 51 2021-09-30T00:12:00+00:00<br/> </p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In a sense, the insert-only pattern maintains a database log directly in the<br/>table itself, making it especially useful if the application needs access to<br/>history. For example, the insert-only pattern would work well for a banking<br/>application designed to present customer address history.<br/>A separate analytics insert-only pattern is often used with regular CRUD<br/>application tables. In the insert-only ETL pattern, data pipelines insert a<br/>new record in the target analytics table anytime an update occurs in the<br/>CRUD table.<br/>Insert-only has a couple of disadvantages. First, tables can grow quite large,<br/>especially if data frequently changes, since each change is inserted into the<br/>table. Sometimes records are purged based on a record sunset date or a<br/>maximum number of record versions to keep table size reasonable. The<br/>second disadvantage is that record lookups incur extra overhead because<br/>looking up the current state involves running MAX(created_timestamp). If<br/>hundreds or thousands of records are under a single ID, this lookup<br/>operation is expensive to run.<br/></p>
<p><b>Messages and Streams<br/></b>Related to event-driven architecture, two terms that you&#8217;ll often see used<br/>interchangeably are <i>message queue</i> and <i>streaming platform</i>, but a subtle but<br/>essential difference exists between the two. Defining and contrasting these<br/>terms is worthwhile since they encompass many big ideas related to source<br/>systems and practices, and technologies spanning the entire data<br/>engineering lifecycle.<br/>A <i>message</i> is raw data communicated across two or more systems<br/>(Figure 5-4). For example, we have System 1 and System 2, where System<br/>1 sends a message to System 2. These systems could be different<br/>microservices, a server sending a message to a serverless function, etc.. A<br/>message is typically sent through a <i>message queue</i> from a publisher to a<br/>consumer, and once the message is delivered, it is removed from the queue.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><i>Figure 5-4. A message passed between two systems<br/></i></p>
<p>Messages are discrete and singular signals in an event-driven system. For<br/>example, an IoT device might send a message with the latest temperature<br/>reading to a message queue. This message is then ingested by a service that<br/>determines whether the furnace should be turned on or off. This service<br/>sends a message to a furnace controller that takes the appropriate action.<br/>Once the message is received, and the action is taken, the message is<br/>removed from the message queue.<br/>By contrast, a <i>stream</i> is an append-only log of event records. (Streams are<br/>ingested and stored in <i>event-streaming platforms</i>, which we discuss at<br/>greater length in &#8220;Messages and Streams&#8221;.) As events occur, they are<br/>accumulated in an ordered sequence (Figure 5-5); a timestamp or an ID<br/>might order events. (Note that events aren&#8217;t always delivered in exact order<br/>because of the subtleties of distributed systems.)<br/></p>
<p><i>Figure 5-5. A stream, which is an ordered append-only log of records<br/></i></p>
<p>You&#8217;ll use streams when you care about what happened over many events.<br/>Because of the append-only nature of streams, records in a stream are<br/>persisted over a long retention window&#8212;often weeks or months&#8212;allowing</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>for complex operations on records such as aggregations on multiple records<br/>or the ability to rewind to a point in time within the stream.<br/>It&#8217;s worth noting that systems that process streams can process messages,<br/>and streaming platforms are frequently used for message passing. We often<br/>accumulate messages in streams when we want to perform message<br/>analytics. In our IoT example, the temperature readings that trigger the<br/>furnace to turn on or off might also be later analyzed to determine<br/>temperature trends and statistics.<br/></p>
<p><b>Types of Time<br/></b>While time is an essential consideration for all data ingestion, it becomes<br/>that much more critical and subtle in the context of streaming, where we<br/>view data as continuous and expect to consume it shortly after it is<br/>produced. Let&#8217;s look at the key types of time you&#8217;ll run into when ingesting<br/>data: the time that the event is generated, when it&#8217;s ingested and processed,<br/>and how long processing took (Figure 5-6).<br/></p>
<p><i>Figure 5-6. Event, ingestion, process, and processing time<br/></i></p>
<p><i>Event time</i> indicates when an event is generated in a source system,<br/>including the timestamp of the original event itself. An undetermined time<br/>lag will occur upon event creation, before the event is ingested and<br/>processed downstream. Always include timestamps for each phase through<br/>which an event travels. Log events as they occur and at each stage of time<br/>&#8212;when they&#8217;re created, ingested, and processed. Use these timestamp logs<br/>to accurately track the movement of your data through your data pipelines.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>After data is created, it is ingested somewhere. <i>Ingestion time</i> indicates<br/>when an event is ingested from source systems into a message queue,<br/>cache, memory, object storage, a database, or any place else that data is<br/>stored (see Chapter 6). After ingestion, data may be processed immediately;<br/>or within minutes, hours, or days; or simply persist in storage indefinitely.<br/><i>Process time</i> occurs after ingestion time, when the data is processed<br/>(typically, a transformation). <i>Processing time</i> is how long the data took to<br/>process, measured in seconds, minutes, hours, etc.<br/>You&#8217;ll want to record these various times, preferably in an automated way.<br/>Set up monitoring along your data workflows to capture when events occur,<br/>when they&#8217;re ingested and processed, and how long it took to process<br/>events.<br/></p>
<p><b>Source System Practical Details<br/></b>This section discusses the practical details of interacting with modern<br/>source systems. We&#8217;ll dig into the details of commonly encountered<br/>databases, APIs, and other aspects. This information will have a shorter<br/>shelf life than the main ideas discussed previously; popular API<br/>frameworks, databases, and other details will continue to change rapidly.<br/>Nevertheless, these details are critical knowledge for working data<br/>engineers. We suggest that you study this information as baseline<br/>knowledge but read extensively to stay abreast of ongoing developments.<br/></p>
<p><b>Databases<br/></b>In this section, we&#8217;ll look at common source system database technologies<br/>that you&#8217;ll encounter as a data engineer and high-level considerations for<br/>working with these systems. There are as many types of databases as there<br/>are use cases for data.<br/><b>Major considerations for understanding database technologies</b></p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Here, we introduce major ideas that occur across a variety of database<br/>technologies, including those that back software applications and those that<br/>support analytics use cases:<br/><i>Database management system<br/></i></p>
<p>A database system used to store and serve data. Abbreviated as DBMS,<br/>it consists of a storage engine, query optimizer, disaster recovery, and<br/>other key components for managing the database system.<br/></p>
<p><i>Lookups<br/></i>How does the database find and retrieve data? Indexes can help speed<br/>up lookups, but not all databases have indexes. Know whether your<br/>database uses indexes; if so, what are the best patterns for designing and<br/>maintaining them? Understand how to leverage for efficient extraction.<br/>It also helps to have a basic knowledge of the major types of indexes,<br/>including B-tree and log-structured merge-trees (LSM).<br/></p>
<p><i>Query optimizer<br/></i>Does the database utilize an optimizer? What are its characteristics?<br/></p>
<p><i>Scaling and distribution<br/></i>Does the database scale with demand? What scaling strategy does it<br/>deploy? Does it scale horizontally (more database nodes) or vertically<br/>(more resources on a single machine)?<br/></p>
<p><i>Modeling patterns<br/></i>What modeling patterns work best with the database (e.g., data<br/>normalization or wide tables)? (See Chapter 8 for our discussion of data<br/>modeling.)</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><i>CRUD<br/></i>How is data queried, created, updated, and deleted in the database?<br/>Every type of database handles CRUD operations differently.<br/></p>
<p><i>Consistency<br/></i>Is the database fully consistent, or does it support a relaxed consistency<br/>model (e.g., eventual consistency)? Does the database support optional<br/>consistency modes for reads and writes (e.g., strongly consistent reads)?<br/></p>
<p>We divide databases into relational and nonrelational categories. In truth,<br/>the nonrelational category is far more diverse, but relational databases still<br/>occupy significant space in application backends.<br/><b>Relational databases<br/></b>A <i>relational database management system</i> (RDBMS) is one of the most<br/>common application backends. Relational databases were developed at IBM<br/>in the 1970s and popularized by Oracle in the 1980s. The growth of the<br/>internet saw the rise of the LAMP stack (Linux, Apache web server,<br/>MySQL, PHP) and an explosion of vendor and open source RDBMS<br/>options. Even with the rise of NoSQL databases (described in the following<br/>section), relational databases have remained extremely popular.<br/>Data is stored in a table of <i>relations</i> (rows), and each relation contains<br/>multiple <i>fields</i> (columns); see Figure 5-7. Note that we use the terms<br/><i>column</i> and <i>field</i> interchangeably throughout this book. Each relation in the<br/>table has the same <i>schema</i> (a sequence of columns with assigned static<br/>types such as string, integer, or float). Rows are typically stored as a<br/>contiguous sequence of bytes on disk.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><i>Figure 5-7. RDBMS stores and retrieves data at a row level<br/></i></p>
<p>Tables are indexed by a <i>primary key</i>, a unique field for each row in the<br/>table. The indexing strategy for the primary key is closely connected with<br/>the layout of the table on disk.<br/>Tables can also have various <i>foreign keys</i>&#8212;fields with values connected<br/>with the values of primary keys in other tables, facilitating joins, and<br/>allowing for complex schemas that spread data across multiple tables. In<br/>particular, it is possible to design a <i>normalized schema</i>. Normalization is a<br/>strategy for ensuring that data in records is not duplicated in multiple<br/>places, thus avoiding the need to update states in multiple locations at once<br/>and preventing inconsistencies (see Chapter 8).<br/>RDBMS systems are typically ACID compliant. Combining a normalized<br/>schema, ACID compliance, and support for high transaction rates makes<br/>relational database systems ideal for storing rapidly changing application<br/>states. The challenge for data engineers is to determine how to capture state<br/>information over time.<br/>A full discussion of the theory, history, and technology of RDBMS is<br/>beyond the scope of this book. We encourage you to study RDBMS<br/>systems, relational algebra, and strategies for normalization because they&#8217;re<br/>widespread, and you&#8217;ll encounter them frequently. See &#8220;Additional<br/>Resources&#8221; for suggested books.<br/><b>Nonrelational databases: NoSQL<br/></b>While relational databases are terrific for many use cases, they&#8217;re not a one-<br/>size-fits-all solution. We often see that people start with a relational</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>database under the impression it&#8217;s a universal appliance and shoehorn in a<br/>ton of use cases and workloads. As data and query requirements morph, the<br/>relational database collapses under its weight. At that point, you&#8217;ll want to<br/>use a database that&#8217;s appropriate for the specific workload under pressure.<br/>Enter nonrelational or NoSQL databases. <i>NoSQL</i>, which stands for <i>not only<br/>SQL</i>, refers to a whole class of databases that abandon the relational<br/>paradigm.<br/>On the one hand, dropping relational constraints can improve performance,<br/>scalability, and schema flexibility. But as always in architecture, trade-offs<br/>exist. NoSQL databases also typically abandon various RDBMS<br/>characteristics, such as strong consistency, joins, or a fixed schema.<br/>A big theme of this book is that data innovation is constant. Let&#8217;s take a<br/>quick look at the history of NoSQL, as it&#8217;s helpful to gain a perspective on<br/>why and how data innovations impact your work as a data engineer. In the<br/>early 2000s, tech companies such as Google and Amazon began to outgrow<br/>their relational databases and pioneered new distributed, nonrelational<br/>databases to scale their web platforms.<br/>While the term <i>NoSQL</i> first appeared in 1998, the modern version was<br/>coined by Eric Evans in the 2000s.  He tells the story in a 2009 blog post:<br/></p>
<p><i>I&#8217;ve spent the last couple of days at nosqleast and one of the hot topics<br/>here is the name &#8220;nosql.&#8221; Understandably, there are a lot of people who<br/>worry that the name is Bad, that it sends an inappropriate or inaccurate<br/>message. While I make no claims to the idea, I do have to accept some<br/>blame for what it is now being called. How&#8217;s that? Johan Oskarsson was<br/>organizing the first meetup and asked the question &#8220;What&#8217;s a good<br/>name?&#8221; on IRC; it was one of three or four suggestions that I spouted off<br/>in the span of like 45 seconds, without thinking.<br/>My regret, however, isn&#8217;t about what the name says; it&#8217;s about what it<br/>doesn&#8217;t. When Johan originally had the idea for the first meetup, he<br/>seemed to be thinking Big Data and linearly scalable distributed systems,<br/>but the name is so vague that it opened the door to talk submissions for<br/>literally anything that stored data, and wasn&#8217;t an RDBMS.<br/></i></p>
<p>1</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>NoSQL remains vague in 2022, but it&#8217;s been widely adopted to describe a<br/>universe of &#8220;new school&#8221; databases, alternatives to relational databases.<br/>There are numerous flavors of NoSQL database designed for almost any<br/>imaginable use case. Because there are far too many NoSQL databases to<br/>cover exhaustively in this section, we consider the following database<br/>types: key-value, document, wide-column, graph, search, and time series.<br/>These databases are all wildly popular and enjoy widespread adoption. A<br/>data engineer should understand these types of databases, including usage<br/>considerations, the structure of the data they store, and how to leverage<br/>each in the data engineering lifecycle.<br/><i>Key-value stores<br/></i>A <i>key-value database</i> is a nonrelational database that retrieves records<br/>using a key that uniquely identifies each record. This is similar to hash map<br/>or dictionary data structures presented in many programming languages but<br/>potentially more scalable. Key-value stores encompass several NoSQL<br/>database types&#8212;for example, document stores and wide column databases<br/>(discussed next).<br/>Different types of key-value databases offer a variety of performance<br/>characteristics to serve various application needs. For example, in-memory<br/>key-value databases are popular for caching session data for web and<br/>mobile applications, where ultra-fast lookup and high concurrency are<br/>required. Storage in these systems is typically temporary; if the database<br/>shuts down, the data disappears. Such caches can reduce pressure on the<br/>main application database and serve speedy responses.<br/>Of course, key-value stores can also serve applications requiring high-<br/>durability persistence. An ecommerce application may need to save and<br/>update massive amounts of event state changes for a user and their orders.<br/>A user logs into the ecommerce application, clicks around various screens,<br/>adds items to a shopping cart, and then checks out. Each event must be<br/>durably stored for retrieval. Key-value stores often persist data to disk and<br/>across multiple nodes to support such use cases.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><i>Document stores<br/></i>As mentioned previously, a <i>document store</i> is a specialized key-value store.<br/>In this context, a <i>document</i> is a nested object; we can usually think of each<br/>document as a JSON object for practical purposes. Documents are stored in<br/>collections and retrieved by key. A <i>collection</i> is roughly equivalent to a<br/>table in a relational database (see Table 5-2).</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p> <br/><i>T<br/>a<br/>b<br/>l<br/>e <br/>5<br/>-<br/>2<br/>. <br/>C<br/>o<br/>m<br/>p<br/>a<br/>r<br/>i<br/>s<br/>o<br/>n <br/>o<br/>f <br/>R<br/>D<br/>B<br/>M<br/>S <br/>a<br/>n<br/>d <br/>d<br/>o<br/>c<br/>u<br/>m</i></p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><i>e<br/>n<br/>t <br/>t<br/>e<br/>r<br/>m<br/>i<br/>n<br/>o<br/>l<br/>o<br/>g<br/>y<br/></i> <br/><b>RDBMS Document database<br/></b> <br/>Table Collection<br/>Row Document, items, entity<br/> <br/></p>
<p>One key difference between relational databases and document stores is that<br/>the latter does not support joins. This means that data cannot be easily<br/><i>normalized</i>, i.e., split across multiple tables. (Applications can still join<br/>manually. Code can look up a document, extract a property, and then<br/>retrieve another document.) Ideally, all related data can be stored in the<br/>same document.<br/>In many cases, the same data must be stored in multiple documents spread<br/>across numerous collections; software engineers must be careful to update a<br/>property everywhere it is stored. (Many document stores support a notion of<br/>transactions to facilitate this.)<br/>Document databases generally embrace all the flexibility of JSON and don&#8217;t<br/>enforce schema or types; this is a blessing and a curse. On the one hand,</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>this allows the schema to be highly flexible and expressive. The schema can<br/>also evolve as an application grows. On the flip side, we&#8217;ve seen document<br/>databases become absolute nightmares to manage and query. If developers<br/>are not careful in managing schema evolution, data may become<br/>inconsistent and bloated over time. Schema evolution can also break<br/>downstream ingestion and cause headaches for data engineers if it&#8217;s not<br/>communicated in a timely fashion (before deployment).<br/>The following is an example of data that is stored in a collection called<br/>users. The collection key is the id. We also have a name (along with first<br/>and last as child elements) and an array of the person&#8217;s favorite bands<br/>within each document:<br/></p>
<p>users:[ <br/>    { <br/>     id: 1234 <br/>        <b>na</b>me: <br/>            <b>f</b>irs<b>t</b>: &quot;Joe&quot; <br/>            las<b>t</b>: &quot;Reis&quot; <br/>        <b>fa</b>vori<b>te</b>_ba<b>n</b>ds: [&quot;AC/DC&quot;, &quot;Slayer&quot;, &quot;WuTang Clan&quot;, &quot;Action Bronson&quot;] <br/>     }, <br/>    { <br/>     id: 1235 <br/>        <b>na</b>me: <br/>            <b>f</b>irs<b>t</b>: &quot;Matt&quot; <br/>            las<b>t</b>: &quot;Housley&quot; <br/>        <b>fa</b>vori<b>te</b>_ba<b>n</b>ds: [&quot;Dave Matthews Band&quot;, &quot;Creed&quot;, &quot;Nickelback&quot;] <br/>    } <br/>]<br/></p>
<p>To query the data in this example, you can retrieve records by key. Note that<br/>most document databases also support the creation of indexes and lookup<br/>tables to allow retrieval of documents by specific properties. This is often<br/>invaluable in application development when you need to search for<br/>documents in various ways. For example, you could set an index on name.<br/>Another critical technical detail for data engineers is that document stores<br/>are generally not ACID compliant, unlike relational databases. Technical<br/>expertise in a particular document store is essential to understanding<br/>performance, tuning, configuration, related effects on writes, consistency,</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>durability, etc. For example, many document stores are <i>eventually<br/>consistent</i>. Allowing data distribution across a cluster is a boon for scaling<br/>and performance but can lead to catastrophes when engineers and<br/>developers don&#8217;t understand the implications.<br/>To run analytics on document stores, engineers generally must run a full<br/>scan to extract all data from a collection or employ a CDC strategy to send<br/>events to a target stream. The full scan approach can have both performance<br/>and cost implications. The scan often slows the database as it runs, and<br/>many serverless cloud offerings charge a significant fee for each full scan.<br/>In document databases, it&#8217;s often helpful to create an index to help speed up<br/>queries. We discuss indexes and query patterns in Chapter 8.<br/><i>Wide-column<br/></i>A <i>wide-column database</i> is optimized for storing massive amounts of data<br/>with high transaction rates and extremely low latency. These databases can<br/>scale to extremely high write rates and vast amounts of data. Specifically,<br/>wide-column databases can support petabytes of data, millions of requests<br/>per second, and sub-10ms latency. These characteristics have made wide-<br/>column databases popular in ecommerce, fintech, ad tech, IoT, and real-<br/>time personalization applications. Data engineers must be aware of the<br/>operational characteristics of the wide-column databases they work with to<br/>set up a suitable configuration, design the schema, and choose an<br/>appropriate row key to optimize performance and avoid common<br/>operational issues.<br/>These databases support rapid scans of massive amounts of data, but they<br/>do not support complex queries. They have only a single index (the row<br/>key) for lookups. Data engineers must generally extract data and send it to a<br/>secondary analytics system to run complex queries to deal with these<br/>limitations. This can be accomplished by running large scans for the<br/>extraction or employing CDC to capture an event stream.<br/><i>Graph databases<br/></i></p>
<p>2</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><i>Graph databases</i> explicitly store data with a mathematical graph structure<br/>(as a set of nodes and edges).  Neo4j has proven extremely popular, while<br/>Amazon, Oracle, and other vendors offer their graph database products.<br/>Roughly speaking, graph databases are a good fit when you want to analyze<br/>the connectivity between elements.<br/>For example, you could use a document database to store one document for<br/>each user describing their properties. You could add an array element for<br/><i>connections</i> that contains directly connected users&#8217; IDs in a social media<br/>context. It&#8217;s pretty easy to determine the number of direct connections a<br/>user has, but suppose you want to know how many users can be reached by<br/>traversing two direct connections. You could answer this question by<br/>writing complex code, but each query would run slowly and consume<br/>significant resources. The document store is simply not optimized for this<br/>use case.<br/>Graph databases are designed for precisely this type of query. Their data<br/>structures allow for queries based on the connectivity between elements;<br/>graph databases are indicated when we care about understanding complex<br/>traversals between elements. In the parlance of graphs, we store <i>nodes<br/></i>(users in the preceding example) and <i>edges</i> (connections between users).<br/>Graph databases support rich data models for both nodes and edges.<br/>Depending on the underlying graph database engine, graph databases utilize<br/>specialized query languages such as SPARQL, Resource Description<br/>Framework (RDF), Graph Query Language (GQL), and Cypher.<br/>As an example of a graph, consider a network of four users. User 1 follows<br/>User 2, who follows User 3 and User 4; User 3 also follows User 4<br/>(Figure 5-8).<br/></p>
<p>3</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><i>Figure 5-8. A social network graph<br/></i></p>
<p>We anticipate that graph database applications will grow dramatically<br/>outside of tech companies; market analyses also predict rapid growth.  Of<br/>course, graph databases are beneficial from an operational perspective and<br/>support the kinds of complex social relationships critical to modern<br/>applications. Graph structures are also fascinating from the perspective of<br/>data science and ML, potentially revealing deep insights into human<br/>interactions and behavior.<br/>This introduces unique challenges for data engineers who may be more<br/>accustomed to dealing with structured relations, documents, or unstructured<br/>data. Engineers must choose whether to do the following:<br/></p>
<p>Map source system graph data into one of their existing preferred<br/>paradigms<br/>Analyze graph data within the source system itself<br/>Adopt graph-specific analytics tools<br/></p>
<p>Graph data can be reencoded into rows in a relational database, which may<br/>be a suitable solution depending on the analytics use case. Transactional<br/>graph databases are also designed for analytics, although large queries may<br/>overload production systems. Contemporary cloud-based graph databases<br/>support read-heavy graph analytics on massive quantities of data.<br/><i>Search<br/></i>A <i>search database</i> is a nonrelational database used to search your data&#8217;s<br/>complex and straightforward semantic and structural characteristics. Two<br/></p>
<p>4</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>prominent use cases exist for a search database: text search and log<br/>analysis. Let&#8217;s cover each of these separately.<br/><i>Text search</i> involves searching a body of text for keywords or phrases,<br/>matching on exact, fuzzy, or semantically similar matches. <i>Log analysis</i> is<br/>typically used for anomaly detection, real-time monitoring, security<br/>analytics, and operational analytics. Queries can be optimized and sped up<br/>with the use of indexes.<br/>Depending on the type of company you work at, you may use search<br/>databases either regularly or not at all. Regardless, it&#8217;s good to be aware<br/>they exist in case you come across them in the wild. Search databases are<br/>popular for fast search and retrieval and can be found in various<br/>applications; an ecommerce site may power its product search using a<br/>search database. As a data engineer, you might be expected to bring data<br/>from a search database (such as Elasticsearch, Apache Solr or Lucene, or<br/>Algolia) into downstream KPI reports or something similar.<br/><i>Time series<br/></i>A <i>time series</i> is a series of values organized by time. For example, stock<br/>prices might move as trades are executed throughout the day, or a weather<br/>sensor will take atmospheric temperatures every minute. Any events that<br/>are recorded over time&#8212;either regularly or sporadically&#8212;are time-series<br/>data. A <i>time-series database</i> is optimized for retrieving and statistical<br/>processing of time-series data.<br/>While time-series data such as orders, shipments, logs, and so forth have<br/>been stored in relational databases for ages, these data sizes and volumes<br/>were often tiny. As data grew faster and bigger, new special-purpose<br/>databases were needed. Time-series databases address the needs of<br/>growing, high-velocity data volumes from IoT, event and application logs,<br/>ad tech, and fintech, among many other use cases. Often these workloads<br/>are write-heavy. As a result, time-series databases often utilize memory<br/>buffering to support fast writes and reads.<br/>We should distinguish between measurement and event-based data,<br/>common in time-series databases. <i>Measurement data</i> is generated regularly,</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>such as temperature or air-quality sensors. <i>Event-based data</i> is irregular and<br/>created every time an event occurs&#8212;for instance, when a motion sensor<br/>detects movement.<br/>The schema for a time series typically contains a timestamp and a small set<br/>of fields. Because the data is time-dependent, the data is ordered by the<br/>timestamp. This makes time-series databases suitable for operational<br/>analytics but not great for BI use cases. Joins are not common, though some<br/>quasi time-series databases such as Apache Druid support joins. Many time-<br/>series databases are available, both as open source and paid options.<br/></p>
<p><b>APIs<br/></b>APIs are now a standard and pervasive way of exchanging data in the<br/>cloud, for SaaS platforms, and between internal company systems. Many<br/>types of API interfaces exist across the web, but we are principally<br/>interested in those built around HTTP, the most popular type on the web<br/>and in the cloud.<br/><b>REST<br/></b>We&#8217;ll first talk about REST, currently the dominant API paradigm. As noted<br/>in Chapter 4, <i>REST</i> stands for <i>representational state transfer</i>. This set of<br/>practices and philosophies for building HTTP web APIs was laid out by<br/>Roy Fielding in 2000 in a PhD dissertation. REST is built around HTTP<br/>verbs, such as GET and PUT; in practice, modern REST uses only a handful<br/>of the verb mappings outlined in the original dissertation.<br/>One of the principal ideas of REST is that interactions are stateless. Unlike<br/>in a Linux terminal session, there is no notion of a session with associated<br/>state variables such as a working directory; each REST call is independent.<br/>REST calls can change the system&#8217;s state, but these changes are global,<br/>applying to the full system rather than a current session.<br/>Critics point out that REST is in no way a full specification.  REST<br/>stipulates basic properties of interactions, but developers utilizing an API<br/></p>
<p>5</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>must gain a significant amount of domain knowledge to build applications<br/>or pull data effectively.<br/>We see great variation in levels of API abstraction. In some cases, APIs are<br/>merely a thin wrapper over internals that provides the minimum<br/>functionality required to protect the system from user requests. In other<br/>examples, a REST data API is a masterpiece of engineering that prepares<br/>data for analytics applications and supports advanced reporting.<br/>A couple of developments have simplified setting up data-ingestion<br/>pipelines from REST APIs. First, data providers frequently supply client<br/>libraries in various languages, especially in Python. Client libraries remove<br/>much of the boilerplate labor of building API interaction code. Client<br/>libraries handle critical details such as authentication and map fundamental<br/>methods into accessible classes.<br/>Second, various services and open source libraries have emerged to interact<br/>with APIs and manage data synchronization. Many SaaS and open source<br/>vendors provide off-the-shelf connectors for common APIs. Platforms also<br/>simplify the process of building custom connectors as required.<br/>There are numerous data APIs without client libraries or out-of-the-box<br/>connector support. As we emphasize throughout the book, engineers would<br/>do well to reduce undifferentiated heavy lifting by using off-the-shelf tools.<br/>However, low-level <i>plumbing</i> tasks still consume many resources. At<br/>virtually any large company, data engineers will need to deal with the<br/>problem of writing and maintaining custom code to pull data from APIs,<br/>which requires understanding the structure of the data as provided,<br/>developing appropriate data-extraction code, and determining a suitable<br/>data synchronization strategy.<br/><b>GraphQL<br/></b><i>GraphQL</i> was created at Facebook as a query language for application data<br/>and an alternative to generic REST APIs. Whereas REST APIs generally<br/>restrict your queries to a specific data model, GraphQL opens up the<br/>possibility of retrieving multiple data models in a single request. This</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>allows for more flexible and expressive queries than with REST. GraphQL<br/>is built around JSON and returns data in a shape resembling the JSON<br/>query.<br/>There&#8217;s something of a holy war between REST and GraphQL, with some<br/>engineering teams partisans of one or the other and some using both. In<br/>reality, engineers will encounter both as they interact with source systems.<br/><b>Webhooks<br/></b><i>Webhooks</i> are a simple event-based data-transmission pattern. The data<br/>source can be an application backend, a web page, or a mobile app. When<br/>specified events happen in the source system, this triggers a call to an<br/>HTTP endpoint hosted by the data consumer. Notice that the connection<br/>goes from the source system to the data sink, the opposite of typical APIs.<br/>For this reason, webhooks are often called <i>reverse APIs</i>.<br/>The endpoint can do various things with the POST event data, potentially<br/>triggering a downstream process or storing the data for future use. For<br/>analytics purposes, we&#8217;re interested in collecting these events. Engineers<br/>commonly use message queues to ingest data at high velocity and volume.<br/>We will talk about message queues and event streams later in this chapter.<br/><b>RPC and gRPC<br/></b>A <i>remote procedure call</i> (RPC) is commonly used in distributed computing.<br/>It allows you to run a procedure on a remote system.<br/><i>gRPC</i> is a remote procedure call library developed internally at Google in<br/>2015 and later released as an open standard. Its use at Google alone would<br/>be enough to merit inclusion in our discussion. Many Google services, such<br/>as Google Ads and GCP, offer gRPC APIs. gRPC is built around the<br/>Protocol Buffers open data serialization standard, also developed by<br/>Google.<br/>gRPC emphasizes the efficient bidirectional exchange of data over HTTP/2.<br/><i>Efficiency</i> refers to aspects such as CPU utilization, power consumption,<br/>battery life, and bandwidth. Like GraphQL, gRPC imposes much more</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>specific technical standards than REST, thus allowing the use of common<br/>client libraries and allowing engineers to develop a skill set that will apply<br/>to any gRPC interaction code.<br/></p>
<p><b>Data Sharing<br/></b>The core concept of cloud data sharing is that a multitenant system supports<br/>security policies for sharing data among tenants. Concretely, any public<br/>cloud object storage system with a fine-grained permission system can be a<br/>platform for data sharing. Popular cloud data-warehouse platforms also<br/>support data-sharing capabilities. Of course, data can also be shared through<br/>download or exchange over email, but a multitenant system makes the<br/>process much easier.<br/>Many modern sharing platforms (especially cloud data warehouses) support<br/>row, column, and sensitive data filtering. Data sharing also streamlines the<br/>notion of the <i>data marketplace</i>, available on several popular clouds and<br/>data platforms. Data marketplaces provide a centralized location for data<br/>commerce, where data providers can advertise their offerings and sell them<br/>without worrying about the details of managing network access to data<br/>systems.<br/>Data sharing can also streamline data pipelines within an organization. Data<br/>sharing allows units of an organization to manage their data and selectively<br/>share it with other units while still allowing individual units to manage their<br/>compute and query costs separately, facilitating data decentralization. This<br/>facilitates decentralized data management patterns such as data mesh.<br/>Data sharing and data mesh align closely with our philosophy of common<br/>architecture components. Choose common components (Chapter 3) that<br/>allow the simple and efficient interchange of data and expertise rather than<br/>embracing the most exciting and sophisticated technology.<br/></p>
<p><b>Third-Party Data Sources<br/></b>The consumerization of technology means every company is essentially<br/>now a technology company. The consequence is that these companies&#8212;and<br/></p>
<p>6</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>increasingly government agencies&#8212;want to make their data available to<br/>their customers and users, either as part of their service or as a separate<br/>subscription. For example, the United States Bureau of Labor Statistics<br/>publishes various statistics about the US labor market. The National<br/>Aeronautics and Space Administration (NASA) publishes various data from<br/>its research initiatives. Facebook shares data with businesses that advertise<br/>on its platform.<br/>Why would companies want to make their data available? Data is sticky,<br/>and a flywheel is created by allowing users to integrate and extend their<br/>application into a user&#8217;s application. Greater user adoption and usage means<br/>more data, which means users can integrate more data into their<br/>applications and data systems. The side effect is there are now almost<br/>infinite sources of third-party data.<br/>Direct third-party data access is commonly done via APIs, through data<br/>sharing on a cloud platform, or through data download. APIs often provide<br/>deep integration capabilities, allowing customers to pull and push data. For<br/>example, many CRMs offer APIs that their users can integrate into their<br/>systems and applications. We see a common workflow to get data from a<br/>CRM, blend the CRM data through the customer scoring model, and then<br/>use reverse ETL to send that data back into CRM for salespeople to contact<br/>better-qualified leads.<br/></p>
<p><b>Message Queues and Event-Streaming Platforms<br/></b>Event-driven architectures are pervasive in software applications and are<br/>poised to grow their popularity even further. First, message queues and<br/>event-streaming platforms&#8212;critical layers in event-driven architectures&#8212;<br/>are easier to set up and manage in a cloud environment. Second, the rise of<br/>data apps&#8212;applications that directly integrate real-time analytics&#8212;are<br/>growing from strength to strength. Event-driven architectures are ideal in<br/>this setting because events can both trigger work in the application and feed<br/>near real-time analytics.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Please note that streaming data (in this case, messages and streams) cuts<br/>across many data engineering lifecycle stages. Unlike an RDBMS, which is<br/>often directly attached to an application, the lines of streaming data are<br/>sometimes less clear-cut. These systems are used as source systems, but<br/>they will often cut across the data engineering lifecycle because of their<br/>transient nature. For example, you can use an event-streaming platform for<br/>message passing in an event-driven application, a source system. The same<br/>event-streaming platform can be used in the ingestion and transformation<br/>stage to process data for real-time analytics.<br/>As source systems, message queues and event-streaming platforms are used<br/>in numerous ways, from routing messages between microservices ingesting<br/>millions of events per second of event data from web, mobile, and IoT<br/>applications. Let&#8217;s look at message queues and event-streaming platforms a<br/>bit more closely.<br/><b>Message queues<br/></b>A <i>message queue</i> is a mechanism to asynchronously send data (usually as<br/>small individual messages, in the kilobytes) between discrete systems using<br/>a publish and subscribe model. Data is published to a message queue and is<br/>delivered to one or more subscribers (Figure 5-9). The subscriber<br/>acknowledges receipt of the message, removing it from the queue.<br/></p>
<p><i>Figure 5-9. A simple message queue<br/></i></p>
<p>Message queues allow applications and systems to be decoupled from each<br/>other and are widely used in microservices architectures. The message<br/>queue buffers messages to handle transient load spikes and makes messages<br/>durable through a distributed architecture with replication.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Message queues are a critical ingredient for decoupled microservices and<br/>event-driven architectures. Some things to keep in mind with message<br/>queues are frequency of delivery, message ordering, and scalability.<br/><i>Message ordering and delivery<br/></i>The order in which messages are created, sent, and received can<br/>significantly impact downstream subscribers. In general, order in<br/>distributed messaging queues is a tricky problem. Message queues often<br/>apply a fuzzy notion of order and first in, first out (FIFO). Strict FIFO<br/>means that if message A is ingested before message B, message A will<br/>always be delivered before message B. In practice, messages might be<br/>published and received out of order, especially in highly distributed<br/>message systems.<br/>For example, Amazon SQS standard queues make the best effort to preserve<br/>message order. SQS also offers FIFO queues, which offer much stronger<br/>guarantees at the cost of extra overhead.<br/>In general, don&#8217;t assume that your messages will be delivered in order<br/>unless your message queue technology guarantees it. You typically need to<br/>design for out-of-order message delivery.<br/><i>Delivery frequency<br/></i>Messages can be sent exactly once or at least once. If a message is sent<br/><i>exactly once</i>, then after the subscriber acknowledges the message, the<br/>message disappears and won&#8217;t be delivered again. Messages sent <i>at least<br/>once</i> can be consumed by multiple subscribers or by the same subscriber<br/>more than once. This is great when duplications or redundancy don&#8217;t matter.<br/>Ideally, systems should be <i>idempotent</i>. In an idempotent system, the<br/>outcome of processing a message once is identical to the outcome of<br/>processing it multiple times. This helps to account for a variety of subtle<br/>scenarios. For example, even if our system can guarantee exactly-once<br/>delivery, a consumer might fully process a message but fail right before<br/>acknowledging processing. The message will effectively be processed<br/>twice, but an idempotent system handles this scenario gracefully.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><i>Scalability<br/></i>The most popular message queues utilized in event-driven applications are<br/>horizontally scalable, running across multiple servers. This allows these<br/>queues to scale up and down dynamically, buffer messages when systems<br/>fall behind, and durably store messages for resilience against failure.<br/>However, this can create a variety of complications, as mentioned<br/>previously (multiple deliveries and fuzzy ordering).<br/><b>Event-streaming platforms<br/></b>In some ways, an <i>event-streaming platform</i> is a continuation of a message<br/>queue in that messages are passed from producers to consumers. As<br/>discussed previously in this chapter, the big difference between messages<br/>and streams is that a message queue is primarily used to route messages<br/>with certain delivery guarantees. In contrast, an event-streaming platform is<br/>used to ingest and process data in an ordered log of records. In an event-<br/>streaming platform, data is retained for a while, and it is possible to replay<br/>messages from a past point in time.<br/>Let&#8217;s describe an event related to an event-streaming platform. As<br/>mentioned in Chapter 3, an event is &#8220;something that happened, typically a<br/>change in the <i>state</i> of something.&#8221; An event has the following features: a<br/>key, a value, and a timestamp. Multiple key-value timestamps might be<br/>contained in a single event. For example, an event for an ecommerce order<br/>might look like this:<br/></p>
<p>Key: &quot;Order # 12345&quot; <br/>Value: &quot;SKU 123, purchase price of $100&quot; <br/>Times<b>ta</b>mp: &quot;2023-01-02 06:01:00&quot;<br/></p>
<p>Let&#8217;s look at some of the critical characteristics of an event-streaming<br/>platform that you should be aware of as a data engineer.<br/><i>Topics<br/></i>In an event-streaming platform, a producer streams events to a topic, a<br/>collection of related events. A topic might contain fraud alerts, customer</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>orders, or temperature readings from IoT devices, for example. A topic can<br/>have zero, one, or multiple producers and customers on most event-<br/>streaming platforms.<br/>Using the preceding event example, a topic might be web orders. Also,<br/>let&#8217;s send this topic to a couple of consumers, such as fulfillment and<br/>marketing. This is an excellent example of blurred lines between analytics<br/>and an event-driven system. The fulfillment subscriber will use events to<br/>trigger a fulfillment process, while marketing runs real-time analytics to<br/>tune marketing campaigns (Figure 5-10).<br/></p>
<p><i>Figure 5-10. An order-processing system generates events (small rectangles) and publishes them to<br/>the web orders topic. Two subscribers&#8212;marketing and fulfillment&#8212;pull events from the topic.<br/></i></p>
<p><i>Stream partitions<br/>Stream partitions</i> are subdivisions of a stream into multiple streams. A good<br/>analogy is a multilane freeway. Having multiple lanes allows for parallelism<br/>and higher throughput. Messages are distributed across partitions by<br/><i>partition key</i>. Messages with the same partition key will always end up in<br/>the same partition.<br/>In Figure 5-11, for example, each message has a numeric ID&#8212; shown<br/>inside the circle representing the message&#8212;that we use as a partition key.<br/>To determine the partition, we divide by 3 and take the remainder. Going<br/>from bottom to top, the partitions have remainder 0, 1, and 2, respectively.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><i>Figure 5-11. An incoming message stream broken into three partitions<br/></i></p>
<p>Set a partition key so that messages that should be processed together have<br/>the same partition key. For example, it is common in IoT settings to want to<br/>send all messages from a particular device to the same processing server.<br/>We can achieve this by using a device ID as the partition key, and then<br/>setting up one server to consume from each partition.<br/>A key concern with stream partitioning is ensuring that your partition key<br/>does not generate <i>hotspotting</i>&#8212;a disproportionate number of messages<br/>delivered to one partition. For example, if each IoT device were known to<br/>be located in a particular US state, we might use the state as the partition<br/>key. Given a device distribution proportional to state population, the<br/>partitions containing California, Texas, Florida, and New York might be<br/>overwhelmed, with other partitions relatively underutilized. Ensure that<br/>your partition key will distribute messages evenly across partitions.<br/><i>Fault tolerance and resilience<br/></i>Event-streaming platforms are typically distributed systems, with streams<br/>stored on various nodes. If a node goes down, another node replaces it, and<br/>the stream is still accessible. This means records aren&#8217;t lost; you may<br/>choose to delete records, but that&#8217;s another story. This fault tolerance and<br/>resilience make streaming platforms a good choice when you need a system<br/>that can reliably produce, store, and ingest event data.<br/></p>
<p><b>Whom You&#8217;ll Work With</b></p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>When accessing source systems, it&#8217;s essential to understand the people with<br/>whom you&#8217;ll work. In our experience, good diplomacy and relationships<br/>with the stakeholders of source systems are an underrated and crucial part<br/>of successful data engineering.<br/>Who are these stakeholders? Typically, you&#8217;ll deal with two categories of<br/>stakeholders: systems and data stakeholders (Figure 5-12). A <i>systems<br/>stakeholder</i> builds and maintains the source systems; these might be<br/>software engineers, application developers, and third parties. Data<br/>stakeholders own and control access to the data you want, generally<br/>handled by IT, a data governance group, or third parties. The systems and<br/>data stakeholders are often different people or teams; sometimes, they are<br/>the same.<br/></p>
<p><i>Figure 5-12. The data engineer&#8217;s upstream stakeholders<br/></i></p>
<p>You&#8217;re often at the mercy of the stakeholder&#8217;s ability to follow correct<br/>software engineering, database management, and development practices.<br/>Ideally, the stakeholders are doing DevOps and working in an agile manner.<br/>We suggest creating a feedback loop between data engineers and<br/>stakeholders of the source systems to create awareness of how data is<br/>consumed and used. This is among the single most overlooked areas where<br/>data engineers can get a lot of value. When something happens to the<br/>upstream source data&#8212;and something will happen, whether it&#8217;s a schema or<br/>data change, a failed server or database, or other important events&#8212;you<br/>want to make sure that you&#8217;re made aware of the impact these issues will<br/>have on your data engineering systems.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>It might help to have a data contract in place with your upstream source<br/>system owners. What is a data contract? James Denmore offers this<br/>definition:<br/></p>
<p><i>A data contract is a written agreement between the owner of a source<br/>system and the team ingesting data from that system for use in a data<br/>pipeline. The contract should state what data is being extracted, via what<br/>method (full, incremental), how often, as well as who (person, team) are<br/>the contacts for both the source system and the ingestion. Data contracts<br/>should be stored in a well-known and easy-to-find location such as a<br/>GitHub repo or internal documentation site. If possible, format data<br/>contracts in a standardized form so they can be integrated into the<br/>development process or queried programmatically.<br/></i></p>
<p>In addition, consider establishing an SLA with upstream providers. An SLA<br/>provides expectations of what you can expect from the source systems you<br/>rely upon. An example of an SLA might be &#8220;data from source systems will<br/>be reliably available and of high quality.&#8221; A service-level objective (SLO)<br/>measures performance against what you&#8217;ve agreed to in the SLA. For<br/>example, given your example SLA, an SLO might be &#8220;source systems will<br/>have 99% uptime.&#8221; If a data contract or SLA/SLO seems too formal, at least<br/>verbally set expectations for source system guarantees for uptime, data<br/>quality, and anything else of importance to you. Upstream owners of source<br/>systems need to understand your requirements so they can provide you with<br/>the data you need.<br/></p>
<p><b>Undercurrents and Their Impact on Source<br/>Systems<br/></b>Unlike other parts of the data engineering lifecycle, source systems are<br/>generally outside the control of the data engineer. There&#8217;s an implicit<br/>assumption (some might call it <i>hope</i>) that the stakeholders and owners of<br/>the source systems&#8212;and the data they produce&#8212;are following best<br/>practices concerning data management, DataOps (and DevOps), DODD<br/>(mentioned in Chapter 2) data architecture, orchestration, and software<br/></p>
<p><i>7</i></p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>engineering. The data engineer should get as much upstream support as<br/>possible to ensure that the undercurrents are applied when data is generated<br/>in source systems. Doing so will make the rest of the steps in the data<br/>engineering lifecycle proceed a lot more smoothly.<br/>How do the undercurrents impact source systems? Let&#8217;s have a look.<br/></p>
<p><b>Security<br/></b>Security is critical, and the last thing you want is to accidentally create a<br/>point of vulnerability in a source system. Here are some areas to consider:<br/></p>
<p>Is the source system architected so data is secure and encrypted, both<br/>with data at rest and while data is transmitted?<br/>Do you have to access the source system over the public internet, or<br/>are you using a virtual private network (VPN)?<br/>Keep passwords, tokens, and credentials to the source system securely<br/>locked away. For example, if you&#8217;re using Secure Shell (SSH) keys,<br/>use a key manager to protect your keys; the same rule applies to<br/>passwords&#8212;use a password manager or a single sign-on (SSO)<br/>provider.<br/>Do you trust the source system? Always be sure to trust but verify that<br/>the source system is legitimate. You don&#8217;t want to be on the receiving<br/>end of data from a malicious actor.<br/></p>
<p><b>Data Management<br/></b>Data management of source systems is challenging for data engineers. In<br/>most cases, you will have only peripheral control&#8212;if any control at all&#8212;<br/>over source systems and the data they produce. To the extent possible, you<br/>should understand the way data is managed in source systems since this will<br/>directly influence how you ingest, store, and transform the data.<br/>Here are some areas to consider:</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><i>Data governance<br/></i>Are upstream data and systems governed in a reliable, easy-to-<br/>understand fashion? Who manages the data?<br/></p>
<p><i>Data quality<br/></i>How do you ensure data quality and integrity in upstream systems?<br/>Work with source system teams to set expectations on data and<br/>communication.<br/></p>
<p><i>Schema<br/></i>Expect that upstream schemas will change. Where possible, collaborate<br/>with source system teams to be notified of looming schema changes.<br/></p>
<p><i>Master data management<br/></i>Is the creation of upstream records controlled by a master data<br/>management practice or system?<br/></p>
<p><i>Privacy and ethics<br/></i>Do you have access to raw data, or will the data be obfuscated? What<br/>are the implications of the source data? How long is it retained? Does it<br/>shift locations based on retention policies?<br/></p>
<p><i>Regulatory<br/></i>Based upon regulations, are you supposed to access the data?<br/></p>
<p><b>DataOps<br/></b>Operational excellence&#8212;DevOps, DataOps, MLOps, <i>X</i>Ops&#8212;should extend<br/>up and down the entire stack and support the data engineering and lifecycle.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>While this is ideal, it&#8217;s often not fully realized.<br/>Because you&#8217;re working with stakeholders who control both the source<br/>systems and the data they produce, you need to ensure that you can observe<br/>and monitor the uptime and usage of the source systems and respond when<br/>incidents occur. For example, when the application database you depend on<br/>for CDC exceeds its I/O capacity and needs to be rescaled, how will that<br/>affect your ability to receive data from this system? Will you be able to<br/>access the data, or will it be unavailable until the database is rescaled? How<br/>will this affect reports? In another example, if the software engineering<br/>team is continuously deploying, a code change may cause unanticipated<br/>failures in the application itself. How will the failure impact your ability to<br/>access the databases powering the application? Will the data be up-to-date?<br/>Set up a clear communication chain between data engineering and the teams<br/>supporting the source systems. Ideally, these stakeholder teams have<br/>incorporated DevOps into their workflow and culture. This will go a long<br/>way to accomplishing the goals of DataOps (a sibling of DevOps), to<br/>address and reduce errors quickly. As we mentioned earlier, data engineers<br/>need to weave themselves into the DevOps practices of stakeholders, and<br/>vice versa. Successful DataOps works when all people are on board and<br/>focus on making systems holistically work.<br/>A few DataOps considerations are as follows:<br/><i>Automation<br/></i></p>
<p>There&#8217;s the automation impacting the source system, such as code<br/>updates and new features. Then there&#8217;s the DataOps automation that<br/>you&#8217;ve set up for your data workflows. Does an issue in the source<br/>system&#8217;s automation impact your data workflow automation? If so,<br/>consider decoupling these systems so they can perform automation<br/>independently.<br/></p>
<p><i>Observability</i></p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>How will you know when there&#8217;s an issue with a source system, such as<br/>an outage or a data-quality issue? Set up monitoring for source system<br/>uptime (or use the monitoring created by the team that owns the source<br/>system). Set up checks to ensure that data from the source system<br/>conforms with expectations for downstream usage. For example, is the<br/>data of good quality? Is the schema conformant? Are customer records<br/>consistent? Is data hashed as stipulated by the internal policy?<br/></p>
<p><i>Incident response<br/></i>What&#8217;s your plan if something bad happens? For example, how will<br/>your data pipeline behave if a source system goes offline? What&#8217;s your<br/>plan to backfill the &#8220;lost&#8221; data once the source system is back online?<br/></p>
<p><b>Data Architecture<br/></b>Similar to data management, unless you&#8217;re involved in the design and<br/>maintenance of the source system architecture, you&#8217;ll have little impact on<br/>the upstream source system architecture. You should also understand how<br/>the upstream architecture is designed, and its strengths and weaknesses.<br/>Talk often with the teams responsible for the source systems to understand<br/>the factors discussed in this section and ensure that their systems can meet<br/>your expectations. Knowing where the architecture performs well and<br/>where it doesn&#8217;t will impact how you design your data pipeline.<br/>Here are some things to consider regarding source system architectures:<br/><i>Reliability<br/></i></p>
<p>All systems suffer from entropy at some point, and outputs will drift<br/>from what&#8217;s expected. Bugs are introduced, and random glitches<br/>happen. Does the system produce predictable outputs? How often can</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>we expect the system to fail? What&#8217;s the mean time to repair to get the<br/>system back to sufficient reliability?<br/></p>
<p><i>Durability<br/></i>Everything fails. A server might die, a cloud&#8217;s zone or region could go<br/>offline, or other issues may arise. You need to account for how an<br/>inevitable failure or outage will affect your managed data systems. How<br/>does the source system handle data loss from hardware failures or<br/>network outages? What&#8217;s the plan for handling outages for an extended<br/>period and limiting the blast radius of an outage?<br/></p>
<p><i>Availability<br/></i>What guarantees that the source system is up, running, and available<br/>when it&#8217;s supposed to be?<br/></p>
<p><i>People<br/></i>Who&#8217;s in charge of the source system&#8217;s design, and how will you know<br/>if breaking changes are made in the architecture? A data engineer needs<br/>to work with the teams who maintain the source systems and ensure that<br/>these systems are architected reliably. Create an SLA with the source<br/>system team to set expectations about potential system failure.<br/></p>
<p><b>Orchestration<br/></b>When orchestrating within your data engineering workflow, you&#8217;ll<br/>primarily be concerned with making sure your orchestration can access the<br/>source system, which requires the correct network access, authentication,<br/>and authorization.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Here are some things to think about concerning orchestration for source<br/>systems:<br/><i>Cadence and frequency<br/></i></p>
<p>Is the data available on a fixed schedule, or can you access new data<br/>whenever you want?<br/></p>
<p><i>Common frameworks<br/></i>Do the software and data engineers use the same container manager,<br/>such as Kubernetes? Would it make sense to integrate application and<br/>data workloads into the same Kubernetes cluster? If you&#8217;re using an<br/>orchestration framework like Airflow, does it make sense to integrate it<br/>with the upstream application team? There&#8217;s no correct answer here, but<br/>you need to balance the benefits of integration with the risks of tight<br/>coupling.<br/></p>
<p><b>Software Engineering<br/></b>As the data landscape shifts to tools that simplify and automate access to<br/>source systems, you&#8217;ll likely need to write code. Here are a few<br/>considerations when writing code to access a source system:<br/><i>Networking<br/></i></p>
<p>Make sure your code will be able to access the network where the<br/>source system resides. Also, always think about secure networking. Are<br/>you accessing an HTTPS URL over the public internet, SSH, or a VPN?<br/></p>
<p><i>Authentication and authorization<br/></i>Do you have the proper credentials (tokens, username/passwords) to<br/>access the source system? Where will you store these credentials so they</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>don&#8217;t appear in your code or version control? Do you have the correct<br/>IAM roles to perform the coded tasks?<br/></p>
<p><i>Access patterns<br/></i>How are you accessing the data? Are you using an API, and how are<br/>you handling REST/GraphQL requests, response data volumes, and<br/>pagination? If you&#8217;re accessing data via a database driver, is the driver<br/>compatible with the database you&#8217;re accessing? For either access<br/>pattern, how are things like retries and timeouts handled?<br/></p>
<p><i>Orchestration<br/></i>Does your code integrate with an orchestration framework, and can it be<br/>executed as an orchestrated workflow?<br/></p>
<p><i>Parallelization<br/></i>How are you managing and scaling parallel access to source systems?<br/></p>
<p><i>Deployment<br/></i>How are you handling the deployment of source code changes?<br/></p>
<p><b>Conclusion<br/></b>Source systems and their data are vital in the data engineering lifecycle.<br/>Data engineers tend to treat source systems as &#8220;someone else&#8217;s problem&#8221;&#8212;<br/>do this at your peril! Data engineers who abuse source systems may need to<br/>look for another job when production goes down.<br/>If there&#8217;s a stick, there&#8217;s also a carrot. Better collaboration with source<br/>system teams can lead to higher-quality data, more successful outcomes,<br/>and better data products. Create a bidirectional flow of communications<br/>with your counterparts on these teams; set up processes to notify of schema</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>and application changes that affect analytics and ML. Communicate your<br/>data needs proactively to assist application teams in the data engineering<br/>process.<br/>Be aware that the integration between data engineers and source system<br/>teams is growing. One example is reverse ETL, which has long lived in the<br/>shadows but has recently risen into prominence. We also discussed that the<br/>event-streaming platform could serve a role in event-driven architectures<br/>and analytics; a source system can also be a data engineering system. Build<br/>shared systems where it makes sense to do so.<br/>Look for opportunities to build user-facing data products. Talk to<br/>application teams about analytics they would like to present to their users or<br/>places where ML could improve the user experience. Make application<br/>teams stakeholders in data engineering, and find ways to share your<br/>successes.<br/>Now that you understand the types of source systems and the data they<br/>generate, we&#8217;ll next look at ways to store this data.<br/></p>
<p><b>Additional Resources<br/></b>&#8220;The What, Why, and When of Single-Table Design with DynamoDB&#8221;<br/>by Alex DeBrie<br/>&#8220;Test Data Quality at Scale with Deequ&#8221; by Dustin Lange et al.<br/>&#8220;Modernizing Business Data Indexing&#8221; by Benjamin Douglas and<br/>Mohammad Mohtasham<br/>Confluent&#8217;s &#8220;Schema Evolution and Compatibility&#8221; documentation<br/>&#8220;NoSQL: What&#8217;s in a Name&#8221; by Eric Evans<br/>&#8220;The Log: What Every Software Engineer Should Know About Real-<br/>Time Data&#8217;s Unifying Abstraction&#8221; by ay Kreps</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><i>Database System Concepts</i> by Abraham (Avi) Silberschatz et al.<br/>(McGraw Hill)<br/><i>Database Internals</i> by Alex Petrov (O&#8217;Reilly)<br/></p>
<p>1  Keith D. Foote, &#8220;A Brief History of Non-Relational Databases,&#8221; Dataversity, June 19, 2018,<br/><i>https://oreil.ly/5Ukg2</i>.<br/></p>
<p>2  Nemil Dalal&#8217;s excellent series on the history of MongoDB recounts some harrowing tales of<br/>database abuse and its consequences for fledgling startups.<br/></p>
<p>3  Martin Kleppmann, <i>Designing Data-Intensive Applications</i> (Sebastopol, CA: O&#8217;Reilly, 2017),<br/>49, <i>https://oreil.ly/v1NhG</i>.<br/></p>
<p>4  Aashish Mehra, &#8220;Graph Database Market Worth $5.1 Billion by 2026: Exclusive Report by<br/>MarketsandMarkets,&#8221; Cision PR Newswire, July 30, 2021, <i>https://oreil.ly/mGVkY</i>.<br/></p>
<p>5  For one example, see Michael S. Mikowski, &#8220;RESTful APIs: The Big Lie,&#8221; August 10, 2015,<br/><i>https://oreil.ly/rqja3</i>.<br/></p>
<p>6  Martin Fowler, &#8220;How to Move Beyond a Monolithic Data Lake to a Distributed Data Mesh,&#8221;<br/>MartinFowler.com, May 20, 2019, <i>https://oreil.ly/TEdJF</i>.<br/></p>
<p>7  Read <i>Data Pipelines Pocket Reference</i> (O&#8217;Reilly) for more information on how a data<br/>contract should be written.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><b>Chapter 6. Storage<br/></b>Storage is the cornerstone of the data engineering lifecycle (Figure 6-1) and<br/>underlies its major stages&#8212;ingestion, transformation, and serving. Data<br/>gets stored many times as it moves through the lifecycle. To paraphrase an<br/>old saying, it&#8217;s storage all the way down. Whether data is needed seconds,<br/>minutes, days, months, or years later, it must persist in storage until systems<br/>are ready to consume it for further processing and transmission. Knowing<br/>the use case of the data and the way you will retrieve it in the future is the<br/>first step to choosing the proper storage solutions for your data architecture.<br/></p>
<p><i>Figure 6-1. Storage plays a central role in the data engineering lifecycle<br/></i></p>
<p>We also discussed storage in Chapter 5, but with a difference in focus and<br/>domain of control. Source systems are generally not maintained or<br/>controlled by data engineers. The storage that data engineers handle<br/>directly, which we&#8217;ll focus on in this chapter, encompasses the data<br/>engineering lifecycle stages of ingesting data from source systems to<br/>serving data to deliver value with analytics, data science, etc. Many forms<br/>of storage undercut the entire data engineering lifecycle in some fashion.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>To understand storage, we&#8217;re going to start by studying the <i>raw ingredients<br/></i>that compose storage systems, including hard drives, solid state drives, and<br/>system memory (see Figure 6-2). It&#8217;s essential to understand the basic<br/>characteristics of physical storage technologies to assess the trade-offs<br/>inherent in any storage architecture. This section also discusses serialization<br/>and compression, key software elements of practical storage. (We defer a<br/>deeper technical discussion of serialization and compression to<br/>Appendix A.) We also discuss <i>caching</i>, which is critical in assembling<br/>storage systems.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><i>Figure 6-2. Raw ingredients, storage systems, and storage abstractions</i></p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Next, we&#8217;ll look at <i>storage systems</i>. In practice, we don&#8217;t directly access<br/>system memory or hard disks. These physical storage components exist<br/>inside servers and clusters that can ingest and retrieve data using various<br/>access paradigms.<br/>Finally, we&#8217;ll look at <i>storage abstractions</i>. Storage systems are assembled<br/>into a cloud data warehouse, a data lake, etc. When building data pipelines,<br/>engineers choose the appropriate abstractions for storing their data as it<br/>moves through the ingestion, transformation, and serving stages.<br/></p>
<p><b>Raw Ingredients of Data Storage<br/></b>Storage is so common that it&#8217;s easy to take it for granted. We&#8217;re often<br/>surprised by the number of software and data engineers who use storage<br/>every day but have little idea how it works behind the scenes or the trade-<br/>offs inherent in various storage media. As a result, we see storage used in<br/>some pretty...interesting ways. Though current managed services potentially<br/>free data engineers from the complexities of managing servers, data<br/>engineers still need to be aware of underlying components&#8217; essential<br/>characteristics, performance considerations, durability, and costs.<br/>In most data architectures, data frequently passes through magnetic storage,<br/>SSDs, and memory as it works its way through the various processing<br/>phases of a data pipeline. Data storage and query systems generally follow<br/>complex recipes involving distributed systems, numerous services, and<br/>multiple hardware storage layers. These systems require the right raw<br/>ingredients to function correctly.<br/>Let&#8217;s look at some of the raw ingredients of data storage: disk drives,<br/>memory, networking and CPU, serialization, compression, and caching.<br/></p>
<p><b>Magnetic Disk Drive<br/></b><i>Magnetic disks</i> utilize spinning platters coated with a ferromagnetic film<br/>(Figure 6-3). This film is magnetized by a read/write head during write<br/>operations to physically encode binary data. The read/write head detects the</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>magnetic field and outputs a bitstream during read operations. Magnetic<br/>disk drives have been around for ages. Still, they form the backbone of bulk<br/>data storage systems because they are significantly cheaper than SSDs per<br/>gigabyte of stored data.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><i>Figure 6-3. Magnetic disk head movement and rotation are essential in random access latency<br/></i></p>
<p>On the one hand, these disks have seen extraordinary improvements in<br/>performance, storage density, and cost.  On the other hand, SSDs1</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>dramatically outperform magnetic disks on various metrics. Currently,<br/>commercial magnetic disk drives cost roughly 3 cents per gigabyte of<br/>capacity. (Note that we&#8217;ll frequently use the abbreviations <i>HDD</i> and <i>SSD</i> to<br/>denote rotating magnetic disk and solid-state drives, respectively.)<br/>IBM developed magnetic disk drive technology in the 1950s. Since then,<br/>magnetic disk capacities have grown steadily. The first commercial<br/>magnetic disk drive, the IBM 350, had a capacity of 3.75 megabytes. As of<br/>this writing, magnetic drives storing 20 TB are commercially available. In<br/>fact, magnetic disks continue to see rapid innovation, with methods such as<br/>heat-assisted magnetic recording (HAMR), shingled magnetic recording<br/>(SMR), and helium-filled disk enclosures being used to realize ever greater<br/>storage densities. In spite of the continuing improvements in drive capacity,<br/>other aspects of HDD performance are hampered by physics.<br/>First, <i>disk transfer speed</i>, the rate at which data can be read and written,<br/>does not scale in proportion with disk capacity. Disk capacity scales with<br/><i>areal density</i> (gigabits stored per square inch), whereas transfer speed scales<br/>with <i>linear density</i> (bits per inch). This means that if disk capacity grows by<br/>a factor of 4, transfer speed increases by only a factor of 2. Consequently,<br/>current data center drives support maximum data transfer speeds of<br/>200&#8211;-300 MB/s. To frame this another way, it takes more than 20 hours to<br/>read the entire contents of a 30 TB magnetic drive, assuming a transfer<br/>speed of 300 MB/s.<br/>A second major limitation is seek time. To access data, the drive must<br/>physically relocate the read/write heads to the appropriate track on the disk.<br/>Third, in order to find a particular piece of data on the disk, the disk<br/>controller must wait for that data to rotate under the read/write heads. This<br/>leads to <i>rotational latency</i>. Typical commercial drives spinning at 7,200<br/>revolutions per minute (RPM) seek time, and rotational latency, leads to<br/>over four milliseconds of overall average latency (time to access a selected<br/>piece of data). A fourth limitation is input/output operations per second<br/>(IOPS), critical for transactional databases. A magnetic drive ranges from<br/>50 to 500 IOPS.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Various tricks can improve latency and transfer speed. Using a higher<br/>rotational speed can increase transfer rate and decrease rotational latency.<br/>Limiting the radius of the disk platter or writing data into only a narrow<br/>band on the disk reduces seek time. However, none of these techniques<br/>makes magnetic drives remotely competitive with SSDs for random access<br/>lookups. SSDs can deliver data with significantly lower latency, higher<br/>IOPS, and higher transfer speeds, partially because there is no physically<br/>rotating disk or magnetic head to wait for.<br/>As mentioned earlier, magnetic disks are still prized in data centers for their<br/>low data-storage costs. In addition, magnetic drives can sustain<br/>extraordinarily high transfer rates through parallelism. This is the critical<br/>idea behind cloud object storage: data can be distributed across thousands<br/>of disks in clusters. Data-transfer rates go up dramatically by reading from<br/>numerous disks simultaneously, limited primarily by network performance<br/>rather than disk transfer rate. Thus, network components and CPUs are also<br/>key raw ingredients in storage systems, and we will return to these topics<br/>shortly.<br/></p>
<p><b>Solid-State Drive<br/></b><i>Solid-state drives</i> (SSDs) store data as charges in flash memory cells. SSDs<br/>eliminate the mechanical components of magnetic drives; the data is read<br/>by purely electronic means. SSDs can look up random data in less than 0.1<br/>ms (100 microseconds). In addition, SSDs can scale both data-transfer<br/>speeds and IOPS by slicing storage into partitions with numerous storage<br/>controllers running in parallel. Commercial SSDs can support transfer<br/>speeds of many gigabytes per second and tens of thousands of IOPS.<br/>Because of these exceptional performance characteristics, SSDs have<br/>revolutionized transactional databases and are the accepted standard for<br/>commercial deployments of OLTP systems. SSDs allow relational<br/>databases such as PostgreSQL, MySQL, and SQL Server to handle<br/>thousands of transactions per second.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>However, SSDs are not currently the default option for high-scale analytics<br/>data storage. Again, this comes down to cost. Commercial SSDs typically<br/>cost 20&#8211;30 cents (USD) per gigabyte of capacity, nearly 10 times the cost<br/>per capacity of a magnetic drive. Thus, object storage on magnetic disks has<br/>emerged as the leading option for large-scale data storage in data lakes and<br/>cloud data warehouses.<br/>SSDs still play a significant role in OLAP systems. Some OLAP databases<br/>leverage SSD caching to support high-performance queries on frequently<br/>accessed data. As low-latency OLAP becomes more popular, we expect<br/>SSD usage in these systems to follow suit.<br/></p>
<p><b>Random Access Memory<br/></b>We commonly use the terms <i>random access memory</i> (RAM) and <i>memory<br/></i>interchangeably. Strictly speaking, magnetic drives and SSDs also serve as<br/>memory that stores data for later random access retrieval, but RAM has<br/>several specific characteristics:<br/></p>
<p>Is attached to a CPU and mapped into CPU address space.<br/>Stores the code that CPUs execute and the data that this code directly<br/>processes.<br/>Is <i>volatile</i>, while magnetic drives and SSDs are <i>nonvolatile</i>. Though<br/>they may occasionally fail and corrupt or lose data, drives generally<br/>retain data when powered off. RAM loses data in less than a second<br/>when it is unpowered.<br/>Offers significantly higher transfer speeds and faster retrieval times<br/>than SSD storage. DDR5 memory&#8212;the latest widely used standard for<br/>RAM&#8212;offers data retrieval latency on the order of 100 ns, roughly<br/>1,000 times faster than SSD. A typical CPU can support 100 GB/s<br/>bandwidth to attached memory and millions of IOPS. (Statistics vary<br/>dramatically depending on the number of memory channels and other<br/>configuration details.)</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Is significantly more expensive than SSD storage, at roughly $10/GB<br/>(at the time of this writing).<br/>Is limited in the amount of RAM attached to an individual CPU and<br/>memory controller. This adds further to complexity and cost. High-<br/>memory servers typically utilize many interconnected CPUs on one<br/>board, each with a block of attached RAM.<br/>Is still significantly slower than CPU cache, a type of memory located<br/>directly on the CPU die or in the same package. Cache stores<br/>frequently and recently accessed data for ultrafast retrieval during<br/>processing. CPU designs incorporate several layers of cache of varying<br/>size and performance characteristics.<br/></p>
<p>When we talk about system memory, we almost always mean <i>dynamic<br/>RAM</i>, a high-density, low-cost form of memory. Dynamic RAM stores data<br/>as charges in capacitors. These capacitors leak over time, so the data must<br/>be frequently <i>refreshed</i> (read and rewritten) to prevent data loss. The<br/>hardware memory controller handles these technical details; data engineers<br/>simply need to worry about bandwidth and retrieval latency characteristics.<br/>Other forms of memory, such as <i>static RAM</i>, are used in specialized<br/>applications such as CPU caches.<br/>Current CPUs virtually always employ the <i>von Neumann architecture</i>, with<br/>code and data stored together in the same memory space. However, CPUs<br/>typically also support the option to disable code execution in specific pages<br/>of memory for enhanced security. This feature is reminiscent of the<br/><i>Harvard architecture</i>, which separates code and data.<br/>RAM is used in various storage and processing systems and can be used for<br/>caching, data processing, or indexes. Several databases treat RAM as a<br/>primary storage layer, allowing ultra-fast read and write performance. In<br/>these applications, data engineers must always keep in mind the volatility of<br/>RAM. Even if data stored in memory is replicated across a cluster, a power<br/>outage that brings down several nodes could cause data loss. Architectures<br/>intended to durably store data may use battery backups and automatically<br/>dump all data to disk in the event of power loss.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><b>Networking and CPU<br/></b>Why are we mentioning networking and CPU as raw ingredients for storing<br/>data? Increasingly, storage systems are distributed to enhance performance,<br/>durability, and availability. We mentioned specifically that individual<br/>magnetic disks offer relatively low-transfer performance, but a cluster of<br/>disks parallelizes reads for significant performance scaling. While storage<br/>standards such as redundant arrays of independent disks (RAID) parallelize<br/>on a single server, cloud object storage clusters operate at a much larger<br/>scale, with disks distributed across a network and even multiple data centers<br/>and availability zones.<br/><i>Availability zones</i> are a standard cloud construct consisting of compute<br/>environments with independent power, water, and other resources.<br/>Multizonal storage enhances both the availability and durability of data.<br/>CPUs handle the details of servicing requests, aggregating reads, and<br/>distributing writes. Storage becomes a web application with an API,<br/>backend service components, and load balancing. Network device<br/>performance and network topology are key factors in realizing high<br/>performance.<br/>Data engineers need to understand how networking will affect the systems<br/>they build and use. Engineers constantly balance the durability and<br/>availability achieved by spreading out data geographically versus the<br/>performance and cost benefits of keeping storage in a small geographic area<br/>and close to data consumers or writers. Appendix B covers cloud<br/>networking and major relevant ideas.<br/></p>
<p><b>Serialization<br/></b><i>Serialization</i> is another raw storage ingredient and a critical element of<br/>database design. The decisions around serialization will inform how well<br/>queries perform across a network, CPU overhead, query latency, and more.<br/>Designing a data lake, for example, involves choosing a base storage<br/>system (e.g., Amazon S3) and standards for serialization that balance<br/>interoperability with performance considerations.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>What is serialization, exactly? Data stored in system memory by software is<br/>generally not in a format suitable for storage on disk or transmission over a<br/>network. Serialization is the process of flattening and packing data into a<br/>standard format that a reader will be able to decode. Serialization formats<br/>provide a standard of data exchange. We might encode data in a row-based<br/>manner as an XML, JSON, or CSV file and pass it to another user who can<br/>then decode it using a standard library. A serialization algorithm has logic<br/>for handling types, imposes rules on data structure, and allows exchange<br/>between programming languages and CPUs. The serialization algorithm<br/>also has rules for handling exceptions. For instance, Python objects can<br/>contain cyclic references; the serialization algorithm might throw an error<br/>or limit nesting depth on encountering a cycle.<br/>Low-level database storage is also a form of serialization. Row-oriented<br/>relational databases organize data as rows on disk to support speedy<br/>lookups and in-place updates. Columnar databases organize data into<br/>column files to optimize for highly efficient compression and support fast<br/>scans of large data volumes. Each serialization choice comes with a set of<br/>trade-offs, and data engineers tune these choices to optimize performance to<br/>requirements.<br/>We provide a more detailed catalog of common data serialization<br/>techniques and formats in Appendix A. We suggest that data engineers<br/>become familiar with common serialization practices and formats,<br/>especially the most popular current formats (e.g., Apache Parquet), hybrid<br/>serialization (e.g., Apache Hudi), and in-memory serialization (e.g., Apache<br/>Arrow).<br/></p>
<p><b>Compression<br/></b><i>Compression</i> is another critical component of storage engineering. On a<br/>basic level, compression makes data smaller, but compression algorithms<br/>interact with other details of storage systems in complex ways.<br/>Highly efficient compression has three main advantages in storage systems.<br/>First, the data is smaller and thus takes up less space on the disk. Second,</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>compression increases the practical scan speed per disk. With a 10:1<br/>compression ratio, we go from scanning 200 MB/s per magnetic disk to an<br/>effective rate of 2 GB/s per disk.<br/>The third advantage is in network performance. Given that a network<br/>connection between an Amazon EC2 instance and S3 provides 10 gigabits<br/>per second (Gbps) of bandwidth, a 10:1 compression ratio increases<br/>effective network bandwidth to 100 Gbps.<br/>Compression also comes with disadvantages. Compressing and<br/>decompressing data entails extra time and resource consumption to read or<br/>write data. We undertake a more detailed discussion of compression<br/>algorithms and trade-offs in Appendix A.<br/></p>
<p><b>Caching<br/></b>We&#8217;ve already mentioned caching in our discussion of RAM. The core idea<br/>of caching is to store frequently or recently accessed data in a fast access<br/>layer. The faster the cache, the higher the cost and the less storage space<br/>available. Less frequently accessed data is stored in cheaper, slower storage.<br/>Caches are critical for data serving, processing, and transformation.<br/>As we analyze storage systems, it is helpful to put every type of storage we<br/>utilize inside a <i>cache hierarchy</i> (Table 6-1). Most practical data systems<br/>rely on many cache layers assembled from storage with varying<br/>performance characteristics. This starts inside CPUs; processors may<br/>deploy up to four cache tiers. We move down the hierarchy to RAM and<br/>SSDs. Cloud object storage is a lower tier that supports long-term data<br/>retention and durability while allowing for data serving and dynamic data<br/>movement in pipelines.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p> <br/><i>T<br/>a<br/>b<br/>l<br/>e <br/>6<br/>-<br/>1<br/>. <br/>A<br/> <br/>h<br/>e<br/>u<br/>r<br/>i<br/>s<br/>ti<br/>c <br/>c<br/>a<br/>c<br/>h<br/>e <br/>h<br/>i<br/>e<br/>r<br/>a<br/>r<br/>c<br/>h<br/>y <br/>d</i></p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><i>i<br/>s<br/>p<br/>l<br/>a<br/>y<br/>i<br/>n<br/>g <br/>s<br/>t<br/>o<br/>r<br/>a<br/>g<br/>e <br/>t<br/>y<br/>p<br/>e<br/>s <br/>w<br/>it<br/>h <br/>a<br/>p<br/>p<br/>r<br/>o<br/>x<br/>i<br/>m<br/>a<br/>t<br/>e <br/>p</i></p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><i>r<br/>i<br/>c<br/>i<br/>n<br/>g <br/>a<br/>n<br/>d <br/>p<br/>e<br/>r<br/>f<br/>o<br/>r<br/>m<br/>a<br/>n<br/>c<br/>e <br/>c<br/>h<br/>a<br/>r<br/>a<br/>c<br/>t<br/>e<br/>r<br/>i<br/>s<br/>ti<br/>c<br/>s<br/></i> </p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><b>Storage type Data fetch latency Bandwidth Price<br/></b> <br/>CPU cache 1 nanosecond 1 TB/s<br/>RAM 0.1 microseconds 100 GB/s $10/GB<br/>SSD 0.1 milliseconds 4 GB/s $0.20/GB<br/>HDD 4 milliseconds 300 MB/s $0.03/GB<br/>Object storage 100 milliseconds 3 GB/s per instance $0.02/GB per <br/></p>
<p>month<br/>Archival storage 12 hours Same as object storage once data is <br/></p>
<p>available<br/>$0.004/GB per <br/>month<br/></p>
<p> <br/>a  A microsecond is 1,000 nanoseconds, and a millisecond is 1,000 microseconds.<br/></p>
<p>We can think of archival storage as a <i>reverse cache</i>. Archival storage<br/>provides inferior access characteristics for low costs. Archival storage is<br/>generally used for data backups and to meet data-retention compliance<br/>requirements. In typical scenarios, this data will be accessed only in an<br/>emergency (e.g., data in a database might be lost and need to be recovered,<br/>or a company might need to look back at historical data for legal<br/>discovery).<br/></p>
<p><b>Data Storage Systems<br/></b>This section covers the major data storage systems you&#8217;ll encounter as a<br/>data engineer. Storage systems exist at a level of abstraction above raw<br/>ingredients. For example, magnetic disks are a raw storage ingredient, while<br/>major cloud object storage platforms and HDFS are storage systems that<br/>utilize magnetic disks. Still higher levels of storage abstraction exist, such<br/>as data lakes and lakehouses (which we cover in &#8220;Data Engineering Storage<br/>Abstractions&#8221;).<br/></p>
<p><b>Single Machine Versus Distributed Storage<br/></b></p>
<p><b>a</b></p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>As data storage and access patterns become more complex and outgrow the<br/>usefulness of a single server, distributing data to more than one server<br/>becomes necessary. Data can be stored on multiple servers, known as<br/><i>distributed storage</i>. This is a distributed system whose purpose is to store<br/>data in a distributed fashion (Figure 6-4).<br/></p>
<p><i>Figure 6-4. Single machine versus distributed storage on multiple servers<br/></i></p>
<p>Distributed storage coordinates the activities of multiple servers to store,<br/>retrieve, and process data faster and at a larger scale, all while providing<br/>redundancy in case a server becomes unavailable. Distributed storage is<br/>common in architectures where you want built-in redundancy and<br/>scalability for large amounts of data. For example, object storage, Apache<br/>Spark, and cloud data warehouses rely on distributed storage architectures.<br/>Data engineers must always be aware of the consistency paradigms of the<br/>distributed systems, which we&#8217;ll explore next.<br/></p>
<p><b>Eventual Versus Strong Consistency<br/></b>A challenge with distributed systems is that your data is spread across<br/>multiple servers. How does this system keep the data consistent?<br/>Unfortunately, distributed systems pose a dilemma for storage and query<br/>accuracy. It takes time to replicate changes across the nodes of a system;<br/>often a balance exists between getting current data and getting &#8220;sorta&#8221;<br/>current data in a distributed database. Let&#8217;s look at two common<br/>consistency patterns in distributed systems: eventual and strong.<br/>We&#8217;ve covered ACID compliance throughout this book, starting in<br/>Chapter 5. Another acronym is <i>BASE</i>, which stands for <i>basically available,<br/>soft-state, eventual consistency</i>. Think of it as the opposite of ACID. BASE<br/>is the basis of eventual consistency. Let&#8217;s briefly explore its components:</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><i>Basically available<br/></i>Consistency is not guaranteed, but efforts at database reads and writes<br/>are made on a best-effort basis, meaning consistent data is available<br/>most of the time.<br/></p>
<p><i>Soft-state<br/></i>The state of the transaction is fuzzy, and it&#8217;s uncertain whether the<br/>transaction is committed or uncommitted.<br/></p>
<p><i>Eventual consistency<br/></i>At <i>some</i> point, reading data will return consistent values.<br/></p>
<p>If reading data in an eventually consistent system is unreliable, why use it?<br/>Eventual consistency is a common trade-off in large-scale, distributed<br/>systems. If you want to scale horizontally (across multiple nodes) to process<br/>data in high volumes, then eventually, consistency is often the price you&#8217;ll<br/>pay. Eventual consistency allows you to retrieve data quickly without<br/>verifying that you have the latest version across all nodes.<br/>The opposite of eventual consistency is <i>strong consistency</i>. With strong<br/>consistency, the distributed database ensures that writes to any node are first<br/>distributed with a consensus and that any reads against the database return<br/>consistent values. You&#8217;ll use strong consistency when you can tolerate<br/>higher query latency and require correct data every time you read from the<br/>database.<br/>Generally, data engineers make decisions about consistency in three places.<br/>First, the database technology itself sets the stage for a certain level of<br/>consistency. Second, configuration parameters for the database will have an<br/>impact on consistency. Third, databases often support some consistency<br/>configuration at an individual query level. For example, DynamoDB<br/>supports eventually consistent reads and strongly consistent reads. Strongly</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>consistent reads are slower and consume more resources, so it is best to use<br/>them sparingly, but they are available when consistency is required.<br/>You should understand how your database handles consistency. Again, data<br/>engineers are tasked with understanding technology deeply and using it to<br/>solve problems appropriately. A data engineer might need to negotiate<br/>consistency requirements with other technical and business stakeholders.<br/>Note that this is both a technology and organizational problem; ensure that<br/>you have gathered requirements from your stakeholders and choose your<br/>technologies appropriately.<br/></p>
<p><b>File Storage<br/></b>We deal with files every day, but the notion of a file is somewhat subtle. A<br/><i>file</i> is a data entity with specific read, write, and reference characteristics<br/>used by software and operating systems. We define a file to have the<br/>following characteristics:<br/><i>Finite length<br/></i></p>
<p>A file is a finite-length stream of bytes.<br/><i>Append operations<br/></i></p>
<p>We can append bytes to the file up to the limits of the host storage<br/>system.<br/></p>
<p><i>Random access<br/></i>We can read from any location in the file or write updates to any<br/>location.<br/></p>
<p><i>Object storage</i> behaves much like file storage but with key differences.<br/>While we set the stage for object storage by discussing file storage first,<br/>object storage is arguably much more important for the type of data</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>engineering you&#8217;ll do today. We will forward-reference the object storage<br/>discussion extensively over the next few pages.<br/>File storage systems organize files into a directory tree. The directory<br/>reference for a file might look like this:<br/></p>
<p>/Users/matthewhousley/output.txt<br/></p>
<p>When I provide this to the operating system, it starts at the root directory /,<br/>finds Users, matthewhousley, and finally output.txt. Working from the<br/>left, each directory is contained inside a parent directory, until we finally<br/>reach the file output.txt. This example uses Unix semantics, but<br/>Windows file reference semantics are similar. The filesystem stores each<br/>directory as metadata about the files and directories that it contains. This<br/>metadata consists of the name of each entity, relevant permission details,<br/>and a pointer to the actual entity. To find a file on disk, the operating system<br/>looks at the metadata at each hierarchy level and follows the pointer to the<br/>next subdirectory entity until finally reaching the file itself.<br/>Note that other file-like data entities generally don&#8217;t necessarily have all<br/>these properties. For example, <i>objects</i> in object storage support only the<br/>first characteristic, finite length, but are still extremely useful. We discuss<br/>this in &#8220;Object Storage&#8221;.<br/>In cases where file storage paradigms are necessary for a pipeline, be<br/>careful with state and try to use ephemeral environments as much as<br/>possible. Even if you must process files on a server with an attached disk,<br/>use object storage for intermediate storage between processing steps. Try to<br/>reserve manual, low-level file processing for one-time ingestion steps or the<br/>exploratory stages of pipeline development.<br/><b>Local disk storage<br/></b>The most familiar type of file storage is an operating system&#8211;managed<br/>filesystem on a local disk partition of SSD or magnetic disk. New<br/>Technology File System (NTFS) and ext4 are popular filesystems on<br/>Windows and Linux, respectively. The operating system handles the details</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>of storing directory entities, files, and metadata. Filesystems are designed to<br/>write data to allow for easy recovery in the event of power loss during a<br/>write, though any unwritten data will still be lost.<br/>Local filesystems generally support full read after write consistency;<br/>reading immediately after a write will return the written data. Operating<br/>systems also employ various locking strategies to manage concurrent<br/>writing attempts to a file.<br/>Local disk filesystems may also support advanced features such as<br/>journaling, snapshots, redundancy, the extension of the filesystem across<br/>multiple disks, full disk encryption, and compression. In &#8220;Block Storage&#8221;,<br/>we also discuss RAID.<br/><b>Network-attached storage<br/></b><i>Network-attached storage</i> (NAS) systems provide a file storage system to<br/>clients over a network. NAS is a prevalent solution for servers; they quite<br/>often ship with built-in dedicated NAS interface hardware. While there are<br/>performance penalties to accessing the filesystem over a network,<br/>significant advantages to storage virtualization also exist, including<br/>redundancy and reliability, fine-grained control of resources, storage<br/>pooling across multiple disks for large virtual volumes, and file sharing<br/>across multiple machines. Engineers should be aware of the consistency<br/>model provided by their NAS solution, especially when multiple clients will<br/>potentially access the same data.<br/>A popular alternative to NAS is a storage area network (SAN), but SAN<br/>systems provide block-level access without the filesystem abstraction. We<br/>cover SAN systems in &#8220;Block Storage&#8221;.<br/><b>Cloud file system services<br/></b>Cloud filesystem services provide a fully managed filesystem for use with<br/>multiple cloud VMs and applications, potentially including clients outside<br/>the cloud environment. Cloud filesystems should not be confused with<br/>standard storage attached to VMs&#8212;generally, block storage with a<br/>filesystem managed by the VM operating system. Cloud filesystems behave</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>much like NAS solutions, but the details of networking, managing disk<br/>clusters, failures, and configuration are fully handled by the cloud vendor.<br/>For example, Amazon Elastic File System (EFS) is an extremely popular<br/>example of a cloud filesystem service. Storage is exposed through the NFS<br/>4 protocol, which is also used by NAS systems. EFS provides automatic<br/>scaling and pay-per-storage pricing with no advanced storage reservation<br/>required. The service also provides <i>local</i> read-after-write consistency (when<br/>reading from the machine that performed the write). It also offers open-<br/>after-close consistency across the full filesystem. In other words, once an<br/>application closes a file, subsequent readers will see changes saved to the<br/>closed file.<br/></p>
<p><b>Block Storage<br/></b>Fundamentally, <i>block storage</i> is the type of raw storage provided by SSDs<br/>and magnetic disks. In the cloud, virtualized block storage is the standard<br/>for VMs. These block storage abstractions allow fine control of storage<br/>size, scalability, and data durability beyond that offered by raw disks.<br/>In our earlier discussion of SSDs and magnetic disks, we mentioned that<br/>with these random-access devices, the operating system can seek, read, and<br/>write any data on the disk. A <i>block</i> is the smallest addressable unit of data<br/>supported by a disk. This was often 512 bytes of usable data on older disks,<br/>but it has now grown to 4,096 bytes for most current disks, making writes<br/>less fine-grained but dramatically reducing the overhead of managing<br/>blocks. Blocks typically contain extra bits for error detection/correction and<br/>other metadata.<br/>Blocks on magnetic disks are geometrically arranged on a physical platter.<br/>Two blocks on the same track can be read without moving the head, while<br/>reading two blocks on separate tracks requires a seek. Seek time can occur<br/>between blocks on an SSD, but this is infinitesimal compared to the seek<br/>time for magnetic disk tracks.<br/><b>Block storage applications</b></p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Transactional database systems generally access disks at a block level to lay<br/>out data for optimal performance. For row-oriented databases, this<br/>originally meant that rows of data were written as continuous streams; the<br/>situation has grown more complicated with the arrival of SSDs and their<br/>associated seek-time performance improvements, but transactional<br/>databases still rely on the high random access performance offered by direct<br/>access to a block storage device.<br/>Block storage also remains the default option for operating system boot<br/>disks on cloud VMs. The block device is formatted much as it would be<br/>directly on a physical disk, but the storage is usually virtualized. (see<br/>&#8220;Cloud virtualized block storage&#8221;.)<br/><b>RAID<br/></b><i>RAID</i> stands for <i>redundant array of independent disks</i>, as noted previously.<br/>RAID simultaneously controls multiple disks to improve data durability,<br/>enhance performance, and combine capacity from multiple drives. An array<br/>can appear to the operating system as a single block device. Many encoding<br/>and parity schemes are available, depending on the desired balance between<br/>enhanced effective bandwidth and higher fault tolerance (tolerance for<br/>many disk failures).<br/><b>Storage area network<br/></b><i>Storage area network</i> (SAN) systems provide virtualized block storage<br/>devices over a network, typically from a storage pool. SAN abstraction can<br/>allow fine-grained storage scaling and enhance performance, availability,<br/>and durability. You might encounter SAN systems if you&#8217;re working with<br/>on-premises storage systems; you might also encounter a cloud version of<br/>SAN, as in the next subsection.<br/><b>Cloud virtualized block storage<br/></b><i>Cloud virtualized block storage</i> solutions are similar to SAN but free<br/>engineers from dealing with SAN clusters and networking details. We&#8217;ll<br/>look at Amazon Elastic Block Store (EBS) as a standard example; other</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>public clouds have similar offerings. EBS is the default storage for Amazon<br/>EC2 virtual machines; other cloud providers also treat virtualized object<br/>storage as a key component of their VM offerings.<br/>EBS offers several tiers of service with different performance<br/>characteristics. Generally, EBS performance metrics are given in IOPS and<br/>throughput (transfer speed). The higher performance tiers of EBS storage<br/>are backed by SSD disks, while magnetic disk-backed storage offers lower<br/>IOPS but costs less per gigabyte.<br/>EBS volumes store data separate from the instance host server but in the<br/>same zone to support high performance and low latency (Figure 6-5). This<br/>allows EBS volumes to persist when an EC2 instance shuts down, when a<br/>host server fails, or even when the instance is deleted. EBS storage is<br/>suitable for applications such as databases, where data durability is a high<br/>priority. In addition, EBS replicates all data to at least two separate host<br/>machines, protecting data if a disk fails.<br/></p>
<p><i>Figure 6-5. EBS volumes replicate data to multiple hosts and disks for high durability and<br/>availability, but are not resilient to the failure of an availability zone<br/></i></p>
<p>EBS storage virtualization also supports several advanced features. For<br/>example, EBS volumes allow instantaneous point-in-time snapshots while<br/>the drive is used. Although it still takes some time for the snapshot to be<br/>replicated to S3, EBS can effectively freeze the state of data blocks when<br/>the snapshot is taken, while allowing the client machine to continue using<br/>the disk. In addition, snapshots after the initial full backup are differential;<br/>only changed blocks are written to S3 to minimize storage costs and backup<br/>time.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>EBS volumes are also highly scalable. At the time of this writing, some<br/>EBS volume classes can scale up to 64 TiB, 256,000 IOPS, and 4,000<br/>MiB/s.<br/><b>Local instance volumes<br/></b>Cloud providers also offer block storage volumes that are physically<br/>attached to the host server running a virtual machine. These storage<br/>volumes are generally very low cost (included with the price of the VM in<br/>the case of Amazon&#8217;s EC2 instance store) and provide low latency and high<br/>IOPS.<br/>Instance store volumes (Figure 6-6) behave essentially like a disk<br/>physically attached to a server in a data center. One key difference is that<br/>when a VM shuts down or is deleted, the contents of the locally attached<br/>disk are lost, whether or not this event was caused by intentional user<br/>action. This ensures that a new virtual machine cannot read disk contents<br/>belonging to a different customer.<br/></p>
<p><i>Figure 6-6. Instance store volumes offer high performance and low cost but do not protect data in the<br/>event of disk failure or VM shutdown</i></p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Locally attached disks support none of the advanced virtualization features<br/>offered by virtualized storage services like EBS. The locally attached disk is<br/>not replicated, so a physical disk failure can lose or corrupt data even if the<br/>host VM continues running. Furthermore, locally attached volumes do not<br/>support snapshots or other backup features.<br/>Despite these limitations, locally attached disks are extremely useful. In<br/>many cases, we use disks as a local cache and hence don&#8217;t need all the<br/>advanced virtualization features of a service like EBS. For example,<br/>suppose we&#8217;re running AWS EMR on EC2 instances. We may be running<br/>an ephemeral job that consumes data from S3, stores it temporarily in the<br/>distributed filesystem running across the instances, processes the data, and<br/>writes the results back to S3. The EMR filesystem builds in replication and<br/>redundancy and is serving as a cache rather than permanent storage. The<br/>EC2 instance store is a perfectly suitable solution in this case and can<br/>enhance performance since data can be read and processed locally without<br/>flowing over a network (see Figure 6-7).<br/></p>
<p><i>Figure 6-7. Instance store volumes can be used as a processing cache in an ephemeral Hadoop<br/>cluster</i></p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>We recommend that engineers think about locally attached storage in worst-<br/>case scenarios. What are the consequences of a local disk failure? Of an<br/>accidental VM or cluster shutdown? Of a zonal or regional cloud outage? If<br/>none of these scenarios will have catastrophic consequences when data on<br/>locally attached volumes is lost, local storage may be a cost-effective and<br/>performant option. In addition, simple mitigation strategies (periodic<br/>checkpoint backups to S3) can prevent data loss.<br/></p>
<p><b>Object Storage<br/></b><i>Object storage</i> contains <i>objects</i> of all shapes and sizes (Figure 6-8). The<br/>term <i>object storage</i> is somewhat confusing because <i>object</i> has several<br/>meanings in computer science. In this context, we&#8217;re talking about a<br/>specialized file-like construct. It could be any type of file&#8212;TXT, CSV,<br/>JSON, images, videos, or audio.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><i>Figure 6-8. Object storage contains immutable objects of all shapes and sizes. Unlike files on a local<br/>disk, objects cannot be modified in place.<br/></i></p>
<p>Object stores have grown in importance and popularity with the rise of big<br/>data and the cloud. Amazon S3, Azure Blob Storage, and Google Cloud<br/>Storage (GCS) are widely used object stores. In addition, many cloud data<br/>warehouses (and a growing number of databases) utilize object storage as<br/>their storage layer, and cloud data lakes generally sit on object stores.<br/>Although many on-premises object storage systems can be installed on<br/>server clusters, we&#8217;ll focus mostly on fully managed cloud object stores.<br/>From an operational perspective, one of the most attractive characteristics<br/>of cloud object storage is that it is straightforward to manage and use.<br/>Object storage was arguably one of the first &#8220;serverless&#8221; services; engineers<br/>don&#8217;t need to consider the characteristics of underlying server clusters or<br/>disks.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>An object store is a key-value store for immutable data objects. We lose<br/>much of the writing flexibility we expect with file storage on a local disk in<br/>an object store. Objects don&#8217;t support random writes or append operations;<br/>instead, they are written once as a stream of bytes. After this initial write,<br/>objects become immutable. To change data in an object or append data to it,<br/>we must rewrite the full object. Object stores generally support random<br/>reads through range requests, but these lookups may perform much worse<br/>than random reads from data stored on an SSD.<br/>For a software developer used to leveraging local random access file<br/>storage, the characteristics of objects might seem like constraints, but less is<br/>more; object stores don&#8217;t need to support locks or change synchronization,<br/>allowing data storage across massive disk clusters. Object stores support<br/>extremely performant parallel stream writes and reads across many disks,<br/>and this parallelism is hidden from engineers, who can simply deal with the<br/>stream rather than communicating with individual disks. In a cloud<br/>environment, write speed scales with the number of streams being written<br/>up to quota limits set by the vendor. Read bandwidth can scale with the<br/>number of parallel requests, the number of virtual machines employed to<br/>read data, and the number of CPU cores. These characteristics make object<br/>storage ideal for serving high-volume web traffic or delivering data to<br/>highly parallel distributed query engines.<br/>Typical cloud object stores save data in several availability zones,<br/>dramatically reducing the odds that storage will go fully offline or be lost in<br/>an unrecoverable way. This durability and availability are built into the<br/>cost; cloud storage vendors offer other storage classes at discounted prices<br/>in exchange for reduced durability or availability. We&#8217;ll discuss this in<br/>&#8220;Storage classes and tiers&#8221;.<br/>Cloud object storage is a key ingredient in separating compute and storage,<br/>allowing engineers to process data with ephemeral clusters and scale these<br/>clusters up and down on demand. This is a key factor in making big data<br/>available to smaller organizations that can&#8217;t afford to own hardware for data<br/>jobs that they&#8217;ll run only occasionally. Some major tech companies will<br/>continue to run permanent Hadoop clusters on their hardware. Still, the</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>general trend is that most organizations will move data processing to the<br/>cloud, using an object store as essential storage and serving layer while<br/>processing data on ephemeral clusters.<br/>In object storage, available storage space is also highly scalable, an ideal<br/>characteristic for big data systems. Storage space is constrained by the<br/>number of disks the storage provider owns, but these providers handle<br/>exabytes of data. In a cloud environment, available storage space is<br/>virtually limitless; in practice, the primary limit on storage space for public<br/>cloud customers is budget. From a practical standpoint, engineers can<br/>quickly store massive quantities of data for projects without planning<br/>months in advance for necessary servers and disks.<br/><b>Object stores for data engineering applications<br/></b>From the standpoint of data engineering, object stores provide excellent<br/>performance for large batch reads and batch writes. This corresponds well<br/>to the use case for massive OLAP systems. A bit of data engineering<br/>folklore says that object stores are not good for updates, but this is only<br/>partially true. Object stores are an inferior fit for transactional workloads<br/>with many small updates every second; these use cases are much better<br/>served by transactional databases or block storage systems. Object stores<br/>work well for a low rate of update operations, where each operation updates<br/>a large volume of data.<br/>Object stores are now the gold standard of storage for data lakes. In the<br/>early days of data lakes, write once, read many (WORM) was the<br/>operational standard, but this had more to do with the complexities of<br/>managing data versions and files than the limitations of HDFS and object<br/>stores. Since then, systems such as Apache Hudi and Delta Lake have<br/>emerged to manage this complexity, and privacy regulations such as GDPR<br/>and CCPA have made deletion and update capabilities imperative. Update<br/>management for object storage is the central idea behind the data lakehouse<br/>concept, which we introduced in Chapter 3.<br/>Object storage is an ideal repository for unstructured data in any format<br/>beyond these structured data applications. Object storage can house any</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>binary data with no constraints on type or structure and frequently plays a<br/>role in ML pipelines for raw text, images, video, and audio.<br/><b>Object lookup<br/></b>As we mentioned, object stores are key-value stores. What does this mean<br/>for engineers? It&#8217;s critical to understand that, unlike file stores, object stores<br/>do not utilize a directory tree to find objects. The object store uses a top-<br/>level logical container (a bucket in S3 and GCS) and references objects by<br/>key. A simple example in S3 might look like this:<br/></p>
<p>S3://oreilly-data-engineering-book/data-example.json<br/></p>
<p>In this case, S3://oreilly-data-engineering-book/ is the bucket name,<br/>and data-example.json is the key pointing to a particular object. S3<br/>bucket names must be unique across all of AWS. Keys are unique within a<br/>bucket. Although cloud object stores may appear to support directory tree<br/>semantics, no true directory hierarchy exists. We might store an object with<br/>the following full path:<br/></p>
<p>S3://oreilly-data-engineering-book/project-data/11/23/2021/data.txt<br/></p>
<p>On the surface, this looks like subdirectories you might find in a regular file<br/>folder system: project-data, 11, 23, and 2021. Many cloud console<br/>interfaces allow users to view the objects inside a &#8220;directory,&#8221; and cloud<br/>command-line tools often support Unix-style commands such as ls inside<br/>an object store directory. However, behind the scenes, the object system<br/>does not traverse a directory tree to reach the object. Instead, it simply sees<br/>a key (project-data/11/23/2021/data.txt) that happens to match<br/>directory semantics. This might seem like a minor technical detail, but<br/>engineers need to understand that certain &#8220;directory&#8221;-level operations are<br/>costly in an object store. To run aws ls S3://oreilly-data-<br/>engineering-book/project-data/11/ the object store must filter keys<br/>on the key prefix project-data/11/. If the bucket contains millions of</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>objects, this operation might take some time, even if the &#8220;subdirectory&#8221;<br/>houses only a few objects.<br/><b>Object consistency and versioning<br/></b>As mentioned, object stores don&#8217;t support in-place updates or appends as a<br/>general rule. We write a new object under the same key to update an object.<br/>When data engineers utilize updates in data processes, they must be aware<br/>of the consistency model for the object store they&#8217;re using. Object stores<br/>may be eventually consistent or strongly consistent. For example, until<br/>recently, S3 was <i>eventually consistent</i>; after a new version of an object was<br/>written under the same key, the object store might sometimes return the old<br/>version of the object. The <i>eventual</i> part of <i>eventual consistency</i> means that<br/>after enough time has passed, the storage cluster reaches a state such that<br/>only the latest version of the object will be returned. This contrasts with the<br/><i>strong consistency</i> model we expect of local disks attached to a server:<br/>reading after a write will return the most recently written data.<br/>It might be desirable to impose strong consistency on an object store for<br/>various reasons, and standard methods are used to achieve this. One<br/>approach is to add a strongly consistent database (e.g., PostgreSQL) to the<br/>mix. Writing an object is now a two-step process:<br/></p>
<p>1. Write the object.<br/>2. Write the returned metadata for the object version to the strongly<br/></p>
<p>consistent database.<br/>The version metadata (an object hash or an object timestamp) can uniquely<br/>identify an object version in conjunction with the object key. To read an<br/>object, a reader undertakes the following steps:<br/></p>
<p>1. Fetch the latest object metadata from the strongly consistent database.<br/>2. Query object metadata using the object key. Read the object data if it<br/></p>
<p>matches the metadata fetched from the consistent database.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>3. If the object metadata does not match, repeat step 2 until the latest<br/>version of the object is returned.<br/></p>
<p>A practical implementation has exceptions and edge cases to consider, such<br/>as when the object gets rewritten during this querying process. These steps<br/>can be managed behind an API so that an object reader sees a strongly<br/>consistent object store at the cost of higher latency for object access.<br/>Object versioning is closely related to object consistency. When we rewrite<br/>an object under an existing key in an object store, we&#8217;re essentially writing<br/>a brand-new object, setting references from the existing key to the object,<br/>and deleting the old object references. Updating all references across the<br/>cluster takes time, hence the potential for stale reads. Eventually, the storage<br/>cluster garbage collector deallocates the space dedicated to the dereferenced<br/>data, recycling disk capacity for use by new objects.<br/>With object versioning turned on, we add additional metadata to the object<br/>that stipulates a version. While the default key reference gets updated to<br/>point to the new object, we retain other pointers to previous versions. We<br/>also maintain a version list so that clients can get a list of all object<br/>versions, and then pull a specific version. Because old versions of the object<br/>are still referenced, they aren&#8217;t cleaned up by the garbage collector.<br/>If we reference an object with a version, the consistency issue with some<br/>object storage systems disappears: the key and version metadata together<br/>form a unique reference to a particular, immutable data object. We will<br/>always get the same object back when we use this pair, provided that we<br/>haven&#8217;t deleted it. The consistency issue still exists when a client requests<br/>the &#8220;default&#8221; or &#8220;latest&#8221; version of an object.<br/>The principal overhead that engineers need to consider with object<br/>versioning is the cost of storage. Historical versions of objects generally<br/>have the same associated storage costs as current versions. Object version<br/>costs may be nearly insignificant or catastrophically expensive, depending<br/>on various factors. The data size is an issue, as is update frequency; more<br/>object versions can lead to significantly larger data size. Keep in mind that</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>we&#8217;re talking about brute-force object versioning. Object storage systems<br/>generally store full object data for each version, not differential snapshots.<br/>Engineers also have the option of deploying storage lifecycle policies.<br/>Lifecycle policies allow automatic deletion of old object versions when<br/>certain conditions are met (e.g., when an object version reaches a certain<br/>age or many newer versions exist). Cloud vendors also offer various<br/>archival data tiers at heavily discounted prices, and the archival process can<br/>be managed using lifecycle policies.<br/><b>Storage classes and tiers<br/></b>Cloud vendors now offer storage classes that discount data storage pricing<br/>in exchange for reduced access or reduced durability. We use the term<br/><i>reduced access</i> here because many of these storage tiers still make data<br/>highly available, but with high retrieval costs in exchange for reduced<br/>storage costs.<br/>Let&#8217;s look at a couple of examples in S3 since Amazon is a benchmark for<br/>cloud service standards. The S3 Standard-Infrequent Access storage class<br/>discounts monthly storage costs for increased data retrieval costs. (See &#8220;A<br/>Brief Detour on Cloud Economics&#8221; for a theoretical discussion of the<br/>economics of cloud storage tiers.) Amazon also offers the Amazon S3 One<br/>Zone-Infrequent Access tier, replicating only to a single zone. Projected<br/>availability drops from 99.9% to 99.5% to account for the possibility of a<br/>zonal outage. Amazon still claims extremely high data durability, with the<br/>caveat that data will be lost if an availability zone is destroyed.<br/>Further down the tiers of reduced access are the archival tiers in S3 Glacier.<br/>S3 Glacier promises a dramatic reduction in long-term storage costs for<br/>much higher access costs. Users have various retrieval speed options, from<br/>minutes to hours, with higher retrieval costs for faster access. For example,<br/>at the time of this writing, S3 Glacier Deep Archive discounts storage costs<br/>even further; Amazon advertises that storage costs start at $1 per terabyte<br/>per month. In exchange, data restoration takes 12 hours. In addition, this<br/>storage class is designed for data that will be stored from 7&#8211;10 years and be<br/>accessed only one to two times per year.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Be aware of how you plan to utilize archival storage, as it&#8217;s easy to get into<br/>and often costly to access data, especially if you need it more often than<br/>expected. See Chapter 4 for a more extensive discussion of archival storage<br/>economics.<br/><b>Object store&#8211;backed filesystems<br/></b>Object store synchronization solutions have become increasingly popular.<br/>Tools like s3fs and Amazon S3 File Gateway allow users to mount an S3<br/>bucket as local storage. Users of these tools should be aware of the<br/>characteristics of writes to the filesystem and how these will interact with<br/>the characteristics and pricing of object storage. File Gateway, for example,<br/>handles changes to files fairly efficiently by combining portions of objects<br/>into a new object using the advanced capabilities of S3. However, high-<br/>speed transactional writing will overwhelm the update capabilities of an<br/>object store. Mounting object storage as a local filesystem works well for<br/>files that are updated infrequently.<br/></p>
<p><b>Cache and Memory-Based Storage Systems<br/></b>As discussed in &#8220;Raw Ingredients of Data Storage&#8221;, RAM offers excellent<br/>latency and transfer speeds. However, traditional RAM is extremely<br/>vulnerable to data loss because a power outage lasting even a second can<br/>erase data. RAM-based storage systems are generally focused on caching<br/>applications, presenting data for quick access and high bandwidth. Data<br/>should generally be written to a more durable medium for retention<br/>purposes.<br/>These ultra-fast cache systems are useful when data engineers need to serve<br/>data with ultra-fast retrieval latency.<br/><b>Example: Memcached and lightweight object caching<br/></b><i>Memcached</i> is a key-value store designed for caching database query<br/>results, API call responses, and more. Memcached uses simple data<br/>structures, supporting either string or integer types. Memcached can deliver</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>results with very low latency while also taking the load off backend<br/>systems.<br/><b>Example: Redis, memory caching with optional persistence<br/></b>Like Memcached, <i>Redis</i> is a key-value store, but it supports somewhat more<br/>complex data types (such as lists or sets) Redis also builds in multiple<br/>persistence mechanisms, including snapshotting and journaling. With a<br/>typical configuration, Redis writes data roughly every two seconds. Redis is<br/>thus suitable for extremely high-performance applications but can tolerate a<br/>small amount of data loss.<br/></p>
<p><b>The Hadoop Distributed File System<br/></b>The Hadoop Distributed File System is based on Google File System (GFS)<br/>and was initially engineered to process data with the MapReduce<br/>programming model. Hadoop is similar to object storage, but with a key<br/>difference: Hadoop combines compute and storage on the same nodes,<br/>where object stores typically have limited support for internal processing.<br/>Hadoop breaks large files into <i>blocks</i>, chunks of data less than a few<br/>hundred megabytes in size. The filesystem is managed by the <i>NameNode</i>,<br/>which maintains directories, file metadata, and a detailed catalog describing<br/>the location of file blocks in the cluster. In a typical configuration, each<br/>block of data is replicated to three nodes. This increases both the durability<br/>and availability of data. If a disk or node fails, the replication factor for<br/>some file blocks will fall below 3. The NameNode will instruct other nodes<br/>to replicate these file blocks so that they again reach the correct replication<br/>factor. Thus, the probability of losing data is very low, barring a <i>correlated<br/>failure</i> (e.g., an asteroid hitting the data center).<br/>Hadoop is not simply a storage system. Hadoop combines compute<br/>resources with storage nodes to allow in-place data processing. This was<br/>originally achieved using the MapReduce programming model, which we<br/>discuss in Chapter 8.<br/><b>Hadoop is dead. Long live Hadoop!</b></p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>We often see claims that Hadoop is dead. This is only partially true. Hadoop<br/>is no longer a hot, bleeding-edge technology. Many Hadoop ecosystem<br/>tools such as Apache Pig are now on life support and primarily used to run<br/>legacy jobs. The pure MapReduce programming model has fallen by the<br/>wayside. HDFS remains widely used in various applications and<br/>organizations.<br/>Hadoop still appears in many legacy installations. Many organizations that<br/>adopted Hadoop during the peak of the big data craze have no immediate<br/>plans to migrate to newer technologies. This is a good choice for companies<br/>that run massive (thousand-node) Hadoop clusters and have the resources to<br/>maintain on-premises systems effectively. Smaller companies may want to<br/>reconsider the cost overhead and scale limitations of running a small<br/>Hadoop cluster against migrating to cloud solutions.<br/>In addition, HDFS is a key ingredient of many current big data engines,<br/>such as Amazon EMR. In fact, Apache Spark is still commonly run on<br/>HDFS clusters. We discuss this in more detail in &#8220;Separation of Compute<br/>from Storage&#8221;.<br/></p>
<p><b>Streaming Storage<br/></b>Streaming data has different storage requirements than nonstreaming data.<br/>In the case of message queues, stored data is temporal and expected to<br/>disappear after a certain duration. However, distributed, scalable streaming<br/>frameworks like Apache Kafka now allow extremely long-duration<br/>streaming data retention. Kafka supports indefinite data retention by<br/>pushing old, infrequently accessed messages down to object storage. Kafka<br/>competitors (including Amazon Kinesis, Apache Pulsar, and Google Cloud<br/>Pub/Sub) also support long data retention<br/>Closely related to data retention in these systems is the notion of replay.<br/><i>Replay</i> allows a streaming system to return a range of historical stored data.<br/>Replay is the standard data-retrieval mechanism for streaming storage<br/>systems. Replay can be used to run batch queries over a time range or to</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>reprocess data in a streaming pipeline. Chapter 7 covers replay in more<br/>depth.<br/>Other storage engines have emerged for real-time analytics applications. In<br/>some sense, transactional databases emerged as the first real-time query<br/>engines; data becomes visible to queries as soon as it is written. However,<br/>these databases have well-known scaling and locking limitations, especially<br/>for analytics queries that run across large volumes of data. While scalable<br/>versions of row-oriented transactional databases have overcome some of<br/>these limitations, they are still not truly optimized for analytics at scale.<br/></p>
<p><b>Indexes, Partitioning, and Clustering<br/></b>Indexes provide a map of the table for particular fields and allow extremely<br/>fast lookup of individual records. Without indexes, a database would need<br/>to scan an entire table to find the records satisfying a WHERE condition.<br/>In most RDBMSs, indexes are used for primary table keys (allowing unique<br/>identification of rows) and foreign keys (allowing joins with other tables).<br/>Indexes can also be applied to other columns to serve the needs of specific<br/>applications. Using indexes, an RDBMS can look up and update thousands<br/>of rows per second.<br/>We do not cover transactional database records in depth in this book;<br/>numerous technical resources are available on this topic. Rather, we are<br/>interested in the evolution away from indexes in analytics-oriented storage<br/>systems and some new developments in indexes for analytics use cases.<br/><b>The evolution from rows to columns<br/></b>An early data warehouse was typically built on the same type of RDBMS<br/>used for transactional applications. The growing popularity of MPP systems<br/>meant a shift toward parallel processing for significant improvements in<br/>scan performance across large quantities of data for analytics purposes.<br/>However, these row-oriented MPPs still used indexes to support joins and<br/>condition checking.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In &#8220;Raw Ingredients of Data Storage&#8221;, we discuss columnar serialization.<br/><i>Columnar serialization</i> allows a database to scan only the columns required<br/>for a particular query, sometimes dramatically reducing the amount of data<br/>read from the disk. In addition, arranging data by column packs similar<br/>values next to each other, yielding high-compression ratios with minimal<br/>compression overhead. This allows data to be scanned more quickly from<br/>disk and over a network.<br/>Columnar databases perform poorly for transactional use cases&#8212;i.e., when<br/>we try to look up large numbers of individual rows asynchronously.<br/>However, they perform extremely well when large quantities of data must<br/>be scanned&#8212;e.g., for complex data transformations, aggregations,<br/>statistical calculations, or evaluation of complex conditions on large<br/>datasets.<br/>In the past, columnar databases performed poorly on joins, so the advice for<br/>data engineers was to denormalize data, using wide schemas, arrays, and<br/>nested data wherever possible. Join performance for columnar databases<br/>has improved dramatically in recent years, so while there can still be<br/>performance advantages in denormalization, this is no longer a necessity.<br/>You&#8217;ll learn more about normalization and denormalization in Chapter 8.<br/><b>From indexes to partitions and clustering<br/></b>While columnar databases allow for fast scan speeds, it&#8217;s still helpful to<br/>reduce the amount of data scanned as much as possible. In addition to<br/>scanning only data in columns relevant to a query, we can partition a table<br/>into multiple subtables by splitting it on a field. It is quite common in<br/>analytics and data science use cases to scan over a time range, so date- and<br/>time-based partitioning is extremely common. Columnar databases<br/>generally support a variety of other partition schemes as well.<br/><i>Clusters</i> allow finer-grained organization of data within partitions. A<br/>clustering scheme applied within a columnar database sorts data by one or a<br/>few fields, colocating similar values. This improves performance for<br/>filtering, sorting, and joining these values.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><b>Example: Snowflake micro-partitioning<br/></b>We mention Snowflake micro-partitioning because it&#8217;s a good example of<br/>recent developments and evolution in approaches to columnar storage.<br/><i>Micro partitions</i> are sets of rows between 50 and 500 megabytes in<br/>uncompressed size. Snowflake uses an algorithmic approach that attempts<br/>to cluster together similar rows. This contrasts the traditional naive<br/>approach to partitioning on a single designated field, such as a date.<br/>Snowflake specifically looks for values that are repeated in a field across<br/>many rows. This allows aggressive <i>pruning</i> of queries based on predicates.<br/>For example, a WHERE clause might stipulate the following:<br/></p>
<p><b>WHERE</b> created_date='2017-01-02'<br/></p>
<p>In such a query, Snowflake excludes any micro-partitions that don&#8217;t include<br/>this date, effectively pruning this data. Snowflake also allows overlapping<br/>micro-partitions, potentially partitioning on multiple fields showing<br/>significant repeats.<br/>Efficient pruning is facilitated by Snowflake&#8217;s metadata database, which<br/>stores a description of each micro-partition, including the number of rows<br/>and value ranges for fields. At each query stage, Snowflake analyzes micro-<br/>partitions to determine which ones need to be scanned. Snowflake uses the<br/>term <i>hybrid columnar storage</i>,  partially referring to the fact that its tables<br/>are broken into small groups of rows, even though storage is fundamentally<br/>columnar. The metadata database plays a role similar to an index in a<br/>traditional relational database.<br/></p>
<p><b>Data Engineering Storage Abstractions<br/></b><i>Data engineering storage abstractions</i> are data organization and query<br/>patterns that sit at the heart of the data engineering lifecycle and are built<br/>atop the data storage systems discussed previously (see Figure 6-3). We<br/>introduced many of these abstractions in Chapter 3, and we will revisit them<br/>here.<br/></p>
<p>2</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>The main types of abstractions we&#8217;ll concern ourselves with are those that<br/>support data science, analytics, and reporting use cases. These include data<br/>warehouse, data lake, data lakehouse, data platforms, and data catalogs. We<br/>won&#8217;t cover source systems, as they are discussed in Chapter 5.<br/>The storage abstraction you require as a data engineer boils down to a few<br/>key considerations:<br/><i>Purpose and use case<br/></i></p>
<p>You must first identify the purpose of storing the data. What is it used<br/>for?<br/></p>
<p><i>Update patterns<br/></i>Is the abstraction optimized for bulk updates, streaming inserts, or<br/>upserts?<br/></p>
<p><i>Cost<br/></i>What are the direct and indirect financial costs? The time to value? The<br/>opportunity costs?<br/></p>
<p><i>Separate storage and compute<br/></i>The trend is toward separating storage and compute, but most systems<br/>hybridize separation and colocation. We cover this in &#8220;Separation of<br/>Compute from Storage&#8221; since it affects purpose, speed, and cost.<br/></p>
<p>You should know that the popularity of separating storage from compute<br/>means the lines between OLAP databases and data lakes are increasingly<br/>blurring. Major cloud data warehouses and data lakes are on a collision<br/>course. In the future, the differences between these two may be in name<br/>only since they might functionally and technically be very similar under the<br/>hood.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><b>The Data Warehouse<br/></b>Data warehouses are a standard OLAP data architecture. As discussed in<br/>Chapter 3, the term <i>data warehouse</i> refers to technology platforms (e.g.,<br/>Google BigQuery and Teradata), an architecture for data centralization, and<br/>an organizational pattern within a company. In terms of storage trends,<br/>we&#8217;ve evolved from building data warehouses atop conventional<br/>transactional databases, row-based MPP systems (e.g., Teradata and IBM<br/>Netezza), and columnar MPP systems (e.g., Vertica and Teradata Columnar)<br/>to cloud data warehouses and data platforms. (See our data warehousing<br/>discussion in Chapter 3 for more details on MPP systems.)<br/>In practice, cloud data warehouses are often used to organize data into a<br/>data lake, a storage area for massive amounts of unprocessed raw data, as<br/>originally conceived by James Dixon.  Cloud data warehouses can handle<br/>massive amounts of raw text and complex JSON documents. The limitation<br/>is that cloud data warehouses cannot handle truly unstructured data, such as<br/>images, video, or audio, unlike a true data lake. Cloud data warehouses can<br/>be coupled with object storage to provide a complete data-lake solution.<br/></p>
<p><b>The Data Lake<br/></b>The <i>data lake</i> was originally conceived as a massive store where data was<br/>retained in raw, unprocessed form. Initially, data lakes were built primarily<br/>on Hadoop systems, where cheap storage allowed for retention of massive<br/>amounts of data without the cost overhead of a proprietary MPP system.<br/>The last five years have seen two major developments in the evolution of<br/>data lake storage. First, a major migration toward <i>separation of compute<br/>and storage</i> has occurred. In practice, this means a move away from<br/>Hadoop toward cloud object storage for long-term retention of data.<br/>Second, data engineers discovered that much of the functionality offered by<br/>MPP systems (schema management; update, merge and delete capabilities)<br/>and initially dismissed in the rush to data lakes was, in fact, extremely<br/>useful. This led to the notion of the data lakehouse.<br/></p>
<p>3</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><b>The Data Lakehouse<br/></b>The <i>data lakehouse</i> is an architecture that combines aspects of the data<br/>warehouse and the data lake. As it is generally conceived, the lakehouse<br/>stores data in object storage just like a lake. However, the lakehouse adds to<br/>this arrangement features designed to streamline data management and<br/>create an engineering experience similar to a data warehouse. This means<br/>robust table and schema support and features for managing incremental<br/>updates and deletes. Lakehouses typically also support table history and<br/>rollback; this is accomplished by retaining old versions of files and<br/>metadata.<br/>A lakehouse system is a metadata and file-management layer deployed with<br/>data management and transformation tools. Databricks has heavily<br/>promoted the lakehouse concept with Delta Lake, an open source storage<br/>management system.<br/>We would be remiss not to point out that the architecture of the data<br/>lakehouse is similar to the architecture used by various commercial data<br/>platforms, including BigQuery and Snowflake. These systems store data in<br/>object storage and provide automated metadata management, table history,<br/>and update/delete capabilities. The complexities of managing underlying<br/>files and storage are fully hidden from the user.<br/>The key advantage of the data lakehouse over proprietary tools is<br/>interoperability. It&#8217;s much easier to exchange data between tools when<br/>stored in an open file format. Reserializing data from a proprietary database<br/>format incurs overhead in processing, time, and cost. In a data lakehouse<br/>architecture, various tools can connect to the metadata layer and read data<br/>directly from object storage.<br/>It is important to emphasize that much of the data in a data lakehouse may<br/>not have a table structure imposed. We can impose data warehouse features<br/>where we need them in a lakehouse, leaving other data in a raw or even<br/>unstructured format.<br/>The data lakehouse technology is evolving rapidly. A variety of new<br/>competitors to Delta Lake have emerged, including Apache Hudi and</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Apache Iceberg. See Appendix A for more details.<br/></p>
<p><b>Data Platforms<br/></b>Increasingly, vendors are styling their products as <i>data platforms</i>. These<br/>vendors have created their ecosystems of interoperable tools with tight<br/>integration into the core data storage layer. In evaluating platforms,<br/>engineers must ensure that the tools offered meet their needs. Tools not<br/>directly provided in the platform can still interoperate, with extra data<br/>overhead for data interchange. Platforms also emphasize close integration<br/>with object storage for unstructured use cases, as mentioned in our<br/>discussion of cloud data warehouses.<br/>At this point, the notion of the data platform frankly has yet to be fully<br/>fleshed out. However, the race is on to create a walled garden of data tools,<br/>both simplifying the work of data engineering and generating significant<br/>vendor lock-in.<br/></p>
<p><b>Stream-to-Batch Storage Architecture<br/></b>The stream-to-batch storage architecture has many similarities to the<br/>Lambda architecture, though some might quibble over the technical details.<br/>Essentially, data flowing through a topic in the streaming storage system is<br/>written out to multiple consumers.<br/>Some of these consumers might be real-time processing systems that<br/>generate statistics on the stream. In addition, a batch storage consumer<br/>writes data for long-term retention and batch queries. The batch consumer<br/>could be AWS Kinesis Firehose, which can generate S3 objects based on<br/>configurable triggers (e.g., time and batch size). Systems such as BigQuery<br/>ingest streaming data into a streaming buffer. This streaming buffer is<br/>automatically reserialized into columnar object storage. The query engine<br/>supports seamless querying of both the streaming buffer and the object data<br/>to provide users a current, nearly real-time view of the table.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><b>Big Ideas and Trends in Storage<br/></b>In this section, we&#8217;ll discuss some big ideas in storage&#8212;key considerations<br/>that you need to keep in mind as you build out your storage architecture.<br/>Many of these considerations are part of larger trends. For example, data<br/>catalogs fit under the trend toward &#8220;enterprisey&#8221; data engineering and data<br/>management. Separation of compute from storage is now largely an<br/>accomplished fact in cloud data systems. And data sharing is an<br/>increasingly important consideration as businesses adopt data technology.<br/></p>
<p><b>Data Catalog<br/></b>A <i>data catalog</i> is a centralized metadata store for all data across an<br/>organization. Strictly speaking, a data catalog is not a top-level data storage<br/>abstraction, but it integrates with various systems and abstractions. Data<br/>catalogs typically work across operational and analytics data sources,<br/>integrate data lineage and presentation of data relationships, and allow user<br/>editing of data descriptions.<br/>Data catalogs are often used to provide a central place where people can<br/>view their data, queries, and data storage. As a data engineer, you&#8217;ll likely<br/>be responsible for setting up and maintaining the various data integrations<br/>of data pipeline and storage systems that will integrate with the data catalog<br/>and the integrity of the data catalog itself.<br/><b>Catalog application integration<br/></b>Ideally, data applications are designed to integrate with catalog APIs to<br/>handle their metadata and updates directly. As catalogs are more widely<br/>used in an organization, it becomes easier to approach this ideal.<br/><b>Automated scanning<br/></b>In practice, cataloging systems typically need to rely on an automated<br/>scanning layer that collects metadata from various systems such as data<br/>lakes, data warehouses, and operational databases. Data catalogs can collect</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>existing metadata and may also use scanning tools to infer metadata such as<br/>key relationships or the presence of sensitive data.<br/><b>Data portal and social layer<br/></b>Data catalogs also typically provide a human access layer through a web<br/>interface, where users can search for data and view data relationships. Data<br/>catalogs can be enhanced with a social layer offering Wiki functionality.<br/>This allows users to provide information on their datasets, request<br/>information from other users, and post updates as they become available.<br/><b>Data catalog use cases<br/></b>Data catalogs have both organizational and technical use cases. Data<br/>catalogs make metadata easily available to systems. For instance, a data<br/>catalog is a key ingredient of the data lakehouse, allowing table<br/>discoverability for queries.<br/>Organizationally, data catalogs allow business users, analysts, data<br/>scientists, and engineers to search for data to answer questions. Data<br/>catalogs streamline cross-organizational communications and collaboration.<br/></p>
<p><b>Data Sharing<br/></b><i>Data sharing</i> allows organizations and individuals to share specific data and<br/>carefully defined permissions with specific entities. Data sharing allows<br/>data scientists to share data from a sandbox with their collaborators within<br/>an organization. Across organizations, data sharing facilitates collaboration<br/>between partner businesses. For example, an ad tech company can share<br/>advertising data with its customers.<br/>A cloud multitenant environment makes interorganizational collaboration<br/>much easier. However, it also presents new security challenges.<br/>Organizations must carefully control policies that govern who can share<br/>data with whom to prevent accidental exposure or deliberate exfiltration.<br/>Data sharing is a core feature of many cloud data platforms. See Chapter 5<br/>for a more extensive discussion of data sharing.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><b>Schema<br/></b>What is the expected form of the data? What is the file format? Is it<br/>structured, semistructured, or unstructured? What data types are expected?<br/>How does the data fit into a larger hierarchy? Is it connected to other data<br/>through shared keys or other relationships?<br/>Note that schema need not be <i>relational</i>. Rather, data becomes more useful<br/>when we have as much information about its structure and organization. For<br/>images stored in a data lake, this schema information might explain the<br/>image format, resolution, and the way the images fit into a larger hierarchy.<br/>Schema can function as a sort of Rosetta stone, instructions that tell us how<br/>to read the data. Two major schema patterns exist: schema on write and<br/>schema on read. <i>Schema on write</i> is essentially the traditional data<br/>warehouse pattern: a table has an integrated schema; any writes to the table<br/>must conform. To support schema on write, a data lake must integrate a<br/>schema metastore.<br/>With <i>schema on read</i>, the schema is dynamically created when data is<br/>written, and a reader must determine the schema when reading the data.<br/>Ideally, schema on read is implemented using file formats that implement<br/>built-in schema information, such as Parquet or JSON. CSV files are<br/>notorious for schema inconsistency and are not recommended in this<br/>setting.<br/>The principal advantage of schema on write is that it enforces data<br/>standards, making data easier to consume and utilize in the future. Schema<br/>on read emphasizes flexibility, allowing virtually any data to be written.<br/>This comes at the cost of greater difficulty consuming data in the future.<br/></p>
<p><b>Separation of Compute from Storage<br/></b>A key idea we revisit throughout this book is the separation of compute<br/>from storage. This has emerged as a standard data access and query pattern<br/>in today&#8217;s cloud era. Data lakes, as we discussed, store data in object stores<br/>and spin up temporary compute capacity to read and process it. Most fully</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>managed OLAP products now rely on object storage behind the scenes. To<br/>understand the motivations for separating compute and storage, we should<br/>first look at the colocation of compute and storage.<br/><b>Colocation of compute and storage<br/></b>Colocation of compute and storage has long been a standard method to<br/>improve database performance. For transactional databases, data colocation<br/>allows fast, low-latency disk reads and high bandwidth. Even when we<br/>virtualize storage (e.g., using Amazon EBS), data is located relatively close<br/>to the host machine.<br/>The same basic idea applies for analytics query systems running across a<br/>cluster of machines. For example, with HDFS and MapReduce, the standard<br/>approach is to locate data blocks that need to be scanned in the cluster, and<br/>then push individual <i>map</i> jobs out to these blocks. The data scan and<br/>processing for the map step are strictly local. The <i>reduce</i> step involves<br/>shuffling data across the cluster, but keeping map steps local effectively<br/>preserves more bandwidth for shuffling, delivering better overall<br/>performance; map steps that filter heavily also dramatically reduce the<br/>amount of data to be shuffled.<br/><b>Separation of compute and storage<br/></b>If colocation of compute and storage delivers high performance, why the<br/>shift toward separation of compute and storage? Several motivations exist.<br/><i>Ephemerality and scalability<br/></i>In the cloud, we&#8217;ve seen a dramatic shift toward ephemerality. In general,<br/>it&#8217;s cheaper to buy and host a server than to rent it from a cloud provider,<br/><i>provided that you&#8217;re running it 24 hours a day nonstop for years on end</i>. In<br/>practice, workloads vary dramatically, and significant efficiencies are<br/>realized with a pay-as-you-go model if servers can scale up and down. This<br/>is true for web servers in online retail, and it is also true for big data batch<br/>jobs that may run only periodically.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Ephemeral compute resources allow engineers to spin up massive clusters<br/>to complete jobs on time, and then delete clusters when these jobs are done.<br/>The performance benefits of temporarily operating at ultra-high scale can<br/>outweigh the bandwidth limitations of object storage.<br/><i>Data durability and availability<br/></i>Cloud object stores significantly mitigate the risk of data loss and generally<br/>provide extremely high uptime (availability). For example, S3 stores data<br/>across multiple zones; if a natural disaster destroys a zone, data is still<br/>available from the remaining zones. Having multiple zones available also<br/>reduces the odds of a data outage. If resources in one zone go down,<br/>engineers can spin up the same resources in a different zone.<br/>The potential for a misconfiguration that destroys data in object storage is<br/>still somewhat scary, but simple-to-deploy mitigations are available.<br/>Copying data to multiple cloud regions reduces this risk since configuration<br/>changes are generally deployed to only one region at a time. Replicating<br/>data to multiple storage providers can further reduce the risk.<br/><b>Hybrid separation and colocation<br/></b>The practical realities of separating compute from storage are more<br/>complicated than we&#8217;ve implied. In reality, we constantly hybridize<br/>colocation and separation to realize the benefits of both approaches. This<br/>hybridization is typically done in two ways: multitier caching and hybrid<br/>object storage.<br/>With <i>multitier caching</i>, we utilize object storage for long-term data<br/>retention and access but spin up local storage to be used during queries and<br/>various stages of data pipelines. Both Google and Amazon offer versions of<br/>hybrid object storage (object storage that is tightly integrated with<br/>compute).<br/>Let&#8217;s look at examples of how some popular processing engines hybridize<br/>separation and colocation of storage and compute.<br/><i>Example: AWS EMR with S3 and HDFS</i></p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Big data services like Amazon EMR spin up temporary HDFS clusters to<br/>process data. Engineers have the option of referencing both S3 and HDFS<br/>as a filesystem. A common pattern is to stand up HDFS on SSD drives, pull<br/>from S3, and save data from intermediate processing steps on local HDFS.<br/>Doing so can realize significant performance gains over processing directly<br/>from S3. Full results are written back to S3 once the cluster completes its<br/>steps, and the cluster and HDFS are deleted. Other consumers read the<br/>output data directly from S3.<br/><i>Example: Apache Spark<br/></i>In practice, Spark generally runs jobs on HDFS or some other ephemeral<br/>distributed filesystem to support performant storage of data between<br/>processing steps. In addition, Spark relies heavily on in-memory storage of<br/>data to improve processing. The problem with owning the infrastructure for<br/>running Spark is that dynamic RAM (DRAM) is extremely expensive; by<br/>separating compute and storage in the cloud, we can rent large quantities of<br/>memory and then release that memory when the job completes.<br/><i>Example: Apache Druid<br/></i>Apache Druid relies heavily on SSDs to realize high performance. Since<br/>SSDs are significantly more expensive than magnetic disks, Druid keeps<br/>only one copy of data in its cluster, reducing &#8220;live&#8221; storage costs by a factor<br/>of three.<br/>Of course, maintaining data durability is still critical, so Druid uses an<br/>object store as its durability layer. When data is ingested, it&#8217;s processed,<br/>serialized into compressed columns, and written to cluster SSDs and object<br/>storage. In the event of node failure or cluster data corruption, data can be<br/>automatically recovered to new nodes. In addition, the cluster can be shut<br/>down and then fully recovered from SSD storage.<br/><i>Example: Hybrid Object Storage<br/></i>Google&#8217;s Colossus file storage system supports fine-grained control of data<br/>block location, although this functionality is not exposed directly to the<br/>public. BigQuery uses this feature to colocate customer tables in a single</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>location, allowing ultra-high bandwidth for queries in that location.  We<br/>refer to this as <i>hybrid object storage</i> because it combines the clean<br/>abstractions of object storage with some advantages of colocating compute<br/>and storage. Amazon also offers some notion of hybrid object storage<br/>through S3 Select, a feature that allows users to filter S3 data directly in S3<br/>clusters before data is returned across the network.<br/>We speculate that public clouds will adopt hybrid object storage more<br/>widely to improve the performance of their offerings and make more<br/>efficient use of available network resources. Some may be already doing so<br/>without disclosing this publicly.<br/>The concept of hybrid object storage underscores that there can still be<br/>advantages to having low-level access to hardware rather than relying on<br/>someone else&#8217;s public cloud. Public cloud services do not expose low-level<br/>details of hardware and systems (e.g., data block locations for Colossus),<br/>but these details can be extremely useful in performance optimization and<br/>enhancement. See our discussion of cloud economics in Chapter 4.<br/>While we&#8217;re now seeing a mass migration of data to public clouds, we<br/>believe that many hyper-scale data service vendors that currently run on<br/>public clouds provided by other vendors may build their data centers in the<br/>future, albeit with deep network integration into public clouds.<br/><b>Zero copy cloning<br/></b>Cloud-based systems based around object storage support <i>zero copy<br/>cloning</i>. This typically means that a new virtual copy of an object is created<br/>(e.g., a new table) without necessarily physically copying the underlying<br/>data. Typically, new pointers are created to the raw data files, and future<br/>changes to these tables will not be recorded in the old table. For those<br/>familiar with the inner workings of object-oriented languages such as<br/>Python, this type of &#8220;shallow&#8221; copying is familiar from other contexts.<br/>Zero copy cloning is a compelling feature, but engineers must understand<br/>its strengths and limitations. For example, cloning an object in a data lake<br/></p>
<p>4</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>environment and then deleting the files in the original object might also<br/>wipe out the new object.<br/>For fully managed object-store-based systems (e.g., Snowflake and<br/>BigQuery), engineers need to be extremely familiar with the exact limits of<br/>shallow copying. Engineers have more access to underlying object storage<br/>in data lake systems such as Databricks&#8212;a blessing and a curse. Data<br/>engineers should exercise great caution before deleting any raw files in the<br/>underlying object store. Databricks and other data lake management<br/>technologies sometimes also support a notion of <i>deep copying</i>, whereby all<br/>underlying data objects are copied. This is a more expensive process, but<br/>also more robust in the event that files are unintentionally lost or deleted.<br/></p>
<p><b>Data Storage Lifecycle and Data Retention<br/></b>Storing data isn&#8217;t as simple as just saving it to object storage or disk and<br/>forgetting about it. You need to think about the data storage lifecycle and<br/>data retention. When you think about access frequency and use cases, ask,<br/>&#8220;How important is the data to downstream users, and how often do they<br/>need to access it?&#8221; This is the data storage lifecycle. Another question you<br/>should ask is, &#8220;How long should I keep this data?&#8221; Do you need to retain<br/>data indefinitely, or are you fine discarding it past a certain time frame?<br/>This is data retention. Let&#8217;s dive into each of these.<br/><b>Hot, warm, and cold data<br/></b>Did you know that data has a temperature? Depending on how frequently<br/>data is accessed, we can roughly bucket the way it is stored into three<br/>categories of persistence: hot, warm, and cold. Query access patterns differ<br/>for each dataset (Figure 6-9). Typically, newer data is queried more often<br/>than older data. Let&#8217;s look at hot, cold, and warm data in that order.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><i>Figure 6-9. Hot, warm, and cold data costs associated with access frequency<br/></i></p>
<p><i>Hot data<br/>Hot data</i> has instant or frequent access requirements. The underlying<br/>storage for hot data is suited for fast access and reads, such as SSD or<br/>memory. Because of the type of hardware involved with hot data, storing<br/>hot data is often the most expensive form of storage. Example use cases for<br/>hot data include retrieving product recommendations and product page<br/>results. The cost of storing hot data is the highest of these three storage<br/>tiers, but retrieval is often inexpensive.<br/>Query results cache is another example of hot data. When a query is run,<br/>some query engines will persist the query results in the cache. For a limited<br/>time, when the same query is run, instead of rerunning the same query<br/>against storage, the query results cache serves the cached results. This<br/>allows for much faster query response times versus redundantly issuing the<br/>same query repeatedly. In upcoming chapters, we cover query results caches<br/>in more detail.<br/><i>Warm data<br/>Warm data</i> is accessed semi-regularly, say, once per month. No hard and<br/>fast rules indicate how often warm data is accessed, but it&#8217;s less than hot<br/>data and more than cold data. The major cloud providers offer object<br/>storage tiers that accommodate warm data. For example,S3 offers an<br/>Infrequently Accessed Tier, and Google Cloud has a similar storage tier<br/>called Nearline. Vendors give their models of recommended access<br/>frequency, and engineers can also do their cost modeling and monitoring.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Storage of warm data is cheaper than hot data, with slightly more expensive<br/>retrieval costs.<br/><i>Cold data<br/></i>On the other extreme, <i>cold data</i> is infrequently accessed data. The hardware<br/>used to archive cold data is typically cheap and durable, such as HDD, tape<br/>storage, and cloud-based archival systems. Cold data is mainly meant for<br/>long-term archival, when there&#8217;s little to no intention to access the data.<br/>Though storing cold data is cheap, retrieving cold data is often expensive.<br/><i>Storage tier considerations<br/></i>When considering the storage tier for your data, consider the costs of each<br/>tier. If you store all of your data in hot storage, all of the data can be<br/>accessed quickly. But this comes at a tremendous price! Conversely, if you<br/>store all data in cold storage to save on costs, you&#8217;ll certainly lower your<br/>storage costs, but at the expense of prolonged retrieval times and high<br/>retrieval costs if you need to access data. The storage price goes down from<br/>faster/higher performing storage to lower storage.<br/>Cold storage is popular for archiving data. Historically, cold storage<br/>involved physical backups and often mailing this data to a third party that<br/>would archive it in a literal vault. Cold storage is increasingly popular in the<br/>cloud. Every cloud vendor offers a cold data solution, and you should weigh<br/>the cost of pushing data into cold storage versus the cost and time to<br/>retrieve the data.<br/>Data engineers need to account for spillover from hot to warm/cold storage.<br/>Memory is expensive and finite. For example, if hot data is stored in<br/>memory, it can be spilled to disk when there&#8217;s too much new data to store<br/>and not enough memory. Some databases may move infrequently accessed<br/>data to warm or cold tiers, offloading the data to either HDD or object<br/>storage. The latter is increasingly more common because of the cost-<br/>effectiveness of object storage. If you&#8217;re in the cloud and using managed<br/>services, disk spillover will happen automatically.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>If you&#8217;re using cloud-based object storage, create automated lifecycle<br/>policies for your data. This will drastically reduce your storage costs. For<br/>example, if your data needs to be accessed only once a month, move the<br/>data to an infrequent access storage tier. If your data is 180 days old and not<br/>accessed for current queries, move it to an archival storage tier. In both<br/>cases, you can automate the migration of data away from regular object<br/>storage, and you&#8217;ll save money. That said, consider the retrieval costs&#8212;<br/>both in time and money&#8212;using infrequent or archival style storage tiers.<br/>Access and retrieval times and costs may vary depending on the cloud<br/>provider. Some cloud providers make it simple and cheap to migrate data<br/>into archive storage, but it is costly and slow to retrieve your data.<br/><b>Data retention<br/></b>Back in the early days of &#8220;big data,&#8221; there was a tendency to err on the side<br/>of accumulating every piece of data possible, regardless of its usefulness.<br/>The expectation was, &#8220;we might need this data in the future.&#8221; This data<br/>hoarding inevitably became unwieldy and dirty, giving rise to data swamps,<br/>and regulatory crackdowns on data retention, among other consequences<br/>and nightmares. Nowadays, data engineers need to consider data retention:<br/>what data do you <i>need</i> to keep, and how <i>long</i> should you keep it? Here are<br/>some things to think about with data retention.<br/><i>Value<br/></i>Data is an asset, so you should know the value of the data you&#8217;re storing. Of<br/>course, value is subjective and depends on what it&#8217;s worth to your<br/>immediate use case and your broader organization. Is this data impossible to<br/>re-create, or can it easily be re-created by querying upstream systems?<br/>What&#8217;s the impact to downstream users if this data is available versus if it is<br/>not?<br/><i>Time<br/></i>The value to downstream users also depends upon the age of the data. New<br/>data is typically more valuable and frequently accessed than older data.<br/>Technical limitations may determine how long you can store data in certain</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>storage tiers. For example, if you store hot data in cache or memory, you&#8217;ll<br/>likely need to set a time to live (TTL), so you can expire data after a certain<br/>point or persist it to warm or cold storage. Otherwise, your hot storage will<br/>become full, and queries against the hot data will suffer from performance<br/>lags.<br/><i>Compliance<br/></i>Certain regulations (e.g., HIPAA and Payment Card Industry, or PCI) might<br/>require you to keep data for a certain time. In these situations, the data<br/>simply needs to be accessible upon request, even if the likelihood of an<br/>access request is low. Other regulations might require you to hold data for<br/>only a limited period of time, and you&#8217;ll need to have the ability to delete<br/>specific information on time and within compliance guidelines. You&#8217;ll need<br/>a storage and archival data process&#8212;along with the ability to search the<br/>data&#8212;that fits the retention requirements of the particular regulation with<br/>which you need to comply. Of course, you&#8217;ll want to balance compliance<br/>against cost.<br/><i>Cost<br/></i>Data is an asset that (hopefully) has an ROI. On the cost side of ROI, an<br/>obvious storage expense is associated with data. Consider the timeline in<br/>which you need to retain data. Given our discussion about hot, warm, and<br/>cold data, implement automatic data lifecycle management practices and<br/>move the data to cold storage if you don&#8217;t need the data past the required<br/>retention date. Or delete data if it&#8217;s truly not needed.<br/></p>
<p><b>Single-Tenant Versus Multitenant Storage<br/></b>In Chapter 3, we covered the trade-offs between single-tenant and<br/>multitenant architecture. To recap, with <i>single-tenant</i> architecture, each<br/>group of tenants (e.g., individual users, groups of users, accounts, or<br/>customers) gets its own dedicated set of resources such as networking,<br/>compute, and storage. A <i>multitenant</i> architecture inverts this and shares<br/>these resources among groups of users. Both architectures are widely used.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>This section looks at the implications of single-tenant and multitenant<br/>storage.<br/>Adopting single-tenant storage means that every tenant gets their dedicated<br/>storage. In the example in Figure 6-10, each tenant gets a database. No data<br/>is shared among these databases, and storage is totally isolated. An example<br/>of using single-tenant storage is that each customer&#8217;s data must be stored in<br/>isolation and cannot be blended with any other customer&#8217;s data. In this case,<br/>each customer gets their own database.<br/></p>
<p><i>Figure 6-10. In single-tenant storage, each tenant gets their own database<br/></i></p>
<p>Separate data storage implies separate and independent schemas, bucket<br/>structures, and everything related to storage. This means you have the<br/>liberty of designing each tenant&#8217;s storage environment to be uniform or let<br/>them evolve however they may. Schema variation across customers can be<br/>an advantage and a complication; as always, consider the trade-offs. If each<br/>tenant&#8217;s schema isn&#8217;t uniform across all tenants, this has major<br/>consequences if you need to query multiple tenants&#8217; tables to create a<br/>unified view of all tenant data.<br/>Multitenant storage allows for the storage of multiple tenants within a<br/>single database. For example, instead of the single-tenant scenario where<br/>customers get their own database, multiple customers may reside in the<br/>same database schemas or tables in a multitenant database. Storing<br/>multitenant data means each tenant&#8217;s data is stored in the same place<br/>(Figure 6-11).</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><i>Figure 6-11. In this multitenant storage, four tenants occupy the same database<br/></i></p>
<p>You need to be aware of querying both single and multitenant storage,<br/>which we cover in more detail in Chapter 8.<br/></p>
<p><b>Whom You&#8217;ll Work With<br/></b>Storage is at the heart of data engineering infrastructure. You&#8217;ll interact<br/>with the people who own your IT infrastructure&#8212;typically, DevOps,<br/>security, and cloud architects. Defining domains of responsibility between<br/>data engineering and other teams is critical. Do data engineers have the<br/>authority to deploy their infrastructure in an AWS account, or must another<br/>team handle these changes? Work with other teams to define streamlined<br/>processes so that teams can work together efficiently and quickly.<br/>The division of responsibilities for data storage will depend significantly on<br/>the maturity of the organization involved. The data engineer will likely<br/>manage the storage systems and workflow if the company is early in its data<br/>maturity. If the company is later in its data maturity, the data engineer will<br/>probably manage a section of the storage system. This data engineer will</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>also likely interact with engineers on either side of storage&#8212;ingestion and<br/>transformation.<br/>The data engineer needs to ensure that the storage systems used by<br/>downstream users are securely available, contain high-quality data, have<br/>ample storage capacity, and perform when queries and transformations are<br/>run.<br/></p>
<p><b>Undercurrents<br/></b>The undercurrents for storage are significant because storage is a critical<br/>hub for all stages of the data engineering lifecycle. Unlike other<br/>undercurrents for which data might be in motion (ingestion) or queried and<br/>transformed, the undercurrents for storage differ because storage is so<br/>ubiquitous.<br/></p>
<p><b>Security<br/></b>While engineers often view security as an impediment to their work, they<br/>should embrace the idea that security is a key enabler. Robust security with<br/>fine-grained data access control allows data to be shared and consumed<br/>more widely within a business. The value of data goes up significantly<br/>when this is possible.<br/>As always, exercise the principle of least privilege. Don&#8217;t give full database<br/>access to anyone unless required. This means most data engineers don&#8217;t<br/>need full database access in practice. Also, pay attention to the column, row,<br/>and cell-level access controls in your database. Give users only the<br/>information they need and no more.<br/></p>
<p><b>Data Management<br/></b>Data management is critical as we read and write data with storage systems.<br/><b>Data catalogs and metadata management</b></p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Data is enhanced by robust metadata. Cataloging enables data scientists,<br/>analysts, and ML engineers by enabling data discovery. Data lineage<br/>accelerates the time to track down data problems and allows consumers to<br/>locate upstream raw sources. As you build out your storage systems, invest<br/>in your metadata. Integration of a data dictionary with these other tools<br/>allows users to share and record institutional knowledge robustly.<br/>Metadata management also significantly enhances data governance. Beyond<br/>simply enabling passive data cataloging and lineage, consider implementing<br/>analytics over these systems to get a clear, active picture of what&#8217;s<br/>happening with your data.<br/><b>Data versioning in object storage<br/></b>Major cloud object storage systems enable data versioning. Data versioning<br/>can help with error recovery when processes fail, and data becomes<br/>corrupted. Versioning is also beneficial for tracking the history of datasets<br/>used to build models. Just as code version control allows developers to<br/>track down commits that cause bugs, data version control can aid ML<br/>engineers in tracking changes that lead to model performance degradation.<br/><b>Privacy<br/></b>GDPR and other privacy regulations have significantly impacted storage<br/>system design. Any data with privacy implications has a lifecycle that data<br/>engineers must manage. Data engineers must be prepared to respond to data<br/>deletion requests and selectively remove data as required. In addition,<br/>engineers can accommodate privacy and security through anonymization<br/>and masking.<br/></p>
<p><b>DataOps<br/></b>DataOps is not orthogonal to data management, and a significant area of<br/>overlap exists. DataOps concerns itself with traditional operational<br/>monitoring of storage systems and monitoring the data itself, inseparable<br/>from metadata and quality.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><b>Systems monitoring<br/></b>Data engineers must monitor storage in a variety of ways. This includes<br/>monitoring infrastructure storage components, where they exist, but also<br/>monitoring object storage and other &#8220;serverless&#8221; systems. Data engineers<br/>should take the lead on FinOps (cost management), security monitoring,<br/>and access monitoring.<br/><b>Observing and monitoring data<br/></b>While metadata systems as we&#8217;ve described are critical, good engineering<br/>must consider the entropic nature of data by actively seeking to understand<br/>its characteristics and watching for major changes. Engineers can monitor<br/>data statistics, apply anomaly detection methods or simple rules, and<br/>actively test and validate for logical inconsistencies.<br/></p>
<p><b>Data Architecture<br/></b>Chapter 3 covers the basics of data architecture, as storage is the critical<br/>underbelly of the data engineering lifecycle.<br/>Consider the following data architecture tips. Design for required reliability<br/>and durability. Understand the upstream source systems and how that data,<br/>once ingested, will be stored and accessed. Understand the types of data<br/>models and queries that will occur downstream.<br/>If data is expected to grow, can you negotiate storage with your cloud<br/>provider? Take an active approach to FinOps, and treat it as a central part of<br/>architecture conversations. Don&#8217;t prematurely optimize, but prepare for<br/>scale if business opportunities exist in operating on large data volumes.<br/>Lean toward fully managed systems, and understand provider SLAs. Fully<br/>managed systems are generally far more robust and scalable than systems<br/>you have to babysit.<br/></p>
<p><b>Orchestration</b></p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Orchestration is highly entangled with storage. Storage allows data to flow<br/>through pipelines, and orchestration is the pump. Orchestration also helps<br/>engineers cope with the complexity of data systems, potentially combining<br/>many storage systems and query engines.<br/></p>
<p><b>Software Engineering<br/></b>We can think about software engineering in the context of storage in two<br/>ways. First, the code you write should perform well with your storage<br/>system. Make sure the code you write stores the data correctly and doesn&#8217;t<br/>accidentally cause data, memory leaks, or performance issues. Second,<br/>define your storage infrastructure as code and use ephemeral compute<br/>resources when it&#8217;s time to process your data. Because storage is<br/>increasingly distinct from compute, you can automatically spin resources up<br/>and down while keeping your data in object storage. This keeps your<br/>infrastructure clean and avoids coupling your storage and query layers.<br/></p>
<p><b>Conclusion<br/></b>Storage is everywhere and underlays many stages of the data engineering<br/>lifecycle. In this chapter, you learned about the raw ingredients, types,<br/>abstractions, and big ideas around storage systems. Gain deep knowledge of<br/>the inner workings and limitations of the storage systems you&#8217;ll use. Know<br/>the types of data, activities, and workloads appropriate for your storage.<br/></p>
<p><b>Additional Resources<br/></b>&#8220;Column-oriented DBMS&#8221; Wikipedia page<br/>&#8220;Rowise vs. Columnar Database? Theory and in Practice&#8221; by Mangat<br/>Rai Modi<br/>&#8220;The Design and Implementation of Modern Column-Oriented<br/>Database Systems&#8221; by Daniel Abadi et al.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>&#8220;Diving Into Delta Lake: Schema Enforcement and Evolution&#8221; by<br/>Burak Yavuz et al.<br/>&#8220;Snowflake Solution Anti-Patterns: The Probable Data Scientist&#8221; by<br/>John Aven<br/>&#8220;The What, When, Why, and How of Incremental Loads&#8221; by Tim<br/>Mitchell<br/>IDC&#8217;s &#8220;Data Creation and Replication Will Grow at a Faster Rate than<br/>Installed Storage Capacity, According to the IDC Global DataSphere<br/>and StorageSphere Forecasts&#8221; press release<br/>&#8220;What Is a Vector Database?&#8221; by Bryan Turriff<br/>&#8220;Hot Data vs. Cold Data: Why It Matters&#8221; by Afzaal Ahmad Zeeshan<br/></p>
<p>1  Andy Klein, &#8220;Hard Disk Drive (HDD) vs. Solid-State Drive (SSD): What&#8217;s the Diff?,&#8221;<br/>Backblaze blog, October 5, 2021, <i>https://oreil.ly/XBps8</i>.<br/></p>
<p>2  Benoit Dageville, &#8220;The Snowflake Elastic Data Warehouse,&#8221; <i>SIGMOD &#8217;16: Proceedings of<br/>the 2016 International Conference on Management of Data</i> (June 2016): 215&#8211;226,<br/><i>https://oreil.ly/Tc1su</i>.<br/></p>
<p>3  James Dixon, &#8220;Data Lakes Revisited,&#8221; <i>James Dixon&#8217;s Blog</i>, September 25, 2014,<br/><i>https://oreil.ly/FH25v</i>.<br/></p>
<p>4  Valliappa Lakshmanan and Jordan Tigani, <i>GoogleBig Query: The Definitive Guide<br/></i>(Seastopol, CA: O&#8217;Reilly, 2019), page 15, <i>https://oreil.ly/5aXXu</i></p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><b>Chapter 7. Ingestion<br/></b>You&#8217;ve learned about the various source systems you&#8217;ll likely encounter as<br/>a data engineer and about ways to store data. Let&#8217;s now turn our attention to<br/>the patterns and choices that apply to ingesting data from various source<br/>systems. In this chapter, we discuss data ingestion (see Figure 7-1), the key<br/>engineering considerations for the ingestion phase, the major patterns for<br/>batch and streaming ingestion, technologies you&#8217;ll encounter, whom you&#8217;ll<br/>work with as you develop your data ingestion pipeline, and how the<br/>undercurrents feature in the ingestion phase.<br/></p>
<p><i>Figure 7-1. To begin processing data, we must ingest it<br/></i></p>
<p><b>What Is Data Ingestion?<br/></b><i>Data ingestion</i> is the process of moving data from one place to another.<br/>Data ingestion implies data movement from source systems into storage in<br/>the data engineering lifecycle, with ingestion as an intermediate step<br/>(Figure 7-2).</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><i>Figure 7-2. Data from system 1 is ingested into system 2<br/></i></p>
<p>It&#8217;s worth quickly contrasting data ingestion with data integration. Whereas<br/><i>data ingestion</i> is data movement from point A to B, <i>data integration<br/></i>combines data from disparate sources into a new dataset. For example, you<br/>can use data integration to combine data from a CRM system, advertising<br/>analytics data, and web analytics to create a user profile, which is saved to<br/>your data warehouse. Furthermore, using reverse ETL, you can send this<br/>newly created user profile <i>back</i> to your CRM so salespeople can use the<br/>data for prioritizing leads. We describe data integration more fully in<br/>Chapter 8, where we discuss data transformations; reverse ETL is covered<br/>in Chapter 9.<br/>We also point out that data ingestion is different from <i>internal ingestion<br/></i>within a system. Data stored in a database is copied from one table to<br/>another, or data in a stream is temporarily cached. We consider this another<br/>part of the general data transformation process covered in Chapter 8.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><b>DATA PIPELINES DEFINED<br/></b>Data pipelines begin in source systems, but ingestion is the stage where<br/>data engineers begin actively designing data pipeline activities. In the<br/>data engineering space, a good deal of ceremony occurs around data<br/>movement and processing patterns, with established patterns such as<br/>ETL, newer patterns such as ELT, and new names for long-established<br/>practices (reverse ETL) and data sharing.<br/>All of these concepts are encompassed in the idea of a <i>data pipeline</i>. It<br/>is essential to understand the details of these various patterns and know<br/>that a modern data pipeline includes all of them. As the world moves<br/>away from a traditional monolithic approach with rigid constraints on<br/>data movement, and toward an open ecosystem of cloud services that<br/>are assembled like LEGO bricks to realize products, data engineers<br/>prioritize using the right tools to accomplish the desired outcome over<br/>adhering to a narrow philosophy of data movement.<br/>In general, here&#8217;s our definition of a data pipeline:<br/><i>A data pipeline is the combination of architecture, systems, and<br/>processes that move data through the stages of the data engineering<br/>lifecycle.<br/></i></p>
<p>Our definition is deliberately fluid&#8212;and intentionally vague&#8212;to allow<br/>data engineers to plug in whatever they need to accomplish the task at<br/>hand. A data pipeline could be a traditional ETL system, where data is<br/>ingested from an on-premises transactional system, passed through a<br/>monolithic processor, and written into a data warehouse. Or it could be<br/>a cloud-based data pipeline that pulls data from 100 sources, combines<br/>it into 20 wide tables, trains five other ML models, deploys them into<br/>production, and monitors ongoing performance. A data pipeline should<br/>be flexible enough to fit any needs along the data engineering lifecycle.<br/>Let&#8217;s keep this notion of data pipelines in mind as we proceed through<br/>this chapter.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><b>Key Engineering Considerations for the<br/>Ingestion Phase<br/></b>When preparing to architect or build an ingestion system, here are some<br/>primary considerations and questions to ask yourself related to data<br/>ingestion:<br/></p>
<p>What&#8217;s the use case for the data I&#8217;m ingesting?<br/>Can I reuse this data and avoid ingesting multiple versions of the same<br/>dataset?<br/>Where is the data going? What&#8217;s the destination?<br/>How often should the data be updated from the source?<br/>What is the expected data volume?<br/>What format is the data in? Can downstream storage and<br/>transformation accept this format?<br/>Is the source data in good shape for immediate downstream use? That<br/>is, is the data of good quality? What post-processing is required to<br/>serve it? What are data-quality risks (e.g., could bot traffic to a website<br/>contaminate the data)?<br/>Does the data require in-flight processing for downstream ingestion if<br/>the data is from a streaming source?<br/></p>
<p>These questions undercut batch and streaming ingestion and apply to the<br/>underlying architecture you&#8217;ll create, build, and maintain. Regardless of<br/>how often the data is ingested, you&#8217;ll want to consider these factors when<br/>designing your ingestion architecture:<br/></p>
<p>Bounded versus unbounded<br/>Frequency<br/>Synchronous versus asynchronous</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Serialization and deserialization<br/>Throughput and elastic scalability<br/>Reliability and durability<br/>Payload<br/>Push versus pull versus poll patterns<br/></p>
<p>Let&#8217;s look at each of these.<br/></p>
<p><b>Bounded Versus Unbounded<br/></b>As you might recall from Chapter 3, data comes in two forms: bounded and<br/>unbounded (Figure 7-3). <i>Unbounded data</i> is data as it exists in reality, as<br/>events happen, either sporadically or continuously, ongoing and flowing.<br/><i>Bounded data</i> is a convenient way of bucketing data across some sort of<br/>boundary, such as time.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><i>Figure 7-3. Bounded versus unbounded data<br/></i></p>
<p>Let us adopt this mantra: <i>All data is unbounded until it&#8217;s bounded.</i> Like<br/>many mantras, this one is not precisely accurate 100% of the time. The<br/>grocery list that I scribbled this afternoon is bounded data. I wrote it as a<br/>stream of consciousness (unbounded data) onto a piece of scrap paper,<br/>where the thoughts now exist as a list of things (bounded data) I need to buy<br/>at the grocery store. However, the idea is correct for practical purposes for<br/>the vast majority of data you&#8217;ll handle in a business context. For example,<br/>an online retailer will process customer transactions 24 hours a day until the<br/>business fails, the economy grinds to a halt, or the sun explodes.<br/>Business processes have long imposed artificial bounds on data by cutting<br/>discrete batches. Keep in mind the true unboundedness of your data;<br/>streaming ingestion systems are simply a tool for preserving the unbounded<br/>nature of data so that subsequent steps in the lifecycle can also process it<br/>continuously.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><b>Frequency<br/></b>One of the critical decisions that data engineers must make in designing<br/>data-ingestion processes is the data-ingestion frequency. Ingestion<br/>processes can be batch, micro-batch, or real-time.<br/>Ingestion frequencies vary dramatically from slow to fast (Figure 7-4). On<br/>the slow end, a business might ship its tax data to an accounting firm once a<br/>year. On the faster side, a CDC system could retrieve new log updates from<br/>a source database once a minute. Even faster, a system might continuously<br/>ingest events from IoT sensors and process these within seconds. Data-<br/>ingestion frequencies are often mixed in a company, depending on the use<br/>case and technologies.<br/></p>
<p><i>Figure 7-4. The spectrum batch to real-time ingestion frequencies<br/></i></p>
<p>We note that &#8220;real-time&#8221; ingestion patterns are becoming increasingly<br/>common. We put &#8220;real-time&#8221; in quotation marks because no ingestion<br/>system is genuinely real-time. Any database, queue or pipeline has inherent<br/>latency in delivering data to a target system. It is more accurate to speak of<br/><i>near real-time</i>, but we often use <i>real-time</i> for brevity. The near real-time<br/>pattern generally does away with an explicit update frequency; events are<br/>processed in the pipeline either one by one as they arrive or in micro-<br/>batches (i.e., batches over concise time intervals). For this book, we will use<br/><i>real-time</i> and <i>streaming</i> interchangeably.<br/>Even with a streaming data-ingestion process, batch processing downstream<br/>is relatively standard. At the time of this writing, ML models are typically<br/>trained on a batch basis, although continuous online training is becoming<br/>more prevalent. Rarely do data engineers have the option to build a purely<br/>near real-time pipeline with no batch components. Instead, they choose<br/>where batch boundaries will occur&#8212;i.e., the data engineering lifecycle data</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>will be broken into batches. Once data reaches a batch process, the batch<br/>frequency becomes a bottleneck for all downstream processing.<br/>In addition, streaming systems are the best fit for many data source types. In<br/>IoT applications, the typical pattern is for each sensor to write events or<br/>measurements to streaming systems as they happen. While this data can be<br/>written directly into a database, a streaming ingestion platform such as<br/>Amazon Kinesis or Apache Kafka is a better fit for the application.<br/>Software applications can adopt similar patterns by writing events to a<br/>message queue as they happen rather than waiting for an extraction process<br/>to pull events and state information from a backend database. This pattern<br/>works exceptionally well for event-driven architectures already exchanging<br/>messages through queues. And again, streaming architectures generally<br/>coexist with batch processing.<br/></p>
<p><b>Synchronous Versus Asynchronous Ingestion<br/></b>With <i>synchronous ingestion</i>, the source, ingestion, and destination have<br/>complex dependencies and are tightly coupled. As you can see in Figure 7-<br/>5, each stage of the data engineering lifecycle has processes A, B, and C<br/>directly dependent upon one another. If process A fails, processes B and C<br/>cannot start; if process B fails, process C doesn&#8217;t start. This type of<br/>synchronous workflow is common in older ETL systems, where data<br/>extracted from a source system must then be transformed before being<br/>loaded into a data warehouse. Processes downstream of ingestion can&#8217;t start<br/>until all data in the batch has been ingested. If the ingestion or<br/>transformation process fails, the entire process must be rerun.<br/></p>
<p><i>Figure 7-5. A synchronous ingestion process runs as discrete batch steps</i></p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Here&#8217;s a mini case study of how <i>not</i> to design your data pipelines. At one<br/>company, the transformation process itself was a series of dozens of tightly<br/>coupled synchronous workflows, with the entire process taking over 24<br/>hours to finish. If any step of that transformation pipeline failed, the whole<br/>transformation process had to be restarted from the beginning! In this<br/>instance, we saw process after process fail, and because of nonexistent or<br/>cryptic error messages, fixing the pipeline was a game of whack-a-mole<br/>that took over a week to diagnose and cure. Meanwhile, the business didn&#8217;t<br/>have updated reports during that time. People weren&#8217;t happy.<br/>With <i>asynchronous ingestion</i>, dependencies can now operate at the level of<br/>individual events, much as they would in a software backend built from<br/>microservices (Figure 7-6). Individual events become available in storage<br/>as soon as they are ingested individually. Take the example of a web<br/>application on AWS that emits events into Amazon Kinesis Data Streams<br/>(here acting as a buffer). The stream is read by Apache Beam, which parses<br/>and enriches events, and then forwards them to a second Kinesis stream;<br/>Kinesis Data Firehose rolls up events and writes objects to Amazon S3.<br/></p>
<p><i>Figure 7-6. Asynchronous processing of an event stream in AWS<br/></i></p>
<p>The big idea is that rather than relying on asynchronous processing, where a<br/>batch process runs for each stage as the input batch closes and certain time<br/>conditions are met, each stage of the asynchronous pipeline can process<br/>data items as they become available in parallel across the Beam cluster. The<br/>processing rate depends on available resources. The Kinesis Data Stream<br/>acts as the shock absorber, moderating the load so that event rate spikes will<br/>not overwhelm downstream processing. Events will move through the<br/>pipeline quickly when the event rate is low, and any backlog has cleared.<br/>Note that we could modify the scenario and use a Kinesis Data Stream for<br/>storage, eventually extracting events to S3 before they expire out of the<br/>stream.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><b>Serialization and Deserialization<br/></b>Moving data from source to destination involves serialization and<br/>deserialization. As a reminder, <i>serialization</i> means encoding the data from a<br/>source and preparing data structures for transmission and intermediate<br/>storage stages.<br/>When ingesting data, ensure that your destination can deserialize the data it<br/>receives. We&#8217;ve seen data ingested from a source but then sitting inert and<br/>unusable in the destination because the data cannot be properly deserialized.<br/>See the more extensive discussion of serialization in Appendix A.<br/></p>
<p><b>Throughput and Scalability<br/></b>In theory, your ingestion should never be a bottleneck. In practice, ingestion<br/>bottlenecks are pretty standard. Data throughput and system scalability<br/>become critical as your data volumes grow and requirements change.<br/>Design your systems to scale and shrink to flexibly match the desired data<br/>throughput.<br/>Where you&#8217;re ingesting data from matters a lot. If you&#8217;re receiving data as<br/>it&#8217;s generated, will the upstream system have any issues that might impact<br/>your downstream ingestion pipelines? For example, suppose a source<br/>database goes down. When it comes back online and attempts to backfill the<br/>lapsed data loads, will your ingestion be able to keep up with this sudden<br/>influx of backlogged data?<br/>Another thing to consider is your ability to handle bursty data ingestion.<br/>Data generation rarely happens at a constant rate and often ebbs and flows.<br/>Built-in buffering is required to collect events during rate spikes to prevent<br/>data from getting lost. Buffering bridges the time while the system scales<br/>and allows storage systems to accommodate bursts even in a dynamically<br/>scalable system.<br/>Whenever possible, use managed services that handle the throughput<br/>scaling for you. While you can manually accomplish these tasks by adding<br/>more servers, shards, or workers, often this isn&#8217;t value-added work, and</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>there&#8217;s a good chance you&#8217;ll miss something. Much of this heavy lifting is<br/>now automated. Don&#8217;t reinvent the data ingestion wheel if you don&#8217;t have<br/>to.<br/></p>
<p><b>Reliability and Durability<br/></b>Reliability and durability are vital in the ingestion stages of data pipelines.<br/><i>Reliability</i> entails high uptime and proper failover for ingestion systems.<br/><i>Durability</i> entails making sure that data isn&#8217;t lost or corrupted.<br/>Some data sources (e.g., IoT devices and caches) may not retain data if it is<br/>not correctly ingested. Once lost, it is gone for good. In this sense, the<br/><i>reliability</i> of ingestion systems leads directly to the <i>durability</i> of generated<br/>data. If data is ingested, downstream processes can theoretically run late if<br/>they break temporarily.<br/>Our advice is to evaluate the risks and build an appropriate level of<br/>redundancy and self-healing based on the impact and cost of losing data.<br/>Reliability and durability have both direct and indirect costs. For example,<br/>will your ingestion process continue if an AWS zone goes down? How<br/>about a whole region? How about the power grid or the internet? Of course,<br/>nothing is free. How much will this cost you? You might be able to build a<br/>highly redundant system and have a team on call 24 hours a day to handle<br/>outages. This also means your cloud and labor costs become prohibitive<br/>(direct costs), and the ongoing work takes a significant toll on your team<br/>(indirect costs). There&#8217;s no correct answer, and you need to evaluate the<br/>costs and benefits of your reliability and durability decisions.<br/>Don&#8217;t assume that you can build a system that will reliably and durably<br/>ingest data in every possible scenario. Even the nearly infinite budget of the<br/>US federal government can&#8217;t guarantee this. In many extreme scenarios,<br/>ingesting data actually won&#8217;t matter. There will be little to ingest if the<br/>internet goes down, even if you build multiple air-gapped data centers in<br/>underground bunkers with independent power. Continually evaluate the<br/>trade-offs and costs of reliability and durability.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><b>Payload<br/></b>A <i>payload</i> is the dataset you&#8217;re ingesting and has characteristics such as<br/>kind, shape, size, schema and data types, and metadata. Let&#8217;s look at some<br/>of these characteristics to understand why this matters.<br/><b>Kind<br/></b>The <i>kind</i> of data you handle directly impacts how it&#8217;s dealt with<br/>downstream in the data engineering lifecycle. Kind consists of type and<br/>format. Data has a type&#8212;tabular, image, video, text, etc. The type directly<br/>influences the data format or the way it is expressed in bytes, names, and<br/>file extensions. For example, a tabular kind of data may be in formats such<br/>as CSV or Parquet, with each of these formats having different byte patterns<br/>for serialization and deserialization. Another kind of data is an image,<br/>which has a format of JPG or PNG and is inherently unstructured.<br/><b>Shape<br/></b>Every payload has a <i>shape</i> that describes its dimensions. Data shape is<br/>critical across the data engineering lifecycle. For instance, an image&#8217;s pixel<br/>and red, green, blue (RGB) dimensions are necessary for training deep<br/>learning models. As another example, if you&#8217;re trying to import a CSV file<br/>into a database table, and your CSV has more columns than the database<br/>table, you&#8217;ll likely get an error during the import process. Here are some<br/>examples of the shapes of various kinds of data:<br/><i>Tabular<br/></i></p>
<p>The number of rows and columns in the dataset, commonly expressed<br/>as <i>M</i> rows and <i>N</i> columns<br/></p>
<p><i>Semistructured JSON<br/></i>The key-value pairs and nesting depth occur with subelements<br/></p>
<p><i>Unstructured text</i></p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Number of words, characters, or bytes in the text body<br/><i>Images<br/></i></p>
<p>The width, height, and RGB color depth (e.g., 8 bits per pixel)<br/><i>Uncompressed audio<br/></i></p>
<p>Number of channels (e.g., two for stereo), sample depth (e.g., 16 bits<br/>per sample), sample rate (e.g., 48 kHz), and length (e.g., 10,003<br/>seconds)<br/></p>
<p><b>Size<br/></b>The <i>size</i> of the data describes the number of bytes of a payload. A payload<br/>may range in size from single bytes to terabytes and larger. To reduce the<br/>size of a payload, it may be compressed into various formats such as ZIP<br/>and TAR (see the discussion of compression in Appendix A).<br/>A massive payload can also be split into chunks, which effectively reduces<br/>the size of the payload into smaller subsections. When loading a huge file<br/>into a cloud object storage or data warehouse, this is a common practice as<br/>the small individual files are easier to transmit over a network (especially if<br/>they&#8217;re compressed). The smaller chunked files are sent to their destination<br/>and then reassembled after all data has arrived.<br/><b>Schema and data types<br/></b>Many data payloads have a schema, such as tabular and semistructured<br/>data. As mentioned earlier in this book, a schema describes the fields and<br/>types of data within those fields. Other data, such as unstructured text,<br/>images, and audio, will not have an explicit schema or data types. However,<br/>they might come with technical file descriptions on shape, data and file<br/>format, encoding, size, etc.<br/>Although you can connect to databases in various ways (such as file export,<br/>CDC, JDBC/ODBC), the connection is easy. The great engineering</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>challenge is understanding the underlying schema. Applications organize<br/>data in various ways, and engineers need to be intimately familiar with the<br/>organization of the data and relevant update patterns to make sense of it.<br/>The problem has been somewhat exacerbated by the popularity of object-<br/>relational mapping (ORM), which automatically generates schemas based<br/>on object structure in languages such as Java or Python. Natural structures<br/>in an object-oriented language often map to something messy in an<br/>operational database. Data engineers may need to familiarize themselves<br/>with the class structure of application code.<br/>Schema is not only for databases. As we&#8217;ve discussed, APIs present their<br/>schema complications. Many vendor APIs have friendly reporting methods<br/>that prepare data for analytics. In other cases, engineers are not so lucky.<br/>The API is a thin wrapper around underlying systems, requiring engineers<br/>to understand application internals to use the data.<br/>Much of the work associated with ingesting from source schemas happens<br/>in the data engineering lifecycle transformation stage, which we discuss in<br/>Chapter 8. We&#8217;ve placed this discussion here because data engineers need to<br/>begin studying source schemas as soon they plan to ingest data from a new<br/>source.<br/>Communication is critical for understanding source data, and engineers also<br/>have the opportunity to reverse the flow of communication and help<br/>software engineers improve data where it is produced. Later in this chapter,<br/>we&#8217;ll return to this topic in &#8220;Whom You&#8217;ll Work With&#8221;.<br/><i>Detecting and handling schema changes in upstream and<br/>downstream systems<br/></i>Schema changes frequently occur in source systems and are often well out<br/>of data engineers&#8217; control. Examples of schema changes include the<br/>following:<br/></p>
<p>Adding a new column<br/>Changing a column type</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Creating a new table<br/>Renaming a column<br/></p>
<p>It&#8217;s becoming increasingly common for ingestion tools to automate the<br/>detection of schema changes and even auto-update target tables. Ultimately,<br/>this is something of a mixed blessing. Schema changes can still break<br/>pipelines downstream of staging and ingestion.<br/>Engineers must still implement strategies to respond to changes<br/>automatically and alert on changes that cannot be accommodated<br/>automatically. Automation is excellent, but the analysts and data scientists<br/>who rely on this data should be informed of the schema changes that violate<br/>existing assumptions. Even if automation can accommodate a change, the<br/>new schema may adversely affect the performance of reports and models.<br/>Communication between those making schema changes and those impacted<br/>by these changes is as important as reliable automation that checks for<br/>schema changes.<br/><i>Schema registries<br/></i>In streaming data, every message has a schema, and these schemas may<br/>evolve between producers and consumers. A <i>schema registry</i> is a metadata<br/>repository used to maintain schema and data type integrity in the face of<br/>constantly changing schemas. Schema registries can also track schema<br/>versions and history. It describes the data model for messages, allowing<br/>consistent serialization and deserialization between producers and<br/>consumers. Schema registries are used in most major data tools and clouds.<br/><b>Metadata<br/></b>In addition to the apparent characteristics we&#8217;ve just covered, a payload<br/>often contains metadata, which we first discussed in Chapter 2. Metadata is<br/>data about data. Metadata can be as critical as the data itself. One of the<br/>significant limitations of the early approach to the data lake&#8212;or data<br/>swamp, which could become a data superfund site&#8212;was a complete lack of<br/>attention to metadata. Without a detailed description of the data, it may be</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>of little value. We&#8217;ve already discussed some types of metadata (e.g.,<br/>schema) and will address them many times throughout this chapter.<br/></p>
<p><b>Push Versus Pull Versus Poll Patterns<br/></b>We mentioned push versus pull when we introduced the data engineering<br/>lifecycle in Chapter 2. A <i>push</i> strategy (Figure 7-7) involves a source<br/>system sending data to a target, while a <i>pull</i> strategy (Figure 7-8) entails a<br/>target reading data directly from a source. As we mentioned in that<br/>discussion, the lines between these strategies are blurry.<br/></p>
<p><i>Figure 7-7. Pushing data from source to destination<br/></i></p>
<p><i>Figure 7-8. A destination pulling data from a source<br/></i></p>
<p>Another pattern related to pulling is <i>polling</i> for data (Figure 7-9). Polling<br/>involves periodically checking a data source for any changes. When<br/>changes are detected, the destination pulls the data as it would in a regular<br/>pull situation.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><i>Figure 7-9. Polling for changes in a source system<br/></i></p>
<p><b>Batch Ingestion Considerations<br/></b>Batch ingestion, which involves processing data in bulk, is often a<br/>convenient way to ingest data. This means that data is ingested by taking a<br/>subset of data from a source system, based either on a time interval or the<br/>size of accumulated data (Figure 7-10).<br/></p>
<p><i>Figure 7-10. Time-interval batch ingestion<br/></i></p>
<p><i>Time-interval batch ingestion</i> is widespread in traditional business ETL for<br/>data warehousing. This pattern is often used to process data once a day,</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>overnight during off-hours, to provide daily reporting, but other frequencies<br/>can also be used.<br/><i>Size-based batch ingestion</i> (Figure 7-11) is quite common when data is<br/>moved from a streaming-based system into object storage; ultimately, you<br/>must cut the data into discrete blocks for future processing in a data lake.<br/>Some size-based ingestion systems can break data into objects based on<br/>various criteria, such as the size in bytes of the total number of events.<br/></p>
<p><i>Figure 7-11. Size-based batch ingestion<br/></i></p>
<p>Some commonly used batch ingestion patterns, which we discuss in this<br/>section, include the following:<br/></p>
<p>Snapshot or differential extraction<br/>File-based export and ingestion<br/>ETL versus ELT<br/>Inserts, updates, and batch size<br/>Data migration<br/></p>
<p><b>Snapshot or Differential Extraction<br/></b>Data engineers must choose whether to capture full snapshots of a source<br/>system or differential (sometimes called <i>incremental</i>) updates. With <i>full<br/>snapshots</i>, engineers grab the entire current state of the source system on<br/>each update read. With the <i>differential update</i> pattern, engineers can pull<br/>only the updates and changes since the last read from the source system.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>While differential updates are ideal for minimizing network traffic and<br/>target storage usage, full snapshot reads remain extremely common because<br/>of their simplicity.<br/></p>
<p><b>File-Based Export and Ingestion<br/></b>Data is quite often moved between databases and systems using files. Data<br/>is serialized into files in an exchangeable format, and these files are<br/>provided to an ingestion system. We consider file-based export to be a<br/><i>push-based</i> ingestion pattern. This is because data export and preparation<br/>work is done on the source system side.<br/>File-based ingestion has several potential advantages over a direct database<br/>connection approach. It is often undesirable to allow direct access to<br/>backend systems for security reasons. With file-based ingestion, export<br/>processes are run on the data-source side, giving source system engineers<br/>complete control over what data gets exported and how the data is<br/>preprocessed. Once files are done, they can be provided to the target system<br/>in various ways. Common file-exchange methods are object storage, secure<br/>file transfer protocol (SFTP), electronic data interchange (EDI), or secure<br/>copy (SCP).<br/></p>
<p><b>ETL Versus ELT<br/></b>Chapter 3 introduced ETL and ELT, both extremely common ingestion,<br/>storage, and transformation patterns you&#8217;ll encounter in batch workloads.<br/>This section covers the extract (<i>E</i>) and the load (<i>L</i>) parts of ETL and ELT.<br/><b>Extract<br/></b><i>Extract</i> means getting data from a source system. While <i>extract</i> seems to<br/>imply <i>pulling</i> data, it can also be push based. Extraction may also require<br/>reading metadata and schema changes.<br/><b>Load</b></p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Once data is extracted, it can either be transformed (ETL) before loading it<br/>into a storage destination or simply loaded into storage for future<br/>transformation. When loading data, you should be mindful of the type of<br/>system you&#8217;re loading, the schema of the data, and the performance impact<br/>of loading. We cover ETL and ELT in Chapter 8.<br/></p>
<p><b>Inserts, Updates, and Batch Size<br/></b>Batch-oriented systems often perform poorly when users attempt to perform<br/>many small-batch operations rather than a smaller number of large<br/>operations. For example, while it is common to insert one row at a time in a<br/>transactional database, this is a bad pattern for many columnar databases as<br/>it forces the creation of many small, suboptimal files, and forces the system<br/>to run a high number of <i>create object</i> operations. Running many small in-<br/>place update operations is an even bigger problem because it causes the<br/>database to scan each existing column file to run the update.<br/>Understand the appropriate update patterns for the database or data store<br/>you&#8217;re working with. Also, understand that certain technologies are<br/>purpose-built for high insert rates. For example, Apache Druid and Apache<br/>Pinot can handle high insert rates. SingleStore can manage hybrid<br/>workloads that combine OLAP and OLTP characteristics. BigQuery<br/>performs poorly on a high rate of vanilla SQL single-row inserts but<br/>extremely well if data is fed in through its stream buffer. Know the limits<br/>and characteristics of your tools.<br/></p>
<p><b>Data Migration<br/></b>Migrating data to a new database or environment is not usually trivial, and<br/>data needs to be moved in bulk. Sometimes this means moving data sizes<br/>that are hundreds of terabytes or much larger, often involving the migration<br/>of specific tables and moving entire databases and systems.<br/>Data migrations probably aren&#8217;t a regular occurrence as a data engineer, but<br/>you should be familiar with them. As is often the case for data ingestion,<br/>schema management is a crucial consideration. Suppose you&#8217;re migrating</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>data from one database system to a different one (say, SQL Server to<br/>Snowflake). No matter how closely the two databases resemble each other,<br/>subtle differences almost always exist in the way they handle schema.<br/>Fortunately, it is generally easy to test ingestion of a sample of data and find<br/>schema issues before undertaking a complete table migration.<br/>Most data systems perform best when data is moved in bulk rather than as<br/>individual rows or events. File or object storage is often an excellent<br/>intermediate stage for transferring data. Also, one of the biggest challenges<br/>of database migration is not the movement of the data itself but the<br/>movement of data pipeline connections from the old system to the new one.<br/>Be aware that many tools are available to automate various types of data<br/>migrations. Especially for large and complex migrations, we suggest<br/>looking at these options before doing this manually or writing your own<br/>migration solution.<br/></p>
<p><b>Message and Stream Ingestion<br/>Considerations<br/></b>Ingesting event data is common. This section covers issues you should<br/>consider when ingesting events, drawing on topics covered in Chapters 5<br/>and 6.<br/></p>
<p><b>Schema Evolution<br/></b>Schema evolution is common when handling event data; fields may be<br/>added or removed, or value types might change (say, a string to an integer).<br/>Schema evolution can have unintended impacts on your data pipelines and<br/>destinations. For example, an IoT device gets a firmware update that adds a<br/>new field to the event it transmits, or a third-party API introduces changes<br/>to its event payload or countless other scenarios. All of these potentially<br/>impact your downstream capabilities.<br/>To alleviate issues related to schema evolution, here are a few suggestions.<br/>First, if your event-processing framework has a schema registry (discussed</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>earlier in this chapter), use it to version your schema changes. Next, a dead-<br/>letter queue (described in &#8220;Error Handling and Dead-Letter Queues&#8221;) can<br/>help you investigate issues with events that are not properly handled.<br/>Finally, the low-fidelity route (and the most effective) is regularly<br/>communicating with upstream stakeholders about potential schema changes<br/>and proactively addressing schema changes with the teams introducing<br/>these changes instead of reacting to the receiving end of breaking changes.<br/></p>
<p><b>Late-Arriving Data<br/></b>Though you probably prefer all event data to arrive on time, event data<br/>might arrive late. A group of events might occur around the same time<br/>frame (similar event times), but some might arrive later than others (late<br/>ingestion times) because of various circumstances.<br/>For example, an IoT device might be late sending a message because of<br/>internet latency issues. This is common when ingesting data. You should be<br/>aware of late-arriving data and the impact on downstream systems and uses.<br/>Suppose you assume that ingestion or process time is the same as the event<br/>time. You may get some strange results if your reports or analysis depend<br/>on an accurate portrayal of when events occur. To handle late-arriving data,<br/>you need to set a cutoff time for when late-arriving data will no longer be<br/>processed.<br/></p>
<p><b>Ordering and Multiple Delivery<br/></b>Streaming platforms are generally built out of distributed systems, which<br/>can cause some complications. Specifically, messages may be delivered out<br/>of order and more than once (at-least-once delivery). See the event-<br/>streaming platforms discussion in Chapter 5 for more details.<br/></p>
<p><b>Replay<br/></b><i>Replay</i> allows readers to request a range of messages from the history,<br/>allowing you to rewind your event history to a particular point in time.<br/>Replay is a key capability in many streaming ingestion platforms and is</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>particularly useful when you need to re-ingest and reprocess data for a<br/>specific time range. For example, RabbitMQ typically deletes messages<br/>after all subscribers consume them. Kafka, Kinesis, and Pub/Sub all support<br/>event retention and replay.<br/></p>
<p><b>Time to Live<br/></b>How long will you preserve your event record? A key parameter is<br/><i>maximum message retention time</i>, also known as the <i>time to live</i> (TTL).<br/>TTL is usually a configuration you&#8217;ll set for how long you want events to<br/>live before they are acknowledged and ingested. Any unacknowledged<br/>event that&#8217;s not ingested after its TTL expires automatically disappears.<br/>This is helpful to reduce backpressure and unnecessary event volume in<br/>your event-ingestion pipeline.<br/>Find the right balance of TTL impact on our data pipeline. An extremely<br/>short TTL (milliseconds or seconds) might cause most messages to<br/>disappear before processing. A very long TTL (several weeks or months)<br/>will create a backlog of many unprocessed messages, resulting in long wait<br/>times.<br/>Let&#8217;s look at how some popular platforms handle TTL at the time of this<br/>writing. Google Cloud Pub/Sub supports retention periods of up to 7 days.<br/>Amazon Kinesis Data Streams retention can be turned up to 365 days.<br/>Kafka can be configured for indefinite retention, limited by available disk<br/>space. (Kafka also supports the option to write older messages to cloud<br/>object storage, unlocking virtually unlimited storage space and retention.)<br/></p>
<p><b>Message Size<br/></b>Message size is an easily overlooked issue: you must ensure that the<br/>streaming framework in question can handle the maximum expected<br/>message size. Amazon Kinesis supports a maximum message size of 1 MB.<br/>Kafka defaults to this maximum size but can be configured for a maximum<br/>of 20 MB or more. (Configurability may vary on managed service<br/>platforms.)</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><b>Error Handling and Dead-Letter Queues<br/></b>Sometimes events aren&#8217;t successfully ingested. Perhaps an event is sent to a<br/>nonexistent topic or message queue, the message size may be too large, or<br/>the event has expired past its TTL. Events that cannot be ingested need to<br/>be rerouted and stored in a separate location called a <i>dead-letter queue</i>.<br/>A dead-letter queue segregates problematic events from events that can be<br/>accepted by the consumer (Figure 7-12). If events are not rerouted to a<br/>dead-letter queue, these erroneous events risk blocking other messages from<br/>being ingested. Data engineers can use a dead-letter queue to diagnose why<br/>event ingestions errors occur and solve data pipeline problems, and might<br/>be able to reprocess some messages in the queue after fixing the underlying<br/>cause of errors.<br/></p>
<p><i>Figure 7-12. &#8220;Good&#8221; events are passed to the consumer, whereas &#8220;bad&#8221; events are stored in a dead-<br/>letter queue<br/></i></p>
<p><b>Consumer Pull and Push<br/></b>A consumer subscribing to a topic can get events in two ways: push and<br/>pull. Let&#8217;s look at the ways some streaming technologies pull and push data.<br/>Kafka and Kinesis support only pull subscriptions. Subscribers read<br/>messages from a topic and confirm when they have been processed. In<br/>addition, to pull subscriptions, Pub/Sub and RabbitMQ support push<br/>subscriptions, allowing these services to write messages to a listener.<br/>Pull subscriptions are the default choice for most data engineering<br/>applications, but you may want to consider push capabilities for specialized</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>applications. Note that pull-only message ingestion systems can still push if<br/>you add an extra layer to handle this.<br/></p>
<p><b>Location<br/></b>It is often desirable to integrate streaming across several locations for<br/>enhanced redundancy and to consume data close to where it is generated.<br/>As a general rule, the closer your ingestion is to where data originates, the<br/>better your bandwidth and latency. However, you need to balance this<br/>against the costs of moving data between regions to run analytics on a<br/>combined dataset. As always, data egress costs can spiral quickly. Do a<br/>careful evaluation of the trade-offs as you build out your architecture.<br/></p>
<p><b>Ways to Ingest Data<br/></b>Now that we&#8217;ve described some of the significant patterns underlying batch<br/>and streaming ingestion, let&#8217;s focus on ways you can ingest data. Although<br/>we will cite some common ways, keep in mind that the universe of data<br/>ingestion practices and technologies is vast and growing daily.<br/></p>
<p><b>Direct Database Connection<br/></b>Data can be pulled from databases for ingestion by querying and reading<br/>over a network connection. Most commonly, this connection is made using<br/>ODBC or JDBC.<br/>ODBC uses a driver hosted by a client accessing the database to translate<br/>commands issued to the standard ODBC API into commands issued to the<br/>database. The database returns query results over the wire, where the driver<br/>receives them and translates them back into a standard form and read by the<br/>client. For ingestion, the application utilizing the ODBC driver is an<br/>ingestion tool. The ingestion tool may pull data through many small queries<br/>or a single large query.<br/>JDBC is conceptually remarkably similar to ODBC. A Java driver connects<br/>to a remote database and serves as a translation layer between the standard</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>JDBC API and the native network interface of the target database. It might<br/>seem strange to have a database API dedicated to a single programming<br/>language, but there are strong motivations for this. The Java Virtual<br/>Machine (JVM) is standard, portable across hardware architectures and<br/>operating systems, and provides the performance of compiled code through<br/>a just-in-time (JIT) compiler. The JVM is an extremely popular compiling<br/>VM for running code in a portable manner.<br/>JDBC provides extraordinary database driver portability. ODBC drivers are<br/>shipped as OS and architecture native binaries; database vendors must<br/>maintain versions for each architecture/OS version that they wish to<br/>support. On the other hand, vendors can ship a single JDBC driver that is<br/>compatible with any JVM language (e.g., Java, Scala, Clojure, or Kotlin)<br/>and JVM data framework (i.e., Spark.) JDBC has become so popular that it<br/>is also used as an interface for non-JVM languages such as Python; the<br/>Python ecosystem provides translation tools that allow Python code to talk<br/>to a JDBC driver running on a local JVM.<br/>JDBC and ODBC are used extensively for data ingestion from relational<br/>databases, returning to the general concept of direct database connections.<br/>Various enhancements are used to accelerate data ingestion. Many data<br/>frameworks can parallelize several simultaneous connections and partition<br/>queries to pull data in parallel. On the other hand, nothing is free; using<br/>parallel connections also increases the load on the source database.<br/>JDBC and ODBC were long the gold standards for data ingestion from<br/>databases, but these connection standards are beginning to show their age<br/>for many data engineering applications. These connection standards<br/>struggle with nested data, and they send data as rows. This means that<br/>native nested data must be re-encoded as string data to be sent over the<br/>wire, and columns from columnar databases must be re-serialized as rows.<br/>As discussed in &#8220;File-Based Export and Ingestion&#8221;, many databases now<br/>support native file export that bypasses JDBC/ODBC and exports data<br/>directly in formats such as Parquet, ORC, and Avro. Alternatively, many<br/>cloud data warehouses provide direct REST APIs.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>JDBC connections should generally be integrated with other ingestion<br/>technologies. For example, we commonly use a reader process to connect to<br/>a database with JDBC, write the extracted data into multiple objects, and<br/>then orchestrate ingestion into a downstream system (see Figure 7-13). The<br/>reader process can run in a wholly ephemeral cloud instance or in an<br/>orchestration system.<br/></p>
<p><i>Figure 7-13. An ingestion process reads from a source database using JDBC, and then writes objects<br/>into object storage. A target database (not shown) can be triggered to ingest the data with an API<br/></i></p>
<p><i>call from an orchestration system.<br/></i></p>
<p><b>Change Data Capture<br/></b><i>Change data capture</i> (CDC). introduced in Chapter 2, is the process of<br/>ingesting changes from a source database system. For example, we might<br/>have a source PostgreSQL system that supports an application and<br/>periodically or continuously ingests table changes for analytics.<br/>Note that our discussion here is by no means exhaustive. We introduce you<br/>to common patterns but suggest that you read the documentation on a<br/>particular database to handle the details of CDC strategies.<br/><b>Batch-oriented CDC<br/></b>If the database table in question has an updated_at field containing the last<br/>time a record was written or updated, we can query the table to find all<br/>updated rows since a specified time. We set the filter timestamp based on<br/>when we last captured changed rows from the tables. This process allows us<br/>to pull changes and differentially update a target table.<br/>This form of batch-oriented CDC has a key limitation: while we can easily<br/>determine which rows have changed since a point in time, we don&#8217;t<br/>necessarily obtain all changes that were applied to these rows. Consider the<br/>example of running batch CDC on a bank account table every 24 hours.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>This operational table shows the current account balance for each account.<br/>When money is moved in and out of accounts, the banking application runs<br/>a transaction to update the balance.<br/>When we run a query to return all rows in the account table that changed in<br/>the last 24 hours, we&#8217;ll see records for each account that recorded a<br/>transaction. Suppose that a certain customer withdrew money five times<br/>using a debit card in the last 24 hours. Our query will return only the last<br/>account balance recorded in the 24 hour period; other records over the<br/>period won&#8217;t appear. This issue can be mitigated by utilizing an insert-only<br/>schema, where each account transaction is recorded as a new record in the<br/>table (see &#8220;Insert-Only&#8221;).<br/><b>Continuous CDC<br/></b><i>Continuous CDC</i> captures all table history and can support near real-time<br/>data ingestion, either for real-time database replication or to feed real-time<br/>streaming analytics. Rather than running periodic queries to get a batch of<br/>table changes, continuous CDC treats each write to the database as an<br/>event.<br/>We can capture an event stream for continuous CDC in a couple of ways.<br/>One of the most common approaches with a transactional database such as<br/>PostgreSQL is <i>log-based CDC</i>. The database binary log records every<br/>change to the database sequentially (see &#8220;Database Logs&#8221;) A CDC tool can<br/>read this log and send the events to a target, such as the Apache Kafka<br/>Debezium streaming platform.<br/>Some databases support a simplified, managed CDC paradigm. For<br/>instance, many cloud-hosted databases can be configured to directly trigger<br/>a serverless function or write to an event stream every time a change<br/>happens in the database. This completely frees engineers from worrying<br/>about the details of how events are captured in the database and forwarded.<br/><b>CDC and database replication<br/></b>CDC can be used to replicate between databases: events are buffered into a<br/>stream and <i>asynchronously</i> written into a second database. However, many</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>databases natively support a tightly coupled version of replication<br/>(synchronous replication) that keeps the replica fully in sync with the<br/>primary database. Synchronous replication typically requires that the<br/>primary database and the replica are of the same type (e.g., PostgreSQL to<br/>PostgreSQL). The advantage of synchronous replication is that the<br/>secondary database can offload work from the primary database by acting<br/>as a read replica; read queries can be redirected to the replica. The query<br/>will return the same results that would be returned from the primary<br/>database.<br/>Read replicas are often used in batch data ingestion patterns to allow large<br/>scans to run without overloading the primary production database. In<br/>addition, an application can be configured to fail over to the replica if the<br/>primary database becomes unavailable. No data will be lost in the failover<br/>because the replica is entirely in sync with the primary database.<br/>The advantage of asynchronous CDC replication is a loosely coupled<br/>architecture pattern. While the replica might be slightly delayed from the<br/>primary database, this is often not a problem for analytics applications, and<br/>events can now be directed to a variety of targets; we might run CDC<br/>replication while simultaneously directing events to object storage and a<br/>streaming analytics processor.<br/><b>CDC considerations<br/></b>Like anything in technology, CDC is not free. CDC consumes various<br/>database resources, such as memory, disk bandwidth, storage, CPU time,<br/>and network bandwidth. Engineers should work with production teams and<br/>run tests before turning on CDC on production systems to avoid operational<br/>problems. Similar considerations apply to synchronous replication.<br/>For batch CDC, be aware that running any large batch query against a<br/>transactional production system can cause excessive load. Either run such<br/>queries only at off-hours or use a read replica to avoid burdening the<br/>primary database.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><b>APIs<br/></b><i>The bulk of software engineering is just plumbing.<br/></i></p>
<p>&#8212;Karl Hughes<br/>As we mentioned in Chapter 5, APIs are a data source that continues to<br/>grow in importance and popularity. A typical organization may have<br/>hundreds of external data sources such as SaaS platforms or partner<br/>companies. The hard reality is that no proper standard exists for data<br/>exchange over APIs. Data engineers can spend a significant amount of time<br/>reading documentation, communicating with external data owners, and<br/>writing and maintaining API connection code.<br/>Three trends are slowly changing this situation. First, many vendors provide<br/>API client libraries for various programming languages that remove much<br/>of the complexity of API access.<br/>Second, numerous data connector platforms are available now as SaaS,<br/>open source, or managed open source. These platforms provide turnkey<br/>data connectivity to many data sources; they offer frameworks for writing<br/>custom connectors for unsupported data sources. See &#8220;Managed Data<br/>Connectors&#8221;.<br/>The third trend is the emergence of data sharing (discussed in Chapter 5)&#8212;<br/>i.e., the ability to exchange data through a standard platform such as<br/>BigQuery, Snowflake, Redshift, or S3. Once data lands on one of these<br/>platforms, it is straightforward to store it, process it, or move it somewhere<br/>else. Data sharing has had a large and rapid impact in the data engineering<br/>space.<br/>Don&#8217;t reinvent the wheel when data sharing is not an option and direct API<br/>access is necessary. While a managed service might look like an expensive<br/>option, consider the value of your time and the opportunity cost of building<br/>API connectors when you could be spending your time on higher-value<br/>work.<br/>In addition, many managed services now support building custom API<br/>connectors. This may provide API technical specifications in a standard</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>format or writing connector code that runs in a serverless function<br/>framework (e.g., AWS Lambda) while letting the managed service handle<br/>the details of scheduling and synchronization. Again, these services can be<br/>a huge time-saver for engineers, both for development and ongoing<br/>maintenance.<br/>Reserve your custom connection work for APIs that aren&#8217;t well supported<br/>by existing frameworks; you will find that there are still plenty of these to<br/>work on. Handling custom API connections has two main aspects: software<br/>development and ops. Follow software development best practices; you<br/>should use version control, continuous delivery, and automated testing. In<br/>addition to following DevOps best practices, consider an orchestration<br/>framework, which can dramatically streamline the operational burden of<br/>data ingestion.<br/></p>
<p><b>Message Queues and Event-Streaming Platforms<br/></b>Message queues and event-streaming platforms are widespread ways to<br/>ingest real-time data from web and mobile applications, IoT sensors, and<br/>smart devices. As real-time data becomes more ubiquitous, you&#8217;ll often find<br/>yourself either introducing or retrofitting ways to handle real-time data in<br/>your ingestion workflows. As such, it&#8217;s essential to know how to ingest<br/>real-time data. Popular real-time data ingestion includes message queues or<br/>event-streaming platforms, which we covered in Chapter 5. Though these<br/>are both source systems, they also act as ways to ingest data. In both cases,<br/>you consume events from the publisher you subscribe to.<br/>Recall the differences between messages and streams. A <i>message</i> is handled<br/>at the individual event level and is meant to be transient. Once a message is<br/>consumed, it is acknowledged and removed from the queue. On the other<br/>hand, a <i>stream</i> ingests events into an ordered log. The log persists for as<br/>long as you wish, allowing events to be queried over various ranges,<br/>aggregated, and combined with other streams to create new transformations<br/>published to downstream consumers. In Figure 7-14, we have two<br/>producers (producers 1 and 2) sending events to two consumers (consumers</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>1 and 2). These events are combined into a new dataset and sent to a<br/>producer for downstream consumption.<br/></p>
<p><i>Figure 7-14. Two datasets are produced and consumed (producers 1 and 2), and then combined, with<br/>the combined data published to a new producer (producer 3)<br/></i></p>
<p>The last point is an essential difference between batch and streaming<br/>ingestion. Whereas batch usually involves static workflows (ingest data,<br/>store it, transform it, and serve it), messages and streams are fluid. Ingestion<br/>can be nonlinear, with data being published, consumed, republished, and re-<br/>consumed. When designing your real-time ingestion workflows, keep in<br/>mind how data will flow.<br/>Another consideration is the throughput of your real-time data pipelines.<br/>Messages and events should flow with as little latency as possible, meaning<br/>you should provision adequate partition (or shard) bandwidth and<br/>throughput. Provide sufficient memory, disk, and CPU resources for event<br/>processing, and if you&#8217;re managing your real-time pipelines, incorporate<br/>autoscaling to handle spikes and save money as load decreases. For these<br/>reasons, managing your streaming platform can entail significant overhead.<br/>Consider managed services for your real-time ingestion pipelines, and focus<br/>your attention on ways to get value from your real-time data.<br/></p>
<p><b>Managed Data Connectors<br/></b>These days, if you&#8217;re considering writing a data ingestion connector to a<br/>database or API, ask yourself: has this already been created? Furthermore,<br/>is there a service that will manage the nitty-gritty details of this connection<br/>for me? &#8220;APIs&#8221; mentions the popularity of managed data connector</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>platforms and frameworks. These tools aim to provide a standard set of<br/>connectors available out of the box to spare data engineers building<br/>complicated plumbing to connect to a particular source. Instead of creating<br/>and managing a data connector, you outsource this service to a third party.<br/>Generally, options in the space allow users to set a target and source, ingest<br/>in various ways (e.g., CDC, replication, truncate and reload), set<br/>permissions and credentials, configure an update frequency, and begin<br/>syncing data. The vendor or cloud behind the scenes fully manages and<br/>monitors data syncs. If data synchronization fails, you&#8217;ll receive an alert<br/>with logged information on the cause of the error.<br/>We suggest using managed connector platforms instead of creating and<br/>managing your connectors. Vendors and OSS projects each typically have<br/>hundreds of prebuilt connector options and can easily create custom<br/>connectors. The creation and management of data connectors is largely<br/>undifferentiated heavy lifting these days and should be outsourced<br/>whenever possible.<br/></p>
<p><b>Moving Data with Object Storage<br/></b>Object storage is a multitenant system in public clouds, and it supports<br/>storing massive amounts of data. This makes object storage ideal for<br/>moving data in and out of data lakes, between teams, and transferring data<br/>between organizations. You can even provide short-term access to an object<br/>with a signed URL, giving a user temporary permission.<br/>In our view, object storage is the most optimal and secure way to handle file<br/>exchange. Public cloud storage implements the latest security standards, has<br/>a robust track record of scalability and reliability, accepts files of arbitrary<br/>types and sizes, and provides high-performance data movement. We<br/>discussed object storage much more extensively in Chapter 6.<br/></p>
<p><b>EDI<br/></b>Another practical reality for data engineers is <i>electronic data interchange<br/></i>(EDI). The term is vague enough to refer to any data movement method. It</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>usually refers to somewhat archaic means of file exchange, such as by email<br/>or flash drive. Data engineers will find that some data sources do not<br/>support more modern means of data transport, often because of archaic IT<br/>systems or human process limitations.<br/>Engineers can at least enhance EDI through automation. For example, they<br/>can set up a cloud-based email server that saves files onto company object<br/>storage as soon as they are received. This can trigger orchestration<br/>processes to ingest and process data. This is much more robust than an<br/>employee downloading the attached file and manually uploading it to an<br/>internal system, which we still frequently see.<br/></p>
<p><b>Databases and File Export<br/></b>Engineers should be aware of how the source database systems handle file<br/>export. Export involves large data scans that significantly load the database<br/>for many transactional systems. Source system engineers must assess when<br/>these scans can be run without affecting application performance and might<br/>opt for a strategy to mitigate the load. Export queries can be broken into<br/>smaller exports by querying over key ranges or one partition at a time.<br/>Alternatively, a read replica can reduce load. Read replicas are especially<br/>appropriate if exports happen many times a day and coincide with a high<br/>source system load.<br/>Major cloud data warehouses are highly optimized for direct file export. For<br/>example, Snowflake, BigQuery, Redshift, and others support direct export<br/>to object storage in various formats.<br/></p>
<p><b>Practical Issues with Common File Formats<br/></b>Engineers should also be aware of the file formats to export. CSV is still<br/>ubiquitous and highly error prone at the time of this writing. Namely,<br/>CSV&#8217;s default delimiter is also one of the most familiar characters in the<br/>English language&#8212;the comma! But it gets worse.<br/>CSV is by no means a uniform format. Engineers must stipulate the<br/>delimiter, quote characters, and escaping to appropriately handle the export</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>of string data. CSV also doesn&#8217;t natively encode schema information or<br/>directly support nested structures. CSV file encoding and schema<br/>information must be configured in the target system to ensure appropriate<br/>ingestion. Autodetection is a convenience feature provided in many cloud<br/>environments but is inappropriate for production ingestion. As a best<br/>practice, engineers should record CSV encoding and schema details in file<br/>metadata.<br/>More robust and expressive export formats include Parquet, Avro, Arrow,<br/>and ORC or JSON. These formats natively encode schema information and<br/>handle arbitrary string data with no particular intervention. Many of them<br/>also handle nested data structures natively so that JSON fields are stored<br/>using internal nested structures rather than simple strings. For columnar<br/>databases, columnar formats (Parquet, Arrow, ORC) allow more efficient<br/>data export because columns can be directly transcoded between formats.<br/>These formats are also generally more optimized for query engines. The<br/>Arrow file format is designed to map data directly into processing engine<br/>memory, providing high performance in data lake environments.<br/>The disadvantage of these newer formats is that many of them are not<br/>natively supported by source systems. Data engineers are often forced to<br/>work with CSV data and then build robust exception handling and error<br/>detection to ensure data quality on ingestion. See Appendix A for a more<br/>extensive discussion of file formats.<br/></p>
<p><b>Shell<br/></b>The <i>shell</i> is an interface by which you may execute commands to ingest<br/>data. The shell can be used to script workflows for virtually any software<br/>tool, and shell scripting is still used extensively in ingestion processes. A<br/>shell script might read data from a database, reserialize it into a different<br/>file format, upload it to object storage, and trigger an ingestion process in a<br/>target database. While storing data on a single instance or server is not<br/>highly scalable, many of our data sources are not particularly large, and<br/>such approaches work just fine.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In addition, cloud vendors generally provide robust CLI-based tools. It is<br/>possible to run complex ingestion processes simply by issuing commands to<br/>the AWS CLI. As ingestion processes grow more complicated and the SLA<br/>grows more stringent, engineers should consider moving to a proper<br/>orchestration system.<br/></p>
<p><b>SSH<br/></b><i>SSH</i> is not an ingestion strategy but a protocol used with other ingestion<br/>strategies. We use SSH in a few ways. First, SSH can be used for file<br/>transfer with SCP, as mentioned earlier. Second, SSH tunnels are used to<br/>allow secure, isolated connections to databases.<br/>Application databases should never be directly exposed on the internet.<br/>Instead, engineers can set up a bastion host&#8212;i.e., an intermediate host<br/>instance that can connect to the database in question. This host machine is<br/>exposed on the internet, although locked down for minimal access from<br/>only specified IP addresses to specified ports. To connect to the database, a<br/>remote machine first opens an SSH tunnel connection to the bastion host,<br/>and then connects from the host machine to the database.<br/></p>
<p><b>SFTP and SCP<br/></b>Accessing and sending data both from secure FTP (SFTP) and secure copy<br/>(SCP) are techniques you should be familiar with, even if data engineers do<br/>not typically use these regularly (IT or security/secOps will handle this).<br/>Engineers rightfully cringe at the mention of SFTP (occasionally, we even<br/>hear instances of FTP being used in production). Regardless, SFTP is still a<br/>practical reality for many businesses. They work with partner businesses<br/>that consume or provide data using SFTP, and are unwilling to rely on other<br/>standards. To avoid data leaks, security analysis is critical in these<br/>situations.<br/>SCP is a file-exchange protocol that runs over an SSH connection related to<br/>SSH. SCP can be a secure file-transfer option if it is configured correctly.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Again, adding additional network access control (defense in depth) to<br/>enhance SCP security is highly recommended.<br/></p>
<p><b>Webhooks<br/></b><i>Webhooks</i>, as we discussed in Chapter 5, are often referred to as <i>reverse<br/>APIs</i>. For a typical REST data API, the data provider gives engineers API<br/>specifications that they use to write their data ingestion code. The code<br/>makes requests and receives data in responses.<br/>With a webhook (Figure 7-15), the data provider defines an API request<br/>specification, but the data provider <i>makes API calls</i> rather than receiving<br/>them; it&#8217;s the data consumer&#8217;s responsibility to provide an API endpoint for<br/>the provider to call. The consumer is responsible for ingesting each request<br/>and handling data aggregation, storage, and processing.<br/></p>
<p><i>Figure 7-15. A basic webhook ingestion architecture built from cloud services<br/></i></p>
<p>Webhook-based data ingestion architectures can be brittle, difficult to<br/>maintain, and inefficient. Using appropriate off-the-shelf tools, data<br/>engineers can build more robust webhook architectures with lower<br/>maintenance and infrastructure costs. For example, a webhook pattern in<br/>AWS might use a serverless function framework (Lambda) to receive<br/>incoming events, a managed event-streaming platform to store and buffer<br/>messages (Kinesis), a stream-processing framework to handle real-time<br/>analytics (Flink), and an object store for long-term storage (S3).<br/>You&#8217;ll notice that this architecture does much more than simply ingest the<br/>data. This underscores ingestion&#8217;s entanglement with the other stages of the</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>data engineering lifecycle; it is often impossible to define your ingestion<br/>architecture without making decisions about storage and processing.<br/></p>
<p><b>Web Interface<br/></b>Web interfaces for data access remain a practical reality for data engineers.<br/>We frequently run into situations where not all data and functionality in a<br/>SaaS platform is exposed through automated interfaces such as APIs and<br/>file drops. Instead, someone must manually access a web interface, generate<br/>a report, and download a file to a local machine. This has obvious<br/>drawbacks, such as people forgetting to run the report or having their laptop<br/>die. Where possible, choose tools and workflows that allow for automated<br/>access to data.<br/></p>
<p><b>Web Scraping<br/></b><i>Web scraping</i> automatically extracts data from web pages, often by<br/>combing the web page&#8217;s various HTML elements. You might scrape<br/>ecommerce sites to extract product pricing information or scrape multiple<br/>news sites for your news aggregator. Web scraping is widespread, and you<br/>may encounter it as a data engineer. It&#8217;s also a murky area where ethical and<br/>legal lines are blurry.<br/>Here is some top-level advice to be aware of before undertaking any web-<br/>scraping project. First, ask yourself if you should be web scraping or if data<br/>is available from a third party. If your decision is to web scrape, be a good<br/>citizen. Don&#8217;t inadvertently create a denial-of-service (DoS) attack, and<br/>don&#8217;t get your IP address blocked. Understand how much traffic you<br/>generate and pace your web-crawling activities appropriately. Just because<br/>you can spin up thousands of simultaneous Lambda functions to scrape<br/>doesn&#8217;t mean you should; excessive web scraping could lead to the<br/>disabling of your AWS account.<br/>Second, be aware of the legal implications of your activities. Again,<br/>generating DoS attacks can entail legal consequences. Actions that violate<br/>terms of service may cause headaches for your employer or you personally.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Third, web pages constantly change their HTML element structure, making<br/>it tricky to keep your web scraper updated. Ask yourself, is the headache of<br/>maintaining these systems worth the effort?<br/>Web scraping has interesting implications for the data engineering lifecycle<br/>processing stage; engineers should think about various factors at the<br/>beginning of a web-scraping project. What do you intend to do with the<br/>data? Are you just pulling required fields from the scraped HTML by using<br/>Python code and then writing these values to a database? Do you intend to<br/>maintain the complete HTML code of the scraped websites and process this<br/>data using a framework like Spark? These decisions may lead to very<br/>different architectures downstream of ingestion.<br/></p>
<p><b>Transfer Appliances for Data Migration<br/></b>For massive data (100 TB or more), transferring data directly over the<br/>internet may be a slow and costly process. At this scale, the fastest, most<br/>efficient way to move data is not over the wire but by truck. Cloud vendors<br/>offer the ability to send your data via a physical &#8220;box of hard drives.&#8221;<br/>Simply order a storage device, called a <i>transfer appliance</i>, load your data<br/>from your servers, and then send it back to the cloud vendor, which will<br/>upload your data.<br/>The suggestion is to consider using a transfer appliance if your data size<br/>hovers around 100 TB. On the extreme end, AWS even offers Snowmobile,<br/>a transfer appliance sent to you in a semitrailer! Snowmobile is intended to<br/>lift and shift an entire data center, in which data sizes are in the petabytes or<br/>greater.<br/>Transfer appliances are handy for creating hybrid-cloud or multicloud<br/>setups. For example, Amazon&#8217;s data transfer appliance (AWS Snowball)<br/>supports import and export. To migrate into a second cloud, users can<br/>export their data into a Snowball device, and then import it into a second<br/>transfer appliance to move data into GCP or Azure. This might sound<br/>awkward, but even when it&#8217;s feasible to push data over the internet between</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>clouds, data egress fees make this a costly proposition. Physical transfer<br/>appliances are a cheaper alternative when the data volumes are significant.<br/>Remember that transfer appliances and data migration services are one-time<br/>data ingestion events and are not suggested for ongoing workloads. Suppose<br/>you have workloads requiring constant data movement in either a hybrid or<br/>multicloud scenario. In that case, your data sizes are presumably batching<br/>or streaming much smaller data sizes on an ongoing basis.<br/></p>
<p><b>Data Sharing<br/></b><i>Data sharing</i> is growing as a popular option for consuming data (see<br/>Chapters 5 and 6.) Data providers will offer datasets to third-party<br/>subscribers, either for free or at a cost. These datasets are often shared in a<br/>read-only fashion, meaning you can integrate these datasets with your own<br/>data (and other third-party datasets), but you do not own the shared dataset.<br/>In the strict sense, this isn&#8217;t ingestion, where you get physical possession of<br/>the dataset. If the data provider decides to remove your access to a dataset,<br/>you&#8217;ll no longer have access to it.<br/>Many cloud platforms offer data sharing, allowing you to share your data<br/>and consume data from various providers. Some of these platforms also<br/>provide data marketplaces where companies and organizations can offer<br/>their data for sale.<br/></p>
<p><b>Whom You&#8217;ll Work With<br/></b>Data ingestion sits at several organizational boundaries. In developing and<br/>managing data ingestion pipelines, data engineers will work with both<br/>people and systems sitting upstream (data producers) and downstream (data<br/>consumers).<br/></p>
<p><b>Upstream Stakeholders<br/></b>A significant disconnect often exists between those responsible for<br/><i>generating data</i>&#8212;typically, software engineers&#8212;and the data engineers</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>who will prepare this data for analytics and data science. Software<br/>engineers and data engineers usually sit in separate organizational silos; if<br/>they think about data engineers, they typically see them simply as<br/>downstream consumers of the data exhaust from their application, not as<br/>stakeholders.<br/>We see this current state of affairs as a problem and a significant<br/>opportunity. Data engineers can improve the quality of their data by inviting<br/>software engineers to be stakeholders in data engineering outcomes. The<br/>vast majority of software engineers are well aware of the value of analytics<br/>and data science but don&#8217;t necessarily have aligned incentives to contribute<br/>to data engineering efforts directly.<br/>Simply improving communication is a significant first step. Often software<br/>engineers have already identified potentially valuable data for downstream<br/>consumption. Opening a communication channel encourages software<br/>engineers to get data into shape for consumers and communicate about data<br/>changes to prevent pipeline regressions.<br/>Beyond communication, data engineers can highlight the contributions of<br/>software engineers to team members, executives, and especially product<br/>managers. Involving product managers in the outcome and treating<br/>downstream data processed as part of a product encourages them to allocate<br/>scarce software development to collaboration with data engineers. Ideally,<br/>software engineers can work partially as extensions of the data engineering<br/>team; this allows them to collaborate on various projects, such as creating<br/>an event-driven architecture to enable real-time analytics.<br/></p>
<p><b>Downstream Stakeholders<br/></b>Who is the ultimate customer for data ingestion? Data engineers focus on<br/>data practitioners and technology leaders such as data scientists, analysts,<br/>and chief technical officers. They would do well also to remember their<br/>broader circle of business stakeholders such as marketing directors, vice<br/>presidents over the supply chain, and CEOs.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Too often, we see data engineers pursuing sophisticated projects (e.g., real-<br/>time streaming buses or complex data systems) while digital marketing<br/>managers next door are left downloading Google Ads reports manually.<br/>View data engineering as a business, and recognize who your customers<br/>are. Often basic automation of ingestion processes has significant value,<br/>especially for departments like marketing that control massive budgets and<br/>sit at the heart of revenue for the business. Basic ingestion work may seem<br/>tedious, but delivering value to these core parts of the company will open<br/>up more budget and more exciting long-term data engineering<br/>opportunities.<br/>Data engineers can also invite more executive participation in this<br/>collaborative process. For a good reason, data-driven culture is quite<br/>fashionable in business leadership circles. Still, it is up to data engineers<br/>and other data practitioners to provide executives with guidance on the best<br/>structure for a data-driven business. This means communicating the value<br/>of lowering barriers between data producers and data engineers while<br/>supporting executives in breaking down silos and setting up incentives to<br/>lead to a more unified data-driven culture.<br/>Once again, <i>communication</i> is the watchword. Honest communication early<br/>and often with stakeholders will go a long way to ensure that your data<br/>ingestion adds value.<br/></p>
<p><b>Undercurrents<br/></b>Virtually all the undercurrents touch the ingestion phase, but we&#8217;ll<br/>emphasize the most salient ones here.<br/></p>
<p><b>Security<br/></b>Moving data introduces security vulnerabilities because you have to transfer<br/>data between locations. The last thing you want is to capture or compromise<br/>the data while moving.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Consider where the data lives and where it is going. Data that needs to<br/>move within your VPC should use secure endpoints and never leave the<br/>confines of the VPC. Use a VPN or a dedicated private connection if you<br/>need to send data between the cloud and an on-premises network. This<br/>might cost money, but the security is a good investment. If your data<br/>traverses the public internet, ensure that the transmission is encrypted. It is<br/>always a good practice to encrypt data over the wire.<br/></p>
<p><b>Data Management<br/></b>Naturally, data management begins at data ingestion. This is the starting<br/>point for lineage and data cataloging; from this point on, data engineers<br/>need to think about master data management, ethics, privacy, and<br/>compliance.<br/><b>Schema changes<br/></b>Schema changes (such as adding, changing, or removing columns in a<br/>database table) remain, from our perspective, an unsettled issue in data<br/>management. The traditional approach is a careful command-and-control<br/>review process. Working with clients at large enterprises, we have been<br/>quoted lead times of six months for the addition of a single field. This is an<br/>unacceptable impediment to agility.<br/>On the opposite end of the spectrum, any schema change in the source<br/>triggers target tables to be re-created with the new schema. This solves<br/>schema problems at the ingestion stage but can still break downstream<br/>pipelines and destination storage systems.<br/>One possible solution, which we, the authors, have meditated on for a<br/>while, is an approach pioneered by Git version control. When Linus<br/>Torvalds was developing Git, many of his choices were inspired by the<br/>limitations of Concurrent Versions System (CVS). CVS is completely<br/>centralized; it supports only one current official version of the code, stored<br/>on a central project server. To make Git a truly distributed system, Torvalds</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>used the notion of a tree; each developer could maintain their processed<br/>branch of the code and then merge to or from other branches.<br/>A few years ago, such an approach to data was unthinkable. On-premises<br/>MPP systems are typically operated at close to maximum storage capacity.<br/>However, storage is cheap in big data and cloud data warehouse<br/>environments. One may quite easily maintain multiple versions of a table<br/>with different schemas and even different upstream transformations. Teams<br/>can support various &#8220;development&#8221; versions of a table by using<br/>orchestration tools such as Airflow; schema changes, upstream<br/>transformation, and code changes can appear in development tables before<br/>official changes to the <i>main</i> table.<br/><b>Data ethics, privacy, and compliance<br/></b>Clients often ask for our advice on encrypting sensitive data in databases,<br/>which generally leads us to ask a fundamental question: do you need the<br/>sensitive data you&#8217;re trying to encrypt? As it turns out, this question often<br/>gets overlooked when creating requirements and solving problems.<br/>Data engineers should always train themselves to ask this question when<br/>setting up ingestion pipelines. They will inevitably encounter sensitive data;<br/>the natural tendency is to ingest it and forward it to the next step in the<br/>pipeline. But if this data is not needed, why collect it at all? Why not simply<br/>drop sensitive fields before data is stored? Data cannot leak if it is never<br/>collected.<br/>Where it is truly necessary to keep track of sensitive identities, it is<br/>common practice to apply tokenization to anonymize identities in model<br/>training and analytics. But engineers should look at where this tokenization<br/>is used. If possible, hash data at ingestion time.<br/>Data engineers cannot avoid working with highly sensitive data in some<br/>cases. Some analytics systems must present identifiable, sensitive<br/>information. Engineers must act under the highest ethical standards<br/>whenever they handle sensitive data. In addition, they can put in place a<br/>variety of practices to reduce the direct handling of sensitive data. Aim as</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>much as possible for <i>touchless production</i> where sensitive data is involved.<br/>This means that engineers develop and test code on simulated or cleansed<br/>data in development and staging environments but automated code<br/>deployments to production.<br/>Touchless production is an ideal that engineers should strive for, but<br/>situations inevitably arise that cannot be fully solved in development and<br/>staging environments. Some bugs may not be reproducible without looking<br/>at the live data that is triggering a regression. For these cases, put a broken-<br/>glass process in place: require at least two people to approve access to<br/>sensitive data in the production environment. This access should be tightly<br/>scoped to a particular issue and come with an expiration date.<br/>Our last bit of advice on sensitive data: be wary of naive technological<br/>solutions to human problems. Both encryption and tokenization are often<br/>treated like privacy magic bullets. Most cloud-based storage systems and<br/>nearly all databases encrypt data at rest and in motion by default. Generally,<br/>we don&#8217;t see encryption problems but data access problems. Is the solution<br/>to apply an extra layer of encryption to a single field or to control access to<br/>that field? After all, one must still tightly manage access to the encryption<br/>key. Legitimate use cases exist for single-field encryption, but watch out for<br/>ritualistic encryption.<br/>On the tokenization front, use common sense and assess data access<br/>scenarios. If someone had the email of one of your customers, could they<br/>easily hash the email and find the customer in your data? Thoughtlessly<br/>hashing data without salting and other strategies may not protect privacy as<br/>well as you think.<br/></p>
<p><b>DataOps<br/></b>Reliable data pipelines are the cornerstone of the data engineering lifecycle.<br/>When they fail, all downstream dependencies come to a screeching halt.<br/>Data warehouses and data lakes aren&#8217;t replenished with fresh data, and data<br/>scientists and analysts can&#8217;t effectively do their jobs; the business is forced<br/>to fly blind.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Ensuring that your data pipelines are properly monitored is a crucial step<br/>toward reliability and effective incident response. If there&#8217;s one stage in the<br/>data engineering lifecycle where monitoring is critical, it&#8217;s in the ingestion<br/>stage. Weak or nonexistent monitoring means the pipelines may or may not<br/>be working. Referring back to our earlier discussion on time, be sure to<br/>track the various aspects of time&#8212;event creation, ingestion, process, and<br/>processing times. Your data pipelines should predictably process data in<br/>batches or streams. We&#8217;ve seen countless examples of reports and ML<br/>models generated from stale data. In one extreme case, an ingestion pipeline<br/>failure wasn&#8217;t detected for six months. (One might question the concrete<br/>utility of the data in this instance, but that&#8217;s another matter.) This was very<br/>much avoidable through proper monitoring.<br/>What should you monitor? Uptime, latency, and data volumes processed are<br/>good places to start. If an ingestion job fails, how will you respond? In<br/>general, build monitoring into your pipelines from the beginning rather than<br/>waiting for deployment.<br/>Monitoring is key, and knowledge of the behavior of the upstream systems<br/>you depend on and how they generate data. You should be aware of the<br/>number of events generated per time interval you&#8217;re concerned with<br/>(events/minute, events/second, and so on) and the average size of each<br/>event. Your data pipeline should handle both the frequency and size of the<br/>events you&#8217;re ingesting.<br/>This also applies to third-party services. In the case of these services, what<br/>you&#8217;ve gained in terms of lean operational efficiencies (reduced headcount)<br/>is replaced by systems you depend on being outside of your control. If<br/>you&#8217;re using a third-party service (cloud, data integration service, etc.), how<br/>will you be alerted if there&#8217;s an outage? What&#8217;s your response plan if a<br/>service you depend on suddenly goes offline?<br/>Sadly, no universal response plan exists for third-party failures. If you can<br/>fail over to other servers, preferably in another zone or region, definitely set<br/>this up.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>If your data ingestion processes are built internally, do you have the proper<br/>testing and deployment automation to ensure that the code functions in<br/>production? And if the code is buggy or fails, can you roll it back to a<br/>working version?<br/><b>Data-quality tests<br/></b>We often refer to data as a silent killer. If quality, valid data is the<br/>foundation of success in today&#8217;s businesses, using bad data to make<br/>decisions is much worse than having no data. Bad data has caused untold<br/>damage to businesses; these data disasters are sometimes called<br/><i>datastrophes.<br/></i>Data is entropic; it often changes in unexpected ways without warning. One<br/>of the inherent differences between DevOps and DataOps is that we expect<br/>software regressions only when we deploy changes, while data often<br/>presents regressions independently because of events outside our control.<br/>DevOps engineers are typically able to detect problems by using binary<br/>conditions. Has the request failure rate breached a certain threshold? How<br/>about response latency? In the data space, regressions often manifest as<br/>subtle statistical distortions. Is a change in search-term statistics a result of<br/>customer behavior? Of a spike in bot traffic that has escaped the net? Of a<br/>site test tool deployed in some other part of the company?<br/>Like system failures in DevOps, some data regressions are immediately<br/>visible. For example, in the early 2000s, Google provided search terms to<br/>websites when users arrived from search. In 2011, Google began<br/>withholding this information in some cases to protect user privacy better.<br/>Analysts quickly saw &#8220;not provided&#8221; bubbling to the tops of their reports.<br/>The truly dangerous data regressions are silent and can come from inside or<br/>outside a business. Application developers may change the meaning of<br/>database fields without adequately communicating with data teams.<br/>Changes to data from third-party sources may go unnoticed. In the best-case<br/>scenario, reports break in obvious ways. Often business metrics are<br/>distorted unbeknownst to decision makers.<br/></p>
<p>1<br/></p>
<p>2</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Whenever possible, work with software engineers to fix data-quality issues<br/>at the source. It&#8217;s surprising how many data-quality issues can be handled<br/>by respecting basic best practices in software engineering, such as logs to<br/>capture the history of data changes, checks (nulls, etc.), and exception<br/>handling (try, catch, etc.).<br/>Traditional data testing tools are generally built on simple binary logic. Are<br/>nulls appearing in a non-nullable field? Are new, unexpected items showing<br/>up in a categorical column? Statistical data testing is a new realm, but one<br/>that is likely to grow dramatically in the next five years.<br/></p>
<p><b>Orchestration<br/></b>Ingestion generally sits at the beginning of a large and complex data graph;<br/>since ingestion is the first stage of the data engineering lifecycle, ingested<br/>data will flow into many more data processing steps, and data from many<br/>sources will commingle in complex ways. As we&#8217;ve emphasized throughout<br/>this book, orchestration is a crucial process for coordinating these steps.<br/>Organizations in an early stage of data maturity may choose to deploy<br/>ingestion processes as simple scheduled cron jobs. However, it is crucial to<br/>recognize that this approach is brittle and can slow the velocity of data<br/>engineering deployment and development.<br/>As data pipeline complexity grows, true orchestration is necessary. By true<br/>orchestration, we mean a system capable of scheduling complete task<br/>graphs rather than individual tasks. An orchestration can start each<br/>ingestion task at the appropriate scheduled time. Downstream processing<br/>and transform steps begin as ingestion tasks are completed. Further<br/>downstream, processing steps lead to additional processing steps.<br/></p>
<p><b>Software Engineering<br/></b>The ingestion stage of the data engineering lifecycle is engineering<br/>intensive. This stage sits at the edge of the data engineering domain and<br/>often interfaces with external systems, where software and data engineers<br/>have to build a variety of custom plumbing.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Behind the scenes, ingestion is incredibly complicated, often with teams<br/>operating open source frameworks like Kafka or Pulsar, or some of the<br/>biggest tech companies running their own forked or homegrown ingestion<br/>solutions. As discussed in this chapter, managed data connectors have<br/>simplified the ingestion process, such as Fivetran, Matillion, and Airbyte.<br/>Data engineers should take advantage of the best available tools&#8212;primarily,<br/>managed tools and services that do a lot of the heavy lifting for you&#8212;and<br/>develop high software development competency in areas where it matters. It<br/>pays to use proper version control and code review processes and<br/>implement appropriate tests even for any ingestion-related code.<br/>When writing software, your code needs to be decoupled. Avoid writing<br/>monolithic systems with tight dependencies on the source or destination<br/>systems.<br/></p>
<p><b>Conclusion<br/></b>In your work as a data engineer, ingestion will likely consume a significant<br/>part of your energy and effort. At the heart, ingestion is plumbing,<br/>connecting pipes to other pipes, ensuring that data flows consistently and<br/>securely to its destination. At times, the minutiae of ingestion may feel<br/>tedious, but the exciting data applications (e.g., analytics and ML) cannot<br/>happen without it.<br/>As we&#8217;ve emphasized, we&#8217;re also in the midst of a sea change, moving from<br/>batch toward streaming data pipelines. This is an opportunity for data<br/>engineers to discover interesting applications for streaming data,<br/>communicate these to the business, and deploy exciting new technologies.<br/></p>
<p><b>Additional Resources<br/></b>Google Cloud&#8217;s &#8220;Streaming Pipelines&#8221; web page<br/>Microsoft&#8217;s &#8220;Snapshot Window (Azure Stream Analytics)&#8221;<br/>documentation</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Airbyte&#8217;s &#8220;Connections and Sync Modes&#8221; web page<br/></p>
<p>1  Andy Petrella, &#8220;Datastrophes,&#8221; <i>Medium</i>, March 1, 2021, <i>https://oreil.ly/h6FRW</i>.<br/>2  Danny Sullivan, &#8220;Dark Google: One Year Since Search Terms Went &#8216;Not Provided,&#8217;&#8221;<br/></p>
<p><i>MarTech</i>, October 19, 2012, https://oreil.ly/Fp8ta</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><b>Chapter 8. Queries, Modeling, and<br/>Transformation<br/></b>Up to this point, the stages of the data engineering lifecycle have primarily been about passing data from one place<br/>to another or storing it. In this chapter, you&#8217;ll learn how to make data useful. By understanding queries, modeling,<br/>and transformations (see Figure 8-1), you&#8217;ll have the tools to turn raw data ingredients into something consumable<br/>by downstream stakeholders.<br/></p>
<p><i>Figure 8-1. Transformations allow us to create value from data<br/></i></p>
<p>We&#8217;ll first discuss queries and the significant patterns underlying them. Second, we will look at the major data<br/>modeling patterns you can use to introduce business logic into your data. Then, we&#8217;ll cover transformations, which<br/>take the logic of your data models and the results of queries and make them useful for more straightforward<br/>downstream consumption. Finally, we&#8217;ll cover whom you&#8217;ll work with and the undercurrents as they relate to this<br/>chapter.<br/>A variety of techniques can be used to query, model, and transform data in SQL and NoSQL databases. This<br/>section focuses on queries made to an OLAP system, such as a data warehouse or data lake. Although many<br/>languages exist for querying, for the sake of convenience and familiarity, throughout most of this chapter, we&#8217;ll<br/>focus heavily on SQL, the most popular and universal query language. Most of the concepts for OLAP databases<br/>and SQL will translate to other types of databases and query languages. This chapter assumes you have an<br/>understanding of the SQL language, and related concepts like primary and foreign keys. If these ideas are<br/>unfamiliar to you, countless resources are available to help you get started.<br/>A note on the terms used in this chapter. For convenience, we&#8217;ll use the term <i>database</i> as a shorthand for a query<br/>engine and the storage it&#8217;s querying; this could be a cloud data warehouse or Apache Spark querying data stored in<br/>S3. We assume the database has a storage engine that organizes the data under the hood. This extends to file-based<br/>queries (loading a CSV file into a Python notebook) and queries against file formats such as Parquet.<br/>Also, note that this chapter focuses mainly on the query, modeling patterns, and transformations related to<br/>structured and semistructured data, which data engineers use often. Many of the practices discussed can also be<br/>applied to working with unstructured data such as images, video, and raw text.<br/>Before we get into modeling and transforming data, let&#8217;s look at queries&#8212;what they are, how they work,<br/>considerations for improving query performance, and queries on streaming data.<br/></p>
<p><b>Queries</b></p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Queries are a fundamental part of data engineering, data science, and analysis. Before you learn about the<br/>underlying patterns and technologies for transformations, you need to understand what queries are, how they work<br/>on various data, and techniques for improving query performance.<br/>This section primarily concerns itself with queries on tabular and semistructured data. As a data engineer, you&#8217;ll<br/>most frequently query and transform these data types. Before we get into more complicated topics about queries<br/>and transformations, let&#8217;s start by answering a pretty simple question: what is a query?<br/></p>
<p><b>What Is a Query?<br/></b>We often run into people who know how to write SQL but are unfamiliar with how a query works under the hood.<br/>Some of this introductory material on queries will be familiar to experienced data engineers; feel free to skip ahead<br/>if this applies to you.<br/>A <i>query</i> allows you to retrieve and act on data. Recall our conversation in Chapter 5 about CRUD. When a query<br/>retrieves data, it is issuing a request to read a pattern of records. This is the <i>R</i> (read) in CRUD. You might issue a<br/>query that gets all records from a table foo, such as SELECT * FROM foo. Or, you might apply a predicate (logical<br/>condition) to filter your data by retrieving only records where the id is 1, using the SQL query SELECT * FROM<br/>foo WHERE id=1.<br/>Many databases allow you to create, update, and delete data. These are the <i>CUD</i> in CRUD; your query will either<br/>create, mutate, or destroy existing records. Let&#8217;s review some other common acronyms you&#8217;ll run into when<br/>working with query languages.<br/><b>Data definition language<br/></b>At a high level, you first need to create the database objects before adding data. You&#8217;ll use <i>data definition<br/>language</i> (DDL) commands to perform operations on database objects, such as the database itself, schemas, tables,<br/>or users; DDL defines the state of objects in your database.<br/>Data engineers use common SQL DDL expressions: CREATE, DROP, and UPDATE. For example, you can create a<br/>database by using the DDL expression CREATE DATABASE bar. After that, you can also create new tables (CREATE<br/>table bar_table) or delete a table (DROP table bar_table).<br/><b>Data manipulation language<br/></b>After using DDL to define database objects, you need to add and alter data within these objects, which is the<br/>primary purpose of <i>data manipulation language</i> (DML). Some common DML commands you&#8217;ll use as a data<br/>engineer are as follows:<br/></p>
<p>SELECT <br/>INSERT <br/>UPDATE <br/>DELETE <br/>COPY <br/>MERGE<br/></p>
<p>For example, you can INSERT new records into a database table, UPDATE existing ones, and SELECT specific<br/>records.<br/><b>Data control language<br/></b>You most likely want to limit access to database objects and finely control <i>who</i> has access to <i>what</i>. <i>Data control<br/>language</i> (DCL) allows you to control access to the database objects or the data by using SQL commands such as<br/>GRANT, DENY, and REVOKE.<br/>Let&#8217;s walk through a brief example using DCL commands. A new data scientist named Sarah joins your company,<br/>and she needs read-only access to a database called <i>data_science_db</i>. You give Sarah access to this database by<br/>using the following DCL command:</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><b>GRANT</b> <b>SELECT</b> <b>ON</b> data_science_db <b>TO</b> user_name Sarah;<br/></p>
<p>It&#8217;s a hot job market, and Sarah has worked at the company for only a few months before getting poached by a big<br/>tech company. So long, Sarah! Being a security-minded data engineer, you remove Sarah&#8217;s ability to read from the<br/>database:<br/></p>
<p><b>REVOKE</b> <b>SELECT</b> <b>ON</b> data_science_db <b>TO</b> user_name Sarah;<br/></p>
<p>Access-control requests and issues are common, and understanding DCL will help you resolve problems if you or<br/>a team member can&#8217;t access the data they need, as well as prevent access to data they don&#8217;t need.<br/><b>TCL<br/></b><i>TCL</i> stands for <i>transaction control language</i>. As the name suggests, TCL supports commands that control the<br/>details of transactions. With TCL, we can define commit checkpoints, conditions when actions will be rolled back,<br/>and more. Two common TCL commands include COMMIT and ROLLBACK.<br/></p>
<p><b>The Life of a Query<br/></b>How does a query work, and what happens when a query is executed? Let&#8217;s cover the high-level basics of query<br/>execution (Figure 8-2), using an example of a typical SQL query executing in a database.<br/></p>
<p><i>Figure 8-2. The life of a SQL query in a database<br/></i></p>
<p>While running a query might seem simple&#8212;write code, run it, and get results&#8212;a lot is going on under the hood.<br/>When you execute a SQL query, here&#8217;s a summary of what happens:<br/></p>
<p>1. The database engine compiles the SQL, parsing the code to check for proper semantics and ensuring that the<br/>database objects referenced exist and that the current user has the appropriate access to these objects.<br/></p>
<p>2. The SQL code is converted into bytecode. This bytecode expresses the steps that must be executed on the<br/>database engine in an efficient, machine-readable format.<br/></p>
<p>3. The database&#8217;s query optimizer analyzes the bytecode to determine how to execute the query, reordering and<br/>refactoring steps to use available resources as efficiently as possible.<br/></p>
<p>4. The query is executed, and results are produced.<br/></p>
<p><b>The Query Optimizer<br/></b>Queries can have wildly different execution times, depending on how they&#8217;re executed. A query optimizer&#8217;s job is<br/>to optimize query performance and minimize costs by breaking the query into appropriate steps in an efficient<br/>order. The optimizer will assess joins, indexes, data scan size, and other factors. The query optimizer attempts to<br/>execute the query in the least expensive manner.<br/>Query optimizers are fundamental to how your query will perform. Every database is different and executes<br/>queries in ways that are obviously and subtly different from each other. You won&#8217;t directly work with a query<br/>optimizer, but understanding some of its functionality will help you write more performant queries. You&#8217;ll need to<br/>know how to analyze a query&#8217;s performance, using things like an explain plan or query analysis, described in the<br/>following section.<br/></p>
<p><b>Improving Query Performance</b></p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In data engineering, you&#8217;ll inevitably encounter poorly performing queries. Knowing how to identify and fix these<br/>queries is invaluable. Don&#8217;t fight your database. Learn to work with its strengths and augment its weaknesses. This<br/>section shows various ways to improve your query performance.<br/><b>Optimize your JOIN strategy and schema<br/></b>A single dataset (such as a table or file) is rarely useful on its own; we create value by combining it with other<br/>datasets. <i>Joins</i> are one of the most common means of combining datasets and creating new ones. We assume that<br/>you&#8217;re familiar with the significant types of joins (e.g., inner, outer, left, cross) and the types of join relationships<br/>(e.g., one to one, one to many, many to one, and many to many).<br/>Joins are critical in data engineering and are well supported and performant in many databases. Even columnar<br/>databases, which in the past had a reputation for slow join performance, now generally offer excellent<br/>performance.<br/>A common technique for improving query performance is to <i>pre-join</i> data. If you find that analytics queries are<br/>joining the same data repeatedly, it often makes sense to join the data in advance and have queries read from the<br/>pre-joined version of the data so that you&#8217;re not repeating computationally intensive work. This may mean<br/>changing the schema and relaxing normalization conditions to widen tables and utilize newer data structures (such<br/>as arrays or structs) for replacing frequently joined entity relationships. Another strategy is maintaining a more<br/>normalized schema but pre-joining tables for the most common analytics and data science use cases. We can<br/>simply create pre-joined tables and train users to utilize these or join inside materialized views (see &#8220;Materialized<br/>Views, Federation, and Query Virtualization&#8221;).<br/>Next, consider the details and complexity of your join conditions. Complex join logic may consume significant<br/>computational resources. We can improve performance for complex joins in a few ways.<br/>Many row-oriented databases allow you to index a result computed from a row. For instance, PostgreSQL allows<br/>you to create an index on a string field converted to lowercase; when the optimizer encounters a query where the<br/>lower() function appears inside a predicate, it can apply the index. You can also create a new derived column for<br/>joining, though you will need to train users to join on this column.<br/></p>
<p><b>ROW EXPLOSION<br/></b>An obscure but frustrating problem is row explosion.  This occurs when we have a large number of many-to-<br/>many matches, either because of repetition in join keys or as a consequence of join logic. Suppose the join key<br/>in table A has the value this repeated five times, and the join key in table B contains this same value repeated<br/>10 times. This leads to a cross-join of these rows: every this row from table A paired with every this row<br/>from table B. This creates 5 &#215; 10 = 50 rows in the output. Now suppose that many other repeats are in the join<br/>key. Row explosion often generates enough rows to consume a massive quantity of database resources or even<br/>cause a query to fail.<br/>It is also essential to know how your query optimizer handles joins. Some databases can reorder joins and<br/>predicates, while others cannot. A row explosion in an early query stage may cause the query to fail, even<br/>though a later predicate should correctly remove many of the repeats in the output. Predicate reordering can<br/>significantly reduce the computational resources required by a query.<br/></p>
<p>Finally, use common table expressions (CTEs) instead of nested subqueries or temporary tables. CTEs allow users<br/>to compose complex queries together in a readable fashion, helping you understand the flow of your query. The<br/>importance of readability for complex queries cannot be understated.<br/>In many cases, CTEs will also deliver better performance than a script that creates intermediate tables; if you have<br/>to create intermediate tables, consider creating temporary tables. If you&#8217;d like to learn more about CTEs, a quick<br/>web search will yield plenty of helpful information.<br/><b>Use the explain plan and understand your query&#8217;s performance<br/></b></p>
<p>1</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>As you learned in the preceding section, the database&#8217;s query optimizer influences the execution of a query. The<br/>query optimizer&#8217;s explain plan will show you how the query optimizer determined its optimum lowest-cost query,<br/>the database objects used (tables, indexes, cache, etc.), and various resource consumption and performance<br/>statistics in each query stage. Some databases provide a visual representation of query stages. In contrast, others<br/>make the explain plan available via SQL with the EXPLAIN command, which displays the sequence of steps the<br/>database will take to execute the query.<br/>In addition to using EXPLAIN to understand <i>how</i> your query will run, you should monitor your query&#8217;s<br/>performance, viewing metrics on database resource consumption. The following are some areas to monitor:<br/></p>
<p>Usage of key resources such as disk, memory, and network.<br/>Data loading time versus processing time.<br/>Query execution time, number of records, the size of the data scanned, and the quantity of data shuffled.<br/>Competing queries that might cause resource contention in your database.<br/>Number of concurrent connections used versus connections available. Oversubscribed concurrent connections<br/>can have negative effects on your users who may not be able to connect to the database.<br/></p>
<p><b>Avoid full table scans<br/></b>All queries scan data, but not all scans are created equal. As a rule of thumb, you should query only the data you<br/>need. When you run SELECT * with no predicates, you&#8217;re scanning the entire table and retrieving every row and<br/>column. This is very inefficient performance-wise and expensive, especially if you&#8217;re using a pay-as-you-go<br/>database that charges you either for bytes scanned or compute resources utilized while a query is running.<br/>Whenever possible, use <i>pruning</i> to reduce the quantity of data scanned in a query. Columnar and row-oriented<br/>databases require different pruning strategies. In a column-oriented database, you should select only the columns<br/>you need. Most column-oriented OLAP databases also provide additional tools for optimizing your tables for<br/>better query performance. For instance, if you have a very large table (several terabytes in size or greater)<br/>Snowflake and BigQuery give you the option to define a cluster key on a table, which orders the table&#8217;s data in a<br/>way that allows queries to more efficiently access portions of very large datasets. BigQuery also allows you to<br/>partition a table into smaller segments, allowing you to query only specific partitions instead of the entire table.<br/>(Be aware that inappropriate clustering and key distribution strategies can degrade performance.)<br/>In row-oriented databases, pruning usually centers around table indexes, which you learned in Chapter 6. The<br/>general strategy is to create table indexes that will improve performance for your most performance-sensitive<br/>queries while not overloading the table with so many indexes such that you degrade performance.<br/><b>Know how your database handles commits<br/></b>A database <i>commit</i> is a change within a database, such as creating, updating, or deleting a record, table, or other<br/>database objects. Many databases support <i>transactions</i>&#8212;i.e., a notion of committing several operations<br/>simultaneously in a way that maintains a consistent state. Please note that the term <i>transaction</i> is somewhat<br/>overloaded; see Chapter 5. The purpose of a transaction is to keep a consistent state of a database both while it&#8217;s<br/>active and in the event of a failure. Transactions also handle isolation when multiple concurrent events might be<br/>reading, writing, and deleting from the same database objects. Without transactions, users would get potentially<br/>conflicting information when querying a database.<br/>You should be intimately familiar with how your database handles commits and transactions, and determine the<br/>expected consistency of query results. Does your database handle writes and updates in an ACID-compliant<br/>manner? Without ACID compliance, your query might return unexpected results. This could result from a dirty<br/>read, which happens when a row is read and an uncommitted transaction has altered the row. Are dirty reads an<br/>expected behavior of your database? If so, how do you handle this? Also, be aware that during update and delete<br/>transactions, some databases create new files to represent the new state of the database and retain the old files for</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>failure checkpoint references. In these databases, running a large number of small commits can lead to clutter and<br/>consume significant storage space that might need to be vacuumed periodically.<br/>Let&#8217;s briefly consider three databases to understand the impact of commits (note these examples are current as of<br/>the time of this writing). First, suppose we&#8217;re looking at a PostgreSQL RDBMS and applying ACID transactions.<br/>Each transaction consists of a package of operations that will either fail or succeed as a group. We can also run<br/>analytics queries across many rows; these queries will present a consistent picture of the database at a point in<br/>time.<br/>The disadvantage of the PostgreSQL approach is that it requires <i>row locking</i> (blocking reads and writes to certain<br/>rows), which can degrade performance in various ways. PostgreSQL is not optimized for large scans or the<br/>massive amounts of data appropriate for large-scale analytics applications.<br/>Next, consider Google BigQuery. It utilizes a point-in-time full table commit model. When a read query is issued,<br/>BigQuery will read from the latest committed snapshot of the table. Whether the query runs for one second or two<br/>hours, it will read only from that snapshot and will not see any subsequent changes. BigQuery does not lock the<br/>table while I read from it. Instead, subsequent write operations will create new commits and new snapshots while<br/>the query continues to run on the snapshot where it started.<br/>To prevent the inconsistent state, BigQuery allows only one write operation at a time. In this sense, BigQuery<br/>provides no write concurrency whatsoever. (In the sense that it can write massive amounts of data in parallel <i>inside<br/>a single write query</i>, it is highly concurrent.) If more than one client attempts to write simultaneously, write queries<br/>are queued in order of arrival. BigQuery&#8217;s commit model is similar to the commit models used by Snowflake,<br/>Spark, and others.<br/>Last, let&#8217;s consider MongoDB. We refer to MongoDB as a <i>variable consistency database</i>. Engineers have various<br/>configurable consistency options, both for the database and at the level of individual queries. MongoDB is<br/>celebrated for its extraordinary scalability and write concurrency but is somewhat notorious for issues that arise<br/>when engineers abuse it.<br/>For instance, in certain modes, MongoDB supports ultra-high write performance. However, this comes at a cost:<br/>the database will unceremoniously and silently discard writes if it gets overwhelmed with traffic. This is perfectly<br/>suitable for applications that can stand to lose some data&#8212;for example, IoT applications where we simply want<br/>many measurements but don&#8217;t care about capturing all measurements. It is not a great fit for applications that need<br/>to capture exact data and statistics.<br/>None of this is to say these are bad databases. They&#8217;re all fantastic databases when they are chosen for appropriate<br/>applications and configured correctly. The same goes for virtually any database technology.<br/>Companies don&#8217;t hire engineers simply to hack on code in isolation. To be worthy of their title, engineers should<br/>develop a deep understanding of the problems they&#8217;re tasked with solving and the technology tools. This applies to<br/>commit and consistency models and every other aspect of technology performance. Appropriate technology<br/>choices and configuration can ultimately differentiate extraordinary success and massive failure. Refer to<br/>Chapter 6 for a deeper discussion of consistency.<br/><b>Vacuum dead records<br/></b>As we just discussed, transactions incur the overhead of creating new records during certain operations, such as<br/>updates, deletes, and index operations, while retaining the old records as pointers to the last state of the database.<br/>As these old records accumulate in the database filesystem, they eventually no longer need to be referenced. You<br/>should remove these dead records in a process called <i>vacuuming</i>.<br/>You can vacuum a single table, multiple tables, or all tables in a database. No matter how you choose to vacuum,<br/>deleting dead database records is important for a few reasons. First, it frees up space for new records, leading to<br/>less table bloat and faster queries. Second, new and relevant records mean query plans are more accurate; outdated<br/>records can lead the query optimizer to generate suboptimal and inaccurate plans. Finally, vacuuming cleans up<br/>poor indexes, allowing for better index performance.<br/></p>
<p>2</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Vacuum operations are handled differently depending on the type of database. For example, in databases backed by<br/>object storage (BigQuery, Snowflake, Databricks), the only downside of old data retention is that it uses storage<br/>space, potentially costing money depending on the storage pricing model for the database. In Snowflake, users<br/>cannot directly vacuum. Instead, they control a &#8220;time-travel&#8221; interval that determines how long table snapshots are<br/>retained before they are auto vacuumed. BigQuery utilizes a fixed seven-day history window. Databricks generally<br/>retains data indefinitely until it is manually vacuumed; vacuuming is important to control direct S3 storage costs.<br/>Amazon Redshift handles its cluster disks in many configurations,  and vacuuming can impact performance and<br/>available storage. VACUUM runs automatically behind the scenes, but users may sometimes want to run it manually<br/>for tuning purposes.<br/>Vacuuming becomes even more critical for relational databases such as PostgreSQL and MySQL. Large numbers<br/>of transactional operations can cause a rapid accumulation of dead records, and engineers working in these systems<br/>need to familiarize themselves with the details and impact of vacuuming.<br/><b>Leverage cached query results<br/></b>Let&#8217;s say you have an intensive query that you often run on a database that charges you for the amount of data you<br/>query. Each time a query is run, this costs you money. Instead of rerunning the same query on the database<br/>repeatedly and incurring massive charges, wouldn&#8217;t it be nice if the results of the query were stored and available<br/>for instant retrieval? Thankfully, many cloud OLAP databases cache query results.<br/>When a query is initially run, it will retrieve data from various sources, filter and join it, and output a result. This<br/>initial query&#8212;a cold query&#8212;is similar to the notion of cold data we explored in Chapter 6. For argument&#8217;s sake,<br/>let&#8217;s say this query took 40 seconds to run. Assuming your database caches query results, rerunning the same query<br/>might return results in 1 second or less. The results were cached, and the query didn&#8217;t need to run cold. Whenever<br/>possible, leverage query cache results to help reduce pressure on your database while providing a better user<br/>experience for frequently run queries. Note also that <i>materialized views</i> provide another form of query caching<br/>(see &#8220;Materialized Views, Federation, and Query Virtualization&#8221;).<br/></p>
<p><b>Queries on Streaming Data<br/></b>Streaming data is constantly in flight. As you might imagine, querying streaming data is different from batch data.<br/>To fully take advantage of a data stream, we must adapt query patterns that reflect its real-time nature. For<br/>example, systems such as Kafka and Pulsar make it easier to query streaming data sources. Let&#8217;s look at some<br/>common ways to do this.<br/><b>Basic query patterns on streams<br/></b>Recall continuous CDC, discussed in Chapter 7. CDC, in this form, essentially sets up an analytics database as a<br/>fast follower to a production database. One of the longest-standing streaming query patterns simply entails<br/>querying the analytics database, retrieving statistical results and aggregations with a slight lag behind the<br/>production database.<br/><i>The fast-follower approach<br/></i>How is this a streaming query pattern? Couldn&#8217;t we accomplish the same thing simply by running our queries on<br/>the production database? In principle, yes; in practice, no. Production databases generally aren&#8217;t equipped to<br/>handle production workloads and simultaneously run large analytics scans across significant quantities of data.<br/>Running such queries can slow the production application or even cause it to crash.  The basic CDC query pattern<br/>allows us to serve real-time analytics with a minimal impact on the production system.<br/>The fast-follower pattern can utilize a conventional transactional database as the follower, but there are significant<br/>advantages to using a proper OLAP-oriented system (Figure 8-3). Both Druid and BigQuery combine a streaming<br/>buffer with long-term columnar storage in a setup somewhat similar to the Lambda architecture (see Chapter 3).<br/>This works extremely well for computing trailing statistics on vast historical data with near real-time updates.<br/></p>
<p>3<br/></p>
<p>4</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><i>Figure 8-3. CDC with a fast-follower analytics database<br/></i></p>
<p>The fast-follower CDC approach has critical limitations. It doesn&#8217;t fundamentally rethink batch query patterns.<br/>You&#8217;re still running SELECT queries against the current table state, and missing the opportunity to dynamically<br/>trigger events off changes in the stream.<br/><i>The Kappa architecture<br/></i>Next, recall the Kappa architecture we discussed in Chapter 3. The principal idea of this architecture is to handle<br/>all data like events and store these events as a stream rather than a table (Figure 8-4). When production application<br/>databases are the source, Kappa architecture stores events from CDC. Event streams can also flow directly from an<br/>application backend, from a swarm of IoT devices, or any system that generates events and can push them over a<br/>network. Instead of simply treating a streaming storage system as a buffer, Kappa architecture retains events in<br/>storage during a more extended retention period, and data can be directly queried from this storage. The retention<br/>period can be pretty long (months or years). Note that this is much longer than the retention period used in purely<br/>real-time oriented systems, usually a week at most.<br/></p>
<p><i>Figure 8-4. The Kappa architecture is built around streaming storage and ingest systems<br/></i></p>
<p>The &#8220;big idea&#8221; in Kappa architecture is to treat streaming storage as a real-time transport layer and a database for<br/>retrieving and querying historical data. This happens either through the direct query capabilities of the streaming<br/>storage system or with the help of external tools. For example, Kafka KSQL supports aggregation, statistical<br/>calculations, and even sessionization. If query requirements are more complex or data needs to be combined with<br/>other data sources, an external tool such as Spark reads a time range of data from Kafka and computes the query<br/>results. The streaming storage system can also feed other applications or a stream processor such as Flink or Beam.<br/><b>Windows, triggers, emitted statistics, and late-arriving data<br/></b>One fundamental limitation of traditional batch queries is that this paradigm generally treats the query engine as an<br/>external observer. An actor external to the data causes the query to run&#8212;perhaps an hourly cron job or a product<br/>manager opening a dashboard.<br/>Most widely used streaming systems, on the other hand, support the notion of computations triggered directly from<br/>the data itself. They might emit mean and median statistics every time a certain number of records are collected in<br/>the buffer or output a summary when a user session closes.<br/>Windows are an essential feature in streaming queries and processing. Windows are small batches that are<br/>processed based on dynamic triggers. Windows are generated dynamically over time in some ways. Let&#8217;s look at<br/>some common types of windows: session, fixed-time, and sliding. We&#8217;ll also look at watermarks.<br/><i>Session window<br/></i>A <i>session window</i> groups events that occur close together, and filters out periods of inactivity when no events<br/>occur. We might say that a user session is any time interval with no inactivity gap of five minutes or more. Our<br/>batch system collects data by a user ID key, orders events, determines the gaps and session boundaries, and</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>calculates statistics for each session. Data engineers often sessionize data retrospectively by applying time<br/>conditions to user activity on web and desktop apps.<br/>In a streaming session, this process can happen dynamically. Note that session windows are per key; in the<br/>preceding example, each user gets their own set of windows. The system accumulates data per user. If a five-<br/>minute gap with no activity occurs, the system closes the window, sends its calculations, and flushes the data. If<br/>new events arrive for the use, the system starts a new session window.<br/>Session windows may also make a provision for late-arriving data. Allowing data to arrive up to five minutes late<br/>to account for network conditions and system latency, the system will open the window if a late-arriving event<br/>indicates activity less than five minutes after the last event. We will have more to say about late-arriving data<br/>throughout this chapter. Figure 8-5 shows three session windows, each separated by five minutes of inactivity.<br/></p>
<p><i>Figure 8-5. Session window with a five-minute timeout for inactivity<br/></i></p>
<p>Making sessionization dynamic and near real-time fundamentally changes its utility. With retrospective<br/>sessionization, we could automate specific actions a day or an hour after a user session closed (e.g., a follow-up<br/>email with a coupon for a product viewed by the user). With dynamic sessionization, the user could get an alert in<br/>a mobile app that is immediately useful based on their activity in the last 15 minutes.<br/><i>Fixed-time windows<br/></i>A <i>fixed-time</i> (aka <i>tumbling</i>) window features fixed time periods that run on a fixed schedule and processes all data<br/>since the previous window is closed. For example, we might close a window every 20 seconds and process all data<br/>arriving from the previous window to give a mean and median statistic (Figure 8-6). Statistics would be emitted as<br/>soon as they could be calculated after the window closed.<br/></p>
<p><i>Figure 8-6. Tumbling/fixed window<br/></i></p>
<p>This is similar to traditional batch ETL processing, where we might run a data update job every day or every hour.<br/>The streaming system allows us to generate windows more frequently and deliver results with lower latency. As<br/>we&#8217;ll repeatedly emphasize, batch is a special case of streaming.<br/><i>Sliding windows<br/></i>Events in a sliding window are bucketed into windows of fixed time length. For example, we could generate a new<br/>60-second window every 30 seconds (Figure 8-7). Just as we did before, we can emit mean and median statistics.<br/>Sliding windows are also known as <i>hopping windows</i>.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><i>Figure 8-7. Sliding windows<br/></i></p>
<p>The sliding can vary. For example, we might think of the window as truly sliding continuously but emitting<br/>statistics only when certain conditions (triggers) are met. Suppose we used a 30-second continuously sliding<br/>window but calculated a statistic only when a user clicked a particular banner. This would lead to an extremely<br/>high rate of output when many users click the banner, and no calculations during a lull.<br/><i>Watermarks<br/></i>We&#8217;ve covered various types of windows and their uses. As discussed in Chapter 7, data is sometimes ingested out<br/>of the order from which it originated. A <i>watermark</i> (Figure 8-8) is a threshold used by a window to determine<br/>whether data in a window is within the established time interval or whether it&#8217;s considered late. If data arrives that<br/>is new to the window but older than the timestamp of the watermark, it is considered to be late-arriving data.<br/></p>
<p><i>Figure 8-8. Watermark acting as a threshold for late-arriving data<br/></i></p>
<p><b>Combining streams with other data<br/></b>As we&#8217;ve mentioned before, we often derive value from data by combining it with other data. Streaming data is no<br/>different. For instance, multiple streams can be combined, or a stream can be combined with batch historical data.<br/><i>Conventional table joins<br/></i>Some tables may be fed by streams (Figure 8-9). The most basic approach to this problem is simply joining these<br/>two tables in a database. A stream can feed one or both of these tables.<br/></p>
<p><i>Figure 8-9. Joining two tables fed by streams<br/></i></p>
<p><i>Enrichment<br/>Enrichment</i> means that we join a stream to other data (Figure 8-10). Typically, this is done to provide enhanced<br/>data into another stream. For example, suppose that an online retailer receives an event stream from a partner</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>business containing product and user IDs. The retailer wishes to enhance these events with product details and<br/>demographic information on the users. The retailer feeds these events to a serverless function that looks up the<br/>product and user in an in-memory database (say, a cache), adds the required information to the event, and outputs<br/>the enhanced events to another stream.<br/></p>
<p><i>Figure 8-10. In this example, a stream is enriched with data residing in object storage, resulting in a new enriched dataset<br/></i></p>
<p>In practice, the enrichment source could originate almost anywhere&#8212;a table in a cloud data warehouse or<br/>RDBMS, or a file in object storage. It&#8217;s simply a question of reading from the source and storing the requisite<br/>enrichment data in an appropriate place for retrieval by the stream.<br/><i>Stream-to-stream joining<br/></i>Increasingly, streaming systems support direct stream-to-stream joining. Suppose that an online retailer wishes to<br/>join its web event data with streaming data from an ad platform. The company can feed both streams into Spark,<br/>but a variety of complications arise. For instance, the streams may have significantly different latencies for arrival<br/>at the point where the join is handled in the streaming system. The ad platform may provide its data with a five-<br/>minute delay. In addition, certain events may be significantly delayed&#8212;for example, a session close event for a<br/>user, or an event that happens on the phone offline and shows up in the stream only after the user is back in mobile<br/>network range.<br/>As such, typical streaming join architectures rely on streaming buffers. The buffer retention interval is<br/>configurable; a longer retention interval requires more storage and other resources. Events get joined with data in<br/>the buffer and are eventually evicted after the retention interval has passed (Figure 8-11).<br/></p>
<p><i>Figure 8-11. An architecture to join streams buffers each stream and joins events if related events are found during the buffer retention interval<br/></i></p>
<p>Now that we&#8217;ve covered how queries work for batch and streaming data, let&#8217;s discuss making your data useful by<br/>modeling it.<br/></p>
<p><b>Data Modeling<br/></b>Data modeling is something that we see overlooked disturbingly often. We often see data teams jump into building<br/>data systems without a game plan to organize their data in a way that&#8217;s useful for the business. This is a mistake.<br/>Well-constructed data architectures must reflect the goals and business logic of the organization that relies on this<br/>data. Data modeling involves deliberately choosing a coherent structure for data, and is a critical step to make data<br/>useful for the business.<br/>Data modeling has been a practice for decades in one form or another. For example, various types of normalization<br/>techniques (discussed in &#8220;Normalization&#8221;) have been used to model data since the early days of RDBMSs; data<br/></p>
<p>5</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>warehousing modeling techniques have been around since at least the early 1990s, and arguably longer. As<br/>pendulums in technology often go, data modeling became somewhat unfashionable in the early to mid-2010s. The<br/>rise of data lake 1.0, NoSQL, and big data systems allowed engineers to bypass traditional data modeling,<br/>sometimes for legitimate performance gains. Other times, the lack of rigorous data modeling created data swamps,<br/>along with lots of redundant, mismatched, or simply wrong data.<br/>Nowadays, the pendulum seems to be swinging back toward data modeling. The growing popularity of data<br/>management (in particular, data governance and data quality) is pushing the need for coherent business logic. The<br/>meteoric rise of data&#8217;s prominence in companies creates a growing recognition that modeling is critical for<br/>realizing value at the higher levels of the Data Science Hierarchy of Needs pyramid. On the other hand, we believe<br/>that new paradigms are required to truly embrace the needs of streaming data and ML. In this section, we survey<br/>current data modeling techniques and briefly muse on the future of data modeling.<br/></p>
<p><b>What Is a Data Model?<br/></b>A <i>data model</i> represents the way data relates to the real world. It reflects how the data must be structured and<br/>standardized to best reflect your organization&#8217;s processes, definitions, workflows, and logic. A good data model<br/>captures how communication and work naturally flow within your organization. In contrast, a poor data model (or<br/>nonexistent one) is haphazard, confusing, and incoherent.<br/>Some data professionals view data modeling as tedious and reserved for &#8220;big enterprises.&#8221; Like most good hygiene<br/>practices&#8212;such as flossing your teeth and getting a good night&#8217;s sleep&#8212;data modeling is acknowledged as a good<br/>thing to do but is often ignored in practice. Ideally, every organization should model its data if only to ensure that<br/>business logic and rules are translated at the data layer.<br/>When modeling data, it&#8217;s critical to focus on translating the model to business outcomes. A good data model<br/>should correlate with impactful business decisions. For example, a <i>customer</i> might mean different things to<br/>different departments in a company. Is someone who&#8217;s bought from you over the last 30 days a customer? What if<br/>they haven&#8217;t bought from you in the previous six months or a year? Carefully defining and modeling this customer<br/>data can have a massive impact on downstream reports on customer behavior or the creation of customer churn<br/>models whereby the time since the last purchase is a critical variable.<br/></p>
<p><b>TIP<br/></b>Can you think of concepts or terms in your company that might mean different things to different people?<br/></p>
<p>Our discussion focuses mainly on batch data modeling since that&#8217;s where most data modeling techniques arose. We<br/>will also look at some approaches to modeling streaming data and general considerations for modeling.<br/></p>
<p><b>Conceptual, Logical, and Physical Data Models<br/></b>When modeling data, the idea is to move from abstract modeling concepts to concrete implementation. Along this<br/>continuum (Figure 8-12), three main data models are conceptual, logical, and physical. These models form the<br/>basis for the various modeling techniques we describe in this chapter:<br/><i>Conceptual<br/></i></p>
<p>Contains business logic and rules and describes the system&#8217;s data, such as schemas, tables, and fields (names<br/>and types). When creating a conceptual model, it&#8217;s often helpful to visualize it in an entity-relationship (ER)<br/>diagram, which is a standard tool for visualizing the relationships among various entities in your data (orders,<br/>customers, products, etc.). For example, an ER diagram might encode the connections among customer ID,</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>customer name, customer address, and customer orders. Visualizing entity relationships is highly<br/>recommended for designing a coherent conceptual data model.<br/></p>
<p><i>Logical<br/></i>Details how the conceptual model will be implemented in practice by adding significantly more detail. For<br/>example, we would add information on the types of customer ID, customer names, and custom addresses. In<br/>addition, we would map out primary and foreign keys.<br/></p>
<p><i>Physical<br/></i>Defines how the logical model will be implemented in a database system. We would add specific databases,<br/>schemas, and tables to our logical model, including configuration details.<br/></p>
<p><i>Figure 8-12. The continuum of data models: conceptual, logical, and physical<br/></i></p>
<p>Successful data modeling involves business stakeholders at the inception of the process. Engineers need to obtain<br/>definitions and business goals for the data. Modeling data should be a full-contact sport whose goal is to provide<br/>the business with quality data for actionable insights and automation. This is a practice that everyone must<br/>continuously participate in.<br/>Another important consideration for data modeling is the <i>grain</i> of the data, which is the resolution at which data is<br/>stored and queried. The grain is typically at the level of a primary key in a table, such as customer ID, order ID,<br/>and product ID; it&#8217;s often accompanied by a date or timestamp for increased fidelity.<br/>For example, suppose that a company has just begun to deploy BI reporting. The company is small enough that the<br/>same person is filling the role of data engineer and analyst. A request comes in for a report that summarizes daily<br/>customer orders. Specifically, the report should list all customers who ordered, the number of orders they placed<br/>that day, and the total amount they spent.<br/>This report is inherently coarse-grained. It contains no details on spending per order or the items in each order. It is<br/>tempting for the data engineer/analyst to ingest data from the production orders database and boil it down to a<br/>reporting table with only the basic aggregated data required for the report. However, this would entail starting over<br/>when a request comes in for a report with finer-grained data aggregation.<br/>Since the data engineer is actually quite experienced, they elect to create tables with detailed data on customer<br/>orders, including each order, item, item cost, item IDs, etc. Essentially, their tables contain all details on customer<br/>orders. The data&#8217;s grain is at the customer-order level. This customer-order data can be analyzed as is, or<br/>aggregated for summary statistics on customer order activity.<br/>In general, you should strive to model your data at the lowest level of grain possible. From here, it&#8217;s easy to<br/>aggregate this highly granular dataset. The reverse isn&#8217;t true, and it&#8217;s generally impossible to restore details that<br/>have been aggregated away.<br/></p>
<p><b>Normalization<br/></b><i>Normalization</i> is a database data modeling practice that enforces strict control over the relationships of tables and<br/>columns within a database. The goal of normalization is to remove the redundancy of data within a database and<br/>ensure referential integrity. Basically, it&#8217;s <i>don&#8217;t repeat yourself</i> (DRY) applied to data in a database.<br/>Normalization is typically applied to relational databases containing tables with rows and columns (we use the<br/>terms <i>column</i> and <i>field</i> interchangeably in this section). It was first introduced by relational database pioneer Edgar<br/>Codd in the early 1970s.<br/></p>
<p>6</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Codd outlined four main objectives of normalization:<br/>To free the collection of relations from undesirable insertion, update, and deletion dependencies<br/>To reduce the need for restructuring the collection of relations, as new types of data are introduced, and thus<br/>increase the lifespan of application programs<br/>To make the relational model more informative to users<br/>To make the collection of relations neutral to the query statistics, where these statistics are liable to change as<br/>time goes by<br/></p>
<p>Codd introduced the idea of <i>normal forms</i>. The normal forms are sequential, with each form incorporating the<br/>conditions of prior forms. We describe Codd&#8217;s first three normal forms here:<br/><i>Denormalized<br/></i></p>
<p>No normalization. Nested and redundant data is allowed.<br/><i>First normal form (1NF)<br/></i></p>
<p>Each column is unique and has a single value. The table has a unique primary key.<br/><i>Second normal form (2NF)<br/></i></p>
<p>The requirements of 1NF, plus partial dependencies are removed.<br/><i>Third normal form (3NF)<br/></i></p>
<p>The requirements of 2NF, plus each table contains only relevant fields related to its primary key and has no<br/>transitive dependencies.<br/></p>
<p>It&#8217;s worth spending a moment to unpack a couple of terms we just threw at you. A <i>unique primary key</i> is a single<br/>field or set of multiple fields that uniquely determines rows in the table. Each key value occurs at most once;<br/>otherwise, a value would map to multiple rows in the table. Thus, every other value in a row is dependent on (can<br/>be determined from) the key. A <i>partial dependency</i> occurs when a subset of fields in a composite key can be used<br/>to determine a nonkey column of the table. A <i>transitive dependency</i> occurs when a nonkey field depends on<br/>another nonkey field.<br/>Let&#8217;s look at stages of normalization&#8212;from denormalized to 3NF&#8212;using an ecommerce example of customer<br/>orders (Table 8-1). We&#8217;ll provide concrete explanations of each of the concepts introduced in the previous<br/>paragraph.<br/></p>
<p>7</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p> <br/><i>T<br/>a<br/>b<br/>l<br/>e<br/> <br/>8<br/>-<br/>1<br/>. <br/>O<br/>r<br/>d<br/>e<br/>r<br/>D<br/>e<br/>t<br/>a<br/>i<br/>l<br/></i> <br/><b>OrderID OrderItems CustomerID CustomerName OrderDate<br/></b> <br/>100<br/></p>
<p>[{ <br/>              <b>&quot;sku&quot;</b>: 1, <br/>              <b>&quot;price&quot;</b>: 50, <br/><b>&quot;quantity&quot;</b>: 1, <br/>              <b>&quot;name:&quot;</b>: &quot;Thingamajig&quot; <br/>}, { <br/>              <b>&quot;sku&quot;</b>: 2, <br/>              <b>&quot;price&quot;</b>: 25, <br/><b>&quot;quantity&quot;</b>: 2, <br/>              <b>&quot;name:&quot;</b>: &quot;Whatchamacallit&quot; <br/>}]<br/></p>
<p> <br/></p>
<p>5 Joe Reis 2022-03-01<br/></p>
<p> <br/></p>
<p>First, this denormalized OrderDetail table contains five fields. The primary key is OrderID. Notice that the<br/>OrderItems field contains a nested object with two SKUs along with their price, quantity, and name.<br/>To convert this data to 1NF, let&#8217;s move OrderItems into four fields (Table 8-2). Now we have an OrderDetail<br/>table in which fields do not contain repeats or nested data.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p> <br/><i>T<br/>a<br/>b<br/>l<br/>e<br/> <br/>8<br/>-<br/>2<br/>. <br/>O<br/>r<br/>d<br/>e<br/>r<br/>D<br/>e<br/>t<br/>a<br/>i<br/>l <br/>w<br/>i<br/>t<br/>h<br/>o<br/>u<br/>t <br/>r<br/>e<br/>p<br/>e<br/>a<br/>t<br/>s<br/> <br/>o<br/>r<br/> <br/>n<br/>e<br/>s<br/>t<br/>e<br/>d<br/> <br/>d<br/>a<br/>t<br/>a<br/></i> <br/><b>OrderID Sku Price Quantity ProductName CustomerID CustomerName Ord</b></p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p> <br/>100 1 50 1 Thingamajig 5 Joe Reis 202<br/>100 2 25 2 Whatchamacallit 5 Joe Reis 202<br/> <br/></p>
<p>The problem is that now we don&#8217;t have a unique primary key. That is, 100 occurs in the OrderID column in two<br/>different rows. To get a better grasp of the situation, let&#8217;s look at a larger sample from our table (Table 8-3).</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p> <br/><i>T<br/>a<br/>b<br/>l<br/>e <br/>8<br/>-<br/>3<br/>. <br/>O<br/>r<br/>d<br/>e<br/>r<br/>D<br/>e<br/>t<br/>a<br/>i<br/>l <br/>w<br/>i<br/>t<br/>h <br/>a <br/>l<br/>a<br/>r<br/>g<br/>e<br/>r <br/>s<br/>a<br/>m<br/>p<br/>l<br/>e<br/></i> <br/><b>OrderID Sku Price Quantity ProductName CustomerID CustomerName Ord<br/></b> <br/>100 1 50 1 Thingamajig 5 Joe Reis 202<br/>100 2 25 2 Whatchamacallit 5 Joe Reis 202<br/>101 3 75 1 Whozeewhatzit 7 Matt Housley 202<br/>102 1 50 1 Thingamajig 7 Matt Housley 202<br/> <br/></p>
<p>To create a unique primary (composite) key, let&#8217;s number the lines in each order by adding a column called<br/>LineItemNumber (Table 8-4).</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p> <br/><i>T<br/>a<br/>b<br/>l<br/>e<br/> <br/>8<br/>-<br/>4<br/>. <br/>O<br/>r<br/>d<br/>e<br/>r<br/>D<br/>e<br/>t<br/>a<br/>i<br/>l <br/>w<br/>i<br/>t<br/>h<br/> <br/>L<br/>i<br/>n<br/>e<br/>I<br/>t<br/>e<br/>m<br/>N<br/>u<br/>m<br/>b<br/>e<br/>r<br/> <br/>c<br/>o<br/>l<br/>u<br/>m<br/>n<br/></i> <br/><b>OrderID LineItemNumber Sku Price Quantity ProductName CustomerID Cus<br/></b> <br/>100 1 1 50 1 Thingamajig 5 Joe </p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>100 2 2 25 2 Whatchamacallit 5 Joe <br/>101 1 3 75 1 Whozeewhatzit 7 Mat<br/>102 1 1 50 1 Thingamajig 7 Mat<br/> <br/></p>
<p>The composite key (OrderID, LineItemNumber) is now a unique primary key.<br/>To reach 2NF, we need to ensure that no partial dependencies exist. A <i>partial dependency</i> is a nonkey column that<br/>is fully determined by a subset of the columns in the unique primary (composite) key; partial dependencies can<br/>occur only when the primary key is composite. In our case, the last three columns are determined by order number.<br/>To fix this problem, let&#8217;s split OrderDetail into two tables: Orders and OrderLineItem (Tables 8-5 and 8-6).<br/> <br/><i>T<br/>a<br/>b<br/>l<br/>e<br/> <br/>8<br/>-<br/>5<br/>. <br/>O<br/>r<br/>d<br/>e<br/>r<br/>s<br/></i> <br/><b>OrderID CustomerID CustomerName OrderDate<br/></b> <br/>100 5 Joe Reis 2022-03-01<br/>101 7 Matt Housley 2022-03-01<br/>102 7 Matt Housley 2022-03-01<br/> </p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p> <br/><i>T<br/>a<br/>b<br/>l<br/>e<br/> <br/>8<br/>-<br/>6<br/>. <br/>O<br/>r<br/>d<br/>e<br/>r<br/>L<br/>i<br/>n<br/>e<br/>I<br/>t<br/>e<br/>m<br/></i> <br/><b>OrderID LineItemNumber Sku Price Quantity ProductName<br/></b> <br/>100 1 1 50 1 Thingamajig<br/>100 2 2 25 2 Whatchamacallit<br/>101 1 3 75 1 Whozeewhatzit<br/>102 1 1 50 1 Thingamajig<br/> <br/></p>
<p>The composite key (OrderID, LineItemNumber) is a unique primary key for OrderLineItem, while OrderID is a<br/>primary key for Orders.<br/>Notice that Sku determines ProductName in OrderLineItem. That is, Sku depends on the composite key, and<br/>ProductName depends on Sku. This is a transitive dependency. Let&#8217;s break OrderLineItem into OrderLineItem<br/>and Skus (Tables 8-7 and 8-8).</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p> <br/><i>T<br/>a<br/>b<br/>l<br/>e<br/> <br/>8<br/>-<br/>7<br/>. <br/>O<br/>r<br/>d<br/>e<br/>r<br/>L<br/>i<br/>n<br/>e<br/>I<br/>t<br/>e<br/>m<br/></i> <br/><b>OrderID LineItemNumber Sku Price Quantity<br/></b> <br/>100 1 1 50 1<br/>100 2 2 25 2<br/>101 1 3 75 1<br/>102 1 1 50 1<br/> </p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p> <br/><i>T<br/>a<br/>b<br/>l<br/>e<br/> <br/>8<br/>-<br/>8<br/>. <br/>S<br/>k<br/>u<br/>s<br/></i> <br/><b>Sku ProductName<br/></b> <br/>1 Thingamajig<br/>2 Whatchamacallit<br/>3 Whozeewhatzit<br/> <br/></p>
<p>Now, both OrderLineItem and Skus are in 3NF. Notice that Orders does not satisfy 3NF. What transitive<br/>dependencies are present? How would you fix this?<br/>Additional normal forms exist (up to 6NF in the Boyce-Codd system), but these are much less common than the<br/>first three. A database is usually considered normalized if it&#8217;s in third normal form, and that&#8217;s the convention we<br/>use in this book.<br/>The degree of normalization that you should apply to your data depends on your use case. No one-size-fits-all<br/>solution exists, especially in databases where some denormalization presents performance advantages. Although<br/>denormalization may seem like an antipattern, it&#8217;s common in many OLAP systems that store semistructured data.<br/>Study normalization conventions and database best practices to choose an appropriate strategy.<br/></p>
<p><b>Techniques for Modeling Batch Analytical Data<br/></b>When describing data modeling for data lakes or data warehouses, you should assume that the raw data takes many<br/>forms (e.g., structured and semistructured), but the output is a structured data model of rows and columns.<br/>However, several approaches to data modeling can be used in these environments. The big approaches you&#8217;ll likely<br/>encounter are Kimball, Inmon, and data vault.<br/>In practice, some of these techniques can be combined. For example, we see some data teams start with data vault<br/>and then add a Kimball star schema alongside it. We&#8217;ll also look at wide and denormalized data models and other<br/>batch data-modeling techniques you should have in your arsenal. As we discuss each of these techniques, we will<br/>use the example of modeling transactions occurring in an ecommerce order system.<br/></p>
<p><b>NOTE<br/></b>Our coverage of the first three approaches&#8212;Inmon, Kimball, and data vault&#8212;is cursory and hardly does justice to their respective<br/>complexity and nuance. At the end of each section, we list the canonical books from their creators. For a data engineer, these books are<br/>must-reads, and we highly encourage you to read them, if only to understand how and why data modeling is central to batch analytical data.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><b>Inmon<br/></b>The father of the data warehouse, Bill Inmon, created his approach to data modeling in 1990. Before the data<br/>warehouse, the analysis would often occur directly on the source system itself, with the obvious consequence of<br/>bogging down production transactional databases with long-running queries. The goal of the data warehouse was<br/>to separate the source system from the analytical system.<br/>Inmon defines a data warehouse the following way:<br/></p>
<p><i>A data warehouse is a subject-oriented, integrated, nonvolatile, and time-variant collection of data in support of<br/>management&#8217;s decisions. The data warehouse contains granular corporate data. Data in the data warehouse is<br/>able to be used for many different purposes, including sitting and waiting for future requirements which are<br/>unknown today.<br/></i></p>
<p>The four critical parts of a data warehouse mean the following:<br/><i>Subject-oriented<br/></i></p>
<p>The data warehouse focuses on a specific subject area, such as sales or marketing.<br/><i>Nonvolatile<br/></i></p>
<p>Data remains unchanged after data is stored in a data warehouse.<br/><i>Integrated<br/></i></p>
<p>Data from disparate sources is consolidated and normalized.<br/><i>Time-variant<br/></i></p>
<p>Varying time ranges can be queried.<br/></p>
<p>Let&#8217;s look at each of these parts to understand its influence on an Inmon data model. First, the logical model must<br/>focus on a specific area. For instance, if the <i>subject orientation</i> is &#8220;sales,&#8221; then the logical model contains all<br/>details related to sales&#8212;business keys, relationships, attributes, etc. Next, these details are <i>integrated</i> into a<br/>consolidated and highly normalized data model. Finally, the data is stored unchanged in a <i>nonvolatile</i> and <i>time-<br/>variant</i> way, meaning you can (theoretically) query the original data for as long as storage history allows. The<br/>Inmon data warehouse must strictly adhere to all four of these critical parts <i>in support of management&#8217;s decisions.<br/></i>This is a subtle point, but it positions the data warehouse for analytics, not OLTP.<br/>Here is another key characteristic of Inmon&#8217;s data warehouse:<br/></p>
<p><i>The second salient characteristic of the data warehouse is that it is integrated. Of all the aspects of a data<br/>warehouse, integration is the most important. Data is fed from multiple, disparate sources into the data<br/>warehouse. As the data is fed, it is converted, reformatted, resequenced, summarized, etc. The result is that data<br/>&#8212;once it resides in the data warehouse &#8212;has a single physical corporate image.<br/></i></p>
<p>With Inmon&#8217;s data warehouse, data is integrated from across the organization in a granular, highly normalized ER<br/>model, with a relentless emphasis on ETL. Because of the subject-oriented nature of the data warehouse, the<br/>Inmon data warehouse consists of key source databases and information systems used in an organization. Data<br/>from key business source systems is ingested and integrated into a highly normalized (3NF) data warehouse that<br/>often closely resembles the normalization structure of the source system itself; data is brought in incrementally,<br/>starting with the highest-priority business areas. The strict normalization requirement ensures as little data<br/>duplication as possible, which leads to fewer downstream analytical errors because data won&#8217;t diverge or suffer<br/>from redundancies. The data warehouse represents a &#8220;single source of truth,&#8221; which supports the overall business&#8217;s<br/>information requirements. The data is presented for downstream reports and analysis via business and department-<br/>specific data marts, which may also be denormalized.<br/></p>
<p><i>8<br/></i></p>
<p><i>9</i></p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Let&#8217;s look at how an Inmon data warehouse is used for ecommerce (Figure 8-13). The business source systems are<br/>orders, inventory, and marketing. The data from these source systems are ETLed to the data warehouse and stored<br/>in 3NF. Ideally, the data warehouse holistically encompasses the business&#8217;s information. To serve data for<br/>department-specific information requests, ETL processes take data from the data warehouse, transform the data,<br/>and place it in downstream data marts to be viewed in reports.<br/></p>
<p><i>Figure 8-13. An ecommerce data warehouse<br/></i></p>
<p>A popular option for modeling data in a data mart is a star schema (discussed in the following section on Kimball),<br/>though any data model that provides easily accessible information is also suitable. In the preceding example, sales,<br/>marketing, and purchasing have their own star schema, fed upstream from the granular data in the data warehouse.<br/>This allows each department to have its own data structure that&#8217;s unique and optimized to its specific needs.<br/>Inmon continues to innovate in the data warehouse space, currently focusing on textual ETL in the data warehouse.<br/>He&#8217;s also a prolific writer and thinker, writing over 60 books and countless articles. For further reading about<br/>Inmon&#8217;s data warehouse, please refer to his books listed in &#8220;Additional Resources&#8221;.<br/><b>Kimball<br/></b>If there are spectrums to data modeling, Kimball is very much on the opposite end of Inmon. Created by Ralph<br/>Kimball in the early 1990s, this approach to data modeling focuses less on normalization, and in some cases<br/>accepting denormalization. As Inmon says about the difference between the data warehouse and data mart, &#8220;A data<br/>mart is never a substitute for a data warehouse.&#8221;<br/>Whereas Inmon integrates data from across the business in the data warehouse, and serves department-specific<br/>analytics via data marts, the Kimball model is bottom-up, encouraging you to model and serve department or<br/>business analytics in the data warehouse itself (Inmon argues this approach skews the definition of a data<br/>warehouse). This may enable faster iteration and modeling than Inmon, with the trade-off of potential looser data<br/>integration, data redundancy, and duplication.<br/>In Kimball&#8217;s approach, data is modeled with two general types of tables: facts and dimensions. You can think of a<br/><i>fact table</i> as a table of numbers, and <i>dimension tables</i> as qualitative data referencing a fact. Dimension tables<br/>surround a single fact table in a relationship called a <i>star schema</i> (Figure 8-14).  Let&#8217;s look at facts, dimensions,<br/>and star schemas.<br/></p>
<p>10<br/></p>
<p>11</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><i>Figure 8-14. A Kimball star schema, with facts and dimensions<br/></i></p>
<p><i>Fact tables<br/></i>The first type of table in a star schema is the fact table, which contains <i>factual</i>, quantitative, and event-related data.<br/>The data in a fact table is immutable because facts relate to events. Therefore, fact tables don&#8217;t change and are<br/>append-only. Fact tables are typically narrow and long, meaning they have not a lot of columns but a lot of rows<br/>that represent events. Fact tables should be at the lowest grain possible.<br/>Queries against a star schema start with the fact table. Each row of a fact table should represent the grain of the<br/>data. Avoid aggregating or deriving data within a fact table. If you need to perform aggregations or derivations, do<br/>so in a downstream query, data mart table, or view. Finally, fact tables don&#8217;t reference other fact tables; they<br/>reference only dimensions.<br/>Let&#8217;s look at an example of an elementary fact table (Table 8-9). A common question in your company might be,<br/>&#8220;Show me gross sales, by each customer order, by date.&#8221; Again, facts should be at the lowest grain possible&#8212;in<br/>this case, the orderID of the sale, customer, date, and gross sale amount. Notice that the data types in the fact table<br/>are all numbers (integers and floats); there are no strings. Also, in this example, CustomerKey 7 has two orders on<br/>the same day, reflecting the grain of the table. Instead, the fact table has keys that reference dimension tables<br/>containing their respective attributes, such as the customer and date information. The gross sales amount represents<br/>the total sale for the sales <i>event</i>.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p> <br/><i>T<br/>a<br/>b<br/>l<br/>e <br/>8<br/>-<br/>9<br/>. <br/>A<br/> <br/>f<br/>a<br/>c<br/>t <br/>t<br/>a<br/>b<br/>l<br/>e<br/></i> <br/><b>OrderID CustomerKey DateKey GrossSalesAmt<br/></b> <br/>100 5 20220301 100.00<br/>101 7 20220301 75.00<br/>102 7 20220301 50.00<br/> <br/></p>
<p><i>Dimension tables<br/></i>The second primary type of table in a Kimball data model is called a <i>dimension</i>. Dimension tables provide the<br/>reference data, attributes, and relational context for the events stored in fact tables. Dimension tables are smaller<br/>than fact tables and take an opposite shape, typically wide and short. When joined to a fact table, dimensions can<br/>describe the events&#8217; what, where, and when. Dimensions are denormalized, with the possibility of duplicate data.<br/>This is OK in the Kimball data model. Let&#8217;s look at the two dimensions referenced in the earlier fact table<br/>example.<br/>In a Kimball data model, dates are typically stored in a date dimension, allowing you to reference the date key<br/>(DateKey) between the fact and date dimension table. With the date dimension table, you can easily answer<br/>questions like, &#8220;What are my total sales in the first quarter of 2022?&#8221; or &#8220;How many more customers shop on<br/>Tuesday than Wednesday?&#8221; Notice we have five fields in addition to the date key (Table 8-10). The beauty of a<br/>date dimension is that you can add as many new fields as makes sense to analyze your data.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p> <br/><i>T<br/>a<br/>b<br/>l<br/>e <br/>8<br/>-<br/>1<br/>0<br/>. <br/>A<br/> <br/>d<br/>a<br/>t<br/>e <br/>d<br/>i<br/>m<br/>e<br/>n<br/>s<br/>i<br/>o<br/>n <br/>t<br/>a<br/>b<br/>l<br/>e<br/></i> <br/><b>DateKey Date-ISO Year Quarter Month Day-of-week<br/></b> <br/>20220301 2022-03-01 2022 1 3 Tuesday<br/>20220302 2022-03-02 2022 1 3 Wednesday<br/>20220303 2022-03-03 2022 1 3 Thursday<br/> <br/></p>
<p>Table 8-11 also references another dimension&#8212;the customer dimension&#8212;by the CustomerKey field. The customer<br/>dimension contains several fields that describe the customer: first and last name, zip code, and a couple of<br/>peculiar-looking date fields. Let&#8217;s look at these date fields, as they illustrate another concept in the Kimball data<br/>model: a Type 2 slowly changing dimension, which we&#8217;ll describe in greater detail next.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p> <br/><i>T<br/>a<br/>b<br/>l<br/>e <br/>8<br/>-<br/>1<br/>1<br/>. <br/>A<br/> <br/>T<br/>y<br/>p<br/>e <br/>2 <br/>c<br/>u<br/>s<br/>t<br/>o<br/>m<br/>e<br/>r <br/>d<br/>i<br/>m<br/>e<br/>n<br/>s<br/>i<br/>o<br/>n <br/>t<br/>a<br/>b<br/>l<br/>e<br/></i> <br/><b>CustomerKey FirstName LastName ZipCode EFF_StartDate EFF_EndDate<br/></b> <br/>5 Joe Reis 84108 2019-01-04 9999-01-01<br/>7 Matt Housley 84101 2020-05-04 2021-09-19<br/>7 Matt Housley 84123 2021-09-19 9999-01-01<br/>11 Lana Belle 90210 2022-02-04 9999-01-01<br/> <br/></p>
<p>For example, take a look at CustomerKey 5, with the EFF_StartDate (EFF_StartDate means <i>effective start date</i>)<br/>of 2019-01-04 and an EFF_EndDate of 9999-01-01. This means Joe Reis&#8217;s customer record was created in the</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>customer dimension table on 2019-01-04 and has an end date of 9999-01-01. Interesting. What does this end date<br/>mean? It means the customer record is active and isn&#8217;t changed.<br/>Now let&#8217;s look at Matt Housley&#8217;s customer record (CustomerKey = 7). Notice the two entries for Housley&#8217;s start<br/>date: 2020-05-04 and 2021-09-19. It looks like Housley changed his zip code on 2021-09-19, resulting in a<br/>change to his customer record. When the data is queried for the most recent customer records, you will query<br/>where the end date is equal to 9999-01-01.<br/>A slowly changing dimension (SCD) is necessary to track changes in dimensions. The preceding example is a<br/>Type 2 SCD: a new record is inserted when an existing record changes. Though SCDs can go up to seven levels,<br/>let&#8217;s look at the three most common ones:<br/><i>Type 1<br/></i></p>
<p>Overwrite existing dimension records. This is super simple and means you have no access to the deleted<br/>historical dimension records.<br/></p>
<p><i>Type 2<br/></i>Keep a full history of dimension records. When a record changes, that specific record is flagged as changed,<br/>and a new dimension record is created that reflects the current status of the attributes. In our example, Housley<br/>moved to a new zip code, which triggered his initial record to reflect an effective end date, and a new record<br/>was created to show his new zip code.<br/></p>
<p><i>Type 3<br/></i>A Type 3 SCD is similar to a Type 2 SCD, but instead of creating a new row, a change in a Type 3 SCD creates<br/>a new field. Using the preceding example, let&#8217;s see what this looks like as a Type 3 SCD in the following<br/>tables.<br/></p>
<p>In Table 8-12, Housley lives in the 84101 zip code. When Housley moves to a new zip code, the Type 3 SCD<br/>creates two new fields, one for his new zip code and the date of the change (Table 8-13). The original zip code<br/>field is also renamed to reflect that this is the older record.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p> <br/><i>T<br/>a<br/>b<br/>l<br/>e <br/>8<br/>-<br/>1<br/>2<br/>. <br/>T<br/>y<br/>p<br/>e <br/>3 <br/>s<br/>l<br/>o<br/>w<br/>l<br/>y <br/>c<br/>h<br/>a<br/>n<br/>g<br/>i<br/>n<br/>g <br/>d<br/>i<br/>m<br/>e<br/>n<br/>s<br/>i<br/>o<br/>n<br/></i> <br/><b>CustomerKey FirstName LastName ZipCode<br/></b> <br/>7 Matt Housley 84101<br/> </p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p> <br/><i>T<br/>a<br/>b<br/>l<br/>e <br/>8<br/>-<br/>1<br/>3<br/>. <br/>T<br/>y<br/>p<br/>e <br/>3 <br/>c<br/>u<br/>s<br/>t<br/>o<br/>m<br/>e<br/>r <br/>d<br/>i<br/>m<br/>e<br/>n<br/>s<br/>i<br/>o<br/>n <br/>t<br/>a<br/>b<br/>l<br/>e<br/></i> <br/><b>CustomerKey FirstName LastName Original ZipCode Current ZipCode CurrentDate<br/></b> <br/>7 Matt Housley 84101 84123 2021-09-19<br/> <br/></p>
<p>Of the types of SCDs described, Type 1 is the default behavior of most data warehouses, and Type 2 is the one we<br/>most commonly see used in practice. There&#8217;s a lot to know about dimensions, and we suggest using this section as<br/>a starting point to get familiar with how dimensions work and how they&#8217;re used.<br/><i>Star schema<br/></i>Now that you have a basic understanding of facts and dimensions, it&#8217;s time to integrate them into a star schema.<br/>The <i>star schema</i> represents the data model of the business. Unlike highly normalized approaches to data modeling,<br/>the star schema is a fact table surrounded by the necessary dimensions. This results in fewer joins than other data</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>models, which speeds up query performance. Another advantage of a star schema is it&#8217;s arguably easier for<br/>business users to understand and use.<br/>Note that the star schema shouldn&#8217;t reflect a particular report, though you can model a report in a downstream data<br/>mart or directly in your BI tool. The star schema should capture the facts and attributes of your <i>business logic</i> and<br/>be flexible enough to answer the respective critical questions.<br/>Because a star schema has one fact table, sometimes you&#8217;ll have multiple star schemas that address different facts<br/>of the business. You should strive to reduce the number of dimensions whenever possible since this reference data<br/>can potentially be reused among different fact tables. A dimension that is reused across multiple star schemas, thus<br/>sharing the same fields, is called a <i>conformed dimension</i>. A conformed dimension allows you to combine multiple<br/>fact tables across multiple star schemas. Remember, redundant data is OK with the Kimball method, but avoid<br/>replicating the same dimension tables to avoid drifting business definitions and data integrity.<br/>The Kimball data model and star schema have a lot of nuance. You should be aware that this mode is appropriate<br/>only for batch data and not for streaming data. Because the Kimball data model is popular, there&#8217;s a good chance<br/>you&#8217;ll run into it<br/><b>Data vault<br/></b>Whereas Kimball and Inmon focus on the structure of business logic in the data warehouse, the <i>data vault</i> offers a<br/>different approach to data modeling.  Created in the 1990s by Dan Linstedt, the data vault model separates the<br/>structural aspects of a source system&#8217;s data from its attributes. Instead of representing business logic in facts,<br/>dimensions, or highly normalized tables, a data vault simply loads data from source systems directly into a handful<br/>of purpose-built tables in an insert-only manner. Unlike the other data modeling approaches you&#8217;ve learned about,<br/>there&#8217;s no notion of good, bad, or conformed data in a data vault.<br/>Data moves fast these days, and data models need to be agile, flexible, and scalable; the data vault model aims to<br/>meet this need. The goal of this model is to keep the data as closely aligned to the business as possible, even while<br/>the business&#8217;s data evolves.<br/>A data vault consists of three main types of tables: hubs, links, and satellites (Figure 8-15). In short, a <i>hub</i> stores<br/>business keys, a <i>link</i> maintains relationships among business keys, and a <i>satellite</i> represents a business key&#8217;s<br/>attributes and context. A user will query a hub, which will link to a satellite table containing the query&#8217;s relevant<br/>attributes. Let&#8217;s explore hubs, links, and satellites in more detail.<br/></p>
<p><i>Figure 8-15. Data vault tables: hubs, links, and satellites connected together<br/></i></p>
<p><i>Hubs<br/></i>Queries often involve searching by a business key, such as a customer ID or an order ID from our ecommerce<br/>example. A hub is the central entity of a data vault that retains a record of all business keys loaded into the data<br/>vault. The hub should reflect the state of the source system from which it loads data.<br/>A hub always contains the following standard fields:<br/><i>Hash key<br/></i></p>
<p>The primary key used to join data between systems. This is a calculated hash field (MD5 or similar)<br/><i>Load date<br/></i></p>
<p>The date the data was loaded into the hub.<br/><i>Record source<br/></i></p>
<p>The source system.<br/></p>
<p>12</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><i>Business key(s)<br/></i>The business key, or record ID, in the source system.<br/></p>
<p>It&#8217;s important to note that a hub is insert-only, and data from source systems is not altered in a hub. Once data is<br/>loaded into a hub, it&#8217;s permanent.<br/>When designing a hub, identifying the business key is critical. Ask yourself: What is the <i>identifiable business<br/>element</i> in a source system?  Put another way, how do users of a source system commonly look for data? Ideally,<br/>this is discovered as you build the conceptual data model of your organization and before you start building your<br/>data vault.<br/>Using our ecommerce scenario, let&#8217;s look at an example of a hub for products. First, let&#8217;s look at the physical<br/>design of a product hub (Table 8-14).<br/></p>
<p>13</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p> <br/><i>T<br/>a<br/>b<br/>l<br/>e <br/>8<br/>-<br/>1<br/>4<br/>. <br/>A<br/> <br/>p<br/>h<br/>y<br/>s<br/>i<br/>c<br/>a<br/>l <br/>d<br/>e<br/>s<br/>i<br/>g<br/>n <br/>f<br/>o<br/>r <br/>a <br/>p<br/>r<br/>o<br/>d<br/>u<br/>c<br/>t <br/>h<br/>u<br/>b<br/></i> <br/><b>HubProduct<br/></b> <br/>ProductHashKey<br/>LoadDate<br/>RecordSource<br/>ProductID<br/> <br/></p>
<p>In practice, the product hub looks like this when populated with data (Table 8-15). In this example, three different<br/>products are loaded into a hub from an ERP system on two separate dates.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p> <br/><i>T<br/>a<br/>b<br/>l<br/>e <br/>8<br/>-<br/>1<br/>5<br/>. <br/>A<br/> <br/>p<br/>r<br/>o<br/>d<br/>u<br/>c<br/>t <br/>h<br/>u<br/>b <br/>p<br/>o<br/>p<br/>u<br/>l<br/>a<br/>t<br/>e<br/>d <br/>w<br/>it<br/>h <br/>d<br/>a<br/>t<br/>a<br/></i> <br/><b>ProductHashKey LoadDate RecordSource ProductID<br/></b> <br/>4041fd80ab68e267490522b4a99048f5 2020-01-02 ERP 1<br/>de8435530d6ca93caabc00cf88982dc1 2021-03-09 ERP 2<br/>cf27369bd80f53d0e60d394817d77bab 2021-03-09 ERP 3<br/> <br/></p>
<p>While we&#8217;re at it, let&#8217;s create another hub for orders (Table 8-16) using the same schema as HubProduct, and<br/>populate it with some sample order data.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p> <br/><i>T<br/>a<br/>b<br/>l<br/>e <br/>8<br/>-<br/>1<br/>6<br/>. <br/>A<br/>n <br/>o<br/>r<br/>d<br/>e<br/>r <br/>h<br/>u<br/>b <br/>p<br/>o<br/>p<br/>u<br/>l<br/>a<br/>t<br/>e<br/>d <br/>w<br/>i<br/>t<br/>h <br/>d<br/>a<br/>t<br/>a<br/></i> <br/><b>OrderHashKey LoadDate RecordSource OrderID<br/></b> <br/>f899139df5e1059396431415e770c6dd 2022-03-01 Website 100<br/>38b3eff8baf56627478ec76a704e9b52 2022-03-01 Website 101<br/>ec8956637a99787bd197eacd77acce5e 2022-03-01 Website 102<br/> <br/></p>
<p><i>Links<br/></i>A <i>link table</i> tracks the relationships of business keys between hubs. Link tables must connect at least two hubs,<br/>ideally at the lowest possible grain. Because link tables connect data from various hubs, they are many to many.<br/>The data vault model&#8217;s relationships are straightforward and handled through changes to the links. This provides<br/>excellent flexibility in the inevitable event that the underlying source data changes. You add a new hub, and this</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>hub needs to connect to existing hubs. Simply add a new relationship to that hub in your link table. That&#8217;s it! Now<br/>let&#8217;s look at ways to view data contextually using satellites.<br/>Back to our ecommerce example, we&#8217;d like to associate orders with products. Let&#8217;s see what a link table might<br/>look like for orders and products (Table 8-17).</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p> <br/><i>T<br/>a<br/>b<br/>l<br/>e <br/>8<br/>-<br/>1<br/>7<br/>. <br/>A<br/> <br/>li<br/>n<br/>k <br/>t<br/>a<br/>b<br/>l<br/>e <br/>f<br/>o<br/>r <br/>p<br/>r<br/>o<br/>d<br/>u<br/>c<br/>t<br/>s <br/>a<br/>n<br/>d <br/>o<br/>r<br/>d<br/>e<br/>r<br/>s<br/></i> <br/><b>LinkOrderProduct<br/></b> <br/>OrderProductHashKey<br/>LoadDate<br/>RecordSource<br/>ProductHashKey<br/>OrderHashKey<br/> </p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>When the LinkOrderProduct table is populated, here&#8217;s what it looks like (Table 8-18). Note that we&#8217;re using the<br/>order&#8217;s record source in this example.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p> <br/><i>T<br/>a<br/>b<br/>l<br/>e <br/>8<br/>-<br/>1<br/>8<br/>. <br/>A<br/> <br/>li<br/>n<br/>k <br/>t<br/>a<br/>b<br/>l<br/>e <br/>c<br/>o<br/>n<br/>n<br/>e<br/>c<br/>ti<br/>n<br/>g <br/>o<br/>r<br/>d<br/>e<br/>r<br/>s <br/>a<br/>n<br/>d <br/>p<br/>r<br/>o<br/>d<br/>u<br/>c<br/>t<br/>s<br/></i> <br/><b>OrderProductHashKey LoadDate RecordSource ProductHashKey OrderHashKey<br/></b> <br/>ff64ec193d7bacf05e8b97ea04b3906<br/>6<br/></p>
<p>2022-03-01 Website 4041fd80ab68e267490522b4a990<br/>48f5<br/></p>
<p>f899139df5e1059396431415e770<br/>c6dd<br/></p>
<p>ff64ec193d7bacf05e8b97ea04b3906 2022-03-01 Website de8435530d6ca93caabc00cf88982 f899139df5e1059396431415e770</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>6 dc1 c6dd<br/>e232628c251e3cad0f53f7e6e13cde<br/>a7<br/></p>
<p>2022-03-01 Website cf27369bd80f53d0e60d394817d7<br/>7bab<br/></p>
<p>38b3eff8baf56627478ec76a704e9<br/>b52<br/></p>
<p>26166a5871b6d21ae12e9c464262b<br/>e67<br/></p>
<p>2022-03-01 Website 4041fd80ab68e267490522b4a990<br/>48f5<br/></p>
<p>ec8956637a99787bd197eacd77ac<br/>ce5e<br/></p>
<p> <br/></p>
<p><i>Satellites<br/></i>We&#8217;ve described relationships between hubs and links that involve keys, load dates, and record sources. How do<br/>you get a sense of what these relationships mean? <i>Satellites</i> are descriptive attributes that give meaning and<br/>context to hubs. Satellites can connect to either hubs or links. The only required fields in a satellite are a primary<br/>key consisting of the business key of the parent hub and a load date. Beyond that, a satellite can contain however<br/>many attributes that make sense.<br/>Let&#8217;s look at an example of a satellite for the Product hub (Table 8-19). In this example, the SatelliteProduct<br/>table contains additional information about the product, such as product name and price.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p> <br/><i>T<br/>a<br/>b<br/>l<br/>e<br/> <br/>8<br/>-<br/>1<br/>9<br/>. <br/>S<br/>a<br/>t<br/>e<br/>l<br/>l<br/>i<br/>t<br/>e<br/>P<br/>r<br/>o<br/>d<br/>u<br/>c<br/>t<br/></i> <br/><b>SatelliteProduct<br/></b> <br/>ProductHashKey<br/>LoadDate<br/>RecordSource<br/>ProductName<br/>Price<br/> <br/></p>
<p>And here&#8217;s the SatelliteProduct table with some sample data (Table 8-20).</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p> <br/><i>T<br/>a<br/>b<br/>l<br/>e <br/>8<br/>-<br/>2<br/>0<br/>. <br/>A<br/> <br/>p<br/>r<br/>o<br/>d<br/>u<br/>c<br/>t <br/>s<br/>a<br/>t<br/>e<br/>ll<br/>it<br/>e <br/>t<br/>a<br/>b<br/>l<br/>e <br/>w<br/>it<br/>h <br/>s<br/>a<br/>m<br/>p<br/>l<br/>e <br/>d<br/>a<br/>t<br/>a<br/></i> <br/><b>ProductHashKey LoadDate RecordSource ProductName Price<br/></b> <br/>4041fd80ab68e267490522b4a99048f5 2020-01-02 ERP Thingamajig 50<br/>de8435530d6ca93caabc00cf88982dc1 2021-03-09 ERP Whatchamacallit 25<br/>cf27369bd80f53d0e60d394817d77bab 2021-03-09 ERP Whozeewhatzit 75<br/> </p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Let&#8217;s tie this all together and join the hub, product, and link tables into a data vault (Figure 8-16).<br/></p>
<p><i>Figure 8-16. The data vault for orders and products<br/></i></p>
<p>Other types of data vault tables exist, including point in time (PIT) and bridge tables. We don&#8217;t cover these here,<br/>but mention them because the data vault is quite comprehensive. Our goal is to simply give you an overview of the<br/>data vault&#8217;s power.<br/>Unlike other data modeling techniques we&#8217;ve discussed, in a data vault, the business logic is created and<br/>interpreted when the data from these tables is queried. Please be aware that the data vault model can be used with<br/>other data modeling techniques. It&#8217;s not unusual for a data vault to be the landing zone for analytical data, after<br/>which it&#8217;s separately modeled in a data warehouse, commonly using a star schema. The data vault model also can<br/>be adapted for NoSQL and streaming data sources. The data vault is a huge topic, and this section is simply meant<br/>to make you aware of its existence.<br/><b>Wide denormalized tables<br/></b>The strict modeling approaches we&#8217;ve described, especially Kimball and Inmon, were developed when data<br/>warehouses were expensive, on premises, and heavily resource-constrained with tightly coupled compute and<br/>storage. While batch data modeling has traditionally been associated with these strict approaches, more relaxed<br/>approaches are becoming more common.<br/>There are reasons for this. First, the popularity of the cloud means that storage is dirt cheap. It&#8217;s cheaper to store<br/>data than agonize over the optimum way to represent the data in storage. Second, the popularity of nested data<br/>(JSON and similar) means schemas are flexible in source and analytical systems.<br/>You have the option to rigidly model your data as we&#8217;ve described, or you can choose to throw all of your data into<br/>a single wide table. A <i>wide table</i> is just what it sounds like: a highly denormalized and very wide collection of<br/>many fields, typically created in a columnar database. A field may be a single value or contain nested data. The<br/>data is organized along with one or multiple keys; these keys are closely tied to the <i>grain</i> of the data.<br/>A wide table can potentially have thousands of columns, whereas fewer than 100 are typical in relational<br/>databases. Wide tables are usually sparse; the vast majority of entries in a given field may be null. This is<br/>extremely expensive in a traditional relational database because the database allocates a fixed amount of space for<br/>each field entry; nulls take up virtually no space in a columnar database. A wide schema in a relational database<br/>dramatically slows reading because each row must allocate all the space specified by the wide schema, and the<br/>database must read the contents of each row in its entirety. On the other hand, a columnar database reads only<br/>columns selected in a query, and reading nulls is essentially free.<br/>Wide tables generally arise through schema evolution; engineers gradually add fields over time. Schema evolution<br/>in a relational database is a slow and resource-heavy process. In a columnar database, adding a field is initially just<br/>a change to metadata. As data is written into the new field, new files are added to the column.<br/>Analytics queries on wide tables often run faster than equivalent queries on highly normalized data requiring many<br/>joins. Removing joins can have a huge impact on scan performance. The wide table simply contains all of the data<br/>you would have joined in a more rigorous modeling approach. Facts and dimensions are represented in the same<br/>table. The lack of data model rigor also means not a lot of thought is involved. Load your data into a wide table<br/>and start querying it. Especially with schemas in source systems becoming more adaptive and flexible, this data<br/>usually results from high-volume transactions, meaning there&#8217;s a lot of data. Storing this as nested data in your<br/>analytical storage has a lot of benefits.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Throwing all of your data into a single table might seem like heresy for a hardcore data modeler, and we&#8217;ve seen<br/>plenty of criticism. What are some of these criticisms? The biggest criticism is as you blend your data, you lose the<br/>business logic in your analytics. Another downside is the performance of updates to things like an element in an<br/>array, which can be very painful.<br/>Let&#8217;s look at an example of a wide table (Table 8-21), using the original denormalized table from our earlier<br/>normalization example. This table can have many more columns&#8212;hundreds or more!&#8212;and we include only a<br/>handful of columns for brevity and ease of understanding. As you can see, this table combines various data types,<br/>represented along a grain of orders for a customer on a date.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p> <br/><i>T<br/>a<br/>b<br/>l<br/>e <br/>8<br/>-<br/>2<br/>1<br/>. <br/>A<br/>n <br/>e<br/>x<br/>a<br/>m<br/>p<br/>l<br/>e <br/>o<br/>f <br/>d<br/>e<br/>n<br/>o<br/>r<br/>m<br/>a<br/>l<br/>i<br/>z<br/>e<br/>d <br/>d<br/>a<br/>t<br/>a<br/></i> <br/><b>OrderID OrderItems CustomerID CustomerName OrderDate Site SiteRegion<br/></b> <br/>100<br/></p>
<p>[{ <br/>              <b>&quot;sku&quot;</b>: <br/>1, <br/>              <br/><b>&quot;price&quot;</b>: 50, <br/><b>&quot;quantity&quot;</b>: 1, <br/>              <br/><b>&quot;name:&quot;</b>: <br/>&quot;Thingamajig&quot; <br/>}, { <br/>              <b>&quot;sku&quot;</b>: <br/>2, <br/>              <br/><b>&quot;price&quot;</b>: 25, <br/></p>
<p>5 Joe Reis 2022-03-01 abc.com US</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><b>&quot;quantity&quot;</b>: 2, <br/>              <br/><b>&quot;name:&quot;</b>: <br/>&quot;Whatchamacallit&quot; <br/>}]<br/></p>
<p> <br/></p>
<p>We suggest using a wide table when you don&#8217;t care about data modeling, or when you have a lot of data that needs<br/>more flexibility than traditional data-modeling rigor provides. Wide tables also lend themselves to streaming data,<br/>which we&#8217;ll discuss next. As data moves toward fast-moving schemas and streaming-first, we expect to see a new<br/>wave of data modeling, perhaps something along the lines of &#8220;relaxed normalization.&#8221;<br/></p>
<p><b>WHAT IF YOU DON&#8217;T MODEL YOUR DATA?<br/></b>You also have the option of <i>not</i> modeling your data. In this case, just query data sources directly. This pattern<br/>is often used, especially when companies are just getting started and want to get quick insights or share<br/>analytics with their users. While it allows you to get answers to various questions, you should consider the<br/>following:<br/></p>
<p>If I don&#8217;t model my data, how do I know the results of my queries are consistent?<br/>Do I have proper definitions of business logic in the source system, and will my query produce truthful<br/>answers?<br/>What query load am I putting on my source systems, and how does this impact users of these systems?<br/></p>
<p>At some point, you&#8217;ll probably gravitate toward a stricter batch data model paradigm and a dedicated data<br/>architecture that doesn&#8217;t rely on the source systems for the heavy lifting.<br/></p>
<p><b>Modeling Streaming Data<br/></b>Whereas many data-modeling techniques are well established for batch, this is not the case for streaming data.<br/>Because of the unbounded and continuous nature of streaming data, translating batch techniques like Kimball to a<br/>streaming paradigm is tricky, if not impossible. For example, given a stream of data, how would you continuously<br/>update a Type-2 slowly changing dimension without bringing your data warehouse to its knees?<br/>The world is evolving from batch to streaming and from on premises to the cloud. The constraints of the older<br/>batch methods no longer apply. That said, big questions remain about how to model data to balance the need for<br/>business logic against fluid schema changes, fast-moving data, and self-service. What is the streaming equivalent<br/>of the preceding batch data model approaches? There isn&#8217;t (yet) a consensus approach on streaming data modeling.<br/>We spoke with many experts in streaming data systems, many of whom told us that traditional batch-oriented data<br/>modeling doesn&#8217;t apply to streaming. A few suggested the data vault as an option for streaming data modeling.<br/>As you may recall, two main types of streams exist: event streams and CDC. Most of the time, the shape of the<br/>data in these streams is semistructured, such as JSON. The challenge with modeling streaming data is that the<br/>payload&#8217;s schema might change on a whim. For example, suppose you have an IoT device that recently upgraded<br/>its firmware and introduced a new field. In that case, it&#8217;s possible that your downstream destination data warehouse<br/>or processing pipeline isn&#8217;t aware of this change and breaks. That&#8217;s not great. As another example, a CDC system<br/>might recast a field as a different type&#8212;say, a string instead of an International Organization for Standardization<br/>(ISO) datetime format. Again, how does the destination handle this seemingly random change?<br/>The streaming data experts we&#8217;ve talked with overwhelmingly suggest you anticipate changes in the source data<br/>and keep a flexible schema. This means there&#8217;s no rigid data model in the analytical database. Instead, assume the<br/>source systems are providing the correct data with the right business definition and logic, as it exists today. And<br/>because storage is cheap, store the recent streaming and saved historical data in a way they can be queried together.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Optimize for comprehensive analytics against a dataset with a flexible schema. Furthermore, instead of reacting to<br/>reports, why not create automation that responds to anomalies and changes in the streaming data instead?<br/>The world of data modeling is changing, and we believe a sea change will soon occur in data model paradigms.<br/>These new approaches will likely incorporate metrics and semantic layers, data pipelines, and traditional analytics<br/>workflows in a streaming layer that sits directly on top of the source system. Since data is being generated in real<br/>time, the notion of artificially separating source and analytics systems into two distinct buckets may not make as<br/>much sense as when data moved more slowly and predictably. Time will tell&#8230;<br/>We have more to say on the future of streaming data in Chapter 11.<br/></p>
<p><b>Transformations<br/></b><i>The net result of transforming data is the ability to unify and integrate data. Once data is transformed, the data<br/>can be viewed as a single entity. But without transforming data, you cannot have a unified view of data across<br/>the organization.<br/></i></p>
<p>&#8212;Bill Inmon<br/>Now that we&#8217;ve covered queries and data modeling, you might be wondering, if I can model data, query it, and get<br/>results, why do I need to think about transformations? Transformations manipulate, enhance, and save data for<br/>downstream use, increasing its value in a scalable, reliable, and cost-effective manner.<br/>Imagine running a query every time you want to view results from a particular dataset. You&#8217;d run the same query<br/>dozens or hundreds of times a day. Imagine that this query involves parsing, cleansing, joining, unioning, and<br/>aggregating across 20 datasets. To further exacerbate the pain, the query takes 30 minutes to run, consumes<br/>significant resources, and incurs substantial cloud charges over several repetitions. You&#8217;d probably go insane.<br/>Thankfully, you can <i>save the results of your query</i> instead, or at least run the most compute-intensive portions only<br/>once, so subsequent queries are simplified.<br/>A transformation differs from a query. A <i>query</i> retrieves the data from various sources based on filtering and join<br/>logic. A <i>transformation</i> persists the results for consumption by additional transformations or queries. These results<br/>may be stored ephemerally or permanently.<br/>Besides persistence, a second aspect that differentiates transformations from queries is complexity. You&#8217;ll likely<br/>build complex pipelines that combine data from multiple sources and reuse intermediate results for multiple final<br/>outputs. These complex pipelines might normalize, model, aggregate, or featurize data. While you can build<br/>complex dataflows in single queries using common table expressions, scripts, or DAGs, this quickly becomes<br/>unwieldy, inconsistent, and intractable. Enter transformations.<br/>Transformations critically rely on one of the major undercurrents in this book: orchestration. Orchestration<br/>combines many discrete operations, such as intermediate transformations, that store data temporarily or<br/>permanently for consumption by downstream transformations or serving. Increasingly, transformation pipelines<br/>span not only multiple tables and datasets, but also multiple systems.<br/></p>
<p><b>Batch Transformations<br/></b><i>Batch transformations</i> run on discrete chunks of data, in contrast to streaming transformations, where data is<br/>processed continuously as it arrives. Batch transformations can run on a fixed schedule (e.g., daily, hourly, or every<br/>15 minutes) to support ongoing reporting, analytics, and ML models. In this section, you&#8217;ll learn various batch<br/>transformation patterns and technologies.<br/><b>Distributed joins<br/></b>The basic idea behind distributed joins is that we need to break a <i>logical join</i> (the join defined by the query logic)<br/>into much smaller <i>node joins</i> that run on individual servers in the cluster. The basic distributed join patterns apply<br/>whether one is in MapReduce (discussed in &#8220;MapReduce&#8221;), BigQuery, Snowflake, or Spark, though the details of</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>intermediate storage between processing steps vary (on disk or in memory). In the best case scenario, the data on<br/>one side of the join is small enough to fit on a single node (<i>broadcast join</i>). Often, a more resource-intensive<br/><i>shuffle hash join</i> is required.<br/><i>Broadcast join<br/></i>A <i>broadcast join</i> is generally asymmetric, with one large table distributed across nodes and one small table that<br/>can easily fit on a single node (Figure 8-17). The query engine &#8220;broadcasts&#8221; the small table (table A) out to all<br/>nodes, where it gets joined to the parts of the large table (table B). Broadcast joins are far less compute intensive<br/>than shuffle hash joins.<br/></p>
<p><i>Figure 8-17. In a broadcast join, the query engine sends table A out to all nodes in the cluster to be joined with the various parts of table B<br/></i></p>
<p>In practice, table A is often a down-filtered larger table that the query engine collects and broadcasts. One of the<br/>top priorities in query optimizers is join reordering. With the early application of filters, and movement of small<br/>tables to the left (for left joins) it is often possible to dramatically reduce the amount of data that is processed in<br/>each join. Pre-filtering data to create broadcast joins where possible can dramatically improve performance and<br/>reduce resource consumption.<br/><i>Shuffle hash join<br/></i>If neither table is small enough to fit on a single node, the query engine will use a <i>shuffle hash join</i>. In Figure 8-18,<br/>the same nodes are represented above and below the dotted line. The area above the dotted line represents the<br/>initial partitioning of tables A and B across the nodes. In general, this partitioning will have no relation to the join<br/>key. A hashing scheme is used to repartition data by join key.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><i>Figure 8-18. Shuffle hash join<br/></i></p>
<p>In this example, the hashing scheme will partition the join key into three parts, with each part assigned to a node.<br/>The data is then reshuffled to the appropriate node, and the new partitions for tables A and B on each node are<br/>joined. Shuffle hash joins are generally more resource intensive than broadcast joins.<br/><b>ETL, ELT and data pipelines<br/></b>As we discussed in Chapter 3, a widespread transformation pattern dating to the early days of relational databases<br/>is a batch ETL. Traditional ETL relies on an external transformation system to pull, transform, and clean data<br/>while preparing it for a target schema, such as a data mart or a Kimball star schema. The transformed data would<br/>then be loaded into a target system, such as a data warehouse, where business analytics could be performed.<br/>The ETL pattern itself was driven by the limitations of both source and target systems. The extract phase tended to<br/>be a major bottleneck, with the constraints of the source RDBMS limiting the rate at which data could be pulled.<br/>And, the transformation was handled in a dedicated system because the target system was extremely resource-<br/>constrained in both storage and CPU capacity.<br/>A now-popular evolution of ETL is ELT. As data warehouse systems have grown in performance and storage<br/>capacity, it has become common to simply extract raw data from a source system, import it into a data warehouse<br/>with minimal transformation, and then clean and transform it directly in the warehouse system. (See our discussion<br/>of data warehouses in Chapter 3 for a more detailed discussion of the difference between ETL and ELT.)<br/>A second, slightly different notion of ELT was popularized with the emergence of data lakes. In this version, the<br/>data is not transformed at the time it&#8217;s loaded. Indeed, massive quantities of data may be loaded with no<br/>preparation and no plan whatsoever. The assumption is that the transformation step will happen at some<br/>undetermined future time. Ingesting data without a plan is a great recipe for a data swamp. As Inmon says:<br/></p>
<p><i>I&#8217;ve always been a fan of ETL because of the fact that ETL forces you to transform data before you put it into a<br/>form where you can work with it. But some organizations want to simply take the data, put it into a database,<br/>then do the transformation&#8230; I&#8217;ve seen too many cases where the organization says, oh we&#8217;ll just put the data in<br/>and transform it later. And guess what? Six months later, that data [has] never been touched.<br/></i></p>
<p>We have also seen that the line between ETL and ELT can become somewhat blurry in a data lakehouse<br/>environment. With object storage as a base layer, it&#8217;s no longer clear what&#8217;s in the database and out of the database.<br/>The ambiguity is further exacerbated with the emergence of data federation, virtualization, and live tables. (We<br/>discuss these topics later in this section.)<br/>Increasingly, we feel that the terms <i>ETL</i> and <i>ELT</i> should be applied only at the micro level (within individual<br/>transformation pipelines) rather than at the macro level (to describe a transformation pattern for a whole<br/></p>
<p><i>14</i></p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>organization). Organizations no longer need to standardize on ETL or ELT but can instead focus on applying the<br/>proper technique on a case-by-case basis as they build data pipelines.<br/><b>SQL and general-purpose code-transformation tools<br/></b>At this juncture, the distinction between SQL-based and non-SQL-based transformation systems feels somewhat<br/>synthetic. Since the introduction of Hive on the Hadoop platform, SQL has become a first-class citizen in the big<br/>data ecosystem. For example, Spark SQL was an early feature of Apache Spark. Streaming-first frameworks such<br/>as Kafka, Flink, and Beam also support SQL, with varying features and functionality.<br/>It is more appropriate to think about SQL-only tools versus those that support more powerful, general-purpose<br/>programming paradigms. SQL-only transformation tools span a wide variety of proprietary and open source<br/>options.<br/><i>SQL is declarative...but it can still build complex data workflows<br/></i>We often hear SQL dismissed because it is &#8220;not procedural.&#8221; This is technically correct. SQL is a declarative<br/>language: instead of coding a data processing procedure, SQL writers stipulate the characteristics of their final data<br/>in set-theoretic language; the SQL compiler and optimizer determine the steps required to put data in this state.<br/>People sometimes imply that because SQL is not procedural, it cannot build out complex pipelines. This is false.<br/>SQL can effectively be used to build complex DAGs using common table expressions, SQL scripts, or an<br/>orchestration tool.<br/>To be clear, SQL has limits, but we often see engineers doing things in Python and Spark that could be more easily<br/>and efficiently done in SQL. For a better idea of the trade-offs we&#8217;re talking about, let&#8217;s look at a couple of<br/>examples of Spark and SQL.<br/><i>Example: When to avoid SQL for batch transformations in Spark<br/></i>When you&#8217;re determining whether to use native Spark or PySpark code instead of Spark SQL or another SQL<br/>engine, ask yourself the following questions:<br/></p>
<p>1. How difficult is it to code the transformation in SQL?<br/>2. How readable and maintainable will the resulting SQL code be?<br/>3. Should some of the transformation code be pushed into a custom library for future reuse across the<br/></p>
<p>organization?<br/>Regarding question 1, many transformations coded in Spark could be realized in fairly simple SQL statements. On<br/>the other hand, if the transformation is not realizable in SQL, or if it would be extremely awkward to implement,<br/>native Spark is a better option. For example, we might be able to implement word stemming in SQL by placing<br/>word suffixes in a table, joining with that table, using a parsing function to find suffixes in words, and then<br/>reducing the word to its stem by using a substring function. However, this sounds like an extremely complex<br/>process with numerous edge cases to consider. A more powerful procedural programming language is a better fit<br/>here.<br/>Question 2 is closely related. The word-stemming query will be neither readable nor maintainable.<br/>Regarding question 3, one of the major limitations of SQL is that it doesn&#8217;t include a natural notion of libraries or<br/>reusable code. One exception is that some SQL engines allow you to maintain user-defined functions (UDFs) as<br/>objects inside a database.  However, these aren&#8217;t committed to a Git repository without an external CI/CD system<br/>to manage deployment. Furthermore, SQL doesn&#8217;t have a good notion of reusability for more complex query<br/>components. Of course, reusable libraries are easy to create in Spark and PySpark.<br/>We will add that it is possible to recycle SQL in two ways. First, we can easily reuse the <i>results</i> of a SQL query by<br/>committing to a table or creating a view. This process is often best handled in an orchestration tool such as Airflow<br/>so that downstream queries can start once the source query has finished. Second, Data Build Tool (dbt) facilitates<br/>the reuse of SQL statements and offers a templating language that makes customization easier.<br/></p>
<p>15</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><i>Example: Optimizing Spark and other processing frameworks<br/></i>Spark acolytes often complain that SQL doesn&#8217;t give them control over data processing. The SQL engine takes<br/>your statements, optimizes them, and compiles them into its processing steps. (In practice, optimization may<br/>happen before or after compilation, or both.)<br/>This is a fair complaint, but a corollary exists. With Spark and other code-heavy processing frameworks, the code<br/>writer becomes responsible for much of the optimization that is handled automatically in a SQL-based engine. The<br/>Spark API is powerful and complex, meaning it is not so easy to identify candidates for reordering, combination,<br/>or decomposition. When embracing Spark, data engineering teams need to actively engage with the problems of<br/>Spark optimization, especially for expensive, long-running jobs. This means building optimization expertise on the<br/>team and teaching individual engineers how to optimize.<br/>A few top-level things to keep in mind when coding in native Spark:<br/></p>
<p>1. Filter early and often.<br/>2. Rely heavily on the core Spark API, and learn to understand the Spark native way of doing things. Try to rely<br/></p>
<p>on well-maintained public libraries if the native Spark API doesn&#8217;t support your use case. Good Spark code is<br/>substantially declarative.<br/></p>
<p>3. Be careful with UDFs.<br/>4. Consider intermixing SQL.<br/></p>
<p>Recommendation 1 applies to SQL optimization as well, the difference being that Spark may not be able to reorder<br/>something that SQL would handle for you automatically. Spark is a big data processing framework, but the less<br/>data you have to process, the less resource-heavy and more performant your code will be.<br/>If you find yourself writing extremely complex custom code, pause and determine whether there&#8217;s a more native<br/>way of doing whatever you&#8217;re trying to accomplish. Learn to understand idiomatic Spark by reading public<br/>examples and working through tutorials. Is there something in the Spark API that can accomplish what you&#8217;re<br/>trying to do? Is there a well-maintained and optimized public library that can help?<br/>The third recommendation is crucial for PySpark. In general, PySpark is an API wrapper for Scala Spark. Your<br/>code pushes work into native Scala code running in the JVM by calling the API. Running Python UDFs forces<br/>data to be passed to Python, where processing is less efficient. If you find yourself using Python UDFs, look for a<br/>more Spark-native way to accomplish what you&#8217;re doing. Go back to the recommendation: is there a way to<br/>accomplish your task by using the core API or a well-maintained library? If you must use UDFs, consider<br/>rewriting them in Scala or Java to improve performance.<br/>As for recommendation 4, using SQL allows us to take advantage of the Spark Catalyst optimizer, which may be<br/>able to squeeze out more performance than we can with native Spark code. SQL is often easier to write and<br/>maintain for simple operations. Combining native Spark and SQL lets us realize the best of both worlds&#8212;<br/>powerful, general-purpose functionality combined with simplicity where applicable.<br/>Much of the optimization advice in this section is fairly generic and would apply just as well to Apache Beam, for<br/>example. The main point is that programmable data processing APIs require a bit more optimization finesse than<br/>SQL, which is perhaps less powerful and easier to use.<br/><b>Update patterns<br/></b>Since transformations persist data, we will often update persisted data in place. Updating data is a major pain point<br/>for data engineering teams, especially as they transition between data engineering technologies. We&#8217;re discussing<br/>DML in SQL, which we introduced earlier in the chapter.<br/>We&#8217;ve mentioned several times throughout the book that the original data lake concept didn&#8217;t really account for<br/>updating data. This now seems nonsensical for several reasons. Updating data has long been a key part of handling<br/>data transformation results, even though the big data community dismissed it. It is silly to rerun significant</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>amounts of work because we have no update capabilities. Thus, the data lakehouse concept now builds in updates.<br/>Also, GDPR and other data deletion standards now <i>require</i> organizations to delete data in a targeted fashion, even<br/>in raw datasets.<br/>Let&#8217;s consider several basic update patterns.<br/><i>Truncate and reload<br/>Truncate</i> is an update pattern that doesn&#8217;t update anything. It simply wipes the old data. In a truncate-and-reload<br/>update pattern, a table is cleared of data, and transformations are rerun and loaded into this table, effectively<br/>generating a new table version.<br/><i>Insert only<br/>Insert only</i> inserts new records without changing or deleting old records. Insert-only patterns can be used to<br/>maintain a current view of data&#8212;for example, if new versions of records are inserted without deleting old records.<br/>A query or view can present the current data state by finding the newest record by primary key. Note that columnar<br/>databases don&#8217;t typically enforce primary keys. The primary key would be a construct used by engineers to<br/>maintain a notion of the current state of the table. The downside to this approach is that it can be extremely<br/>computationally expensive to find the latest record at query time. Alternatively, we can use a materialized view<br/>(covered later in the chapter), an insert-only table that maintains all records, and a truncate-and-reload target table<br/>that holds the current state for serving data.<br/></p>
<p><b>WARNING<br/></b>When inserting data into a column-oriented OLAP database, the common problem is that engineers transitioning from row-oriented systems<br/>attempt to use single-row inserts. This antipattern puts a massive load on the system. It also causes data to be written in many separate files;<br/>this is extremely inefficient for subsequent reads, and the data must be reclustered later. Instead, we recommend loading data in a periodic<br/>micro-batch or batch fashion.<br/></p>
<p>We&#8217;ll mention an exception to the advice not to insert frequently: the enhanced Lambda architecture used by<br/>BigQuery and Apache Druid, which hybridizes a streaming buffer with columnar storage. Deletes and in-place<br/>updates can still be expensive, as we&#8217;ll discuss next.<br/><i>Delete<br/></i>Deletion is critical when a source system deletes data and satisfies recent regulatory changes. In columnar systems<br/>and data lakes, deletes are more expensive than inserts.<br/>When deleting data, consider whether you need to do a hard or soft delete. A <i>hard delete</i> permanently removes a<br/>record from a database, while a <i>soft delete</i> marks the record as &#8220;deleted.&#8221; Hard deletes are useful when you need to<br/>remove data for performance reasons (say, a table is too big), or if there&#8217;s a legal or compliance reason to do so.<br/>Soft deletes might be used when you don&#8217;t want to delete a record permanently but also want to filter it out of<br/>query results.<br/>A third approach to deletes is closely related to soft deletes: <i>insert deletion</i> inserts a new record with a deleted<br/>flag without modifying the previous version of the record. This allows us to follow an insert-only pattern but still<br/>account for deletions. Just note that our query to get the latest table state gets a little more complicated. We must<br/>now deduplicate, find the latest version of each record by key, and not show any record whose latest version shows<br/>deleted.<br/><i>Upsert/merge<br/></i>Of these update patterns, the upsert and merge patterns are the ones that consistently cause the most trouble for<br/>data engineering teams, especially for people transitioning from row-based data warehouses to column-based<br/>cloud systems.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><i>Upserting</i> takes a set of source records and looks for matches against a target table by using a primary key or<br/>another logical condition. (Again, it&#8217;s the responsibility of the data engineering team to manage this primary key<br/>by running appropriate queries. Most columnar systems will not enforce uniqueness.) When a key match occurs,<br/>the target record gets updated (replaced by the new record). When no match exists, the database inserts the new<br/>record. The merge pattern adds to this the ability to delete records.<br/>So, what&#8217;s the problem? The upsert/merge pattern was originally designed for row-based databases. In row-based<br/>databases, updates are a natural process: the database looks up the record in question and changes it in place.<br/>On the other hand, file-based systems don&#8217;t actually support in-place file updates. All of these systems utilize copy<br/>on write (COW). If one record in a file is changed or deleted, the whole file must be rewritten with the new<br/>changes.<br/>This is part of the reason that early adopters of big data and data lakes rejected updates: managing files and<br/>updates seemed too complicated. So they simply used an insert-only pattern and assumed that data consumers<br/>would determine the current state of the data at query time or in downstream transformations. In reality, columnar<br/>databases such as Vertica have long supported in-place updates by hiding the complexity of COW from users.<br/>They scan files, change the relevant records, write new files, and change file pointers for the table. The major<br/>columnar cloud data warehouses support updates and merges, although engineers should investigate update<br/>support if they consider adopting an exotic technology.<br/>There are a few key things to understand here. Even though distributed columnar data systems support native<br/>update commands, merges come at a cost: the performance impact of updating or deleting a single record can be<br/>quite high. On the other hand, merges can be extremely performant for large update sets and may even outperform<br/>transactional databases.<br/>In addition, it is important to understand that COW seldom entails rewriting the whole table. Depending on the<br/>database system in question, COW can operate at various resolutions (partition, cluster, block). To realize<br/>performant updates, focus on developing an appropriate partitioning and clustering strategy based on your needs<br/>and the innards of the database in question.<br/>As with inserts, be careful with your update or merge frequency. We&#8217;ve seen many engineering teams transition<br/>between database systems and try to run near real-time merges from CDC just as they did on their old system. It<br/>simply doesn&#8217;t work. No matter how good your CDC system is, this approach will bring most columnar data<br/>warehouses to their knees. We&#8217;ve seen systems fall weeks behind on updates, where an approach that simply<br/>merged every hour would make much more sense.<br/>We can use various approaches to bring columnar databases closer to real time. For example, BigQuery allows us<br/>to stream insert new records into a table, and then supports specialized materialized views that present an efficient,<br/>near real-time deduplicated table view. Druid uses two-tier storage and SSDs to support ultrafast real-time queries.<br/><b>Schema updates<br/></b>Data has entropy, and may change without your control or consent. External data sources may change their<br/>schema, or application development teams may add new fields to the schema. One advantage of columnar systems<br/>over row-based systems is that while updating the data is more difficult, updating the schema is easier. Columns<br/>can typically be added, deleted, and renamed.<br/>In spite of these technological improvements, practical organizational schema management is more challenging.<br/>Will some schema updates be automated? (This is the approach that Fivetran uses when replicating from sources.)<br/>As convenient as this sounds, there&#8217;s a risk that downstream transformations will break.<br/>Is there a straightforward schema update request process? Suppose a data science team wants to add a column<br/>from a source that wasn&#8217;t previously ingested. What will the review process look like? Will downstream processes<br/>break? (Are there queries that run SELECT * rather than using explicit column selection? This is generally bad<br/>practice in columnar databases.) How long will it take to implement the change? Is it possible to create a table fork<br/>&#8212;i.e., a new table version specific to this project?</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>A new interesting option has emerged for semistructured data. Borrowing an idea from document stores, many<br/>cloud data warehouses now support data types that encode arbitrary JSON data. One approach stores raw JSON in<br/>a field while storing frequently accessed data in adjacent flattened fields. This takes up additional storage space but<br/>allows for the convenience of flattened data, with the flexibility of semistructured data for advanced users.<br/>Frequently accessed data in the JSON field can be added directly into the schema over time.<br/>This approach works extremely well when data engineers must ingest data from an application document store<br/>with a frequently changing schema. Semistructured data available as a first-class citizen in data warehouses is<br/>extremely flexible and opens new opportunities for data analysts and data scientists since data is no longer<br/>constrained to rows and columns.<br/><b>Data wrangling<br/></b><i>Data wrangling</i> takes messy, malformed data and turns it into useful, clean data. This is generally a batch<br/>transformation process.<br/>Data wrangling has long been a major source of pain and job security for ETL developers. For example, suppose<br/>that developers receive EDI data (see Chapter 7) from a partner business regarding transactions and invoices,<br/>potentially a mix of structured data and text. The typical process of wrangling this data involves first trying to<br/>ingest it. Often, the data is so malformed that a good deal of text preprocessing is involved. Developers may<br/>choose to ingest the data as a single text field table&#8212;an entire row ingested as a single field. Developers then begin<br/>writing queries to parse and break apart the data. Over time, they discover data anomalies and edge cases.<br/>Eventually, they will get the data into rough shape. Only then can the process of downstream transformation begin.<br/>Data wrangling tools aim to simplify significant parts of this process. These tools often put off data engineers<br/>because they claim to be no code, which sounds unsophisticated. We prefer to think of data wrangling tools as<br/>integrated development environments (IDEs) for malformed data. In practice, data engineers spend way too much<br/>time parsing nasty data; automation tools allow data engineers to spend time on more interesting tasks. Wrangling<br/>tools may also allow engineers to hand some parsing and ingestion work off to analysts.<br/>Graphical data-wrangling tools typically present a sample of data in a visual interface, with inferred types,<br/>statistics including distributions, anomalous data, outliers, and nulls. Users can then add processing steps to fix<br/>data issues. A step might provide instructions for dealing with mistyped data, splitting a text field into multiple<br/>parts, or joining with a lookup table.<br/>Users can run the steps on a full dataset when the full job is ready. The job typically gets pushed to a scalable data<br/>processing system such as Spark for large datasets. After the job runs, it will return errors and unhandled<br/>exceptions. The user can further refine the recipe to deal with these outliers.<br/>We highly recommend that both aspiring and seasoned engineers experiment with wrangling tools; major cloud<br/>providers sell their version of data-wrangling tools, and many third-party options are available. Data engineers<br/>may find that these tools significantly streamline certain parts of their jobs. Organizationally, data engineering<br/>teams may want to consider training specialists in data wrangling if they frequently ingest from new, messy data<br/>sources.<br/><b>Example: Data transformation in Spark<br/></b>Let&#8217;s look at a practical, concrete example of data transformation. Suppose we build a pipeline that ingests data<br/>from three API sources in JSON format. This initial ingestion step is handled in Airflow. Each data source gets its<br/>prefix (filepath) in an S3 bucket.<br/>Airflow then triggers a Spark job by calling an API. This Spark job ingests each of the three sources into a<br/>dataframe, converting the data into a relational format, with nesting in certain columns. The Spark job combines<br/>the three sources into a single table, and then filters the results with a SQL statement. The results are finally<br/>written out to a Parquet-formatted Delta Lake table stored in S3.<br/>In practice, Spark creates a DAG of steps based on the code that we write for ingesting, joining, and writing out<br/>the data. The basic ingestion of data happens in cluster memory, although one of the data sources is large enough</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>that it must spill to disk during the ingestion process. (This data gets written to cluster storage; it will be reloaded<br/>into memory for subsequent processing steps.)<br/>The join requires a shuffle operation. A key is used to redistribute data across the cluster; once again, a spill to disk<br/>occurs as the data is written to each node. The SQL transformation filters through the rows in memory and<br/>discards the unused rows. Finally, Spark converts the data into Parquet format, compresses it, and writes it back to<br/>S3. Airflow periodically calls back to Spark to see if the job is completed. Once it confirms that the job has<br/>finished, it marks the full Airflow DAG as completed. (Note that we have two DAG constructs here, an Airflow<br/>DAG and a DAG specific to the Spark job.)<br/><b>Business logic and derived data<br/></b>One of the most common use cases for transformation is to render business logic. We&#8217;ve placed this discussion<br/>under batch transformations because this is where this type of transformation happens most frequently, but note<br/>that it could also happen in a streaming pipeline.<br/>Suppose that a company uses multiple specialized internal profit calculations. One version might look at profits<br/>before marketing costs, and another might look at a profit after subtracting marketing costs. Even though this<br/>appears to be a straightforward accounting exercise, each of these metrics is highly complex to render.<br/>Profit before marketing costs might need to account for fraudulent orders; determining a reasonable profit estimate<br/>for the previous business day entails estimating what percentage of revenue and profit will ultimately be lost to<br/>orders canceled in the coming days as the fraud team investigates suspicious orders. Is there a special flag in the<br/>database that indicates an order with a high probability of fraud, or one that has been automatically canceled? Does<br/>the business assume that a certain percentage of orders will be canceled because of fraud even before the fraud-risk<br/>evaluation process has been completed for specific orders?<br/>For profits after marketing costs, we must account for all the complexities of the previous metric, plus the<br/>marketing costs attributed to the specific order. Does the company have a naive attribution model&#8212;e.g., marketing<br/>costs attributed to items weighted by price? Marketing costs might also be attributed per department, or item<br/>category, or&#8212;in the most sophisticated organizations&#8212;per individual item based on user ad clicks.<br/>The business logic transformation that generates this nuanced version of profit must integrate all the subtleties of<br/>attribution&#8212;i.e., a model that links orders to specific ads and advertising costs. Is attribution data stored in the guts<br/>of ETL scripts, or is it pulled from a table that is automatically generated from ad platform data?<br/>This type of reporting data is a quintessential example of <i>derived data</i>&#8212;data computed from other data stored in a<br/>data system. Derived data critics will point out that it is challenging for the ETL to maintain consistency in the<br/>derived metrics.  For example, if the company updates its attribution model, this change may need to be merged<br/>into many ETL scripts for reporting. (ETL scripts are notorious for breaking the DRY principle.) Updating these<br/>ETL scripts is a manual and labor-intensive process, involving domain expertise in processing logic and previous<br/>changes. Updated scripts must also be validated for consistency and accuracy.<br/>From our perspective, these are legitimate criticisms but not necessarily very constructive because the alternative<br/>to derived data in this instance is equally distasteful. Analysts will need to run their reporting queries if profit data<br/>is not stored in the data warehouse, including profit logic. Updating complex ETL scripts to represent changes to<br/>business logic accurately is an overwhelming, labor-intensive task, but getting analysts to update their reporting<br/>queries consistently is well-nigh impossible.<br/>One interesting alternative is to push business logic into a <i>metrics layer</i>,  but still leverage the data warehouse or<br/>other tool to do the computational heavy lifting. A metrics layer encodes business logic and allows analysts and<br/>dashboard users to build complex analytics from a library of defined metrics. The metrics layer generates queries<br/>from the metrics and sends these to the database. We discuss semantic and metrics layers in more detail in<br/>Chapter 9.<br/><b>MapReduce<br/></b></p>
<p>16<br/></p>
<p>17</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>No discussion of batch transformation can be complete without touching on MapReduce. This isn&#8217;t because<br/>MapReduce is widely used by data engineers these days. MapReduce was the defining batch data transformation<br/>pattern of the big data era, it still influences many distributed systems data engineers use today, and it&#8217;s useful for<br/>data engineers to understand at a basic level. MapReduce was introduced by Google in a follow-up to its paper on<br/>GFS. It was initially the de facto processing pattern of Hadoop, the open source analogue technology of GFS that<br/>we introduced in Chapter 6.<br/>A simple MapReduce job consists of a collection of map tasks that read individual data blocks scattered across the<br/>nodes, followed by a shuffle that redistributes result data across the cluster and a reduce step that aggregates data<br/>on each node. For example, suppose that we wanted to run the following SQL query:<br/></p>
<p><b>SELECT</b> <b>COUNT</b>(*), user_id <br/><b>FROM</b> user_events <br/><b>GROUP</b> <b>BY</b> user_id;<br/></p>
<p>The table data is spread across nodes in data blocks; the MapReduce job generates one map task per block. Each<br/>map task essentially runs the query on a single block&#8212;i.e., it generates a count for each user ID that appears in the<br/>block. While a block might contain hundreds of megabytes, the full table could be petabytes in size. However, the<br/>map portion of the job is a nearly perfect example of embarrassing parallelism; the data scan rate across the full<br/>cluster essentially scales linearly with the number of nodes.<br/>We then need to aggregate (reduce) to gather results from the full cluster. We&#8217;re not gathering results to a single<br/>node; rather, we redistribute results by key so that each key ends up on one and only one node. This is the shuffle<br/>step, which is often executed using a hashing algorithm on keys. Once the map results have been shuffled, we sum<br/>the results for each key. The key/count pairs can be written to the local disk on the node where they are computed.<br/>We collect the results stored across nodes to view the full query results.<br/>Real-world MapReduce jobs can be far more complex than what we describe here. A complex query that filters<br/>with a WHERE clause joins three tables and applies a window function that would consist of many map and reduce<br/>stages.<br/><b>After MapReduce<br/></b>Google&#8217;s original MapReduce model is extremely powerful but is now viewed as excessively rigid. It utilizes<br/>numerous short-lived ephemeral tasks that read from and write to disk. In particular, no intermediate state is<br/>preserved in memory; all data is transferred between tasks by storing it to disk or pushing it over the network. This<br/>simplifies state and workflow management, and minimizes memory consumption, but it can also drive high-disk<br/>bandwidth utilization and increase processing time.<br/>The MapReduce paradigm was constructed around the idea that magnetic disk capacity and bandwidth were so<br/>cheap that it made sense to simply throw a massive amount of disk at data to realize ultra-fast queries. This worked<br/>to an extent; MapReduce repeatedly set data processing records during the early days of Hadoop.<br/>However, we have lived in a post-MapReduce world for quite some time. Post-MapReduce processing does not<br/>truly discard MapReduce; it still includes the elements of map, shuffle, and reduce, but it relaxes the constraints of<br/>MapReduce to allow for in-memory caching.  Recall that RAM is much faster than SSD and HDDs in transfer<br/>speed and seek time. Persisting even a tiny amount of judiciously chosen data in memory can dramatically speed<br/>up specific data processing tasks and utterly crush the performance of MapReduce.<br/>For example, Spark, BigQuery, and various other data processing frameworks were designed around in-memory<br/>processing. These frameworks treat data as a distributed set that resides in memory. If data overflows available<br/>memory, this causes a <i>spill to disk</i>. The disk is treated as a second-class data-storage layer for processing, though it<br/>is still highly valuable.<br/>The cloud is one of the drivers for the broader adoption of memory caching; it is much more effective to lease<br/>memory during a specific processing job than to own it 24 hours a day. Advancements in leveraging memory for<br/>transformations will continue to yield gains for the foreseeable future.<br/></p>
<p>18</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><b>Materialized Views, Federation, and Query Virtualization<br/></b>In this section, we look at several techniques that virtualize query results by presenting them as table-like objects.<br/>These techniques can become part of a transformation pipeline or sit right before end-user data consumption.<br/><b>Views<br/></b>First, let&#8217;s review views to set the stage for materialized views. A <i>view</i> is a database object that we can select from<br/>just like any other table. In practice, a view is just a query that references other tables. When we select from a<br/>view, that database creates a new query that combines the view subquery with our query. The query optimizer then<br/>optimizes and runs the full query.<br/>Views play a variety of roles in a database. First, views can serve a security role. For example, views can select<br/>only specific columns and filter rows, thus providing restricted data access. Various views can be created for job<br/>roles depending on user data access.<br/>Second, a view might be used to provide a current deduplicated picture of data. If we&#8217;re using an insert-only<br/>pattern, a view may be used to return a deduplicated version of a table showing only the latest version of each<br/>record.<br/>Third, views can be used to present common data access patterns. Suppose that marketing analysts must frequently<br/>run a query that joins five tables. We could create a view that joins together these five tables into a wide table.<br/>Analysts can then write queries that filter and aggregate on top of this view.<br/><b>Materialized views<br/></b>We mentioned materialized views in our earlier discussion of query caching. A potential disadvantage of<br/>(nonmaterialized) views is that they don&#8217;t do any precomputation. In the example of a view that joins five tables,<br/>this join must run every time a marketing analyst runs a query on this view, and the join could be extremely<br/>expensive.<br/>A materialized view does some or all of the view computation in advance. In our example, a materialized view<br/>might save the five table join results every time a change occurs in the source tables. Then, when a user references<br/>the view, they&#8217;re querying from the prejoined data. A materialized view is a de facto transformation step, but the<br/>database manages execution for convenience.<br/>Materialized views may also serve a significant query optimization role depending on the database, even for<br/>queries that don&#8217;t directly reference them. Many query optimizers can identify queries that &#8220;look like&#8221; a<br/>materialized view. An analyst may run a query that uses a filter that appears in a materialized view. The optimizer<br/>will rewrite the query to select from the precomputed results.<br/><b>Composable materialized views<br/></b>In general, materialized views do not allow for composition&#8212;that is, a materialized view cannot select from<br/>another materialized view. However, we&#8217;ve recently seen the emergence of tools that support this capability. For<br/>example, Databricks has introduced the notion of <i>live tables</i>. Each table is updated as data arrives from sources.<br/>Data flows down to subsequent tables asynchronously.<br/><b>Federated queries<br/></b><i>Federated queries</i> are a database feature that allows an OLAP database to select from an external data source, such<br/>as object storage or RDBMS. For example, let&#8217;s say you need to combine data across object storage and various<br/>tables in MySQL and PostgreSQL databases. Your data warehouse can issue a federated query to these sources and<br/>return the combined results (Figure 8-19).</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><i>Figure 8-19. An OLAP database issues a federated query that gets data from object storage, MySQL, and PostgreSQL and returns a query result with the<br/>combined data<br/></i></p>
<p>As another example, Snowflake supports the notion of external tables defined on S3 buckets. An external data<br/>location and a file format are defined when creating the table, but data is not yet ingested into the table. When the<br/>external table is queried, Snowflake reads from S3 and processes the data based on the parameters set at the time<br/>of the table&#8217;s creation. We can even join S3 data to internal database tables. This makes Snowflake and similar<br/>databases more compatible with a data lake environment.<br/>Some OLAP systems can convert federated queries into materialized views. This gives us much of the<br/>performance of a native table without the need to manually ingest data every time the external source changes. The<br/>materialized view gets updated whenever the external data changes.<br/><b>Data virtualization<br/></b><i>Data virtualization</i> is closely related to federated queries, but this typically entails a data processing and query<br/>system that doesn&#8217;t store data internally. Right now, Trino (e.g., Starburst) and Presto are examples par excellence.<br/>Any query/processing engine that supports external tables can serve as a data virtualization engine. The most<br/>significant considerations with data virtualization are supported external sources and performance.<br/>A closely related concept is the notion of <i>query pushdown</i>. Suppose I wanted to query data from Snowflake, join<br/>data from a MySQL database, and filter the results. Query pushdown aims to move as much work as possible to<br/>the source databases. The engine might look for ways to push filtering predicates into the queries on the source<br/>systems. This serves two purposes: first, it offloads computation from the virtualization layer, taking advantage of<br/>the query performance of the source. Second, it potentially reduces the quantity of data that must push across the<br/>network, a critical bottleneck for virtualization performance.<br/>Data virtualization is a good solution for organizations with data stored across various data sources. However, data<br/>virtualization should not be used haphazardly. For example, virtualizing a production MySQL database doesn&#8217;t<br/>solve the core problem of analytics queries adversely impacting the production system&#8212;because Trino does not<br/>store data internally, it will pull from MySQL every time it runs a query.<br/>Alternatively, data virtualization can be used as a component of data ingestion and processing pipelines. For<br/>instance, Trino might be used to select from MySQL once a day at midnight when the load on the production<br/>system is low. Results could be saved into S3 for consumption by downstream transformations and daily queries,<br/>protecting MySQL from direct analytics queries.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Data virtualization can be viewed as a tool that expands the data lake to many more sources by abstracting away<br/>barriers used to silo data between organizational units. An organization can store frequently accessed, transformed<br/>data in S3 and virtualize access between various parts of the company. This fits closely with the notion of a <i>data<br/>mesh</i> (discussed in Chapter 3), wherein small teams are responsible for preparing their data for analytics and<br/>sharing it with the rest of the company; virtualization can serve as a critical access layer for practical sharing.<br/></p>
<p><b>Streaming Transformations and Processing<br/></b>We&#8217;ve already discussed stream processing in the context of queries. The difference between streaming<br/>transformations and streaming queries is subtle and warrants more explanation.<br/><b>Basics<br/></b>Streaming queries run dynamically to present a current view of data, as discussed previously. <i>Streaming<br/>transformations</i> aim to prepare data for downstream consumption.<br/>For instance, a data engineering team may have an incoming stream carrying events from an IoT source. These IoT<br/>events carry a device ID and event data. We wish to dynamically enrich these events with other device metadata,<br/>which is stored in a separate database. The stream-processing engine queries a separate database containing this<br/>metadata by device ID, generates new events with the added data, and passes it on to another stream. Live queries<br/>and triggered metrics run on this enriched stream (see Figure 8-20).<br/></p>
<p><i>Figure 8-20. An incoming stream is carried by a streaming event platform and passed into a stream processor<br/></i></p>
<p><b>Transformations and queries are a continuum<br/></b>The line between transformations and queries is also blurry in batch processing, but the differences become even<br/>more subtle in the domain of streaming. For example, if we dynamically compute roll-up statistics on windows,<br/>and then send the output to a target stream, is this a transformation or a query?<br/>Maybe we will eventually adopt new terminology for stream processing that better represents real-world use cases.<br/>For now, we will do our best with the terminology we have.<br/><b>Streaming DAGs<br/></b>One interesting notion closely related to stream enrichment and joins is the <i>streaming DAG</i>.  We first talked<br/>about this idea in our discussion of orchestration in Chapter 2. Orchestration is inherently a batch concept, but<br/>what if we wanted to enrich, merge, and split multiple streams in real time?<br/>Let&#8217;s take a simple example where streaming DAG would be useful. Suppose that we want to combine website<br/>clickstream data with IoT data. This will allow us to get a unified view of user activity by combining IoT events<br/>with clicks. Furthermore, each data stream needs to be preprocessed into a standard format (see Figure 8-21).<br/></p>
<p>19</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><i>Figure 8-21. A simple streaming DAG<br/></i></p>
<p>This has long been possible by combining a streaming store (e.g., Kafka) with a stream processor (e.g., Flink).<br/>Creating the DAG amounted to building a complex Rube Goldberg machine, with numerous topics and processing<br/>jobs connected.<br/>Pulsar dramatically simplifies this process by treating DAGs as a core streaming abstraction. Rather than<br/>managing flows across several systems, engineers can define their streaming DAGs as code inside a single system.<br/><b>Micro-batch versus true streaming<br/></b>A long-running battle has been ongoing between micro-batch and true streaming approaches. Fundamentally, it&#8217;s<br/>important to understand your use case, the performance requirements, and the performance capabilities of the<br/>framework in question.<br/>Micro-batching is a way to take a batch-oriented framework and apply it in a streaming situation. A micro-batch<br/>might run anywhere from every two minutes to every second. Some micro-batch frameworks (e.g., Apache Spark<br/>Streaming) are designed for this use case and will perform well with appropriately allocated resources at a high<br/>batch frequency. (In truth, DBAs and engineers have long used micro-batching with more traditional databases;<br/>this often led to horrific performance and resource consumption.)<br/>True streaming systems (e.g., Beam and Flink) are designed to process one event at a time. However, this comes<br/>with significant overhead. Also, it&#8217;s important to note that even in these true streaming systems, many processes<br/>will still occur in batches. A basic enrichment process that adds data to individual events can deliver one event at a<br/>time with low latency. However, a triggered metric on windows may run every few seconds, every few minutes,<br/>etc.<br/>When you&#8217;re using windows and triggers (hence, batch processing), what&#8217;s the window frequency? What&#8217;s the<br/>acceptable latency? If you are collecting Black Friday sales metrics published every few minutes, micro-batches<br/>are probably just fine as long as you set an appropriate micro-batch frequency. On the other hand, if your ops team<br/>is computing metrics every second to detect DDoS attacks, true streaming may be in order.<br/>When should you use one over the other? Frankly, there is no universal answer. The term <i>micro-batch</i> has often<br/>been used to dismiss competing technologies, but it may work just fine for your use case, and can be superior in<br/>many respects depending on your needs. If your team already has expertise in Spark, you will be able to spin up a<br/>Spark (micro-batch) streaming solution extremely fast.<br/>There&#8217;s no substitute for domain expertise and real-world testing. Talk to experts who can present an even-handed<br/>opinion. You can also easily test the alternatives by spinning up tests on cloud infrastructure. Also, watch out for<br/>spurious benchmarks provided by vendors. Vendors are notorious for cherry-picking benchmarks and setting up</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>artificial examples that don&#8217;t match reality (recall our conversation on benchmarks in Chapter 4). Frequently,<br/>vendors will show massive advantages in their benchmark results but fail to deliver in the real world for your use<br/>case.<br/></p>
<p><b>Whom You&#8217;ll Work With<br/></b>Queries, transformations, and modeling impact all stakeholders up and down the data engineering lifecycle. The<br/>data engineer is responsible for several things at this stage in the lifecycle. From a technical angle, the data<br/>engineer designs, builds, and maintains the integrity of the systems that query and transform data. The data<br/>engineer also implements data models within this system. This is the most &#8220;full-contact&#8221; stage where your focus is<br/>to add as much value as possible, both in terms of functioning systems and reliable and trustworthy data.<br/></p>
<p><b>Upstream Stakeholders<br/></b>When it comes to transformations, upstream stakeholders can be broken into two broad categories: those who<br/>control the business definitions and those who control the systems generating data.<br/>When interfacing with upstream stakeholders about business definitions and logic, you&#8217;ll need to know the data<br/>sources&#8212;what they are, how they&#8217;re used, and the business logic and definitions involved. You&#8217;ll work with the<br/>engineers in charge of these source systems and the business stakeholders who oversee the complementary<br/>products and apps. A data engineer might work alongside &#8220;the business&#8221; and technical stakeholders on a data<br/>model.<br/>The data engineer needs to be involved in designing the data model and later updates because of changes in<br/>business logic or new processes. Transformations are easy enough to do; just write a query and plop the results into<br/>a table or view. Creating them so they&#8217;re both performant and valuable to the business is another matter. Always<br/>keep the requirements and expectations of the business top of mind when transforming data.<br/>The stakeholders of the upstream systems want to make sure your queries and transformations minimally impact<br/>their systems. Ensure bidirectional communication about changes to the data models (column and index changes,<br/>for example) in source systems, as these can directly impact queries, transformations, and analytical data models.<br/>Data engineers should know about schema changes, including the addition or deletion of fields, data type changes,<br/>and anything else that might materially impact the ability to query and transform data.<br/></p>
<p><b>Downstream Stakeholders<br/></b>Transformations are where data starts providing utility to downstream stakeholders. Your downstream stakeholders<br/>include many people, including data analysts, data scientists, ML engineers, and &#8220;the business.&#8221; Collaborate with<br/>them to ensure the data model and transformations you provide are performant and useful. In terms of<br/>performance, queries should execute as quickly as possible in the most cost-effective way. What do we mean by<br/><i>useful</i>? Analysts, data scientists, and ML engineers should be able to query a data source with the confidence the<br/>data is of the highest quality and completeness, and can be integrated into their workflows and data products. The<br/>business should be able to trust that transformed data is accurate and actionable.<br/></p>
<p><b>Undercurrents<br/></b>The transformation stage is where your data mutates and morphs into something useful for the business. Because<br/>there are many moving parts, the undercurrents are especially critical at this stage.<br/></p>
<p><b>Security<br/></b>Queries and transformations combine disparate datasets into new datasets. Who has access to this new dataset? If<br/>someone does have access to a dataset, continue to control who has access to a dataset&#8217;s column, row, and cell-</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>level access.<br/>Be aware of attack vectors against your database at query time. Read/write privileges to the database must be<br/>tightly monitored and controlled. Query access to the database must be controlled in the same way as you normally<br/>control access to your organization&#8217;s systems and environments.<br/>Keep credentials hidden; avoid copying and pasting passwords, access tokens, or other credentials into code or<br/>unencrypted files. It&#8217;s shockingly common to see code in GitHub repositories with database usernames and<br/>passwords pasted directly in the codebase! It goes without saying, don&#8217;t share passwords with other users. Finally,<br/>never allow unsecured or unencrypted data to traverse the public internet.<br/></p>
<p><b>Data Management<br/></b>Though data management is essential at the source system stage (and every other stage of the data engineering<br/>lifecycle), it&#8217;s especially critical at the transformation stage. Transformation inherently creates new datasets that<br/>need to be managed. As with other stages of the data engineering lifecycle, it&#8217;s critical to involve all stakeholders<br/>in data models and transformations and manage their expectations. Also, make sure everyone agrees on naming<br/>conventions that align with the respective business definitions of the data. Proper naming conventions should be<br/>reflected in easy-to-understand field names. Users can also check in a data catalog for more clarity on what the<br/>field means when it was created, who maintains the dataset, and other relevant information.<br/>Accounting for definitional accuracy is key at the transformation stage. Does the transformation adhere to the<br/>expected business logic? Increasingly, the notion of a semantic or metrics layer that sits independent of<br/>transformations is becoming popular. Instead of enforcing business logic within the transformation at runtime, why<br/>not keep these definitions as a standalone stage before your transformation layer? While it&#8217;s still early days, expect<br/>to see semantic and metrics layers becoming more popular and commonplace in data engineering and data<br/>management.<br/>Because transformations involve mutating data, it&#8217;s critical to ensure that the data you&#8217;re using is free of defects<br/>and represents ground truth. If MDM is an option at your company, pursue its implementation. Conformed<br/>dimensions and other transformations rely on MDM to preserve data&#8217;s original integrity and ground truth. If MDM<br/>isn&#8217;t possible, work with upstream stakeholders who control the data to ensure that any data you&#8217;re transforming is<br/>correct and complies with the agreed-upon business logic.<br/>Data transformations make it potentially difficult to know how a dataset was derived along the same lines. In<br/>Chapter 6, we discussed data catalogs. As we transform data, <i>data lineage</i> tools become invaluable. Data lineage<br/>tools help both data engineers, who must understand previous transformation steps as they create new<br/>transformations, and analysts, who need to understand where data came from as they run queries and build reports.<br/>Finally, what impact does regulatory compliance have on your data model and transformations? Are sensitive<br/>fields data masked or obfuscated if necessary? Do you have the ability to delete data in response to deletion<br/>requests? Does your data lineage tracking allow you to see data derived from deleted data, and rerun<br/>transformations to remove data downstream of raw sources?<br/></p>
<p><b>DataOps<br/></b>With queries and transformations, DataOps has two areas of concern: data and systems. You need to monitor and<br/>be alerted for changes or anomalies in these areas. The field of data observability is exploding right now, with a<br/>big focus on data reliability. There&#8217;s even a recent job title called <i>data reliability engineer</i>. This section emphasizes<br/>data observability and data health, which focuses on the query and transformation stage.<br/>Let&#8217;s start with the data side of DataOps. When you query data, are the inputs and outputs correct? How do you<br/>know? If this query is saved to a table, is the schema correct? How about the shape of the data and related statistics<br/>such as min/max values, null counts, and more? You should run data-quality tests on the input datasets and the<br/>transformed dataset, which will ensure that the data meets the expectations of upstream and downstream users. If</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>there&#8217;s a data-quality issue in the transformation, you should have the ability to flag this issue, roll back the<br/>changes, and investigate the root cause.<br/>Now let&#8217;s look at the Ops part of DataOps. How are the systems performing? Monitor metrics such as query queue<br/>length, query concurrency, memory usage, storage utilization, network latency, and disk I/O. Use metric data to<br/>spot bottlenecks and poor-performing queries that might be candidates for refactoring and tuning. If the query is<br/>perfectly fine, you&#8217;ll have a good idea of where to tune the database itself (for instance, by clustering a table for<br/>faster lookup performance). Or, you may need to upgrade the database&#8217;s compute resources. Today&#8217;s cloud and<br/>SaaS databases give you a ton of flexibility for quickly upgrading (and downgrading) your system. Take a data-<br/>driven approach and use your observability metrics to pinpoint whether you have a query or a systems-related<br/>issue.<br/>The shift toward SaaS-based analytical databases changes the cost profile of data consumption. In the days of on-<br/>premises data warehouses, the system and licenses were purchased up front, with no additional usage cost.<br/>Whereas traditional data engineers would focus on performance optimization to squeeze the maximum utility out<br/>of their expensive purchases, data engineers working with cloud data warehouses that charge on a consumption<br/>basis need to focus on cost management and cost optimization. This is the practice of <i>FinOps</i> (see Chapter 4).<br/></p>
<p><b>Data Architecture<br/></b>The general rules of good data architecture in Chapter 3 apply to the transformation stage. Build robust systems<br/>that can process and transform data without imploding. Your choices for ingestion and storage will directly impact<br/>your general architecture&#8217;s ability to perform reliable queries and transformations. If the ingestion and storage are<br/>appropriate to your query and transformation patterns, you should be in a great place. On the other hand, if your<br/>queries and transformations don&#8217;t work well with your upstream systems, you&#8217;re in for a world of pain.<br/>For example, we often see data teams using the wrong data pipelines and databases for the job. A data team might<br/>connect a real-time data pipeline to an RDBMS or Elasticsearch and use this as their data warehouse. These<br/>systems are not optimized for high-volume aggregated OLAP queries and will implode under this workload. This<br/>data team clearly didn&#8217;t understand how their architectural choices would impact query performance. Take the time<br/>to understand the trade-offs inherent in your architecture choices; be clear about how your data model will work<br/>with ingestion and storage systems, and how queries will perform.<br/></p>
<p><b>Orchestration<br/></b>Data teams often manage their transformation pipelines using simple time-based schedules&#8212;e.g., cron jobs. This<br/>works reasonably well at first but turns into a nightmare as workflows grow more complicated. Use orchestration<br/>to manage complex pipelines using a dependency-based approach. Orchestration is also the glue that allows us to<br/>assemble pipelines that span multiple systems.<br/></p>
<p><b>Software Engineering<br/></b>When writing transformation code, you can use many languages such as SQL, Python, and JVM-based languages,<br/>and platforms ranging from data warehouses to distributed computing clusters, and everything in between. Each<br/>language and platform has its strengths and quirks, so you should know the best practices of your tools. For<br/>example, you might write data transformations in Python, powered by a distributed system such as Spark or Dask.<br/>When running a data transformation, are you using a UDF when a native function might work much better? We&#8217;ve<br/>seen cases where poorly written, sluggish UDFs were replaced by a built-in SQL command, with instant and<br/>dramatic improvement in performance.<br/>The rise of analytics engineering brings software engineering practices to end users, with the notion of <i>analytics as<br/>code</i>. Analytics engineering transformation tools like dbt have exploded in popularity, giving analysts and data<br/>scientists the ability to write in-database transformations using SQL, without the direct intervention of a DBA or a<br/>data engineer. In this case, the data engineer is responsible for setting up the code repository and CI/CD pipeline<br/>used by the analysts and data scientists. This is a big change in the role of a data engineer, who would historically</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>build and manage the underlying infrastructure and create the data transformations. As data tools lower the barriers<br/>to entry and become more democratized across data teams, it will be interesting to see how the workflows of data<br/>teams change.<br/>Using a GUI-based low-code tool, you&#8217;ll get useful visualizations of the transformation workflow. You still need to<br/>understand what&#8217;s going on under the hood. These GUI-based transformation tools will often generate SQL or<br/>some other language behind the scenes. While the point of a low-code tool is to alleviate the need to be involved in<br/>low-level details, understanding the code behind the scenes will help with debugging and performance<br/>optimization. Blindly assuming that the tool is generating performant code is a mistake.<br/>We suggest that data engineers pay particular attention to software engineering best practices at the query and<br/>transformation stage. While it&#8217;s tempting to simply throw more processing resources at a dataset, knowing how to<br/>write clean, performant code is a much better approach.<br/></p>
<p><b>Conclusion<br/></b>Transformations sit at the heart of data pipelines. Frankly, transformations are the &#8220;sexy&#8221; part of data engineering.<br/>It&#8217;s critical to keep in mind the purpose of transformations. Ultimately, engineers are not hired to play with the<br/>latest technological toys but to serve their customers.<br/>Our opinion is that it is possible to adopt exciting transformation technologies <i>and</i> serve stakeholders. Chapter 11<br/>talks about the <i>live data stack</i>, essentially reconfiguring the data stack around streaming data ingestion and<br/>bringing transformation workflows closer to the source system applications themselves. Engineering teams that<br/>think about real-time data as the technology for the sake of technology will repeat the mistakes of the big data era.<br/>But in reality, the majority of organizations that we work with have a business use case that would benefit from<br/>streaming data. Identifying these use cases and focusing on the value before choosing technologies and complex<br/>systems is key.<br/>As we head into the serving stage of the data engineering lifecycle in Chapter 9, reflect on technology as a tool for<br/>realizing organizational goals. If you&#8217;re a working data engineer, think about how improvements in transformation<br/>systems could help you to serve your end customers better. If you&#8217;re just embarking on a path toward data<br/>engineering, think about the kinds of business problems you&#8217;re interested in solving with technology.<br/></p>
<p><b>Additional Resources<br/></b>&#8220;How a SQL Database Engine Works,&#8221; by Dennis Pham<br/>&#8220;A Detailed Guide on SQL Query Optimization&#8221; tutorial by Megha<br/>&#8220;Modeling of Real-Time Streaming Data?&#8221; Stack Exchange thread<br/>&#8220;Eventual vs. Strong Consistency in Distributed Databases&#8221; by Saurabh.v<br/>&#8220;Caching in Snowflake Data Warehouse&#8221; Snowflake Community page<br/>Google Cloud&#8217;s &#8220;Using Cached Query Results&#8221; documentation<br/>Holistics&#8217; &#8220;Cannot Combine Fields Due to Fan-Out Issues?&#8221; FAQ page<br/>&#8220;A Simple Explanation of Symmetric Aggregates or &#8216;Why on Earth Does My SQL Look Like That?&#8217;&#8221; by<br/>Lloyd Tabb<br/>&#8220;Building a Real-Time Data Vault in Snowflake&#8221; by Dmytro Yaroshenko and Kent Graziano<br/>&#8220;Streaming Event Modeling&#8221; by Paul Stanton<br/>Oracle&#8217;s &#8220;Slowly Changing Dimensions&#8221; tutorial</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>&#8220;The New &#8216;Unified Star Schema&#8217; Paradigm in Analytics Data Modeling Review&#8221; by Andriy Zabavskyy<br/>&#8220;Data Warehouse: The Choice of Inmon vs. Kimball&#8221; by Ian Abramson<br/>&#8220;Inmon or Kimball: Which Approach Is Suitable for Your Data Warehouse?&#8221; by Sansu George<br/>ScienceDirect&#8217;s &#8220;Corporate Information Factory&#8221; web page<br/>Zentut&#8217;s &#8220;Bill Inmon Data Warehouse&#8221; web page<br/>&#8220;The Evolution of the Corporate Information Factory&#8221; by Bill Inmon<br/>&#8220;Types of Data Warehousing Architecture&#8221; by Amritha Fernando<br/>&#8220;Difference Between Kimball and Inmon&#8221; by manmeetjuneja5<br/>&#8220;Introduction to Data Warehousing&#8221;, &#8220;Introduction to Dimensional Modelling for Data Warehousing&#8221;, and<br/>&#8220;Introduction to Data Vault for Data Warehousing&#8221; by Simon Kitching<br/>Gavroshe USA&#8217;s &#8220;DW 2.0&#8221; web page<br/>US patent for &#8220;Method and Apparatus for Functional Integration of Metadata&#8221;<br/>Kimball Group&#8217;s &#8220;Four-Step Dimensional Design Process&#8221;, &#8220;Conformed Dimensions&#8221;, and &#8220;Dimensional<br/>Modeling Techniques&#8221; web pages<br/>&#8220;Kimball vs. Inmon vs. Vault&#8221; Reddit thread<br/>&#8220;How Should Organizations Structure Their Data?&#8221; by Michael Berk<br/>&#8220;Data Vault&#8212;An Overview&#8221; by John Ryan<br/>&#8220;Introduction to Data Vault Modeling&#8221; document compiled by Kent Graziano and Dan Linstedt<br/>&#8220;Data Vault 2.0 Modeling Basics&#8221; by Kent Graziano<br/><i>Building the Data Warehouse</i> (Wiley), <i>Corporate Information Factory</i>, and <i>The Unified Star Schema<br/></i>(Technics Publications) by W.H. (Bill) Inmon<br/><i>The Data Warehouse Toolkit</i> by Ralph Kimball and Margy Ross (Wiley)<br/><i>Building a Scalable Data Warehouse with Data Vault 2.0</i> (Morgan Kaufmann) by Daniel Linstedt and<br/>Michael Olschimke<br/></p>
<p>1  &#8220;Rewriting Slow-Running Queries Which Have Explosive Equi-Join Conditions and Additional Join Conditions Based on Like Operator, &#8221;<br/>Snowflake Community website, May 1, 2020, <i>https://oreil.ly/kUsO9</i>.<br/></p>
<p>2  See, for example, Emin G&#252;n Sirer, &#8220;NoSQL Meets Bitcoin and Brings Down Two Exchanges: The Story of Flexcoin and Poloniex,&#8221; Hacking,<br/>Distributed, April 6, 2014, <i>https://oreil.ly/RM3QX</i>.<br/></p>
<p>3  Some Redshift configurations rely on object storage instead.<br/>4  The authors are aware of an incident involving a new analyst at a large grocery store chain running SELECT * on a production database and<br/></p>
<p>bringing down a critical inventory database for three days.<br/>5  Figure 8-11 and the example it depicts are significantly based on &#8220;Introducing Stream&#8212;Stream Joins in Apache 2.3&#8221; by Tathagata Das and Joseph<br/></p>
<p>Torres, March 13, 2018.<br/>6  For more details on the DRY principle, see <i>The Pragmatic Programmer</i> by David Thomas and Andrew Hunt (Addison-Wesley Professional,<br/></p>
<p>2019).<br/>7  E.F. Codd, &#8220;Further Normalization of the Data Base Relational Model,&#8221; IBM Research Laboratory (1971), <i>https://oreil.ly/Muajm</i>.<br/>8  H.W. Inmon, <i>Building the Data Warehouse</i> (Hoboken: Wiley, 2005).<br/>9  Inmon, <i>Building the Data Warehouse</i>.<br/>10  Inmon, <i>Building the Data Warehouse</i>.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>11  Although dimensions and facts are often associated with Kimball, they were first used at General Mills and Dartmouth University in the 1960s,<br/>and had early adoption at Nielsen and IRI, among other companies.<br/></p>
<p>12  The data vault has two versions, 1.0 and 2.0. This section focuses on data vault 2.0, but we&#8217;ll call it <i>data vault</i> for the sake of brevity.<br/>13  Kent Graziano, &#8220;Data Vault 2.0 Modeling Basics,&#8221; Vertabelo, October 20, 2015, <i>https://oreil.ly/iuW1U</i>.<br/>14  Alex Woodie, &#8220;Lakehouses Prevent Data Swamps, Bill Inmon Says,&#8221; Datanami, June 1, 2021, <i>https://oreil.ly/XMwWc</i>.<br/>15  We remind you to use UDFs responsibly. SQL UDFs often perform reasonably well. We&#8217;ve seen JavaScript UDFs increase query time from a few<br/></p>
<p>minutes to several hours.<br/>16  Michael Blaha, &#8220;Be Careful with Derived Data,&#8221; Dataversity, December 5, 2016, <i>https://oreil.ly/garoL</i>.<br/>17  Benn Stancil, &#8220;The Missing Piece of the Modern Data Stack,&#8221; <i>benn.substack</i>, April 22, 2021, <i>https://oreil.ly/GYf3Z</i>.<br/>18  &#8220;What Is the Difference Between Apache Spark and Hadoop MapReduce?,&#8221; Knowledge Powerhouse YouTube video, May 20, 2017,<br/></p>
<p><i>https://oreil.ly/WN0eX</i>.<br/>19  For a detailed application of the concept of a streaming DAG, see &#8220;Why We Moved from Apache Kafka to Apache Pulsar&#8221; by Simba Khadder,<br/></p>
<p>StreamNative blog, April 21, 2020, <i>https://oreil.ly/Rxfko</i>.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><b>Chapter 9. Serving Data for<br/>Analytics, Machine Learning,<br/>and Reverse ETL<br/></b>Congratulations! You&#8217;ve reached the final stage of the data engineering<br/>lifecycle&#8212;serving data for downstream use cases (see Figure 9-1). In this<br/>chapter, you&#8217;ll learn about various ways to serve data for three major use<br/>cases you&#8217;ll encounter as a data engineer. First, you&#8217;ll serve data for<br/>analytics and BI. You&#8217;ll prepare data for use in statistical analysis,<br/>reporting, and dashboards. This is the most traditional area of data serving.<br/>Arguably, it predates IT and databases, but it is as important as ever for<br/>stakeholders to have visibility into the business, organizational, and<br/>financial processes.<br/></p>
<p><i>Figure 9-1. Serving delivers data for use cases<br/></i></p>
<p>Second, you&#8217;ll serve data for ML applications. ML is not possible without<br/>high-quality data, appropriately prepared. Data engineers work with data</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>scientists and ML engineers to acquire, transform, and deliver the data<br/>necessary for model training.<br/>Third, you&#8217;ll serve data through reverse ETL. <i>Reverse ETL</i> is the process of<br/>sending data back to data sources. For example, we might acquire data from<br/>an ad tech platform, run a statistical process on this data to determine cost-<br/>per-click bids, and then feed this data back into the ad tech platform.<br/>Reverse ETL is highly entangled with BI and ML.<br/>Before we get into these three major ways of serving data, let&#8217;s look at<br/>some general considerations.<br/></p>
<p><b>General Considerations for Serving Data<br/></b>Before we get further into serving data, we have a few big considerations.<br/>First and foremost is trust. People need to trust the data you&#8217;re providing.<br/>Additionally, you need to understand your use cases and users, the data<br/>products that will be produced, how you&#8217;ll be serving data (self-service or<br/>not), data definitions and logic, and data mesh. The considerations we&#8217;ll<br/>discuss here are general and apply to any of the three ways of serving data.<br/>Understanding these considerations will help you be much more effective in<br/>serving your data customers.<br/></p>
<p><b>Trust<br/></b><i>It takes 20 years to build a reputation and five minutes to ruin it. If you<br/>think about that, you&#8217;ll do things differently.<br/></i></p>
<p>&#8212;Warren Buffett<br/>Above all else, trust is the root consideration in serving data; end users need<br/>to trust the data they&#8217;re receiving. The fanciest, most sophisticated data<br/>architecture and serving layer are irrelevant if end users don&#8217;t believe the<br/>data is a reliable representation of their business. A loss of trust is often a<br/>silent death knell for a data project, even if the project isn&#8217;t officially<br/>canceled until months or years later. The job of a data engineer is to serve</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>the best data possible, so you&#8217;ll want to make sure your data products<br/>always contain high-quality and trustworthy data.<br/>As you learn to serve data throughout this chapter, we&#8217;ll reinforce the idea<br/>of baking trust into your data and discuss pragmatic ways to accomplish<br/>this. We see too many cases in which data teams are fixated on pushing out<br/>data without asking whether stakeholders trust it in the first place. Often,<br/>stakeholders lose trust in the data. Once trust is gone, earning it back is<br/>insanely difficult. This inevitably leads to the business not performing to its<br/>fullest potential with data, and data teams losing credibility (and possibly<br/>being dissolved).<br/>To realize data quality and build stakeholder trust, utilize data validation<br/>and data observability processes, in conjunction with visually inspecting<br/>and confirming validity with stakeholders. <i>Data validation</i> is analyzing data<br/>to ensure that it accurately represents financial information, customer<br/>interactions, and sales. <i>Data observability</i> provides an ongoing view of data<br/>and data processes. These processes must be applied <i>throughout the data<br/>engineering lifecycle</i> to realize a good result as we reach the end. We&#8217;ll<br/>discuss these further in &#8220;Undercurrents&#8221;.<br/>In addition to building trust in data quality, it is incumbent on engineers to<br/>build trust in their SLAs and SLOs with their end users and upstream<br/>stakeholders. Once users come to depend on data to accomplish business<br/>processes, they will require that data is consistently available and up-to-date<br/>per the commitments made by data engineers. High-quality data is of little<br/>value if it&#8217;s not available as expected when it&#8217;s time to make a critical<br/>business decision. Note, the SLAs and SLOs may also take the form of <i>data<br/>contracts</i> (see Chapter 5), formally or informally.<br/>We talked about SLAs in Chapter 5, but discussing them again here is<br/>worthwhile. SLAs come in a variety of forms. Regardless of its form, an<br/>SLA tells users what to expect from your data product; it is a contract<br/>between you and your stakeholders. An example of an SLA might be, &#8220;Data<br/>will be reliably available and of high quality.&#8221; An SLO is a key part of an<br/>SLA, and describes the ways you&#8217;ll measure performance against what</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>you&#8217;ve agreed to. For example, given the preceding example SLA, an SLO<br/>might be, &#8220;Our data pipelines to your dashboard or ML workflow will have<br/>99% uptime, with 95% of data free of defects.&#8221; Be sure expectations are<br/>clear and you have the ability to verify you&#8217;re operating within your agreed<br/>SLA and SLO parameters.<br/>It&#8217;s not enough to simply agree on an SLA. Ongoing communication is a<br/>central feature of a good SLA. Have you communicated possible issues that<br/>might affect your SLA or SLO expectations? What&#8217;s your process for<br/>remediation and improvement?<br/>Trust is everything. It takes a long time to earn, and it&#8217;s easy to lose.<br/></p>
<p><b>What&#8217;s the Use Case, and Who&#8217;s the User?<br/></b>The serving stage is about data in action. But what is a <i>productive</i> use of<br/>data? You need to consider two things to answer this question: what&#8217;s the<br/>use case, and who&#8217;s the user?<br/>The use case for data goes well beyond viewing reports and dashboards.<br/>Data is at its best when it leads to <i>action</i>. Will an executive make a strategic<br/>decision from a report? Will a user of a mobile food delivery app receive a<br/>coupon that entices them to purchase in the next two minutes? The data is<br/>often used in more than one use case&#8212;e.g., to train an ML model that does<br/>lead scoring and populates a CRM (reverse ETL). High-quality, high-<br/>impact data will inherently attract many interesting use cases. But in<br/>seeking use cases, always ask, &#8220;What <i>action</i> will this data trigger, and <i>who<br/></i>will be performing this action?,&#8221; with the appropriate follow-up question,<br/>&#8220;Can this action be automated?&#8221;<br/>Whenever possible, prioritize use cases with the highest possible ROI. Data<br/>engineers love to obsess over the technical implementation details of the<br/>systems they build while ignoring the basic question of purpose. Engineers<br/>want to do what they do best: engineer things. When engineers recognize<br/>the need to focus on value and use cases, they become much more valuable<br/>and effective in their roles.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>When starting a new data project, working backward is helpful. While it&#8217;s<br/>tempting to focus on tools, we encourage you to start with the use case and<br/>the users. Here are some questions to ask yourself as you get started:<br/></p>
<p>Who will use the data, and how will they use it?<br/>What do stakeholders expect?<br/>How can I collaborate with data stakeholders (e.g., data scientists,<br/>analysts, business users) to understand how the data I&#8217;m working with<br/>will be used?<br/></p>
<p>Again, always approach data engineering from the perspective of the user<br/>and their use case. By understanding their expectations and goals, you can<br/>work backward to create amazing data products more easily. Let&#8217;s take a<br/>moment to expand on our discussion of a data product.<br/></p>
<p><b>Data Products<br/></b><i>A good definition of a data product is a product that facilitates an end<br/>goal through the use of data.<br/></i></p>
<p>&#8212;DJ Patil<br/>Data products aren&#8217;t created in a vacuum. Like so many other<br/>organizational processes that we&#8217;ve discussed, making data products is a<br/>full-contact sport, involving a mix of product and business alongside<br/>technology. It&#8217;s important to involve key stakeholders in developing a data<br/>product. In most companies, a data engineer is a couple of steps removed<br/>from the end users of a data product; a good data engineer will seek to fully<br/>understand outcomes for direct users such as data analysts and data<br/>scientists or customers external to the company.<br/>When creating a data product, it&#8217;s useful to think of the &#8220;jobs to be done.&#8221;<br/>A user &#8220;hires&#8221; a product for a &#8220;job to be done.&#8221; This means you need to<br/>know what the user wants&#8212;i.e., their motivation for &#8220;hiring&#8221; your product.<br/>A classic engineering mistake is simply building without understanding the<br/></p>
<p>1</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>requirements, needs of the end user, or product/market fit. This disaster<br/>happens when you build data products nobody wants to use.<br/>A good data product has positive feedback loops. More usage of a data<br/>product generates more useful data, which is used to improve the data<br/>product. Rinse and repeat.<br/>When building a data product, keep these considerations in mind:<br/></p>
<p>When someone uses the data product, what do they hope to<br/>accomplish? All too often, data products are made without a clear<br/>understanding of the outcome expected by the user.<br/>Will the data product serve internal or external users? In Chapter 2, we<br/>discussed internal- and external-facing data engineering. When<br/>creating a data product, knowing whether your customer is internal or<br/>external facing will impact the way data is served.<br/>What are the outcomes and ROI of the data product you&#8217;re building?<br/></p>
<p>Building data products that people will use and love is critical. Nothing will<br/>ruin the adoption of a data product more than unwanted utility and loss of<br/>trust in the data outputs. Pay attention to the adoption and usage of data<br/>products, and be willing to adjust to make users happy.<br/></p>
<p><b>Self-Service or Not?<br/></b>How will users interface with your data product? Will a business director<br/>request a report from the data team, or can this director simply build the<br/>report? Self-service data products&#8212;giving the user the ability to build data<br/>products on their own&#8212;have been a common aspiration of data users for<br/>many years. What&#8217;s better than just giving the end user the ability to<br/>directly create reports, analyses, and ML models?<br/>Today, self-service BI and data science is still mostly aspirational. While we<br/>occasionally see companies successfully doing self-service with data, this is<br/>rare. Most of the time, attempts at self-service data begin with great<br/>intentions but ultimately fail; self-service data is tough to implement in</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>practice. Thus, the analyst or data scientist is left to perform the heavy<br/>lifting of providing ad hoc reports and maintaining dashboards.<br/>Why is self-service data so hard? The answer is nuanced, but it generally<br/>involves understanding the end user. If the user is an executive who needs<br/>to understand how the business is doing, that person probably just wants a<br/>predefined dashboard of clear and actionable metrics. The executive will<br/>likely ignore any self-serve tools for creating custom data views. If reports<br/>provoke further questions, they might have analysts at their disposal to<br/>pursue a deeper investigation. On the other hand, a user who is an analyst<br/>might already be pursuing self-service analytics via more powerful tools<br/>such as SQL. Self-service analytics through a BI layer is not useful. The<br/>same considerations apply to data science. Although granting self-service<br/>ML to &#8220;citizen data scientists&#8221; has been a goal of many automated ML<br/>vendors, adoption is still nascent for the same reasons as self-service<br/>analytics. In these two extreme cases, a self-service data product is a wrong<br/>tool for the job.<br/>Successful self-service data projects boil down to having the right audience.<br/>Identify the self-service users and the &#8220;job&#8221; they want to do. What are they<br/>trying to accomplish by using a self-service data product versus partnering<br/>with a data analyst to get the job done? A group of executives with a<br/>background in data forms an ideal audience for self-service; they likely<br/>want to slice and dice data themselves without needing to dust off their<br/>languishing SQL skills. Business leaders willing to invest the time to learn<br/>data skills through a company initiative and training program could also<br/>realize significant value from self-service.<br/>Determine how you will provide data to this group. What are their time<br/>requirements for new data? What happens if they inevitably want more data<br/>or change the scope of what&#8217;s required from self-service? More data often<br/>means more questions, which requires more data. You&#8217;ll need to anticipate<br/>the growing needs of your self-service users. You also need to understand<br/>the fine balance between flexibility and guardrails that will help your<br/>audience find value and insights without incorrect results and confusion.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><b>Data Definitions and Logic<br/></b>As we&#8217;ve emphatically discussed, the utility of data in an organization is<br/>ultimately derived from its correctness and trustworthiness. Critically, the<br/>correctness of data goes beyond faithful reproduction of event values from<br/>source systems. Data correctness also encompasses proper data definitions<br/>and logic; these must be baked into data through all lifecycle stages, from<br/>source systems to data pipelines to BI tools and much more.<br/><i>Data definition</i> refers to the meaning of data as it is understood throughout<br/>the organization. For example, <i>customer</i> has a precise meaning within a<br/>company and across departments. When the definition of a customer varies,<br/>these must be documented and made available to everyone who uses the<br/>data.<br/><i>Data logic</i> stipulates formulas for deriving metrics from data&#8212;say, gross<br/>sales or customer lifetime value. Proper data logic must encode data<br/>definitions and details of statistical calculations. To compute customer<br/>churn metrics, we would need a definition: who is a customer? To calculate<br/>net profits, we would need a set of logical rules to determine which<br/>expenses to deduct from gross revenue.<br/>Frequently, we see data definitions and logic taken for granted, often passed<br/>around the organization in the form of tribal knowledge. <i>Tribal knowledge<br/></i>takes on a life of its own, often at the expense of anecdotes replacing data-<br/>driven insights, decisions, and actions. Instead, formally declaring data<br/>definitions and logic both in a data catalog and within the systems of the<br/>data engineering lifecycle goes a long way to ensuring data correctness,<br/>consistency, and trustworthiness.<br/>Data definitions can be served in many ways, sometimes explicitly, but<br/>mostly implicitly. By <i>implicit</i>, we mean that anytime you serve data for a<br/>query, a dashboard, or an ML model, the data and derived metrics are<br/>presented consistently and correctly. When you write a SQL query, you&#8217;re<br/>implicitly assuming that the inputs to this query are correct, including<br/>upstream pipeline logic and definitions. This is where data modeling</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>(described in Chapter 8) is incredibly useful to capture data definitions and<br/>logic in a way that&#8217;s understandable and usable by multiple end users.<br/>Using a semantic layer, you consolidate business definitions and logic in a<br/>reusable fashion. Write once, use anywhere. This paradigm is an object-<br/>oriented approach to metrics, calculations, and logic. We&#8217;ll have more to<br/>say in &#8220;Semantic and Metrics Layers&#8221;.<br/></p>
<p><b>Data Mesh<br/></b>Data mesh will increasingly be a consideration when serving data. Data<br/>mesh fundamentally changes the way data is served within an organization.<br/>Instead of siloed data teams serving their internal constituents, every<br/>domain team takes on two aspects of decentralized, peer-to-peer data<br/>serving.<br/>First, teams are responsible for serving data <i>to other teams</i> by preparing it<br/>for consumption. Data must be good for use in data apps, dashboards,<br/>analytics, and BI tools across the organization. Second, each team<br/>potentially runs its dashboards and analytics for <i>self-service</i>. Teams<br/>consume data from across the organization based on the particular needs in<br/>their domain. Data consumed from other teams may also make its way into<br/>the software designed by a domain team through user-facing analytics or an<br/>ML feature.<br/>This dramatically changes the details and structure of serving. We<br/>introduced the concept of a data mesh in Chapter 3. Now that we&#8217;ve<br/>covered some general considerations for serving data, let&#8217;s look at the first<br/>major area: analytics.<br/></p>
<p><b>Analytics<br/></b>The first data serving use case you&#8217;ll likely encounter is <i>analytics</i>, which is<br/>discovering, exploring, identifying, and making visible key insights and<br/>patterns within data. Analytics has many aspects. As a practice, analytics is<br/>carried out using statistical methods, reporting, BI tools, and more. As a</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>data engineer, knowing the various types and techniques of analytics is key<br/>to accomplishing your work. This section aims to show how you&#8217;ll serve<br/>data for analytics and presents some points to think about to help your<br/>analysts succeed.<br/>Before you even serve data for analytics, the first thing you need to do<br/>(which should sound familiar after reading the preceding section) is identify<br/>the end use case. Is the user looking at historical trends? Should users be<br/>immediately and automatically notified of an anomaly, such as a fraud<br/>alert? Is someone consuming a real-time dashboard on a mobile<br/>application? These examples highlight the differences between business<br/>analytics (usually BI), operational analytics, and user-facing analytics. Each<br/>of these analytics categories has different goals and unique serving<br/>requirements. Let&#8217;s look at how you&#8217;ll serve data for these types of<br/>analytics.<br/></p>
<p><b>Business Analytics<br/></b><i>Business analytics</i> uses historical and current data to make strategic and<br/>actionable decisions. The types of decisions tend to factor in longer-term<br/>trends, and often involve a mix of statistical and trend analysis, alongside<br/>domain expertise and human judgment. Business analysis is as much an art<br/>as it is a science.<br/>Business analytics typically falls into a few big areas&#8212;dashboards, reports,<br/>and ad hoc analysis. A business analyst might focus on one or all of these<br/>categories. Let&#8217;s quickly look at the differences between these practices and<br/>related tools. Understanding an analyst&#8217;s workflow will help you, the data<br/>engineer, understand how to serve data.<br/>A <i>dashboard</i> concisely shows decision makers how an organization is<br/>performing against a handful of core metrics, such as sales and customer<br/>retention. These core metrics are presented as visualizations (e.g., charts or<br/>heatmaps), summary statistics, or even a single number. This is similar to a<br/>car dashboard, which gives you a single readout of the critical things you<br/>need to know while driving a vehicle. An organization may have more than</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>one dashboard, with C-level executives using an overarching dashboard,<br/>and their direct reports using dashboards with their particular metrics, KPIs,<br/>or objectives and key results (OKRs). Analysts help create and maintain<br/>these dashboards. Once business stakeholders embrace and rely on a<br/>dashboard, the analyst usually responds to requests to look into a potential<br/>issue with a metric or add a new metric to the dashboard. Currently, you<br/>might use BI platforms to create dashboards, such as Tableau, Looker,<br/>Sisense, Power BI, or Apache Superset/Preset.<br/>Analysts are often tasked by business stakeholders with creating a <i>report</i>.<br/>The goal of a report is to use data to drive insights and action. An analyst<br/>working at an online retail company is asked to investigate which factors<br/>are driving a higher-than-expected rate of returns for women&#8217;s running<br/>shorts. The analyst runs some SQL queries in the data warehouse,<br/>aggregates the return codes that customers provide as the reason for their<br/>return, and discovers that the fabric in the running shorts is of inferior<br/>quality, often wearing out within a few uses. Stakeholders such as<br/>manufacturing and quality control are notified of these findings.<br/>Furthermore, the findings are summarized in a report and distributed in the<br/>same BI tool where the dashboard resides.<br/>The analyst was asked to dig into a potential issue and come back with<br/>insights. This represents an example of <i>ad hoc analysis</i>. Reports typically<br/>start as ad hoc requests. If the results of the ad hoc analysis are impactful,<br/>they often end up in a report or dashboard. The technologies used for<br/>reports and ad hoc analysis are similar to dashboards but may include<br/>Excel, Python, R-based notebooks, SQL queries, and much more.<br/>Good analysts constantly engage with the business and dive into the data to<br/>answer questions and uncover hidden and counterintuitive trends and<br/>insights. They also work with data engineers to provide feedback on data<br/>quality, reliability issues, and requests for new datasets. The data engineer<br/>is responsible for addressing this feedback and providing new datasets for<br/>the analyst to use.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Returning to the running shorts example, suppose that after communicating<br/>their findings, analysts learn that manufacturing can provide them with<br/>various supply-chain details regarding the materials used in the running<br/>shorts. Data engineers undertake a project to ingest this data into the data<br/>warehouse. Once the supply-chain data is present, analysts can correlate<br/>specific garment serial numbers with the supplier of the fabric used in the<br/>item. They discover that most failures are tied to one of their three<br/>suppliers, and the factory stops using fabric from this supplier.<br/>The data for business analytics is frequently served in batch mode from a<br/>data warehouse or a data lake. This varies wildly across companies,<br/>departments, and even data teams within companies. New data might be<br/>available every second, minute, every 30 minutes, every day, or once a<br/>week. The frequency of the batches can vary for several reasons. One key<br/>thing to note is that engineers working on analytics problems should<br/>consider various potential applications of data, current, and future. It is<br/>common to have mixed data update frequencies to serve use cases<br/>appropriately but remember that the frequency of ingestion sets a ceiling on<br/>downstream frequency. If streaming applications exist for the data, it should<br/>be ingested as a stream even if some downstream processing and serving<br/>steps are handled in batches.<br/>Of course, data engineers must address various backend technical<br/>considerations in serving business analytics. Some BI tools store data in an<br/>internal storage layer. Other tools run queries on your data lake or data<br/>warehouse. This is advantageous because you can take full advantage of<br/>your OLAP database&#8217;s power. As we&#8217;ve discussed in earlier chapters, the<br/>downside is cost, access control, and latency.<br/></p>
<p><b>Operational Analytics<br/></b>If business analytics is about using data to discover actionable insights, then<br/>operational analytics uses data to take <i>immediate action</i>:<br/></p>
<p>Operational analytics versus business analytics = immediate action<br/>versus actionable insights</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>The big difference between operational and business analytics is <i>time</i>. Data<br/>used in business analytics takes a longer view of the question under<br/>consideration. Up-to-the-second updates are nice to know but won&#8217;t<br/>materially impact the quality or outcome. Operational analytics is quite the<br/>opposite, as real-time updates can be impactful in addressing a problem<br/>when it occurs.<br/>An example of operational analytics is real-time application monitoring.<br/>Many software engineering teams want to know how their application is<br/>performing; if issues arise, they want to be notified immediately. The<br/>engineering team might have a dashboard (see, e.g., Figure 9-2) that shows<br/>the key metrics such as requests per second, database I/O, or whatever<br/>metrics are important. Certain conditions can trigger scaling events, adding<br/>more capacity if servers are overloaded. If certain thresholds are breached,<br/>the monitoring system might also send alerts via text message, group chat,<br/>and email.<br/></p>
<p><i>Figure 9-2. An operational analytics dashboard showing some key metrics from Google Compute<br/>Engine</i></p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><b>BUSINESS AND OPERATIONAL ANALYTICS<br/></b>The line between business and operational analytics has begun to blur.<br/>As streaming and low-latency data become more pervasive, it is only<br/>natural to apply operational approaches to business analytics problems;<br/>in addition to monitoring website performance on Black Friday, an<br/>online retailer could also analyze and present sales, revenue and the<br/>impact of advertising campaigns in real time.<br/>The data architectures will change to fit into a world where you can<br/>have both your red hot and warm data in one place. The central question<br/>you should always ask yourself, and your stakeholders, is this: if you<br/>have streaming data, what are you going to do with it? What action<br/>should you take? Correct action creates impact and value. Real-time<br/>data without action is an unrelenting distraction.<br/>In the long term, we predict that streaming will supplant batch. Data<br/>products over the next 10 years will likely be streaming-first, with the<br/>ability to seamlessly blend historical data. After real-time collection,<br/>data can still be consumed and processed in batches as required.<br/></p>
<p>Let&#8217;s return once again to our running shorts example. Using analytics to<br/>discover bad fabric in the supply chain was a huge success; business leaders<br/>and data engineers want to find more opportunities to utilize data to<br/>improve product quality. The data engineers suggest deploying real-time<br/>analytics at the factory. The plant already uses a variety of machines<br/>capable of streaming real-time data. In addition, the plant has cameras<br/>recording video on the manufacturing line. Right now, technicians watch<br/>the footage in real time, look for defective items, and alert those running the<br/>line when they see a high rate of snags appearing in items.<br/>Data engineers realize that they can use an off-the-shelf cloud machine<br/>vision tool to identify defects in real time automatically. Defect data is tied<br/>to specific item serial numbers and streamed. From here, a real-time<br/>analytics process can tie defective items to streaming events from machines<br/>further up the assembly line.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Using this approach, factory floor analysts discover that the quality of raw<br/>fabric stock varies significantly from box to box. When the monitoring<br/>system shows a high rate of snag defects, line workers can remove the<br/>defective box and charge it back to the supplier.<br/>Seeing the success of this quality improvement project, the supplier decides<br/>to adopt similar quality-control processes. Data engineers from the retailer<br/>work with the supplier to deploy their real-time data analytics, dramatically<br/>improving the quality of their fabric stock.<br/></p>
<p><b>Embedded Analytics<br/></b>Whereas business and operational analytics are internally focused, a recent<br/>trend is external- or user-facing analytics. With so much data powering<br/>applications, companies increasingly provide analytics to end users. These<br/>are typically referred to as <i>data applications</i>, often with analytics<br/>dashboards embedded within the application itself. Also known as<br/><i>embedded analytics</i>, these end-user-facing dashboards give users key<br/>metrics about their relationship with the application.<br/>A smart thermostat has a mobile application that shows the temperature in<br/>real time and up-to-date power consumption metrics, allowing the user to<br/>create a better energy-efficient heating or cooling schedule. In another<br/>example, a third-party ecommerce platform provides its sellers a real-time<br/>dashboard on sales, inventory, and returns. The seller has the option to use<br/>this information to offer deals to customers in near real time. In both cases,<br/>an application allows users to make real-time decisions (manually or<br/>automatically) based on data.<br/>The landscape of embedded analytics is snowballing, and we expect that<br/>such data applications will become increasingly pervasive within the next<br/>few years. As a data engineer, you&#8217;re probably not creating the embedded<br/>analytics frontend, as the application developers handle that. Since you&#8217;re<br/>responsible for the databases serving the embedded analytics, you&#8217;ll need to<br/>understand the speed and latency requirements for user-facing analytics.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Performance for embedded analytics encompasses three problems. First,<br/>app users are not as tolerant of infrequent batch processing as internal<br/>company analysts; users of a recruiting SaaS platform may expect to see a<br/>change in their statistics as soon as they upload a new resume. Users want<br/>low <i>data latency</i>. Second, users of data apps expect fast <i>query performance</i>.<br/>When they adjust parameters in an analytics dashboard, they want to see<br/>refreshed results appear in seconds. Third, data apps must often support<br/>extremely high query rates across many dashboards and numerous<br/>customers. High <i>concurrency</i> is critical.<br/>Google and other early major players in the data apps space developed<br/>exotic technologies to cope with these challenges. For new startups, the<br/>default is to use conventional transactional databases for data applications.<br/>As their customer bases expand, they outgrow their initial architecture.<br/>They have access to a new generation of databases that combine high<br/>performance&#8212;fast queries, high concurrency, and near real-time updates&#8212;<br/>with relative ease of use (e.g., SQL-based analytics).<br/></p>
<p><b>Machine Learning<br/></b>The second major area for serving data is machine learning. ML is<br/>increasingly common, so we&#8217;ll assume you&#8217;re at least familiar with the<br/>concept. With the rise of ML engineering (itself almost a parallel universe<br/>to data engineering), you might ask yourself where a data engineer fits into<br/>the picture.<br/>Admittedly, the boundary between ML, data science, data engineering, and<br/>ML engineering is increasingly fuzzy, and this boundary varies dramatically<br/>between organizations. In some organizations, ML engineers take over data<br/>processing for ML applications right after data collection or may even form<br/>an entirely separate and parallel data organization that handles the entire<br/>lifecycle for all ML applications. Data engineers handle all data processing<br/>in other settings and then hand off data to ML engineers for model training.<br/>Data engineers may even handle some extremely ML-specific tasks, such<br/>featurization of data.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Let&#8217;s return to our example of quality for control of running shorts<br/>produced by an online retailer. Suppose that streaming data has been<br/>implemented in the factory that makes the raw fabric stock for the shorts.<br/>Data scientists discovered that the quality of the manufactured fabric is<br/>susceptible to characteristics of the input raw polyester, temperature,<br/>humidity, and various tunable parameters of the loom that weaves the<br/>fabric. Data scientists develop a basic model to optimize loom parameters.<br/>ML engineers automate model training and set up a process to<br/>automatically tune the loom based on input parameters. Data and ML<br/>engineers work together to design a featurization pipeline, and data<br/>engineers implement and maintain the pipeline.<br/></p>
<p><b>What a Data Engineer Should Know About ML<br/></b>Before we discuss serving data for ML, you may ask yourself how much<br/>ML you need to know as a data engineer. ML is an incredibly vast topic,<br/>and we won&#8217;t attempt to teach you the field; countless books and courses<br/>are available to learn ML.<br/>While a data engineer doesn&#8217;t need to have a deep understanding of ML, it<br/>helps tremendously to know the basics of how classical ML works and the<br/>fundamentals of deep learning. Knowing the basics of ML will go a long<br/>way in helping you work alongside data scientists in building data products.<br/>Here are some areas of ML that we think a data engineer should be familiar<br/>with:<br/></p>
<p>The difference between supervised, unsupervised, and semisupervised<br/>learning.<br/>The difference between classification and regression techniques.<br/>The various techniques for handling time-series data. This includes<br/>time-series analysis, as well as time-series forecasting.<br/>When to use the &#8220;classical&#8221; techniques (logistic regression, tree-based<br/>learning, support vector machines) versus deep learning. We constantly</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>see data scientists immediately jump to deep learning when it&#8217;s<br/>overkill. As a data engineer, your basic knowledge of ML can help you<br/>spot whether an ML technique is appropriate and scales the data you&#8217;ll<br/>need to provide.<br/>When would you use automated machine learning (AutoML) versus<br/>handcrafting an ML model? What are the trade-offs with each<br/>approach regarding the data being used?<br/>What are data-wrangling techniques used for structured and<br/>unstructured data?<br/>All data that is used for ML is converted to numbers. If you&#8217;re serving<br/>structured or semistructured data, ensure that the data can be properly<br/>converted during the feature-engineering process.<br/>How to encode categorical data and the embeddings for various types<br/>of data.<br/>The difference between batch and online learning. Which approach is<br/>appropriate for your use case?<br/>How does the data engineering lifecycle intersect with the ML<br/>lifecycle at your company? Will you be responsible for interfacing<br/>with or supporting ML technologies such as feature stores or ML<br/>observability?<br/>Know when it&#8217;s appropriate to train locally, on a cluster, or at the edge.<br/>When would you use a GPU over a CPU? The type of hardware you<br/>use largely depends on the type of ML problem you&#8217;re solving, the<br/>technique you&#8217;re using, and the size of your dataset.<br/>Know the difference between the applications of batch and streaming<br/>data in training ML models. For example, batch data often fits well<br/>with offline model training, while streaming data works with online<br/>training.<br/>What are data cascades, and how might they impact ML models?</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Are results returned in real time or in batch? For example, a batch<br/>speech transcription model might process speech samples and return<br/>text in batch after an API call. A product recommendation model<br/>might need to operate in real time as the customer interacts with an<br/>online retail site.<br/>The use of structured versus unstructured data. We might cluster<br/>tabular (structured) customer data or recognize images (unstructured)<br/>by using a neural net.<br/></p>
<p>ML is a <i>vast</i> subject area, and this book won&#8217;t teach you these topics, or<br/>even ML generalities. If you&#8217;d like to learn more about ML, we suggest<br/>reading <i>Hands on Machine Learning with Scikit-Learn, Keras, and<br/>TensorFlow</i> by Aur&#233;lien G&#233;ron (O&#8217;Reilly); countless other ML courses and<br/>books are available online. Because the books and online courses evolve so<br/>rapidly, do your research on what seems like a good fit for you.<br/></p>
<p><b>Ways to Serve Data for Analytics and ML<br/></b>As with analytics, data engineers provide data scientists and ML engineers<br/>with the data they need to do their jobs. We have placed serving for ML<br/>alongside analytics because the pipelines and processes are extremely<br/>similar. There are many ways to serve data for analytics and ML. Some<br/>common ways to serve this data include files, databases, query engines, and<br/>data sharing. Let&#8217;s briefly look at each.<br/></p>
<p><b>File Exchange<br/></b>File exchange is ubiquitous in data serving. We process data and generate<br/>files to pass to data consumers.<br/>Keep in mind that a file might be used for many purposes. A data scientist<br/>might load a text file (unstructured data) of customer messages to analyze<br/>the sentiments of customer complaints. A business unit might receive<br/>invoice data from a partner company as a collection of CSVs (structured<br/>data), and an analyst must perform some statistical analysis on these files.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Or, a data vendor might provide an online retailer with images of products<br/>on a competitor&#8217;s website (unstructured data) for automated classification<br/>using computer vision.<br/>The way you serve files depends on several factors, such as these:<br/></p>
<p>Use case&#8212;business analytics, operational analytics, user-facing<br/>analytics<br/>The data consumer&#8217;s data-handling processes<br/>The size and number of individual files in storage<br/>Who is accessing this file<br/>Data type&#8212;structured, semistructured, or unstructured<br/></p>
<p>The second bullet point is one of the main considerations. It is often<br/>necessary to serve data through files rather than data sharing because the<br/>data consumer cannot use a sharing platform.<br/>The simplest file to serve is something along the lines of emailing a single<br/>Excel file. This is still a common workflow even in an era when files can be<br/>collaboratively shared. The problem with emailing files is each recipient<br/>gets their version of the file. If a recipient edits the file, these edits are<br/>specific to that user&#8217;s file. Deviations among files inevitably result. If you<br/>need a coherent, consistent version of a file, we suggest using a<br/>collaboration platform such as Microsoft 365 or Google Docs.<br/>Of course, serving single files is hard to scale, and your needs will<br/>eventually outgrow simple cloud file storage. You&#8217;ll likely grow into an<br/>object storage bucket if you have a handful of large files, or a data lake if<br/>you have a steady supply of files. Object storage can store any type of blob<br/>file, and is especially useful for semistructured or unstructured files.<br/>We&#8217;ll note that we generally consider file exchange through object storage<br/>(data lake) to land under &#8220;data sharing&#8221; rather than file exchange since the<br/>process can be significantly more scalable and streamlined than ad hoc file<br/>exchange.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><b>Databases<br/></b>Databases are a critical layer in serving data for analytics and ML. For this<br/>discussion, we&#8217;ll implicitly keep our focus on serving data from OLAP<br/>databases (e.g., data warehouses and data lakes). In the previous chapter,<br/>you learned about querying databases. Serving data involves querying a<br/>database, and then consuming those results for a use case. An analyst or<br/>data scientist might query a database by using a SQL editor and export<br/>those results to a CSV file for consumption by a downstream application, or<br/>analyze the results in a notebook (described in &#8220;Serving Data in<br/>Notebooks&#8221;).<br/>Serving data from a database carries a variety of benefits. A database<br/>imposes order and structure on the data through schema; databases can offer<br/>fine-grained permission controls at the table, column, and row level,<br/>allowing database administrators to craft complex access policies for<br/>various roles; and, databases can offer high serving performance for large,<br/>computationally intensive queries, high query concurrency, or both.<br/>BI systems usually share the data processing workload with a source<br/>database, but the boundary between processing in the two systems varies.<br/>For example, a Tableau server runs an initial query to pull data from a<br/>database and stores it locally. Basic OLAP/BI slicing and dicing (interactive<br/>filtering and aggregation) runs directly on the server from the local data<br/>copy. On the other hand, Looker relies on a computational model called<br/><i>query pushdown</i>; Looker encodes data processing logic in a specialized<br/>language (LookML), combines this with dynamic user input to generate<br/>SQL queries, runs these against the source database, and presents the<br/>output. (See &#8220;Semantic and Metrics Layers&#8221;.) Both Tableau and Looker<br/>have various configuration options for caching results to reduce the<br/>processing burden for frequently run queries.<br/>A data scientist might connect to a database, extract data, and perform<br/>feature engineering and selection. This converted dataset is then fed into an<br/>ML model; the offline model is trained and produces predictive results.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Data engineers are quite often tasked with managing the database serving<br/>layer. This includes management of performance and costs. In databases<br/>that separate compute and storage, this is a somewhat more subtle<br/>optimization problem than in the days of fixed on-premises infrastructure.<br/>For example, it is now possible to spin up a new Spark cluster or Snowflake<br/>warehouse for each analytical or ML workload. It is generally<br/>recommended to at least split out clusters by major use cases, such as ETL<br/>and serving for analytics and data science. Often data teams choose to slice<br/>more finely, assigning one warehouse per major area. This makes it possible<br/>for different teams to budget for their query costs under the supervision of a<br/>data engineering team.<br/>Also, recall the three performance considerations that we discussed in<br/>&#8220;Embedded Analytics&#8221;. These are data latency, query performance, and<br/>concurrency. A system that can ingest directly from a stream can lower data<br/>latency. And many database architectures rely on SSD or memory caching<br/>to enhance query performance and concurrency to serve the challenging use<br/>cases inherent in user-facing analytics.<br/>Increasingly, data platforms like Snowflake and Databricks allow analysts<br/>and data scientists to operate under a single environment, providing SQL<br/>editors and data science notebooks under one roof. Because compute and<br/>storage are separated, the analysts and data scientists can consume the<br/>underlying data in various ways without interfering with each other. This<br/>will allow high throughput and faster delivery of data products to<br/>stakeholders.<br/></p>
<p><b>Streaming Systems<br/></b>Streaming analytics are increasingly important in the realm of serving. At a<br/>high level, understand that this type of serving may involve <i>emitted metrics</i>,<br/>which are different from traditional queries.<br/>Also, we see operational analytics databases playing a growing role in this<br/>area (see &#8220;Operational Analytics&#8221;). These databases allow queries to run<br/>across a large range of historical data, encompassing up-to-the-second</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>current data. Essentially, they combine aspects of OLAP databases with<br/>stream-processing systems. Increasingly, you&#8217;ll work with streaming<br/>systems to serve data for analytics and ML, so get familiar with this<br/>paradigm.<br/>You&#8217;ve learned about streaming systems throughout the book. For an idea<br/>of where it&#8217;s going, read about the live data stack in Chapter 11.<br/></p>
<p><b>Query Federation<br/></b>As you learned in Chapter 8, query federation pulls data from multiple<br/>sources, such as data lakes, RDBMSs, and data warehouses. Federation is<br/>becoming more popular as distributed query virtualization engines gain<br/>recognition as ways to serve queries without going through the trouble of<br/>centralizing data in an OLAP system. Today, you can find OSS options like<br/>Trino and Presto and managed services such as Starburst. Some of these<br/>offerings describe themselves as ways to enable the data mesh; time will tell<br/>how that unfolds.<br/>When serving data for federated queries, you should be aware that the end<br/>user might be querying several systems&#8212;OLTP, OLAP, APIs, filesystems,<br/>etc. (Figure 9-3). Instead of serving data from a single system, you&#8217;re now<br/>serving data from multiple systems, each with its usage patterns, quirks, and<br/>nuances. This poses challenges for serving data. If federated queries touch<br/>live production source systems, you must ensure that the federated query<br/>won&#8217;t consume excessive resources in the source.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><i>Figure 9-3. A federated query with three data sources<br/></i></p>
<p>In our experience, federated queries are ideally suited when you want<br/>flexibility in analyzing data or the source data needs to be tightly controlled.<br/>Federation allows ad hoc queries for performing exploratory analysis,<br/>blending data from various systems without the complexity of setting up<br/>data pipelines or ETL. This will allow you to determine whether the<br/>performance of a federated query is sufficient for ongoing purposes or you<br/>need to set up ingestion on some or all data sources and centralize the data<br/>in an OLAP database or data lake.<br/>Federated queries also provide read-only access to source systems, which is<br/>great when you don&#8217;t want to serve files, database access, or data dumps.<br/>The end user reads only the version of the data they&#8217;re supposed to access<br/>and nothing more. Query federation is a great option to explore for<br/>situations where access and compliance are critical.<br/></p>
<p><b>Data Sharing<br/></b>Chapter 5 includes an extensive discussion of data sharing. Any data<br/>exchange between organizations or units within a larger organization can be<br/>viewed as data sharing. Still, we mean specifically sharing through</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>massively multitenant storage systems in a cloud environment. Data sharing<br/>generally turns data serving into a security and access control problem.<br/>The actual queries are now handled by the data consumers (analysts and<br/>data scientists) rather than the engineers sourcing the data. Whether serving<br/>data in a data mesh within an organization, providing data to the public, or<br/>serving to partner businesses, data sharing is a compelling serving model.<br/>Data sharing is increasingly a core feature of major data platforms like<br/>Snowflake, Redshift, and BigQuery allowing companies to share data safely<br/>and securely with each other.<br/></p>
<p><b>Semantic and Metrics Layers<br/></b>When data engineers think about serving, they naturally tend to gravitate<br/>toward the data processing and storage technologies&#8212;i.e., will you use<br/>Spark or a cloud data warehouse? Is your data stored in object storage or<br/>cached in a fleet of SSDs? But powerful processing engines that deliver<br/>quick query results across vast datasets don&#8217;t inherently make for quality<br/>business analytics. When fed poor-quality data or poor-quality queries,<br/>powerful query engines quickly return bad results.<br/>Where data quality focuses on characteristics of the data itself and various<br/>techniques to filter or improve bad data, query quality is a question of<br/>building a query with appropriate logic that returns accurate answers to<br/>business questions. Writing high-quality ETL queries and reporting is time-<br/>intensive, detailed work. Various tools can help automate this process while<br/>facilitating consistency, maintenance, and continuous improvement.<br/>Fundamentally, a <i>metrics layer</i> is a tool for maintaining and computing<br/>business logic.  (A <i>semantic layer</i> is extremely similar conceptually,  and<br/><i>headless BI</i> is another closely related term.) This layer can live in a BI tool<br/>or in software that builds transformation queries. Two concrete examples<br/>are Looker and Data Build Tool (dbt).<br/>For instance, Looker&#8217;s LookML allows users to define virtual, complex<br/>business logic. Reports and dashboards point to specific LookML for<br/>computing metrics. Looker allows users to define standard metrics and<br/></p>
<p>2 3</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>reference them in many downstream queries; this is meant to solve the<br/>traditional problem of repetition and inconsistency in traditional ETL<br/>scripts. Looker uses LookML to generate SQL queries, which are pushed<br/>down to the database. Results can be persisted in the Looker server or in the<br/>database itself for large result sets.<br/>dbt allows users to define complex SQL data flows encompassing many<br/>queries and standard definitions of business metrics, much like Looker.<br/>Unlike Looker, dbt runs exclusively in the transform layer, although this<br/>can include pushing queries into views that are computed at query time.<br/>Whereas Looker focuses on serving queries and reporting, dbt can serve as<br/>a robust data pipeline orchestration tool for analytics engineers.<br/>We believe that metrics layer tools will grow more popular with wider<br/>adoption and more entrants, as well as move upstream toward the<br/>application. Metrics layer tools help solve a central question in analytics<br/>that has plagued organizations since people have analyzed data: &#8220;Are these<br/>numbers correct?&#8221; Many new entrants are in the space beside the ones<br/>we&#8217;ve mentioned.<br/></p>
<p><b>Serving Data in Notebooks<br/></b>Data scientists often use notebooks in their day-to-day work. Whether it&#8217;s<br/>exploring data, engineering features, or training a model, the data scientist<br/>will likely use a notebook. At this writing, the most popular notebook<br/>platform is Jupyter Notebook, along with its next-generation iteration,<br/>JupyterLab. Jupyter is open source and can be hosted locally on a laptop, on<br/>a server, or through various cloud-managed services. <i>Jupyter</i> stands for<br/><i>Julia, Python, and R</i> &#8212;the latter two are popular for data science<br/>applications, especially notebooks. Regardless of the language used, the<br/>first thing you&#8217;ll need to consider is how data can be accessed from a<br/>notebook.<br/>Data scientists will programmatically connect to a data source, such as an<br/>API, a database, a data warehouse, or a data lake (Figure 9-4). In a<br/>notebook, all connections are created using the appropriate built-in or</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>imported libraries to load a file from a filepath, connect to an API endpoint,<br/>or make an ODBC connection to a database. A remote connection may<br/>require the correct credentials and privileges to establish a connection. Once<br/>connected, a user may need the correct access to tables (and rows/columns)<br/>or files stored in object storage. The data engineer will often assist the data<br/>scientist in finding the right data, and then ensure that they have the right<br/>permissions to access the rows and columns required.<br/></p>
<p><i>Figure 9-4. A notebook can be served data from many sources, such as object storage, a database,<br/>data warehouse, or data lake</i></p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><b>CREDENTIAL HANDLING<br/></b>Incorrectly handled credentials in notebooks and data science code are a<br/>major security risk; we constantly see credentials mishandled in this<br/>domain. It is common to embed credentials directly in code, where they<br/>often leak into version control repos. Credentials are also frequently<br/>passed around through messages and email.<br/>We encourage data engineers to audit data science security practices<br/>and work collaboratively on improvements. Data scientists are highly<br/>receptive to these conversations if they are given alternatives. Data<br/>engineers should set standards for handling credentials. Credentials<br/>should never be embedded in code; ideally, data scientists use credential<br/>managers or CLI tools to manage access.<br/></p>
<p>Let&#8217;s look at an incredibly common workflow for data scientists: running a<br/>local notebook and loading data into a pandas dataframe. <i>Pandas</i> is a<br/>prevalent Python library used for data manipulation and analysis and is<br/>commonly used to load data (say, a CSV file) into a Jupyter notebook.<br/>When pandas loads a dataset, it stores this dataset in memory.<br/>What happens when the dataset size exceeds the local machine&#8217;s available<br/>memory? This inevitably happens given the limited memory of laptops and<br/>workstations: it stops a data science project dead in its tracks. It&#8217;s time to<br/>consider more scalable options. First, move to a cloud-based notebook<br/>where the underlying storage and memory for the notebook can be flexibly<br/>scaled. Upon outgrowing this option, look at distributed execution systems;<br/>popular Python-based options include Dask, Ray, and Spark. If a full-<br/>fledged cloud-managed offering seems appealing, consider setting up a data<br/>science workflow using Amazon SageMaker, Google Cloud Vertex AI, or<br/>Microsoft Azure Machine Learning. Finally, open source end-to-end ML<br/>workflow options such as Kubeflow and MLflow make it easy to scale ML<br/>workloads in Kubernetes and Spark, respectively. The point is to get data<br/>scientists off their laptops and take advantage of the cloud&#8217;s power and<br/>scalability.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Data engineers and ML engineers play a key role in facilitating the move to<br/>scalable cloud infrastructure. The exact division of labor depends a great<br/>deal on the details of your organization. They should take the lead in setting<br/>up cloud infrastructure, overseeing the management of environments, and<br/>training data scientists on cloud-based tools.<br/>Cloud environments require significant operational work, such as managing<br/>versions and updates, controlling access, and maintaining SLAs. As with<br/>other operational work, a significant payoff can result when &#8220;data science<br/>ops&#8221; are done well.<br/>Notebooks may even become a part of production data science; notebooks<br/>are widely deployed at Netflix. This is an interesting approach with<br/>advantages and trade-offs. Productionized notebooks allow data scientists to<br/>get their work into production much faster, but they are also inherently a<br/>substandard form of production. The alternative is to have ML and data<br/>engineers convert notebooks for production use, placing a significant<br/>burden on these teams. A hybrid of these approaches may be ideal, with<br/>notebooks used for &#8220;light&#8221; production and a full productionization process<br/>for high-value projects.<br/></p>
<p><b>Reverse ETL<br/></b>Today, <i>reverse ETL</i> is a buzzword that describes serving data by loading it<br/>from an OLAP database back into a source system. That said, any data<br/>engineer who&#8217;s worked in the field for more than a few years has probably<br/>done some variation of reverse ETL. Reverse ETL grew in popularity in the<br/>late 2010s/early 2020s and is increasingly recognized as a formal data<br/>engineering responsibility.<br/>A data engineer might pull customers and order data from a CRM and store<br/>it in a data warehouse. This data is used to train a lead scoring model,<br/>whose results are returned to the data warehouse. Your company&#8217;s sales<br/>team wants access to these scored leads to try to generate more sales. You<br/>have a few options to get the results of this lead scoring model into the</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>hands of the sales team. You can put the results in a dashboard for them to<br/>view. Or you might email the results to them as an Excel file.<br/>The challenge with these approaches is that they are not connected to the<br/>CRM, where a salesperson does their work. Why not just put the scored<br/>leads back into the CRM? As we mentioned, successful data products<br/>reduce friction with the end user. In this case, the end user is the sales team.<br/>Using reverse ETL and loading the scored leads back into the CRM is the<br/>easiest and best approach for this data product. Reverse ETL takes<br/>processed data from the output side of the data engineering lifecycle and<br/>feeds it back into source systems (Figure 9-5).<br/></p>
<p><i>Figure 9-5. Reverse ETL</i></p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><b>NOTE<br/></b>Instead of reverse ETL, we, the authors, half-jokingly call it bidirectional load and<br/>transform (BLT). The term <i>reverse ETL</i> doesn&#8217;t quite accurately describe what&#8217;s<br/>happening in this process. Regardless, the term has stuck in the popular imagination and<br/>press, so we&#8217;ll use it throughout the book. More broadly, whether the term <i>reverse ETL<br/></i>sticks around is anyone&#8217;s guess, but the practice of loading data from OLAP systems<br/>back into source systems will remain important.<br/></p>
<p><b>Ways to Serve Data with Reverse ETL<br/></b>How do you begin serving data with reverse ETL? While you can roll your<br/>reverse ETL solution, many off-the-shelf reverse ETL options are available.<br/>We suggest using open source, or a commercial managed service. That said,<br/>the reverse ETL space is changing extremely quickly. No clear winners<br/>have emerged, and many reverse ETL products will be absorbed by major<br/>clouds or other data product vendors. Choose carefully.<br/>We do have a few words of warning regarding reverse ETL. Reverse ETL<br/>inherently creates feedback loops. For example, imagine that we download<br/>Google Ads data, use a model to compute new bids, load the bids back into<br/>Google Ads, and start the process again. Suppose that because of an error in<br/>your bid model, the bids trend ever higher, and your ads get more and more<br/>clicks. You can quickly waste massive amounts of money! Be careful, and<br/>build in monitoring and guardrails.<br/></p>
<p><b>Whom You&#8217;ll Work With<br/></b>As we&#8217;ve discussed, in the serving stage, a data engineer will interface with<br/>a lot of stakeholders. These include (but aren&#8217;t limited to) the following:<br/></p>
<p>Data analysts<br/>Data scientists<br/>MLOps/ML engineers</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>The business&#8212;nondata or nontechnical stakeholders, managers, and<br/>executives<br/></p>
<p>As a reminder, the data engineer operates in a <i>support</i> role for these<br/>stakeholders and is not necessarily responsible for the end uses of data. For<br/>example, a data engineer supplies the data for a report that analysts<br/>interpret, but the data engineer isn&#8217;t responsible for these interpretations.<br/>Instead, the data engineer is responsible for producing the highest-quality<br/>data products possible.<br/>A data engineer should be aware of feedback loops between the data<br/>engineering lifecycle and the broader use of data once it&#8217;s in the hands of<br/>stakeholders. Data is rarely static, and the outside world will influence the<br/>data that is ingested and served and reingested and re-served.<br/>A big consideration for data engineers in the serving stage of the lifecycle is<br/>the separation of duties and concerns. If you&#8217;re at an early-stage company,<br/>the data engineer may also be an ML engineer or data scientist; this is not<br/>sustainable. As the company grows, you need to establish a clear division of<br/>duties with other data team members.<br/>Adopting a data mesh dramatically reorganizes team responsibilities, and<br/>every domain team takes on aspects of serving. For a data mesh to be<br/>successful, each team must work effectively on its data serving<br/>responsibilities, and teams must also effectively collaborate to ensure<br/>organizational success.<br/></p>
<p><b>Undercurrents<br/></b>The undercurrents come to finality with serving. Remember that the data<br/>engineering lifecycle is just that&#8212;a lifecycle. What goes around comes<br/>around. We see many instances where serving data highlights something<br/>missed earlier in the lifecycle. Always be on the lookout for how the<br/>undercurrents can help you spot ways to improve data products.<br/>We&#8217;re fond of saying, &#8220;Data is a silent killer,&#8221; and the undercurrents come<br/>to a head in the serving stage. Serving is your final chance to make sure</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>your data is in great shape before it gets into the hands of end users.<br/></p>
<p><b>Security<br/></b>The same security principles apply whether sharing data with people or<br/>systems. We often see data shared indiscriminately, with little to no access<br/>controls or thought as to what the data will be used for. This is a huge<br/>mistake that can have catastrophic results, such as a data breach and the<br/>resulting fines, bad press, and lost jobs. Take security seriously, especially<br/>in this stage of the lifecycle. Of all the lifecycle stages, serving presents the<br/>largest security surface.<br/>As always, exercise the principle of least privilege both for people and<br/>systems, and provide only the access required for the purpose at hand, and<br/>the job to be done. What data does an executive need versus an analyst or<br/>data scientist? What about an ML pipeline or reverse ETL process? These<br/>users and destinations all have different data needs, and access should be<br/>provided accordingly. Avoid giving carte blanche permissions to everyone<br/>and everything.<br/>Serving data is often read-only unless a person or process needs to update<br/>data in the system from which it is queried. People should be given read-<br/>only access to specific databases and datasets unless their role requires<br/>something more advanced like write or update access. This can be<br/>accomplished by combining groups of users with certain IAM roles (i.e.,<br/>analysts group, data scientist group) or custom IAM roles if this makes<br/>sense. For systems, provide service accounts and roles in a similar fashion.<br/>For both users and systems, narrow access to a dataset&#8217;s fields, rows,<br/>columns, and cells if this is warranted. Access controls should be as fine-<br/>grained as possible and revoked when access is no longer required.<br/>Access controls are critical when serving data in a multitenant environment.<br/>Make sure users can access only <i>their</i> data and nothing more. A good<br/>approach is to mediate access through filtered views, thus alleviating the<br/>security risks inherent in sharing access to a common table. Another</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>suggestion is to use data sharing in your workflows, which allows for read-<br/>only granular controls between you and people consuming your data.<br/>Check how often data products are used, and whether it makes sense to stop<br/>sharing certain data products. It&#8217;s extremely common for an executive to<br/>urgently request an analyst to create a report, only to have this report very<br/>quickly go unused. If data products aren&#8217;t used, ask the users if they&#8217;re still<br/>needed. If not, kill off the data product. This means one less security<br/>vulnerability floating around.<br/>Finally, you should view access control and security not as impediments to<br/>serving but as key enablers. We&#8217;re aware of many instances where complex,<br/>advanced data systems were built, potentially having a significant impact on<br/>a company. Because security was not implemented correctly, few people<br/>were allowed to access the data, so it languished. Fine-grained, robust<br/>access control means that more interesting data analytics and ML can be<br/>done while still protecting the business and its customers.<br/></p>
<p><b>Data Management<br/></b>You&#8217;ve been incorporating data management along the data engineering<br/>lifecycle, and the impact of your efforts will soon become apparent as<br/>people use your data products. At the serving stage, you&#8217;re mainly<br/>concerned with ensuring that people can access high-quality and<br/>trustworthy data.<br/>As we mentioned at the beginning of this chapter, trust is perhaps the most<br/>critical variable in data serving. If people trust their data, they will use it;<br/>untrusted data will go unused. Be sure to make data trust and data<br/>improvement an active process by providing feedback loops. As users<br/>interact with data, they can report problems and request improvements.<br/>Actively communicate back to your users as changes are made.<br/>What data do people need to do their jobs? Especially with regulatory and<br/>compliance concerns weighing on data teams, giving people access to the<br/>raw data&#8212;even with limited fields and rows&#8212;poses a problem of tracing<br/>data back to an entity, such as a person or a group of people. Thankfully,</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>advancements in data obfuscation allow you to serve synthetic, scrambled,<br/>or anonymized data to end users. These &#8220;fake&#8221; datasets should sufficiently<br/>allow an analyst or data scientist to get the necessary signal from the data,<br/>but in a way that makes identifying protected information difficult. Though<br/>this isn&#8217;t a perfect process&#8212;with enough effort, many datasets can be de-<br/>anonymized or reverse-engineered&#8212;it at least reduces the risk of data<br/>leakage.<br/>Also, incorporate semantic and metrics layers into your serving layer,<br/>alongside rigorous data modeling that properly expresses business logic and<br/>definitions. This provides a single source of truth, whether for analytics,<br/>ML, reverse ETL, or other serving uses.<br/></p>
<p><b>DataOps<br/></b>The steps you take in data management&#8212;data quality, governance, and<br/>security&#8212;are monitored in DataOps. Essentially, DataOps operationalizes<br/>data management. The following are some things to monitor:<br/></p>
<p>Data health and data downtime<br/>Latency of systems serving data&#8212;dashboards, databases, etc.<br/>Data quality<br/>Data and system security and access<br/>Data and model versions being served<br/>Uptime to achieve an SLO<br/></p>
<p>A variety of new tools have sprung up to address various monitoring<br/>aspects. For example, many popular data observability tools aim to<br/>minimize <i>data downtime</i> and maximize data quality. Observability tools<br/>may cross over from data to ML, supporting monitoring of models and<br/>model performance. More conventional DevOps monitoring is also critical<br/>to DataOps&#8212;e.g., you need to monitor whether connections are stable<br/>among storage, transformation, and serving.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>As in every stage of the data engineering lifecycle, version-control code and<br/>operationalize deployment. This applies to analytical code, data logic code,<br/>ML scripts, and orchestration jobs. Use multiple stages of deployment (dev,<br/>test, prod) for reports and models.<br/></p>
<p><b>Data Architecture<br/></b>Serving data should have the same architectural considerations as other data<br/>engineering lifecycle stages. At the serving stage, feedback loops must be<br/>fast and tight. Users should be able to access the data they need as quickly<br/>as possible when they need it.<br/>Data scientists are notorious for doing most development on their local<br/>machines. As discussed earlier, encourage them to migrate these workflows<br/>to common systems in a cloud environment, where data teams can<br/>collaborate in dev, test, and production environments and create proper<br/>production architectures. Facilitate your analysts and data scientists by<br/>supporting tools for publishing data insights with little encumbrance.<br/></p>
<p><b>Orchestration<br/></b>Data serving is the last stage of the data engineering lifecycle. Because<br/>serving is downstream of so many processes, it&#8217;s an area of extremely<br/>complex overlap. Orchestration is not simply a way of organizing and<br/>automating complex work but a means of coordinating data flow across<br/>teams so that data is made available to consumers at the promised time.<br/>Ownership of orchestration is a key organizational decision. Will<br/>orchestration be centralized or decentralized? A decentralized approach<br/>allows small teams to manage their data flows, but it can increase the<br/>burden of cross-team coordination. Instead of simply managing flows<br/>within a single system, directly triggering the completion of DAGs or tasks<br/>belonging to other teams, teams must pass messages or queries between<br/>systems.<br/>A centralized approach means that work is easier to coordinate, but<br/>significant gatekeeping must also exist to protect a single production asset.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>For example, a poorly written DAG can bring Airflow to a halt. The<br/>centralized approach would mean bringing down data processes and serving<br/>across the whole organization. Centralized orchestration management<br/>requires high standards, automated testing of DAGs, and gatekeeping.<br/>If orchestration is centralized, who will own it? When a company has a<br/>DataOps team, orchestration usually lands here. Often, a team involved in<br/>serving is a natural fit because it has a fairly holistic view of all data<br/>engineering lifecycle stages. This could be the DBAs, analytics engineers,<br/>data engineers, or ML engineers. ML engineers coordinate complex model-<br/>training processes but may or may not want to add the operational<br/>complexity of managing orchestration to an already crowded docket of<br/>responsibilities.<br/></p>
<p><b>Software Engineering<br/></b>Compared to a few years ago, serving data has become simpler. The need to<br/>write code has been drastically simplified. Data has also become more<br/>code-first, with the proliferation of open source frameworks focused on<br/>simplifying the serving of data. Many ways exist to serve data to end users,<br/>and a data engineer&#8217;s focus should be on knowing how these systems work<br/>and how data is delivered.<br/>Despite the simplicity of serving data, if code is involved, a data engineer<br/>should still understand how the main serving interfaces work. For example,<br/>a data engineer may need to translate the code a data scientist is running<br/>locally on a notebook and convert it into a report or a basic ML model to<br/>operate.<br/>Another area where data engineers will be useful is understanding the<br/>impact of how code and queries will perform against the storage systems.<br/>Analysts can generate SQL in various programmatic ways, including<br/>LookML, Jinja via dbt, various object-relational mapping (ORM) tools, and<br/>metrics layers. When these programmatic layers compile to SQL, how will<br/>this SQL perform? A data engineer can suggest optimizations where the<br/>SQL code might not perform as well as handwritten SQL.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>The rise of analytics and ML IaC means the role of writing code is moving<br/>toward building the systems that support data scientists and analysts. Data<br/>engineers might be responsible for setting up the CI/CD pipelines and<br/>building processes for their data team. They would also do well to train and<br/>support their data team in using the Data/MLOps infrastructure they&#8217;ve<br/>built so that these data teams can be as self-sufficient as possible.<br/>For embedded analytics, data engineers may need to work with application<br/>developers to ensure that queries are returned quickly and cost-effectively.<br/>The application developer will control the frontend code that users deal<br/>with. The data engineer is there to ensure that developers receive the correct<br/>payloads as they&#8217;re requested.<br/></p>
<p><b>Conclusion<br/></b>The data engineering lifecycle has a logical ending at the serving stage. As<br/>with all lifecycles, a feedback loop occurs (Figure 9-6). You should view<br/>the serving stage as a chance to learn what&#8217;s working and what can be<br/>improved. Listen to your stakeholders. If they bring up issues&#8212;and they<br/>inevitably will&#8212;try not to take offense. Instead, use this as an opportunity<br/>to improve what you&#8217;ve built.<br/></p>
<p><i>Figure 9-6. Build, learn, improve</i></p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>A good data engineer is always open to new feedback and constantly finds<br/>ways to improve their craft. Now that we&#8217;ve taken a journey through the<br/>data engineering lifecycle, you know how to design, architect, build,<br/>maintain, and improve your data engineering systems and products. Let&#8217;s<br/>turn our attention to Part III of the book, where we&#8217;ll cover some aspects of<br/>data engineering we&#8217;re constantly asked about and, frankly, deserve more<br/>attention.<br/></p>
<p><b>Additional Resources<br/></b>&#8220;Designing Data Products&#8221; by Seth O&#8217;Regan<br/>&#8220;Data Jujitsu: The Art of Turning Data into Product&#8221; by DJ Patil<br/>&#8220;What Is User-Facing Analytics?&#8221; by Chinmon Soman<br/>&#8220;Data as a Product vs. Data Products: What Are the Differences?&#8221; by<br/>Xavier Gumara Rigol<br/>&#8220;How to Build Great Data Products&#8221; by Emily Glassberg Sands<br/>&#8220;The Evolution of Data Products&#8221; and &#8220;What Is Data Science&#8221; by<br/>Mike Loukides<br/>&#8220;Know Your Customers&#8217; &#8216;Jobs to Be Done&#8217;&#8221; by Clayton M.<br/>Christensen et al.<br/>&#8220;Data Mesh Principles and Logical Architecture&#8221; by Martin Fowler<br/>&#8220;Understanding the Superset Semantic Layer&#8221; by Srini Kadamati<br/>&#8220;The Future of BI Is Headless&#8221; by ZD<br/>&#8220;How to Structure a Data Analytics Team&#8221; by Niall Napier<br/>&#8220;What Is Operational Analytics (and How Is It Changing How We<br/>Work with Data)?&#8221; by Sylvain Giuliani</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>&#8220;Fundamentals of Self-Service Machine Learning&#8221; by Paramita (Guha)<br/>Ghosh<br/>&#8220;What Do Modern Self-Service BI and Data Analytics Really Mean?&#8221;<br/>by Harry Dix<br/>&#8220;Self-Service Analytics&#8221; in the Gartner Glossary<br/>&#8220;The Missing Piece of the Modern Data Stack&#8221; and &#8220;Why Is Self-<br/>Serve Still a Problem?&#8221; by Benn Stancil<br/>Forrester&#8217;s &#8220;Self-Service Business Intelligence: Dissolving the Barriers<br/>to Creative Decision-Support Solutions&#8221; blog article<br/><i>Data Mesh</i> by Zhamak Dehghani (O&#8217;Reilly)<br/></p>
<p>1  &#8220;Know Your Customers&#8217; &#8216;Jobs to Be Done&#8217;&#8221; by Clayton M. Christensen et al., <i>Harvard<br/>Business Review</i>, September 2016, <i>https://oreil.ly/3uU4j</i>.<br/></p>
<p>2  Benn Stancil, &#8220;The Missing Piece of the Modern Data Stack,&#8221; <i>benn.substack</i>, April 22, 2021,<br/><i>https://oreil.ly/wQyPb</i>.<br/></p>
<p>3  Srini Kadamati, &#8220;Understanding the Superset Semantic Layer,&#8221; Preset blog, December 21,<br/>2021, <i>https://oreil.ly/6smWC</i>.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><b>Part III. Security, Privacy, and<br/>the Future of Data Engineering</b></p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><b>Chapter 10. Security and<br/>Privacy<br/></b>Security is vital to the practice of data engineering. This should be<br/>blindingly obvious, but we&#8217;re constantly amazed at how often data<br/>engineers view security as an afterthought. We believe that security is the<br/>first thing a data engineer needs to think about in every aspect of their job<br/>and every stage of the data engineering lifecycle. You deal with sensitive<br/>data, information, and access daily. Your organization, customers, and<br/>business partners expect these valuable assets to be handled with the utmost<br/>care and concern. One security breach or a data leak can leave your<br/>business dead in the water; your career and reputation are ruined if it&#8217;s your<br/>fault.<br/>Security is a key ingredient for privacy. Privacy has long been critical to<br/>trust in the corporate information technology space; engineers directly or<br/>indirectly handle data related to people&#8217;s private lives. This includes<br/>financial information, data on private communications (emails, texts, phone<br/>calls), medical history, educational records, and job history. A company that<br/>leaked this information or misused it could find itself a pariah when the<br/>breach came to light.<br/>Increasingly, privacy is a matter of significant legal importance. For<br/>example, the Family Educational Rights and Privacy Act (FERPA) went<br/>into effect in the US in the 1970s; the Health Insurance Portability and<br/>Accountability Act (HIPAA) followed in the 1990s; GDPR was passed in<br/>Europe in the mid-2010s. Several US-based privacy bills have passed or<br/>will soon. This is just a tiny sampling of privacy-related statutes (and we<br/>believe just the beginning). Still, the penalties for violation of any of these<br/>laws can be significant, even devastating, to a business. And because data<br/>systems are woven into the fabric of education, health care, and business,<br/>data engineers handle sensitive data related to each of these laws.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>A data engineer&#8217;s exact security and privacy responsibilities will vary<br/>significantly between organizations. At a small startup, a data engineer may<br/>do double duty as a data security engineer. A large tech company will have<br/>armies of security engineers and security researchers. Even in this situation,<br/>data engineers will often be able to identify security practices and<br/>technology vulnerabilities within their own teams and systems that they can<br/>report and mitigate in collaboration with dedicated security personnel.<br/>Because security and privacy are critical to data engineering (security being<br/>an undercurrent), we want to spend some more time covering security and<br/>privacy. In this chapter, we lay out some things data engineers should<br/>consider around security, particularly in people, processes, and technology<br/>(in that order). This isn&#8217;t a complete list, but lays out the major things we&#8217;d<br/>wish would improve based on our experience.<br/></p>
<p><b>People<br/></b>The weakest link in security and privacy is <i>you</i>. Security is often<br/>compromised at the human level, so conduct yourself as if you&#8217;re always a<br/>target. A bot or human actor is trying to infiltrate your sensitive credentials<br/>and information at any given time. This is our reality, and it&#8217;s not going<br/>away. Take a defensive posture with everything you do online and offline.<br/>Exercise the power of negative thinking and always be paranoid.<br/></p>
<p><b>The Power of Negative Thinking<br/></b>In a world obsessed with positive thinking, negative thinking is distasteful.<br/>However, American surgeon Atul Gawande wrote a 2007 op-ed in the <i>New<br/>York Times</i> on precisely this subject. His central thesis is that positive<br/>thinking can blind us to the possibility of terrorist attacks or medical<br/>emergencies and deter preparation. Negative thinking allows us to consider<br/>disastrous scenarios and act to prevent them.<br/>Data engineers should actively think through the scenarios for data<br/>utilization and collect sensitive data only if there is an actual need</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>downstream. The best way to protect private and sensitive data is to avoid<br/>ingesting this data in the first place.<br/>Data engineers should think about the attack and leak scenarios with any<br/>data pipeline or storage system they utilize. When deciding on security<br/>strategies, ensure that your approach delivers proper security and not just<br/>the illusion of safety.<br/></p>
<p><b>Always Be Paranoid<br/></b>Always exercise caution when someone asks you for your credentials.<br/>When in doubt&#8212;and you should always be in extreme doubt when asked<br/>for credentials&#8212;hold off and get second opinions from your coworkers and<br/>friends. Confirm with other people that the request is indeed legitimate. A<br/>quick chat or phone call is cheaper than a ransomware attack triggered<br/>through an email click. Trust nobody at face value when asked for<br/>credentials, sensitive data, or confidential information, including from your<br/>coworkers.<br/>You are also the first line of defense in respecting privacy and ethics. Are<br/>you uncomfortable with sensitive data you&#8217;ve been tasked to collect? Do<br/>you have ethical questions about the way data is being handled in a project?<br/>Raise your concerns with colleagues and leadership. Ensure that your work<br/>is both legally compliant and ethical.<br/></p>
<p><b>Processes<br/></b>When people follow regular security processes, security becomes part of the<br/>job. Make security a habit, regularly practice real security, exercise the<br/>principle of least privilege, and understand the shared responsibility model<br/>in the cloud.<br/></p>
<p><b>Security Theater Versus Security Habit<br/></b>With our corporate clients, we see a pervasive focus on compliance (with<br/>internal rules, laws, recommendations from standards bodies), but not</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>enough attention to potentially bad scenarios. Unfortunately, this creates an<br/>illusion of security but often leaves gaping holes that would be evident with<br/>a few minutes of reflection.<br/>Security needs to be simple and effective enough to become habitual<br/>throughout an organization. We&#8217;re amazed at the number of companies with<br/>security policies in the hundreds of pages that nobody reads, the annual<br/>security policy review that people immediately forget, all in checking a box<br/>for a security audit. This is security theater, where security is done in the<br/>letter of compliance (SOC-2, ISO 27001, and related) without real<br/><i>commitment</i>.<br/>Instead, pursue the spirit of genuine and habitual security; bake a security<br/>mindset into your culture. Security doesn&#8217;t need to be complicated. For<br/>example, at our company, we run security training and policy review at least<br/>once a month to ingrain this into our team&#8217;s DNA and update each other on<br/>security practices we can improve. Security must not be an afterthought for<br/>your data team. Everyone is responsible and has a role to play. It must be<br/>the priority for you and everyone else you work with.<br/></p>
<p><b>Active Security<br/></b>Returning to the idea of negative thinking, <i>active security</i> entails thinking<br/>about and researching security threats in a dynamic and changing world.<br/>Rather than simply deploying scheduled simulated phishing attacks, you<br/>can take an active security posture by researching successful phishing<br/>attacks and thinking through your organizational security vulnerabilities.<br/>Rather than simply adopting a standard compliance checklist, you can think<br/>about internal vulnerabilities specific to your organization and incentives<br/>employees might have to leak or misuse private information.<br/>We have more to say about active security in &#8220;Technology&#8221;.<br/></p>
<p><b>The Principle of Least Privilege<br/></b>The <i>principle of least privilege</i> means that a person or system should be<br/>given only the privileges and data they need to complete the task at hand</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>and nothing more. Often, we see an antipattern in the cloud: a regular user<br/>is given administrative access to everything, when that person may need<br/>just a handful of IAM roles to do their work. Giving someone carte blanche<br/>administrative access is a huge mistake and should never happen under the<br/>principle of least privilege.<br/>Instead, provide the user (or group they belong to) the IAM roles they need<br/>when they need them. When these roles are no longer needed, take them<br/>away. The same rule applies to service accounts. Treat humans and<br/>machines the same way: give them only the privileges and data they need to<br/>do their jobs, and only for the timespan when needed.<br/>Of course, the principle of least privilege is also critical to privacy. Your<br/>users and customers expect that people will look at their sensitive data only<br/>when necessary. Make sure that this is the case. Implement column, row,<br/>and cell-level access controls around sensitive data; consider masking PII<br/>and other sensitive data and create views that contain only the information<br/>the viewer needs to access. Some data must be retained, but should be<br/>accessed only in an emergency. Put this data behind a <i>broken glass process</i>:<br/>users can access it only after going through an emergency approval process<br/>to fix a problem, query critical historical information, etc. Access is<br/>revoked immediately once the work is done.<br/></p>
<p><b>Shared Responsibility in the Cloud<br/></b>Security is a shared responsibility in the cloud. The cloud vendor is<br/>responsible for ensuring the physical security of its data center and<br/>hardware. At the same time, you are responsible for the security of the<br/>applications and systems you build and maintain in the cloud. Most cloud<br/>security breaches continue to be caused by end users, not the cloud.<br/>Breaches occur because of unintended misconfigurations, mistakes,<br/>oversights, and sloppiness.<br/></p>
<p><b>Always Back Up Your Data</b></p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Data disappears. Sometimes it&#8217;s a dead hard drive or server; in other cases,<br/>someone might accidentally delete a database or an object storage bucket. A<br/>bad actor can also lock away data. Ransomware attacks are widespread<br/>these days. Some insurance companies are reducing payouts in the event of<br/>an attack, leaving you on the hook both to recover your data and pay the<br/>bad actor who&#8217;s holding it hostage. You need to back up your data regularly,<br/>both for disaster recovery and continuity of business operations, if a version<br/>of your data is compromised in a ransomware attack. Additionally, test the<br/>restoration of your data backups on a regular basis.<br/>Data backup doesn&#8217;t strictly fit under security and privacy practices; it goes<br/>under the larger heading of <i>disaster prevention</i>, but it&#8217;s adjacent to security,<br/>especially in the era of ransomware attacks.<br/></p>
<p><b>An Example Security Policy<br/></b>This section presents a sample security policy regarding credentials,<br/>devices, and sensitive information. Notice that we don&#8217;t overcomplicate<br/>things; instead, we give people a short list of practical actions they can take<br/>immediately.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><b>EXAMPLE SECURITY POLICY<br/>Protect Your Credentials<br/></b>Protect your credentials at all costs. Here are some ground rules for<br/>credentials:<br/></p>
<p>Use a single-sign-on (SSO) for everything. Avoid passwords<br/>whenever possible, and use SSO as the default.<br/>Use multifactor authentication with SSO.<br/>Don&#8217;t share passwords or credentials. This includes client<br/>passwords and credentials. If in doubt, see the person you report<br/>to. If that person is in doubt, keep digging until you find an<br/>answer.<br/>Beware of phishing and scam calls. Don&#8217;t ever give your<br/>passwords out. (Again, prioritize SSO.)<br/>Disable or delete old credentials. Preferably the latter.<br/>Don&#8217;t put your credentials in code. Handle secrets as configuration<br/>and never commit them to version control. Use a secrets manager<br/>where possible.<br/>Always exercise the principle of least privilege. Never give more<br/>access than is required to do the job. This applies to all credentials<br/>and privileges in the cloud and on premises.<br/></p>
<p><b>Protect Your Devices<br/></b>Use device management for all devices used by employees. If an<br/>employee leaves the company or your device gets lost, the device<br/>can be remotely wiped.<br/>Use multifactor authentication for all devices.<br/>Sign in to your device using your company email credentials.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>All policies covering credentials and behavior apply to your<br/>device(s).<br/>Treat your device as an extension of yourself. Don&#8217;t let your<br/>assigned device(s) out of your sight.<br/>When screen sharing, be aware of exactly what you&#8217;re sharing to<br/>protect sensitive information and communications. Share only<br/>single documents, browser tabs, or windows, and avoid sharing<br/>your full desktop. Share only what&#8217;s required to convey your point.<br/>Use &#8220;do not disturb&#8221; mode when on video calls; this prevents<br/>messages from appearing during calls or recordings.<br/></p>
<p><b>Software Update Policy<br/></b>Restart your web browser when you see an update alert.<br/>Run minor OS updates on company and personal devices.<br/>The company will identify critical major OS updates and provide<br/>guidance.<br/>Don&#8217;t use the beta version of an OS.<br/>Wait a week or two for new major OS version releases.<br/></p>
<p>These are some basic examples of how security can be simple and effective.<br/>Based on your company&#8217;s security profile, you may need to add more<br/>requirements for people to follow. And again, always remember that people<br/>are your weakest link in security.<br/></p>
<p><b>Technology<br/></b>After you&#8217;ve addressed security with people and processes, it&#8217;s time to look<br/>at how you leverage technology to secure your systems and data assets. The<br/>following are some significant areas you should prioritize.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><b>Patch and Update Systems<br/></b>Software gets stale, and security vulnerabilities are constantly discovered.<br/>To avoid exposing a security flaw in an older version of the tools you&#8217;re<br/>using, always patch and update operating systems and software as new<br/>updates become available. Thankfully, many SaaS and cloud-managed<br/>services automatically perform upgrades and other maintenance without<br/>your intervention. To update your own code and dependencies, either<br/>automate builds or set alerts on releases and vulnerabilities so you can be<br/>prompted to perform the updates manually.<br/></p>
<p><b>Encryption<br/></b>Encryption is not a magic bullet. It will do little to protect you in the event<br/>of a <i>human</i> security breach that grants access to credentials. Encryption is a<br/>baseline requirement for any organization that respects security and privacy.<br/>It will protect you from basic attacks, such as network traffic interception.<br/>Let&#8217;s look separately at encryption at rest and in transit.<br/><b>Encryption at rest<br/></b>Be sure your data is encrypted when it is at rest (on a storage device). Your<br/>company laptops should have full-disk encryption enabled to protect data if<br/>a device is stolen. Implement server-side encryption for all data stored in<br/>servers, filesystems, databases, and object storage in the cloud. All data<br/>backups for archival purposes should also be encrypted. Finally, incorporate<br/>application-level encryption where applicable.<br/><b>Encryption over the wire<br/></b>Encryption over the wire is now the default for current protocols. For<br/>instance, HTTPS is generally required for modern cloud APIs. Data<br/>engineers should always be aware of how keys are handled; bad key<br/>handling is a significant source of data leaks. In addition, HTTPS does<br/>nothing to protect data if bucket permissions are left open to the public,<br/>another cause of several data scandals over the last decade.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Engineers should also be aware of the security limitations of older<br/>protocols. For example, FTP is simply not secure on a public network.<br/>While this may not appear to be a problem when data is already public, FTP<br/>is vulnerable to man-in-the-middle attacks, whereby an attacker intercepts<br/>downloaded data and changes it before it arrives at the client. It is best to<br/>simply avoid FTP.<br/>Make sure everything is encrypted over the wire, even with legacy<br/>protocols. When in doubt, use robust technology with encryption baked in.<br/></p>
<p><b>Logging, Monitoring, and Alerting<br/></b>Hackers and bad actors typically don&#8217;t announce that they&#8217;re infiltrating<br/>your systems. Most companies don&#8217;t find out about security incidents until<br/>well after the fact. Part of DataOps is to observe, detect, and alert on<br/>incidents. As a data engineer, you should set up automated monitoring,<br/>logging, and alerting to be aware of peculiar events when they happen in<br/>your systems. If possible, set up automatic anomaly detection.<br/>Here are some areas you should monitor:<br/><i>Access<br/></i></p>
<p>Who&#8217;s accessing what, when, and from where? What new accesses were<br/>granted? Are there strange patterns with your current users that might<br/>indicate their account is compromised, such as trying to access systems<br/>they don&#8217;t usually access or shouldn&#8217;t have access to? Do you see new<br/>unrecognized users accessing your system? Be sure to regularly comb<br/>through access logs, users, and their roles to ensure that everything<br/>looks OK.<br/></p>
<p><i>Resources<br/></i>Monitor your disk, CPU, memory, and I/O for patterns that seem out of<br/>the ordinary. Did your resources suddenly change? If so, this might</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>indicate a security breach.<br/><i>Billing<br/></i></p>
<p>Especially with SaaS and cloud-managed services, you need to oversee<br/>costs. Set up budget alerts to make sure your spending is within<br/>expectations. If an unexpected spike occurs in your billing, this might<br/>indicate someone or something is utilizing your resources for malicious<br/>purposes.<br/></p>
<p><i>Excess permissions<br/></i>Increasingly, vendors are providing tools that monitor for permissions<br/>that are <i>not utilized</i> by a user or service account over some time. These<br/>tools can often be configured to automatically alert an administrator or<br/>remove permissions after a specified elapsed time.<br/>For example, suppose that a particular analyst hasn&#8217;t accessed Redshift<br/>for six months. These permissions can be removed, closing a potential<br/>security hole. If the analyst needs to access Redshift in the future, they<br/>can put in a ticket to restore permissions.<br/></p>
<p>It&#8217;s best to combine these areas in your monitoring to get a cross-sectional<br/>view of your resource, access, and billing profile. We suggest setting up a<br/>dashboard for everyone on the data team to view monitoring and receive<br/>alerts when something seems out of the ordinary. Couple this with an<br/>effective incident response plan to manage security breaches when they<br/>occur, and run through the plan on a regular basis so you are prepared.<br/></p>
<p><b>Network Access<br/></b>We often see data engineers doing pretty wild things regarding network<br/>access. In several instances, we&#8217;ve seen publicly available Amazon S3<br/>buckets housing lots of sensitive data. We&#8217;ve also witnessed Amazon EC2<br/>instances with inbound SSH access open to the whole world for 0.0.0.0/0</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>(all IPs) or databases with open access to all inbound requests over the<br/>public internet. These are just a few examples of terrible network security<br/>practices.<br/>In principle, network security should be left to security experts at your<br/>company. (In practice, you may need to assume significant responsibility<br/>for network security in a small company.) As a data engineer, you will<br/>encounter databases, object storage, and servers so often that you should at<br/>least be aware of simple measures you can take to make sure you&#8217;re in line<br/>with good network access practices. Understand what IPs and ports are<br/>open, to whom, and why. Allow the incoming IP addresses of the systems<br/>and users that will access these ports and avoid broadly opening<br/>connections for any reason. When accessing the cloud or a SaaS tool, use an<br/>encrypted connection. For example, don&#8217;t use an unencrypted website from<br/>a coffee shop.<br/>Also, while this book has focused almost entirely on running workloads in<br/>the cloud, we add a brief note here about hosting on-premises servers.<br/>Recall that in Chapter 3, we discussed the difference between a hardened<br/>perimeter and zero-trust security. The cloud is generally closer to zero-trust<br/>security&#8212;every action requires authentication. We believe that the cloud is<br/>a more secure option for most organizations because it imposes zero-trust<br/>practices and allows companies to leverage the army of security engineers<br/>employed by the public clouds.<br/>However, sometimes hardened perimeter security still makes sense; we find<br/>some solace in the knowledge that nuclear missile silos are air gapped (not<br/>connected to any networks). Air-gapped servers are the ultimate example of<br/>a hardened security perimeter. Just keep in mind that even on premises, air-<br/>gapped servers are vulnerable to human security failings.<br/></p>
<p><b>Security for Low-Level Data Engineering<br/></b>For engineers who work in the guts of data storage and processing systems,<br/>it is critical to consider the security implications of every element. Any<br/>software library, storage system, or compute node is a potential security</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>vulnerability. A flaw in an obscure logging library might allow attackers to<br/>bypass access controls or encryption. Even CPU architectures and<br/>microcode represent potential vulnerabilities; sensitive data can be<br/>vulnerable when it&#8217;s at rest in memory or a CPU cache. No link in the chain<br/>can be taken for granted.<br/>Of course, this book is principally about high-level data engineering&#8212;<br/>stitching together tools to handle the entire lifecycle. Thus, we&#8217;ll leave it to<br/>you to dig into the gory technical details.<br/><b>Internal security research<br/></b>We discussed the idea of <i>active security</i> in &#8220;Processes&#8221;. We also highly<br/>recommend adopting an <i>active security</i> approach to technology.<br/>Specifically, this means that every technology employee should think about<br/>security problems.<br/>Why is this important? Every technology contributor develops a domain of<br/>technical expertise. Even if your company employs an army of security<br/>researchers, data engineers will become intimately familiar with specific<br/>data systems and cloud services in their purview. Experts in a particular<br/>technology are well positioned to identify security holes in this technology.<br/>Encourage every data engineer to be actively involved in security. When<br/>they identify potential security risks in their systems, they should think<br/>through mitigations and take an active role in deploying these.<br/></p>
<p><b>Conclusion<br/></b>Security needs to be a habit of mind and action; treat data like your wallet<br/>or smartphone. Although you won&#8217;t likely be in charge of security for your<br/>company, knowing basic security practices and keeping security top of<br/>mind will help reduce the risk of data security breaches at your<br/>organization.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><b>Additional Resources<br/></b>Open Web Application Security Project (OWASP) publications<br/><i>Building Secure and Reliable Systems</i> by Heather Adkins et al.<br/>(O&#8217;Reilly)<br/><i>Practical Cloud Security</i> by Chris Dotson (O&#8217;Reilly)</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><b>Chapter 11. The Future of Data<br/>Engineering<br/></b>This book grew out of the authors&#8217; recognition that warp speed changes in<br/>the field have created a significant knowledge gap for existing data<br/>engineers, people interested in moving into a career in data engineering,<br/>technology managers, and executives who want to better understand how<br/>data engineering fits into their companies. When we started thinking about<br/>how to organize this book, we got quite a bit of pushback from friends<br/>who&#8217;d ask, &#8220;How dare you write about a field that is changing so quickly?!&#8221;<br/>In many ways, they&#8217;re right. It certainly feels like the field of data<br/>engineering&#8212;and, really, all things data&#8212;is changing daily. Sifting through<br/>the noise and finding the signal of <i>what&#8217;s unlikely to change</i> was among the<br/>most challenging parts of organizing and writing this book.<br/>In this book, we focus on big ideas that we feel will be useful for the next<br/>several years&#8212;hence the continuum of the data engineering lifecycle and<br/>its undercurrents. The order of operations and names of best practices and<br/>technologies might change, but the primary stages of the lifecycle will<br/>likely remain intact for many years to come. We&#8217;re keenly aware that<br/>technology continues to change at an exhausting pace; working in the<br/>technology sector in our present era can feel like a rollercoaster ride or<br/>perhaps a hall of mirrors.<br/>Several years ago, data engineering didn&#8217;t even exist as a field or job title.<br/>Now you&#8217;re reading a book called <i>Fundamentals of Data Engineering</i>!<br/>You&#8217;ve learned all about the fundamentals of data engineering&#8212;its<br/>lifecycle, undercurrents, technologies, and best practices. You might be<br/>asking yourself, what&#8217;s next in data engineering? While nobody can predict<br/>the future, we have a good perspective on the past, the present, and current<br/>trends. We&#8217;ve been fortunate to watch the genesis and evolution of data<br/>engineering from a front-row seat. This final chapter presents our thoughts</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>on the future, including observations of ongoing developments and wild<br/>future speculation.<br/></p>
<p><b>The Data Engineering Lifecycle Isn&#8217;t Going<br/>Away<br/></b>While data science has received the bulk of the attention in recent years,<br/>data engineering is rapidly maturing into a distinct and visible field. It&#8217;s one<br/>of the fastest-growing careers in technology, with no signs of losing<br/>momentum. As companies realize they first need to build a data foundation<br/>before moving to &#8220;sexier&#8221; things like AI and ML, data engineering will<br/>continue growing in popularity and importance. This progress centers<br/>around the data engineering lifecycle.<br/>Some question whether increasingly simple tools and practices will lead to<br/>the disappearance of data engineers. This thinking is shallow, lazy, and<br/>shortsighted. As organizations leverage data in new ways, new foundations,<br/>systems, and workflows will be needed to address these needs. Data<br/>engineers sit at the center of designing, architecting, building, and<br/>maintaining these systems. If tooling becomes easier to use, data engineers<br/>will move up the value chain to focus on higher-level work. The data<br/>engineering lifecycle isn&#8217;t going away anytime soon.<br/></p>
<p><b>The Decline of Complexity and the Rise of<br/>Easy-to-Use Data Tools<br/></b>Simplified, easy-to-use tools continue to lower the barrier to entry for data<br/>engineering. This is a great thing, especially given the shortage of data<br/>engineers we&#8217;ve discussed. The trend toward simplicity will continue. Data<br/>engineering isn&#8217;t dependent on a particular technology or data size. It&#8217;s also<br/>not just for large companies. In the 2000s, deploying &#8220;big data&#8221;<br/>technologies required a large team and deep pockets. The ascendance of<br/>SaaS-managed services has largely removed the complexity of</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>understanding the guts of various &#8220;big data&#8221; systems. Data engineering is<br/>now something that <i>all</i> companies can do.<br/>Big data is a victim of its extraordinary success. For example, Google<br/>BigQuery, a descendant of GFS and MapReduce, can query petabytes of<br/>data. Once reserved for internal use at Google, this insanely powerful<br/>technology is now available to anybody with a GCP account. Users simply<br/>pay for the data they store and query rather than having to build a massive<br/>infrastructure stack. Snowflake, Amazon EMR, and many other hyper-<br/>scalable cloud data solutions compete in the space and offer similar<br/>capabilities.<br/>The cloud is responsible for a significant shift in the usage of open source<br/>tools. Even in the early 2010s, using open source typically entailed<br/>downloading the code and configuring it yourself. Nowadays, many open<br/>source data tools are available as managed cloud services that compete<br/>directly with proprietary services. Linux is available preconfigured and<br/>installed on server instances on all major clouds. Serverless platforms like<br/>AWS Lambda and Google Cloud Functions allow you to deploy event-<br/>driven applications in minutes, using mainstream languages such as Python,<br/>Java, and Go running atop Linux behind the scenes. Engineers wishing to<br/>use Apache Airflow can adopt Google&#8217;s Cloud Composer or AWS&#8217;s<br/>managed Airflow service. Managed Kubernetes allows us to build highly<br/>scalable microservice architectures. And so on.<br/>This fundamentally changes the conversation around open source code. In<br/>many cases, managed open source is just as easy to use as its proprietary<br/>service competitors. Companies with highly specialized needs can also<br/>deploy managed open source, then move to self-managed open source later<br/>if they need to customize the underlying code.<br/>Another significant trend is the growth in popularity of off-the-shelf data<br/>connectors (at the time of this writing, popular ones include Fivetran and<br/>Airbyte). Data engineers have traditionally spent a lot of time and resources<br/>building and maintaining plumbing to connect to external data sources. The<br/>new generation of managed connectors is highly compelling, even for</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>highly technical engineers, as they begin to recognize the value of<br/>recapturing time and mental bandwidth for other projects. API connectors<br/>will be an outsourced problem so that data engineers can focus on the<br/>unique issues that drive their businesses.<br/>The intersection of red-hot competition in the data-tooling space with a<br/>growing number of data engineers means data tools will continue<br/>decreasing in complexity while adding even more functionality and<br/>features. This simplification will only grow the practice of data engineering,<br/>as more and more companies find opportunities to discover value in data.<br/></p>
<p><b>The Cloud-Scale Data OS and Improved<br/>Interoperability<br/></b>Let&#8217;s briefly review some of the inner workings of (single-device) operating<br/>systems, then tie this back to data and the cloud. Whether you&#8217;re utilizing a<br/>smartphone, a laptop, an application server, or a smart thermostat, these<br/>devices rely on an operating system to provide essential services and<br/>orchestrate tasks and processes. For example, I can see roughly 300<br/>processes running on the MacBook Pro that I&#8217;m typing on. Among other<br/>things, I see services such as WindowServer (responsible for providing<br/>windows in a graphical interface) and CoreAudio (tasked with providing<br/>low-level audio capabilities).<br/>When I run an application on this machine, it doesn&#8217;t directly access sound<br/>and graphics hardware. Instead, it sends commands to operating system<br/>services to draw windows and play sound. These commands are issued to<br/>standard APIs; a specification tells software developers how to<br/>communicate with operating system services. The operating system<br/><i>orchestrates</i> a boot process to provide these services, starting each service<br/>in the correct order based on dependencies among them; it also maintains<br/>services by monitoring them and restarting them in the correct order in case<br/>of a failure.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Now let&#8217;s return to data in the cloud. The simplified data services that<br/>we&#8217;ve mentioned throughout this book (e.g., Google Cloud BigQuery,<br/>Azure Blob Storage, Snowflake, and AWS Lambda) resemble operating<br/>system services, but at a much larger scale, running across many machines<br/>rather than a single server.<br/>Now that these simplified services are available, the next frontier of<br/>evolution for this notion of a cloud data operating system will happen at a<br/>higher level of abstraction. Benn Stancil called for the emergence of<br/>standardized data APIs for building data pipelines and data applications.<br/>We predict that data engineering will gradually coalesce around a handful<br/>of data interoperability standards. Object storage in the cloud will grow in<br/>importance as a batch interface layer between various data services. New<br/>generation file formats (such as Parquet and Avro) are already taking over<br/>for the purposes of cloud data interchange, significantly improving on the<br/>dreadful interoperability of CSV, and the poor performance of raw JSON.<br/>Another critical ingredient of a data API ecosystem is a metadata catalog<br/>that describes schemas and data hierarchies. Currently, this role is largely<br/>filled by the legacy Hive Metastore. We expect that new entrants will<br/>emerge to take its place. Metadata will play a crucial role in data<br/>interoperability, both across applications and systems, and across clouds<br/>and networks, driving automation and simplification.<br/>We will also see significant improvements in the scaffolding that manages<br/>cloud data services. Apache Airflow has emerged as the first truly cloud-<br/>oriented data orchestration platform, but we are on the cusp of significant<br/>enhancement. Airflow will grow in capabilities, building on its massive<br/>mindshare. New entrants such as Dagster and Prefect will compete by<br/>rebuilding orchestration architecture from the ground up.<br/>This next generation of data orchestration platforms will feature enhanced<br/>data integration and data awareness. Orchestration platforms will integrate<br/>with data cataloging and lineage, becoming significantly more data-aware<br/>in the process. In addition, orchestration platforms will build IaC<br/>capabilities (similar to Terraform) and code deployment features (like<br/></p>
<p>1</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>GitHub Actions and Jenkins). This will allow engineers to code a pipeline,<br/>and then pass it to the orchestration platform to automatically build, test,<br/>deploy and monitor. Engineers will be able to write infrastructure<br/>specifications directly into their pipelines; missing infrastructure and<br/>services (e.g., Snowflake databases, Databricks clusters, and Amazon<br/>Kinesis streams) will be deployed the first time the pipeline runs.<br/>We will also see significant enhancements in the domain of <i>live data</i>&#8212;e.g.,<br/>streaming pipelines and databases capable of ingesting and querying<br/>streaming data. In the past, building a streaming DAG was an extremely<br/>complex process with a high ongoing operational burden (see Chapter 8).<br/>Tools like Apache Pulsar point the way toward a future in which streaming<br/>DAGs can be deployed with complex transformations using relatively<br/>simple code. We have already seen the emergence of managed stream<br/>processors (such as Amazon Kinesis Data Analytics and Google Cloud<br/>Dataflow), but we will see a new generation of orchestration tools for<br/>managing these services, stitching them together, and monitoring them. We<br/>discuss live data in &#8220;The Live Data Stack&#8221;.<br/>What does this enhanced abstraction mean for data engineers? As we&#8217;ve<br/>already argued in this chapter, the role of the data engineer won&#8217;t go away,<br/>but it will evolve significantly. By comparison, more sophisticated mobile<br/>operating systems and frameworks have not eliminated mobile app<br/>developers. Instead, mobile app developers can now focus on building<br/>better-quality, more sophisticated applications. We expect similar<br/>developments for data engineering as the cloud-scale data OS paradigm<br/>increases interoperability and simplicity across various applications and<br/>systems.<br/></p>
<p><b>&#8220;Enterprisey&#8221; Data Engineering<br/></b>The increasing simplification of data tools and the emergence and<br/>documentation of best practices means data engineering will become more<br/>&#8220;enterprisey.&#8221;  This will make many readers violently cringe. The term<br/><i>enterprise</i>, for some, conjures Kafkaesque nightmares of faceless<br/></p>
<p>2</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>committees dressed in overly starched blue shirts and khakis, endless red<br/>tape, and waterfall-managed development projects with constantly slipping<br/>schedules and ballooning budgets. In short, some of you read &#8220;enterprise&#8221;<br/>and imagine a soulless place where innovation goes to die.<br/>Fortunately, this is not what we&#8217;re talking about; we&#8217;re referring to some of<br/>the <i>good</i> things that larger companies do with data&#8212;management,<br/>operations, governance, and other &#8220;boring&#8221; stuff. We&#8217;re presently living<br/>through the golden age of &#8220;enterprisey&#8221; data management tools.<br/>Technologies and practices once reserved for giant organizations are<br/>trickling downstream. The once hard parts of big data and streaming data<br/>have now largely been abstracted away, with the focus shifting to ease of<br/>use, interoperability, and other refinements.<br/>This allows data engineers working on new tooling to find opportunities in<br/>the abstractions of data management, DataOps, and all the other<br/>undercurrents of data engineering. Data engineers will become<br/>&#8220;enterprisey.&#8221; Speaking of which&#8230;<br/></p>
<p><b>Titles and Responsibilities Will Morph...<br/></b>While the data engineering lifecycle isn&#8217;t going anywhere anytime soon, the<br/>boundaries between software engineering, data engineering, data science,<br/>and ML engineering are increasingly fuzzy. In fact, like the authors, many<br/>data scientists are transformed into data engineers through an organic<br/>process; tasked with doing &#8220;data science&#8221; but lacking the tools to do their<br/>jobs, they take on the job of designing and building systems to serve the<br/>data engineering lifecycle.<br/>As simplicity moves up the stack, data scientists will spend a smaller slice<br/>of their time gathering and munging data. But this trend will extend beyond<br/>data scientists. Simplification also means data engineers will spend less<br/>time on low-level tasks in the data engineering lifecycle (managing servers,<br/>configuration, etc.), and &#8220;enterprisey&#8221; data engineering will become more<br/>prevalent.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>As data becomes more tightly embedded in every business&#8217;s processes, new<br/>roles will emerge in the realm of data and algorithms. One possibility is a<br/>role that sits between ML engineering and data engineering. As ML toolsets<br/>become easier to use and managed cloud ML services grow in capabilities,<br/>ML is shifting away from ad hoc exploration and model development to<br/>become an operational discipline.<br/>This new ML-focused engineer who straddles this divide will know<br/>algorithms, ML techniques, model optimization, model monitoring, and<br/>data monitoring. However, their primary role will be to create or utilize the<br/>systems that automatically train models, monitor performance, and<br/>operationalize the full ML process for model types that are well understood.<br/>They will also monitor data pipelines and quality, overlapping into the<br/>current realm of data engineering. ML engineers will become more<br/>specialized to work on model types that are closer to research and less well<br/>understood.<br/>Another area in which titles may morph is at the intersection of software<br/>engineering and data engineering. Data applications, which blend<br/>traditional software applications with analytics, will drive this trend.<br/>Software engineers will need to have a much deeper understanding of data<br/>engineering. They will develop expertise in things like streaming, data<br/>pipelines, data modeling, and data quality. We will move beyond the &#8220;throw<br/>it over the wall&#8221; approach that is now pervasive. Data engineers will be<br/>integrated into application development teams, and software developers will<br/>acquire data engineering skills. The boundaries that exist between<br/>application backend systems and data engineering tools will be lowered as<br/>well, with deep integration through streaming and event-driven<br/>architectures.<br/></p>
<p><b>Moving Beyond the Modern Data Stack,<br/>Toward the Live Data Stack<br/></b>We&#8217;ll be frank: the modern data stack (MDS) isn&#8217;t so modern. We applaud<br/>the MDS for bringing a great selection of powerful data tools to the masses,</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>lowering prices, and empowering data analysts to take control of their data<br/>stack. The rise of ELT, cloud data warehouses, and the abstraction of SaaS<br/>data pipelines certainly changed the game for many companies, opening up<br/>new powers for BI, analytics, and data science.<br/>Having said that, the MDS is basically a repackaging of old data warehouse<br/>practices using modern cloud and SaaS technologies; because the MDS is<br/>built around the cloud data warehouse paradigm, it has some serious<br/>limitations when compared to the potential of next-generation real-time data<br/>applications. From our point of view, the world is moving beyond the use of<br/>data-warehouse-based internal-facing analytics and data science, toward<br/>powering entire businesses and applications in real time with next<br/>generation real-time databases.<br/>What&#8217;s driving this evolution? In many cases, analytics (BI and operational<br/>analytics) will be replaced by automation. Presently, most dashboards and<br/>reports answer questions concerning <i>what</i> and <i>when</i>. Ask yourself, &#8220;If I&#8217;m<br/>asking a <i>what</i> or <i>when</i> question, what action do I take next?&#8221; If the action is<br/>repetitive, it is a candidate for automation. Why look at a report to<br/>determine whether to take action when you can instead automate the action<br/>based on events as they occur?<br/>And it goes much further than this. Why does using a product like TikTok,<br/>Uber, Google, or DoorDash feel like magic? While it seems to you like a<br/>click of a button to watch a short video, order a ride or a meal, or find a<br/>search result, a lot is happening under the hood. These products are<br/>examples of true real-time data applications, delivering the actions you need<br/>at the click of a button while performing extremely sophisticated data<br/>processing and ML behind the scenes with miniscule latency. Presently, this<br/>level of sophistication is locked away behind custom-built technologies at<br/>large technology companies, but this sophistication and power are<br/>becoming democratized, similar to the way the MDS brought cloud-scale<br/>data warehouses and pipelines to the masses. The data world will soon go<br/>&#8220;live.&#8221;<br/></p>
<p><b>The Live Data Stack</b></p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>This democratization of real-time technologies will lead us to the successor<br/>to the MDS: the <i>live data stack</i> will soon be accessible and pervasive<i>.</i> The<br/>live data stack, depicted in Figure 11-1, will fuse real-time analytics and<br/>ML into applications by using streaming technologies, covering the full<br/>data lifecycle from application source systems to data processing to ML,<br/>and back.<br/></p>
<p><i>Figure 11-1. In the live data stack, data and intelligence moves in real time between the application<br/>and supporting systems<br/></i></p>
<p>Just as the MDS took advantage of the cloud and brought on-premises data<br/>warehouse and pipeline technologies to the masses, the live data stack takes<br/>real-time data application technologies used at elite tech companies and<br/>makes them available to companies of all sizes as easy-to-use cloud-based<br/>offerings. This will open up a new world of possibilities for creating even<br/>better user experiences and business value.<br/></p>
<p><b>Streaming Pipelines and Real-Time Analytical Databases<br/></b>The MDS limits itself to batch techniques that treat data as bounded. In<br/>contrast, real-time data applications treat data as an unbounded, continuous<br/>stream. Streaming pipelines and real-time analytical databases are the two<br/>core technologies that will facilitate the move from the MDS to the live data</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>stack. While these technologies have been around for some time, rapidly<br/>maturing managed cloud services will see them be deployed much more<br/>widely.<br/>Streaming technologies will continue to see extreme growth for the<br/>foreseeable future. This will happen in conjunction with a clearer focus on<br/>the business utility of streaming data. Up to the present, streaming systems<br/>have frequently been treated like an expensive novelty or a dumb pipe for<br/>getting data from A to B. In the future, streaming will radically transform<br/>organizational technology and business processes; data architects and<br/>engineers will take the lead in these fundamental changes.<br/>Real-time analytical databases enable both fast ingestion and subsecond<br/>queries on this data. This data can be enriched or combined with historical<br/>datasets. When combined with a streaming pipeline and automation, or<br/>dashboard that is capable of real-time analytics, a whole new level of<br/>possibilities opens up. No longer are you constrained by slow-running ELT<br/>processes, 15-minute updates, or other slow-moving parts. Data moves in a<br/>continuous flow. As streaming ingestion becomes more prevalent, batch<br/>ingestion will be less and less common. Why create a batch bottleneck at<br/>the head of your data pipeline? We&#8217;ll eventually look at batch ingestion the<br/>same way we now look at dial-up modems.<br/>In conjunction with the rise of streams, we expect a back-to-the-future<br/>moment for data transformations. We&#8217;ll shift away from ELT&#8212;in database<br/>transformations&#8212;to something that looks more like ETL. We provisionally<br/>refer to this as <i>stream, transform, and load</i> (STL). In a streaming context,<br/>extraction is an ongoing, continuous process. Of course, batch<br/>transformations won&#8217;t entirely go away. Batch will still be very useful for<br/>model training, quarterly reporting, and more. But streaming transformation<br/>will become the norm.<br/>While the data warehouse and data lake are great for housing large amounts<br/>of data and performing ad hoc queries, they are not so well optimized for<br/>low-latency data ingestion or queries on rapidly moving data. The live data<br/>stack will be powered by OLAP databases that are purpose-built for</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>streaming. Today, databases like Druid, ClickHouse, Rockset, and Firebolt<br/>are leading the way in powering the backend of the next generation of data<br/>applications. We expect that streaming technologies will continue to evolve<br/>rapidly and that new technologies will proliferate.<br/>Another area we think is ripe for disruption is data modeling, where there<br/>hasn&#8217;t been serious innovation since the early 2000s. The traditional batch-<br/>oriented data modeling techniques you learned about in Chapter 8 aren&#8217;t<br/>suited for streaming data. New data-modeling techniques will occur not<br/>within the data warehouse, but in the systems that generate the data. We<br/>expect data modeling will involve some notion of an upstream definitions<br/>layer&#8212;including semantics, metrics, lineage, and data definitions (see<br/>Chapter 9)&#8212;beginning where data is generated in the application. Modeling<br/>will also happen at every stage as data flows and evolves through the full<br/>lifecycle.<br/></p>
<p><b>The Fusion of Data with Applications<br/></b>We expect the next revolution will be the fusion of the application and data<br/>layers. Right now, applications sit in one area, and the MDS sits in another.<br/>To make matters worse, data is created with no regard for how it will be<br/>used for analytics. Consequently, lots of duct tape is needed to make<br/>systems talk with one another. This patchwork, siloed setup is awkward and<br/>ungainly.<br/>Soon, application stacks will be data stacks, and vice versa. Applications<br/>will integrate real-time automation and decision making, powered by the<br/>streaming pipelines and ML. The data engineering lifecycle won&#8217;t<br/>necessarily change, but the time between stages of the lifecycle will<br/>drastically shorten. A lot of innovation will occur in new technologies and<br/>practices that will improve the experience of engineering the live data stack.<br/>Pay attention to emerging database technologies designed to address the<br/>mix of OLTP and OLAP use cases; feature stores may also play a similar<br/>role for ML use cases.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><b>The Tight Feedback Between Applications and ML<br/></b>Another area we&#8217;re excited about is the fusion of applications and ML.<br/>Today, applications and ML are disjointed systems, like applications and<br/>analytics. Software engineers do their thing over here, data scientists and<br/>ML engineers do their thing over there.<br/>ML is well-suited for scenarios where data is generated at such a high rate<br/>and volume that humans cannot feasibly process it by hand. As data sizes<br/>and velocity grow, this applies to every scenario. High volumes of fast-<br/>moving data, coupled with sophisticated workflows and actions, are<br/>candidates for ML. As data feedback loops become shorter, we expect most<br/>applications to integrate ML. As data moves more quickly, the feedback<br/>loop between applications and ML will tighten. The applications in the live<br/>data stack are intelligent and able to adapt in real time to changes in the<br/>data. This creates a cycle of ever-smarter applications and increasing<br/>business value.<br/></p>
<p><b>Dark Matter Data and the Rise of...Spreadsheets?!<br/></b>We&#8217;ve talked about fast-moving data and how feedback loops will shrink as<br/>applications, data, and ML work more closely together. This section might<br/>seem odd, but we need to address something that&#8217;s widely ignored in<br/>today&#8217;s data world, especially by engineers.<br/>What&#8217;s the most widely used data platform? It&#8217;s the humble spreadsheet.<br/>Depending on the estimates you read, the user base of spreadsheets is<br/>between 700 million and 2 billion people. Spreadsheets are the dark matter<br/>of the data world. A good deal of data analytics runs in spreadsheets and<br/>never makes its way into the sophisticated data systems that we describe in<br/>this book. In many organizations, spreadsheets handle financial reporting,<br/>supply-chain analytics, and even CRM.<br/>At heart, what is a spreadsheet? A <i>spreadsheet</i> is an interactive data<br/>application that supports complex analytics. Unlike purely code-based tools<br/>such as pandas (Python Data Analysis Library), spreadsheets are accessible<br/>to a whole spectrum of users, ranging from those who just know how to</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>open files and look at reports to power users who can script sophisticated<br/>procedural data processing. So far, BI tools have failed to bring comparable<br/>interactivity to databases. Users who interact with the UI are typically<br/>limited to slicing and dicing data within certain guardrails, not general-<br/>purpose programmable analytics.<br/>We predict that a new class of tools will emerge that combines the<br/>interactive analytics capabilities of a spreadsheet with the backend power of<br/>cloud OLAP systems. Indeed, some candidates are already in the running.<br/>The ultimate winner in this product category may continue to use<br/>spreadsheet paradigms, or may define entirely new interface idioms for<br/>interacting with data.<br/></p>
<p><b>Conclusion<br/></b>Thank you for joining us on this journey through data engineering! We<br/>traversed good architecture, the stages of the data engineering lifecycle, and<br/>security best practices. We&#8217;ve discussed strategies for choosing<br/>technologies at a time when our field continues to change at an<br/>extraordinary pace. In this chapter, we laid out our wild speculation about<br/>the near and intermediate future.<br/>Some aspects of our prognostication sit on a relatively secure footing. The<br/>simplification of managed tooling and the rise of &#8220;enterprisey&#8221; data<br/>engineering have proceeded day by day as we&#8217;ve written this book. Other<br/>predictions are much more speculative in nature; we see hints of an<br/>emerging <i>live data stack</i>, but this entails a significant paradigm shift for<br/>both individual engineers and the organizations that employ them. Perhaps<br/>the trend toward real-time data will stall once again, with most companies<br/>continuing to focus on basic batch processing. Surely, other trends exist that<br/>we have completely failed to identify. The evolution of technology involves<br/>complex interactions of technology and culture. Both are unpredictable.<br/>Data engineering is a vast topic; while we could not go into any technical<br/>depth in individual areas, we hope that we have succeeded in creating a<br/>kind of travel guide that will help current data engineers, future data</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>engineers, and those who work adjacent to the field to find their way in a<br/>domain that is in flux. We advise you to continue exploration on your own.<br/>As you discover interesting topics and ideas in this book, continue the<br/>conversation as part of a community. Identify domain experts who can help<br/>you to uncover the strengths and pitfalls of trendy technologies and<br/>practices. Read extensively from the latest books, blog posts, and papers.<br/>Participate in meetups and listen to talks. Ask questions and share your own<br/>expertise. Keep an eye on vendor announcements to stay abreast of the<br/>latest developments, taking all claims with a healthy grain of salt.<br/>Through this process, you can choose technology. Next, you will need to<br/>adopt technology and develop expertise, perhaps as an individual<br/>contributor, perhaps within your team as a lead, perhaps across an entire<br/>technology organization. As you do this, don&#8217;t lose sight of the larger goals<br/>of data engineering. Focus on the lifecycle, on serving your customers&#8212;<br/>internal and external&#8212;on your business, on serving and on your larger<br/>goals.<br/>Regarding the future, many of you will play a role in determining what<br/>comes next. Technology trends are defined not only by those who create the<br/>underlying technology, but by those who adopt it and put it to good use.<br/>Successful tool <i>use</i> is as critical as tool <i>creation</i>. Find opportunities to apply<br/>real-time technology that will improve the user experience, create value,<br/>and define entirely new types of applications. It is this kind of practical<br/>application that will materialize the <i>live data stack</i> as a new industry<br/>standard; or perhaps some other new technology trend that we failed to<br/>identify will win the day.<br/>Finally, we wish you an exciting career! We chose to work in data<br/>engineering, to consult, and to write this book not simply because it was<br/>trendy, but because it was fascinating. We hope that we&#8217;ve managed to<br/>convey to you a bit of the joy we&#8217;ve found working in this field.<br/></p>
<p>1  Benn Stancil, &#8220;The Data OS,&#8221; <i>benn.substack</i>, September 3, 2021, <i>https://oreil.ly/HetE9</i>.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>2  Ben Rogojan, &#8220;Three Data Engineering Experts Share Their Thoughts on Where Data Is<br/>Headed,&#8221; <i>Better Programming</i>, May 27, 2021, <i>https://oreil.ly/IsY4W</i>.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><b>Appendix A. Serialization and<br/>Compression Technical Details<br/></b>Data engineers working in the cloud are generally freed from the<br/>complexities of managing object storage systems. Still, they need to<br/>understand details of serialization and deserialization formats. As we<br/>mentioned in Chapter 6 about storage raw ingredients, serialization and<br/>compression algorithms go hand in hand.<br/></p>
<p><b>Serialization Formats<br/></b>Many serialization algorithms and formats are available to data engineers.<br/>While the abundance of options is a significant source of pain in data<br/>engineering, they are also a massive opportunity for performance<br/>improvements. We&#8217;ve sometimes seen job performance improve by a factor<br/>of 100 simply by switching from CSV to Parquet serialization. As data<br/>moves through a pipeline, engineers will also manage reserialization&#8212;<br/>conversion from one format to another. Sometimes data engineers have no<br/>choice but to accept data in an ancient, nasty form; they must design<br/>processes to deserialize this format and handle exceptions, and then clean<br/>up and convert data for consistent, fast downstream processing and<br/>consumption.<br/></p>
<p><b>Row-Based Serialization<br/></b>As its name suggests, <i>row-based serialization</i> organizes data by row. CSV<br/>format is an archetypal row-based format. For semistructured data (data<br/>objects that support nesting and schema variation), row-oriented<br/>serialization entails storing each object as a unit.<br/><b>CSV: The nonstandard standard</b></p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>We discussed CSV in Chapter 7. CSV is a serialization format that data<br/>engineers love to hate. The term <i>CSV</i> is essentially a catchall for delimited<br/>text, but there is flexibility in conventions of escaping, quote characters,<br/>delimiter, and more.<br/>Data engineers should avoid using CSV files in pipelines because they are<br/>highly error-prone and deliver poor performance. Engineers are often<br/>required to use CSV format to exchange data with systems and business<br/>processes outside their control. CSV is a common format for data archival.<br/>If you use CSV for archival, include a complete technical description of the<br/>serialization configuration for your files so that future consumers can ingest<br/>the data.<br/><b>XML<br/></b>Extensible Markup Language (XML) was popular when HTML and the<br/>internet were new, but is now viewed as legacy; it is generally slow to<br/>deserialize and serialize for data engineering applications. XML is another<br/>standard that data engineers are often forced to interact with as they<br/>exchange data with legacy systems and software. JSON has largely replaced<br/>XML for plain-text object serialization.<br/><b>JSON and JSONL<br/></b>JavaScript Object Notation (JSON) has emerged as the new standard for<br/>data exchange over APIs, and it has also become an extremely popular<br/>format for data storage. In the context of databases, the popularity of JSON<br/>has grown apace with the rise of MongoDB and other document stores.<br/>Databases such as Snowflake, BigQuery, and SQL Server also offer<br/>extensive native support, facilitating easy data exchange between<br/>applications, APIs, and database systems.<br/>JSON Lines (JSONL) is a specialized version of JSON for storing bulk<br/>semistructured data in files. JSONL stores a sequence of JSON objects,<br/>with objects delimited by line breaks. From our perspective, JSONL is an<br/>extremely useful format for storing data right after it is ingested from API<br/>or applications. However, many columnar formats offer significantly better</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>performance. Consider moving to another format for intermediate pipeline<br/>stages and serving.<br/><b>Avro<br/></b>Avro is a row-oriented data format designed for RPCs and data<br/>serialization. Avro encodes data into a binary format, with schema metadata<br/>specified in JSON. Avro is popular in the Hadoop ecosystem and is also<br/>supported by various cloud data tools.<br/></p>
<p><b>Columnar Serialization<br/></b>The serialization formats we&#8217;ve discussed so far are row-oriented. Data is<br/>encoded as complete relations (CSV) or documents (XML and JSON), and<br/>these are written into files sequentially.<br/>With <i>columnar serialization</i>, data organization is essentially pivoted by<br/>storing each column into its own set of files. One obvious advantage to<br/>columnar storage is that it allows us to read data from only a subset of fields<br/>rather than having to read full rows at once. This is a common scenario in<br/>analytics applications and can dramatically reduce the amount of data that<br/>must be scanned to execute a query.<br/>Storing data as columns also puts similar values next to each other,<br/>allowing us to encode columnar data efficiently. One common technique<br/>involves looking for repeated values and tokenizing these, a simple but<br/>highly efficient compression method for columns with large numbers of<br/>repeats.<br/>Even when columns don&#8217;t contain large numbers of repeated values, they<br/>may manifest high redundancy. Suppose that we organized customer<br/>support messages into a single column of data. We likely see the same<br/>themes and verbiage again and again across these messages, allowing data<br/>compression algorithms to realize a high ratio. For this reason, columnar<br/>storage is usually combined with compression, allowing us to maximize<br/>disk and network bandwidth resources.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Columnar storage and compression come with some disadvantages too. We<br/>cannot easily access individual data records; we must reconstruct records by<br/>reading data from several column files. Record updates are also<br/>challenging. To change one field in one record, we must decompress the<br/>column file, modify it, recompress it, and write it back to storage. To avoid<br/>rewriting full columns on each update, columns are broken into many files,<br/>typically using partitioning and clustering strategies that organize data<br/>according to query and update patterns for the table. Even so, the overhead<br/>for updating a single row is horrendous. Columnar databases are a terrible<br/>fit for transactional workloads, so transactional databases generally utilize<br/>some form of row- or record-oriented storage.<br/><b>Parquet<br/></b>Parquet stores data in a columnar format and is designed to realize excellent<br/>read and write performance in a data lake environment. Parquet solves a<br/>few problems that frequently bedevil data engineers. Parquet-encoded data<br/>builds in schema information and natively supports nested data, unlike CSV.<br/>Furthermore, Parquet is portable; while databases such as BigQuery and<br/>Snowflake serialize data in proprietary columnar formats and offer<br/>excellent query performance on data stored internally, a huge performance<br/>hit occurs when interoperating with external tools. Data must be<br/>deserialized, reserialized into an exchangeable format, and exported to use<br/>data lake tools such as Spark and Presto. Parquet files in a data lake may be<br/>a superior option to proprietary cloud data warehouses in a polyglot tool<br/>environment.<br/>Parquet format is used with various compression algorithms; speed<br/>optimized compression algorithms such as Snappy (discussed later in this<br/>appendix) are especially popular.<br/><b>ORC<br/></b>Optimized Row Columnar (ORC) is a columnar storage format similar to<br/>Parquet. ORC was very popular for use with Apache Hive; while still<br/>widely used, we generally see it much less than Apache Parquet, and it</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>enjoys somewhat less support in modern cloud ecosystem tools. For<br/>example, Snowflake and BigQuery support Parquet file import and export;<br/>while they can read from ORC files, neither tool can export to ORC.<br/><b>Apache Arrow or in-memory serialization<br/></b>When we introduced serialization as a storage raw ingredient at the<br/>beginning of this chapter, we mentioned that software could store data in<br/>complex objects scattered in memory and connected by pointers, or more<br/>orderly, densely packed structures such as Fortran and C arrays. Generally,<br/>densely packed in-memory data structures were limited to simple types<br/>(e.g., INT64) or fixed-width data structures (e.g., fixed-width strings). More<br/>complex structures (e.g., JSON documents) could not be densely stored in<br/>memory and required serialization for storage and transfer between<br/>systems.<br/>The idea of Apache Arrow is to rethink serialization by utilizing a binary<br/>data format that is suitable for both in-memory processing and export.  This<br/>allows us to avoid the overhead of serialization and deserialization; we<br/>simply use the same format for in-memory processing, export over the<br/>network, and long-term storage. Arrow relies on columnar storage, where<br/>each column essentially gets its own chunks of memory. For nested data,<br/>we use a technique called <i>shredding</i>, which maps each location in the<br/>schema of JSON documents into a separate column.<br/>This technique means that we can store a data file on disk, swap it directly<br/>into program address space by using virtual memory, and begin running a<br/>query against the data without deserialization overhead. In fact, we can<br/>swap chunks of the file into memory as we scan it, and then swap them<br/>back out to avoid running out of memory for large datasets.<br/>One obvious headache with this approach is that different programming<br/>languages serialize data in different ways. To address this issue, the Arrow<br/>Project has created software libraries for a variety of programming<br/>languages (including C, Go, Java, JavaScript, MATLAB, Python, R, and<br/>Rust) that allow these languages to interoperate with Arrow data in<br/>memory. In some cases, these libraries use an interface between the chosen<br/></p>
<p>1</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>language and low-level code in another language (e.g., C) to read and write<br/>from Arrow. This allows high interoperability between languages without<br/>extra serialization overhead. For example, a Scala program can use the Java<br/>library to write arrow data and then pass it as a message to a Python<br/>program.<br/>Arrow is seeing rapid uptake with a variety of popular frameworks such as<br/>Apache Spark. Arrow has also spanned a new data warehouse product;<br/>Dremio is a query engine and data warehouse built around Arrow<br/>serialization to support fast queries.<br/></p>
<p><b>Hybrid Serialization<br/></b>We use the term <i>hybrid serialization</i> to refer to technologies that combine<br/>multiple serialization techniques or integrate serialization with additional<br/>abstraction layers, such as schema management. We cite as examples<br/>Apache Hudi and Apache Iceberg.<br/><b>Hudi<br/></b><i>Hudi</i> stands for <i>Hadoop Update Delete Incremental</i>. This table<br/>management technology combines multiple serialization techniques to<br/>allow columnar database performance for analytics queries while also<br/>supporting atomic, transactional updates. A typical Hudi application is a<br/>table that is updated from a CDC stream from a transactional application<br/>database. The stream is captured into a row-oriented serialization format,<br/>while the bulk of the table is retained in a columnar format. A query runs<br/>over both columnar and row-oriented files to return results for the current<br/>state of the table. Periodically, a repacking process runs that combines the<br/>row and columnar files into updated columnar files to maximize query<br/>efficiency.<br/><b>Iceberg<br/></b>Like Hudi, Iceberg is a table management technology. Iceberg can track all<br/>files that make up a table. It can also track files in each table snapshot over</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>time, allowing table time travel in a data lake environment. Iceberg supports<br/>schema evolution and can readily manage tables at a petabyte scale.<br/></p>
<p><b>Database Storage Engines<br/></b>To round out the discussion of serialization, we briefly discuss database<br/>storage engines. All databases have an underlying storage engine; many<br/>don&#8217;t expose their storage engines as a separate abstraction (for example,<br/>BigQuery, Snowflake). Some (notably, MySQL) support fully pluggable<br/>storage engines. Others (e.g., SQL Server) offer major storage engine<br/>configuration options (columnar versus row-based storage) that<br/>dramatically affect database behavior.<br/>Typically, the storage engine is a separate software layer from the query<br/>engine. The storage engine manages all aspects of how data is stored on a<br/>disk, including serialization, the physical arrangement of data, and indexes.<br/>Storage engines have seen significant innovation in the 2000s and 2010s.<br/>While storage engines in the past were optimized for direct access to<br/>spinning disks, modern storage engines are much better optimized to<br/>support the performance characteristics of SSDs. Storage engines also offer<br/>improved support for modern types and data structures, such as variable-<br/>length strings, arrays, and nested data.<br/>Another major change in storage engines is a shift toward columnar storage<br/>for analytics and data warehouse applications. SQL Server, PostgreSQL,<br/>and MySQL offer robust columnar storage support.<br/></p>
<p><b>Compression: gzip, bzip2, Snappy, etc.<br/></b>The math behind compression algorithms is complex, but the basic idea is<br/>easy to understand: compression algorithms look for redundancy and<br/>repetition in data, then re-encode data to reduce redundancy. When we want<br/>to read the raw data, we <i>decompress</i> it by reversing the algorithm and<br/>putting the redundancy back in.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>For example, you&#8217;ve noticed that certain words appear repeatedly in reading<br/>this book. Running some quick analytics on the text, you could identify the<br/>words that occur most frequently and create shortened tokens for these<br/>words. To compress, you would replace common words with their tokens;<br/>to decompress, you would replace the tokens with their respective words.<br/>Perhaps we could use this naive technique to realize a compression ratio of<br/>2:1 or more. Compression algorithms utilize more sophisticated<br/>mathematical techniques to identify and remove redundancy; they can often<br/>realize compression ratios of 10: 1 on text data.<br/>Note that we&#8217;re talking about <i>lossless compression algorithms</i>.<br/>Decompressing data encoded with a lossless algorithm recovers a bit-for-bit<br/>exact copy of the original data. <i>Lossy compression algorithms</i> for audio,<br/>images, and video aim for sensory fidelity; decompression recovers<br/>something that sounds like or looks like the original but is not an exact<br/>copy. Data engineers might deal with lossy compression algorithms in<br/>media processing pipelines but not in serialization for analytics, where<br/>exact data fidelity is required.<br/>Traditional compression engines such as gzip and bzip2 compress text data<br/>extremely well; they are frequently applied to JSON, JSONL, XML, CSV,<br/>and other text-based data formats. Engineers have created a new generation<br/>of compression algorithms that prioritize speed over compression ratio in<br/>recent years. Major examples are Snappy, Zstandard, LZFSE, and LZ4.<br/>These algorithms are frequently used to compress data in data lakes or<br/>columnar databases to optimize fast query performance.<br/></p>
<p>1  Dejan Simic, &#8220;Apache Arrow: Read DataFrame with Zero Memory,&#8221; Towards Data Science,<br/>June 25, 2020, <i>https://oreil.ly/TDAdY</i>.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><b>Appendix B. Cloud Networking<br/></b>This appendix discusses some factors data engineers should consider about<br/>networking in the cloud. Data engineers frequently encounter networking in<br/>their careers, and often ignore it despite its importance.<br/></p>
<p><b>Cloud Network Topology<br/></b>A <i>cloud network topology</i> describes how various components in the cloud<br/>are arranged and connected, such as cloud services, networks, locations<br/>(zones, regions), and more. Data engineers should always know how cloud<br/>network topology will affect connectivity across the data systems they<br/>build. Microsoft Azure, Google Cloud Platform (GCP), and Amazon Web<br/>Services (AWS) all use remarkably similar resource hierarchies of<br/>availability zones and regions. At the time of this writing, GCP has added<br/>one additional layer, discussed in &#8220;GCP-Specific Networking and<br/>Multiregional Redundancy&#8221;.<br/></p>
<p><b>Data Egress Charges<br/></b>Chapter 4 discusses cloud economics and how actual provider costs don&#8217;t<br/>necessarily drive cloud pricing. Regarding networking, clouds allow<br/>inbound traffic for free but charge for outbound traffic to the internet.<br/>Outbound traffic is not inherently cheaper, but clouds use this method to<br/>create a moat around their services and increase the stickiness of stored<br/>data, a practice that has been widely criticized.  Note that data egress<br/>charges can also apply to data passing between availability zones and<br/>regions within a cloud.<br/></p>
<p><b>Availability Zones<br/></b></p>
<p>1</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>The <i>availability zone</i> is the smallest unit of network topology that public<br/>clouds make visible to customers (Figure B-1). While a zone can potentially<br/>consist of multiple data centers, cloud customers cannot control resource<br/>placement at this level.<br/></p>
<p><i>Figure B-1. Availability zones in two separate regions<br/></i></p>
<p>Generally, clouds support their highest network bandwidth and lowest<br/>latency between systems and services within a zone. High throughput data<br/>workloads should run on clusters located in a single zone for performance<br/>and cost reasons. For example, an ephemeral Amazon EMR cluster should<br/>generally sit in a single availability zone.<br/>In addition, network traffic sent to VMs within a zone is free, but with a<br/>significant caveat: traffic must be sent to private IP addresses. The major<br/>clouds utilize virtual networks known as <i>virtual private clouds</i> (VPCs).<br/>Virtual machines have private IP addresses within the VPC. They may also<br/>be assigned public IP addresses to communicate with the outside world and<br/>receive traffic from the internet, but communications using external IP<br/>addresses can incur data egress charges.<br/></p>
<p><b>Regions</b></p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>A <i>region</i> is a collection of two or more availability zones. Data centers<br/>require many resources to run (electrical power, water, etc.). The resources<br/>of separate availability zones are independent so that a local power outage<br/>doesn&#8217;t take down multiple availability zones. Engineers can build highly<br/>resilient, separate infrastructure even within a single region by running<br/>servers in multiple zones or creating automated cross-zone failover<br/>processes.<br/>Offering multiple regions allows engineers to put resources close to any of<br/>their users. <i>Close</i> means that users can realize good network performance in<br/>connecting to services, minimizing physical distance along the network<br/>path, and a minimal number of hops through routers. Both physical distance<br/>and network hops can increase latency and decrease performance. Major<br/>cloud providers continue to add new regions.<br/>In general, regions support fast, low-latency networking between zones;<br/>networking performance between zones will be worse than within a single<br/>zone and incur nominal data egress charges between VMs. Network data<br/>movement between regions is even slower and may incur higher egress<br/>fees.<br/>In general, object storage is a regional resource. Some data may pass<br/>between zones to reach a virtual machine, but this is mainly invisible to<br/>cloud customers, and there are no direct networking charges for this. (Of<br/>course, customers are still responsible for object access costs.)<br/>Despite regions&#8217; geo-redundant design, many major cloud service failures<br/>have affected entire regions, an example of <i>correlated failure</i>. Engineers<br/>often deploy code and configuration to entire regions; the regional failures<br/>we&#8217;ve observed have generally resulted from code or configuration<br/>problems occurring at the regional level.<br/></p>
<p><b>GCP-Specific Networking and Multiregional Redundancy<br/></b>GCP offers a handful of unique abstractions that engineers should be aware<br/>of if they work in this cloud. The first is the <i>multiregion</i>, a layer in the<br/>resource hierarchy; a multiregion contains multiple regions. Current</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>multiregions are US (data centers in the United States), EU (data centers in<br/>European Union member states), and ASIA.<br/>Several GCP resources support multiregions, including Cloud Storage and<br/>BigQuery. Data is stored in multiple zones within the multiregion in a geo-<br/>redundant manner so that it should remain available in the event of a<br/>regional failure. Multiregional storage is also designed to deliver data<br/>efficiently to users within the multiregion without setting up complex<br/>replication processes between regions. In addition, there are no data egress<br/>fees for VMs in a multiregion to access Cloud Storage data in the same<br/>multiregion.<br/>Cloud customers can set up multiregional infrastructure on AWS or Azure.<br/>In the case of databases or object storage, this involves duplicating data<br/>between regions to increase redundancy and put data closer to users.<br/>Google also essentially owns significantly more global scale networking<br/>resources than other cloud providers, something which it offers to its<br/>customers as <i>premium tier networking</i>. Premium tier networking allows<br/>traffic between zones and regions to pass entirely over Google-owned<br/>networks without traversing the public internet.<br/></p>
<p><b>Direct Network Connections to the Clouds<br/></b>Each major public cloud offers enhanced connectivity options, allowing<br/>customers to integrate their networks with a cloud region or VPC directly.<br/>For example, Amazon offers AWS Direct Connect. In addition to providing<br/>higher bandwidth and lower latency, these connection options often offer<br/>dramatic discounts on data egress charges. In a typical scenario in the US,<br/>AWS egress charges drop from 9 cents per gigabyte over the public internet<br/>to 2 cents per gigabyte over direct connect.<br/></p>
<p><b>CDNs<br/></b><i>Content delivery networks</i> (CDNs) can offer dramatic performance<br/>enhancements and discounts for delivering data assets to the public or</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>customers. Cloud providers offer CDN options and many other providers,<br/>such as Cloudflare. CDNs work best when delivering the same data<br/>repeatedly, but make sure that you read the fine print. Remember that CDNs<br/>don&#8217;t work everywhere, and certain countries may block internet traffic and<br/>CDN delivery.<br/></p>
<p><b>The Future of Data Egress Fees<br/></b>Data egress fees are a significant impediment to interoperability, data<br/>sharing, and data movement to the cloud. Right now, data egress fees are a<br/>moat designed to prevent public cloud customers from leaving or deploying<br/>across multiple clouds.<br/>But interesting signals indicate that change may be on the horizon. In<br/>particular, Zoom&#8217;s announcement in 2020 near the beginning of the<br/>COVID-19 pandemic that it chose Oracle as its cloud infrastructure<br/>provider caught the attention of many cloud watchers.  How did Oracle win<br/>this significant cloud contract for critical remote work infrastructure against<br/>the cloud heavyweights? AWS expert Corey Quinn offers a reasonably<br/>straightforward answer.  By his back-of-the-envelope calculation, Zoom&#8217;s<br/>AWS monthly data egress fees would run over $11 million dollars at list<br/>price; Oracle&#8217;s would cost less than $2 million.<br/>We suspect that one of GCP, AWS, or Azure will announce significant cuts<br/>in egress fees in the next few years, leading to a sea change in the cloud<br/>business model.<br/></p>
<p>1  Matthew Prince and Nitin Rao, &#8220;AWS&#8217;s Egregious Egress,&#8221; <i>The Cloudflare Blog</i>, July 23,<br/>2021, <i>https://oreil.ly/NZqKa</i>.<br/></p>
<p>2  Mark Haranas and Steven Burke, &#8220;Oracle Bests Cloud Rivals to Win Blockbuster Cloud<br/>Deal,&#8221; CRN, April 28, 2020, <i>https://oreil.ly/LkqOi</i>.<br/></p>
<p>3  Corey Quinn, &#8220;Why Zoom Chose Oracle Cloud Over AWS and Maybe You Should Too,&#8221;<br/>Last Week in AWS, April 28, 2020, <i>https://oreil.ly/Lx5uu</i>.<br/></p>
<p>2<br/></p>
<p>3</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><b>Index</b></p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><b>About the Authors<br/>Joe Reis</b> is a business-minded data nerd who&#8217;s worked in the data industry<br/>for 20 years, with responsibilities ranging from statistical modeling,<br/>forecasting, machine learning, data engineering, data architecture, and<br/>almost everything else in between. Joe is the CEO and Co-Founder of<br/>Ternary Data, a data engineering and architecture consulting firm based in<br/>Salt Lake City, Utah. In addition, he volunteers with several technology<br/>groups and teaches at the University of Utah. In his spare time, Joe likes to<br/>rock climb, produce electronic music, and take his kids on crazy<br/>adventures.<br/><b>Matt Housley</b> is a data engineering consultant and cloud specialist. After<br/>some early programming experience with Logo, Basic and 6502 assembly,<br/>he completed a PhD in mathematics at the University of Utah. Matt then<br/>began working in data science, eventually specializing in cloud based data<br/>engineering. He co-founded Ternary Data with Joe Reis, where he leverages<br/>his teaching experience to train future data engineers and advise teams on<br/>robust data architecture. Matt and Joe also pontificate on all things data on<br/>The Monday Morning Data Chat.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p><b>Colophon<br/></b>The animal on the cover of <i>Fundamentals of Data Engineering</i> is the white-<br/>eared puffbird (<i>Nystalus chacuru</i>).<br/>So named for the conspicuous patch of white at their ears, as well as for<br/>their fluffy plumage, these small, rotund birds are found across a wide<br/>swath of central South America, where they inhabit forest edges and<br/>savanna.<br/>White-eared puffbirds are sit-and-wait hunters, perching in open spaces for<br/>long periods and feeding opportunistically on insects, lizards, and even<br/>small mammals that happen to come near. They are most often found alone<br/>or in pairs and are relatively quiet birds, vocalizing only rarely.<br/>The International Union for Conservation of Nature has listed the white-<br/>eared puffbird as being of <i>least concern</i>, due, in part, to their extensive<br/>range and stable population. Many of the animals on O&#8217;Reilly covers are<br/>endangered; all of them are important to the world.<br/>The cover illustration is by Karen Montgomery, based on an antique line<br/>engraving from Shaw&#8217;s <i>General Zoology</i>. The cover fonts are Gilroy<br/>Semibold and Guardian Sans. The text font is Adobe Minion Pro; the<br/>heading font is Adobe Myriad Condensed; and the code font is Dalton<br/>Maag&#8217;s Ubuntu Mono.</p>

</div></div>
</body></html>